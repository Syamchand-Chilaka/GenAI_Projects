{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10dc1c190>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x10dc54b80>, model_name='Gemma-7b-It', groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(groq_api_key=api_key, model=\"Gemma-7b-It\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = \"\"\"\n",
    "I am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation.\n",
    "\n",
    "Five score years ago, a great American, in whose symbolic shadow we stand today, signed the Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions of Negro slaves who had been seared in the flames of withering injustice. It came as a joyous daybreak to end the long night of their captivity.\n",
    "\n",
    "But one hundred years later, the Negro still is not free. One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination. One hundred years later, the Negro lives on a lonely island of poverty in the midst of a vast ocean of material prosperity. One hundred years later, the Negro is still languished in the corners of American society and finds himself an exile in his own land. And so we've come here today to dramatize a shameful condition.\n",
    "\n",
    "In a sense we've come to our nation's capital to cash a check. When the architects of our republic wrote the magnificent words of the Constitution and the Declaration of Independence, they were signing a promissory note to which every American was to fall heir. This note was a promise that all men, yes, black men as well as white men, would be guaranteed the \"unalienable Rights\" of \"Life, Liberty and the pursuit of Happiness.\" It is obvious today that America has defaulted on this promissory note, insofar as her citizens of color are concerned. Instead of honoring this sacred obligation, America has given the Negro people a bad check, a check which has come back marked \"insufficient funds.\"\n",
    "\n",
    "But we refuse to believe that the bank of justice is bankrupt. We refuse to believe that there are insufficient funds in the great vaults of opportunity of this nation. And so, we've come to cash this check, a check that will give us upon demand the riches of freedom and the security of justice.\n",
    "\n",
    "We have also come to this hallowed spot to remind America of the fierce urgency of Now. This is no time to engage in the luxury of cooling off or to take the tranquilizing drug of gradualism. Now is the time to make real the promises of democracy. Now is the time to rise from the dark and desolate valley of segregation to the sunlit path of racial justice. Now is the time to lift our nation from the quicksands of racial injustice to the solid rock of brotherhood. Now is the time to make justice a reality for all of God's children.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message = [\n",
    "    SystemMessage(content=\"You are an expert with expertise in sumarizing speeches\"),\n",
    "    HumanMessage(content=f\"Please provide a short and concise summary of the following speech:{speech}\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm-venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Speech Summary:**\\n\\nThe speaker expresses deep dissatisfaction with the persistent racial inequalities faced by African Americans in the United States. He argues that despite the promises of the Constitution and Declaration of Independence, America has failed to uphold its obligation to guarantee equal rights and opportunities for all. The speaker calls for immediate action to dismantle segregation, end discrimination, and achieve racial justice. He urges the nation to fulfill its promise of freedom and justice for all.', response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 560, 'total_tokens': 648, 'completion_time': 0.09969926, 'prompt_time': 0.399759056, 'queue_time': 0.0012193249999999933, 'total_time': 0.499458316}, 'model_name': 'Gemma-7b-It', 'system_fingerprint': 'fp_7d8efeb0b1', 'finish_reason': 'stop', 'logprobs': None}, id='run-69446c92-9acf-4706-b3cb-22813f098a3e-0', usage_metadata={'input_tokens': 560, 'output_tokens': 88, 'total_tokens': 648})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(chat_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template - Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['language', 'speech'], template='\\nWrite a summary of the following speech:\\n{speech}\\nTranslate the precise summary to {language}\\n')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "generic_template = \"\"\"\n",
    "Write a summary of the following speech:\n",
    "{speech}\n",
    "Translate the precise summary to {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"speech\", \"language\"],\n",
    "    template = generic_template\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = prompt.format(speech = speech, language= \"Hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speech': '\\nI am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation.\\n\\nFive score years ago, a great American, in whose symbolic shadow we stand today, signed the Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions of Negro slaves who had been seared in the flames of withering injustice. It came as a joyous daybreak to end the long night of their captivity.\\n\\nBut one hundred years later, the Negro still is not free. One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination. One hundred years later, the Negro lives on a lonely island of poverty in the midst of a vast ocean of material prosperity. One hundred years later, the Negro is still languished in the corners of American society and finds himself an exile in his own land. And so we\\'ve come here today to dramatize a shameful condition.\\n\\nIn a sense we\\'ve come to our nation\\'s capital to cash a check. When the architects of our republic wrote the magnificent words of the Constitution and the Declaration of Independence, they were signing a promissory note to which every American was to fall heir. This note was a promise that all men, yes, black men as well as white men, would be guaranteed the \"unalienable Rights\" of \"Life, Liberty and the pursuit of Happiness.\" It is obvious today that America has defaulted on this promissory note, insofar as her citizens of color are concerned. Instead of honoring this sacred obligation, America has given the Negro people a bad check, a check which has come back marked \"insufficient funds.\"\\n\\nBut we refuse to believe that the bank of justice is bankrupt. We refuse to believe that there are insufficient funds in the great vaults of opportunity of this nation. And so, we\\'ve come to cash this check, a check that will give us upon demand the riches of freedom and the security of justice.\\n\\nWe have also come to this hallowed spot to remind America of the fierce urgency of Now. This is no time to engage in the luxury of cooling off or to take the tranquilizing drug of gradualism. Now is the time to make real the promises of democracy. Now is the time to rise from the dark and desolate valley of segregation to the sunlit path of racial justice. Now is the time to lift our nation from the quicksands of racial injustice to the solid rock of brotherhood. Now is the time to make justice a reality for all of God\\'s children.\\n',\n",
       " 'language': 'Spanish',\n",
       " 'text': '**Resumen:**\\n\\nEstoy feliz de estar aquí hoy para celebrar lo que será recordado como la mayor manifestación a favor de la libertad en la historia de nuestra nación. Hace cincuenta años, un gran americano, en cuya sombra simbólica estamos hoy, firmó la Declaración de Emancipación. Este importante decreto fue como una fuente de esperanza para millones de negros esclavizados. Sin embargo, cien años después, el negro aún no está libre. La vida del negro sigue siendo limitada por la segregación y la discriminación. A pesar de los progresos realizados, el negro aún vive en la pobreza y se siente exiliado en su propio país.\\n\\nVenimos aquí hoy para recordar la deuda que tiene América con su población de color. Cuando los arquitectos de nuestra república escribieron las palabras impresionantes de la Constitución y la Declaración de Independencia, prometían que todos los ciudadanos, blancos y negros, tenían derecho a los derechos inalienables de \"Vida, Libertad y la búsqueda del Felicidad\". Sin embargo, hoy es evidente que América ha incumplido esta promesa. En lugar de cumplir con esta obligación sagrada, América ha entregado a los ciudadanos de color un cheque que ha resultado insuficiente.\\n\\nSin embargo, no creemos que el banco de justicia esté sin fondos. No creemos que no haya suficientes fondos en los grandes tesoros de oportunidades de esta nación. Venimos aquí para cobrar nuestro cheque, un cheque que nos dará acceso a la riqueza de la libertad y la seguridad de la justicia. También estamos aquí para recordar a América la urgencia de la actualidad. Este no es el momento para pensar en enfriarse o tomar la droga tranquilizante del gradualismo. Es el momento de hacer realidad las promesas de la democracia. Es el momento de levantar nuestra nación de las profundidades de la injusticia racial hasta la cima de la justicia racial. Es el momento de hacer que la justicia sea una realidad para todos los hijos de Dios.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "summary = llm_chain({\"speech\":speech, \"language\":\"Spanish\"})\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When the size of documents is large we use the following methods to overcome the problem of token size:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stuff Documents Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='TECHNOLOGY READINESS LEVELS\\nFOR MACHINE LEARNING SYSTEMS\\nAlexander Lavin∗\\nPasteur LabsCiarán M. Gilligan-Lee\\nSpotifyAlessya Visnjic\\nWhyLabsSiddha Ganju\\nNvidiaDava Newman\\nMIT\\nAtılım Güne¸ s Baydin\\nUniversity of OxfordSujoy Ganguly\\nUnity AIDanny Lange\\nUnity AIAmit Sharma\\nMicrosoft Research\\nStephan Zheng\\nSalesforce ResearchEric P. Xing\\nPetuumAdam Gibson\\nKonduitJames Parr\\nNASA Frontier Development Lab\\nChris Mattmann\\nNASA Jet Propulsion LabYarin Gal\\nAlan Turing Institute\\nABSTRACT\\nThe development and deployment of machine learning (ML) systems can be executed easily with\\nmodern tools, but the process is typically rushed and means-to-an-end. The lack of diligence can\\nlead to technical debt, scope creep and misaligned objectives, model misuse and failures, and\\nexpensive consequences. Engineering systems, on the other hand, follow well-deﬁned processes\\nand testing standards to streamline development for high-quality, reliable results. The extreme is\\nspacecraft systems, where mission critical measures and robustness are ingrained in the development\\nprocess. Drawing on experience in both spacecraft engineering and ML (from research through\\nproduct across domain areas), we have developed a proven systems engineering approach for machine\\nlearning development and deployment. Our Machine Learning Technology Readiness Levels (MLTRL)\\nframework deﬁnes a principled process to ensure robust, reliable, and responsible systems while\\nbeing streamlined for ML workﬂows, including key distinctions from traditional software engineering.\\nEven more, MLTRL deﬁnes a lingua franca for people across teams and organizations to work\\ncollaboratively on artiﬁcial intelligence and machine learning technologies. Here we describe the\\nframework and elucidate it with several real world use-cases of developing ML methods from basic\\nresearch through productization and deployment, in areas such as medical diagnostics, consumer\\ncomputer vision, satellite imagery, and particle physics.\\nKeywords: Machine Learning; Systems Engineering; Data Management; Medical AI; Space Sciences\\nIntroduction\\nThe accelerating use of artiﬁcial intelligence (AI) and machine learning (ML) technologies in systems of software,\\nhardware, data, and people introduces vulnerabilities and risks due to dynamic and unreliable behaviors; fundamentally,\\nML systems learn from data, introducing known and unknown challenges in how these systems behave and interact with\\ntheir environment. Currently the approach to building AI technologies is siloed: models and algorithms are developed\\nin testbeds isolated from real-world environments, and without the context of larger systems or broader products they’ll\\nbe integrated within for deployment. A main concern is models are typically trained and tested on only a handful of\\ncurated datasets, without measures and safeguards for future scenarios, and oblivious of the downstream tasks and\\nusers. Even more, models and algorithms are often integrated into a software stack without regard for the inherent\\nstochasticity –for instance, the massive effect random seeds have on deep reinforcement learning model performance\\n[1] – and failure modes of the ML components, which can be dangerously hidden in layers of software and abstraction.\\nOther domains of engineering, such as civil and aerospace, follow well-deﬁned processes and testing standards to\\nstreamline development for high-quality, reliable results. Technology Readiness Level (TRL) is a systems engineering\\nprotocol for deep tech[ 2] and scientiﬁc endeavors at scale, ideal for integrating many interdependent components\\n∗lavin@simulation.science\\nPreprint. Under review.arXiv:2101.03989v2  [cs.LG]  29 Nov 2021'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='andcross-functional teams of people. It is no surprise that TRL is standard process and parlance in NASA[ 3] and\\nDARPA[4].\\nFor a spaceﬂight project there are several deﬁned phases, from pre-concept to prototyping to deployed operations to\\nend-of-life, each with a series of exacting development cycles and reviews. This is in stark contrast to common machine\\nlearning and software workﬂows, which promote quick iteration, rapid deployment, and simple linear progressions. Yet\\nthe NASA technology readiness process for spacecraft systems is overkill; we need robust ML technologies integrated\\nwith larger systems of software, hardware, data, and humans, but not necessarily for missions to Mars. We aim to bring\\nsystems engineering to AI and ML by deﬁning and putting into action a lean Machine Learning Technology Readiness\\nLevels (MLTRL) framework. We draw on decades of AI and ML development, from research through production,\\nacross domains and diverse data scenarios: for example, computer vision in medical diagnostics and consumer apps,\\nautomation in self-driving vehicles and factory robotics, tools for scientiﬁc discovery and causal inference, streaming\\ntime-series in predictive maintenance and ﬁnance.\\nIn this paper we deﬁne our framework for developing and deploying robust, reliable, and responsible ML and data\\nsystems, with several real test cases of advancing models and algorithms from R&D through productization and\\ndeployment, including essential data considerations. Additionally, MLTRL prioritizes the role of AI ethics and\\nfairness, and our systems AI approach can help curb the large societal issues that can result from poorly deployed and\\nmaintained AI and ML technologies, such as the automation of systemic human bias, denial of individual autonomy,\\nand unjustiﬁable outcomes (see the Alan Turing Institute Report on Ethical AI [5]). The adoption and proliferation of\\nMLTRL provides a common nomenclature and metric across teams and industries. The standardization of MLTRL\\nacross the AI industry should help teams and organizations develop principled, safe, and trusted technologies.\\nFigure 1: MLTRL spans research through prototyping, productization, and deployment. Most ML workﬂows prescribe\\nan isolated, linear process of data processing, training, testing, and serving a model [ 6]. Those workﬂows fail to deﬁne\\nhow ML development must iterate over that basic process to become more mature and robust, and how to integrate with\\na much larger system of software, hardware, data, and people. Not to mention MLTRL continues beyond deployment:\\nmonitoring and feedback cycles are important for continuous reliability and improvement over the product lifetime.\\nResults\\nMLTRL deﬁnes technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment.\\nA TRL represents the maturity of a model or algorithmii, data pipelines, software module, or composition thereof; a\\ntypical ML system consists of many interconnected subsystems and components, and the TRL of the system is the\\nlowest level of its constituent parts [ 7]. The anatomy of a level is marked by gated reviews, evolving working groups,\\nrequirements documentation with risk calculations, progressive code and testing standards, and deliverables such as\\nTRL Cards (Figure 3) and ethics checklists.iiiThese components—which are crucial for implementing the levels in a\\niiNote we use “model” and “algorithm” somewhat interchangeably when referring to the technology under development. The\\nsame MLTRL process and methods apply for a machine translation model and for an A/B testing algorithm, for example.\\niiiTemplates and examples for MLTRL deliverables will be open-sourced upon publication at github.com/alan-turing-institute.\\n2'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='systematic fashion—as well as MLTRL metrics and methods are concretely described in examples and in the Methods\\nsection. Lastly, to emphasize the importance of data tasks in ML, from data curation [ 8] to data governance [ 9], we\\nstate several important data considerations at each MLTRL level.\\nMACHINE LEARNING TECHNOLOGY READINESS LEVELS\\nThe levels are brieﬂy deﬁned as follows and in Figure 1, and elucidated with real-world examples later.\\nLevel 0 - First Principles This is a stage for greenﬁeld AI research, initiated with a novel idea, guiding question, or\\npoking at a problem from new angles. The work mainly consists of literature review, building mathematical foundations,\\nwhite-boarding concepts and algorithms, and building an understanding of the data – for work in theoretical AI and ML,\\nhowever, there will not yet be data to work with (for example, a novel algorithm for Bayesian optimization[ 10], which\\ncould eventually be used for many domains and datasets). The outcome of Level 0 is a set of concrete ideas with sound\\nmathematical formulation, to pursue through low-level experimentation in the next stage. When relevant, this level\\nexpects conclusions about data readiness, including strategies for getting the data to be suitable for the speciﬁc ML task.\\nTo graduate, the basic principles, hypotheses, data readiness, and research plans need to be stated, referencing relevant\\nliterature. With graduation, a TRL Card should be started to succinctly document the methods and insights thus far –\\nthis key MLTRL deliverable is detailed in the Methods section and Figure 3.\\nLevel 0 data – Not a hard requirement at this stage because this is largely theoretical machine learning. That being said,\\ndata availability needs to be considered for deﬁning any research project to move past theory.\\nLevel 0 review – The reviewer here is solely the lead of the research lab or team, for instance a PhD supervisor. We\\nassess hypotheses and explorations for mathematical validity and potential novelty or utility, not necessarily code nor\\nend-to-end experiment results.\\nLevel 1 - Goal-Oriented Research To progress from basic principles to practical use, we design and run low-level\\nexperiments to analyze speciﬁc model or algorithm properties (rather than end-to-end runs for a performance benchmark\\nscore). This involves collection and processing of sample data to train and evaluate the model. This sample data need\\nnot be the full data; it may be a smaller sample that is currently available or more convenient to collect. In some\\ncases it may sufﬁce to use synthetic data as the representative sample – in the medical domain, for example, acquiring\\ndatasets can take many months due to security and privacy constraints, so generating sample data can mitigate this\\nblocker from early ML development. Further, working with the sample data provides a blueprint for the data collection\\nand processing pipeline (including answering whether it is even possible to collect all necessary data), that can be\\nscaled up for the for the next steps. The experiments, good results or not, and mathematical foundations need to pass a\\nreview process with fellow researchers before graduating to Level 2. The application is still speculative, but through\\ncomparison studies and analyses we start to understand if/how/where the technology offers potential improvements and\\nutility. Code is research-caliber : The aim here is to be quick and dirty, moving fast through iterations of experiments.\\nHacky code is okay, and full test coverage is actually discouraged, as long as the overall codebase is organized and\\nmaintainable. It is important to start semantic versioning practices early in the project lifecycle, which should cover\\ncode, models, anddatasets. This is crucial for retrospectives and reproducibility, issues with which can be costly and\\nsevere at later stages. This versioning information and additional progress should be reported on the TRL Card (see for\\nexample Figure 3).\\nLevel 1 data – At minimum we work with sample data that is representative of downstream real datasets, which can be\\na subset of real data, synthetic data, or both. Beyond driving low-level ML experiments, the sample data forces us to\\nconsider data acquisition and processing strategies at an early stage before it becomes a blocker later.\\nLevel 1 review – The panel for this gated review is entirely members of the research team, reviewing for scientiﬁc rigor\\nin early experimentation, and pointing to important concepts and prior work from their respective areas of expertise.\\nThere may be several iterations of feedback and additional experiments.\\nLevel 2 - Proof of Principle (PoP) Development Active R&D is initiated, mainly by developing and running in\\ntestbeds : simulated environments and/or simulated data that closely matches the conditions and data of real scenarios –\\nnote these are driven by model-speciﬁc technical goals, not necessarily application or product goals (yet). An important\\ndeliverable at this stage is the formal research requirements document (with well-speciﬁed veriﬁcation and validation\\n(V&V) steps)iv. Here is one of several key decision points in the broader process: The R&D team considers several\\npaths forward and sets the course: (A) prototype development towards Level 3, (B) continued R&D for longer-term\\nivArequirement is a singular documented physical or functional need that a particular design, product, or process aims to satisfy.\\nRequirements aim to specify all stakeholders’ needs while not specifying a speciﬁc solution. Deﬁnitions are incomplete without\\n3'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='research initiatives and/or publications, or some combination of A and B. We ﬁnd the culmination of this stage is often\\na bifurcation: some work moves to applied AI, while some circles back for more research. This common MLTRL cycle\\nis an instance of the non-monotonic discovery switchback mechanism (detailed in the Methods section).\\nLevel 2 data – Datasets at this stage may include publicly available benchmark datasets, semi-simulated data based\\non the data sample in Level 1, or fully simulated data based on certain assumptions about the potential deployment\\nenvironments. The data should allow researchers to characterize model properties, and highlight corner cases or\\nboundary conditions, in order to justify the utility of continuing R&D on the model.\\nLevel 2 review – To graduate from the PoP stage, the technology needs to satisfy research claims made in previous\\nstages (brought to be bare by the aforementioned PoP data in both quantitative and qualitative ways) with the analyses\\nwell-documented and reproducible.\\nLevel 3 - System Development Here we have checkpoints that push code development towards interoperability,\\nreliability, maintainability, extensibility, and scalability. Code becomes prototype-caliber : A signiﬁcant step up from\\nresearch code in robustness and cleanliness. This needs to be well-designed, well-architected for dataﬂow and interfaces,\\ngenerally covered by unit and integration tests, meet team style standards, and sufﬁciently-documented. Note the\\nprogrammers’ mentality remains that this code will someday be refactored/scrapped for productization; prototype code\\nis relatively primitive with regard to efﬁciency and reliability of the eventual system. With the transition to Level 4 and\\nproof-of-concept mode, the working group should evolve to include product engineering to help deﬁne service-level\\nagreements and objectives (SLAs and SLOs) of the eventual production system.\\nLevel 3 data – For the most part consistent with Level 2; in general, the previous level review can elucidate potential\\ngaps in data coverage and robustness to be addressed in the subsequent level. However, for test suites developed at this\\nstage, it is useful to deﬁne dedicated subsets of the experiment data as default testing sources, as well as setup mock\\ndata for speciﬁc functionalities and scenarios to be tested.\\nLevel 3 review – Teammates from applied AI and engineering are brought into the review to focus on sound software\\npractices, interfaces and documentation for future development, and version control for models and datasets. There are\\nlikely domain- or organization-speciﬁc data management considerations going forward that this review should point out\\n– e.g. standards for data tracking and compliance in healthcare [11].\\nLevel 4 - Proof of Concept (PoC) Development This stage is the seed of application-driven development; for many\\norganizations this is the ﬁrst touch-point with product managers and stakeholders beyond the R&D group. Thus TRL\\nCards and requirements documentation are instrumental in communicating the project status and onboarding new\\npeople. The aim is to demonstrate the technology in a real scenario: quick proof-of-concept examples are developed to\\nexplore candidate application areas and communicate the quantitative and qualitative results. It is essential to use real\\nand representative data for these potential applications. Thus data engineering for the PoC largely involves scaling up\\nthe data collection and processing from Level 1, which may include collecting new data or processing all available data\\nusing scaled experiment pipelines from Level 3. In some scenarios there will new datasets brought in for the PoC, for\\nexample, from an external research partner as a means of validation. Hand-in-hand with the evolution from sample to\\nreal data, the experiment metrics should evolve from ML research to the applied setting: proof-of-concept evaluations\\nshould quantify model and algorithm performance (e.g., precision and recall and various data splits), computational\\ncosts (e.g., CPU vs GPU runtimes), and also metrics that are more relevant to the eventual end-user (e.g., number\\nof false positives in the top-N predictions of a recommender system). We ﬁnd this PoC exploration reveals speciﬁc\\ndifferences between clean and controlled research data versus noisy and stochastic real-world data. The issues can\\nbe readily identiﬁed because of the well-deﬁned distinctions between those development stages in MLTRL, and then\\ntargeted for further development.\\nAI ethics processes vary across organizations, but all should engage in ethics conversations at this stage, including ethics\\nof data collection, and potential of any harm or discriminatory impacts due to the model (as the AI capabilities and\\ndatasets are known). MLTRL requires ethics considerations to be reported on TRL Cards at all stages, which generally\\nlink to an extended ethics checklist. The key decision point here is to push onward with application development or not.\\nIt is common to pause projects that pass Level 4 review, waiting for a better time to dedicate resources, and/or pull the\\ntechnology into a different project.\\nLevel 4 data – Unlike the previous stages, having real-world and representative data is critical for the PoC; even with\\nmethods for verifying that data distributions in synthetic data reliably mirror those of real data [], sufﬁcient conﬁdence\\nin the technology must be achieved with real-world data of the use-case. Further, one must consider how to obtain\\ncorresponding measures for veriﬁcation and validation (V&V). Veriﬁcation: Are we building the product right? Validation: Are we\\nbuilding the right product?\\n4'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='high-quality and consistent data required for the future model inference: generation of the data pipeline PoC that will\\nresemble the future inference pipeline that will take data from intended sources, transform it into features, and send it to\\nthe model for inference.\\nLevel 4 review – Demonstrate the utility towards one or more practical applications (each with multiple datasets), taking\\ncare to communicate assumptions and limitations, and again reviewing data-readiness: evaluating the real-world data\\nfor quality, validity, and availability. The review also evaluates security and privacy considerations – deﬁning these in\\nthe requirements document with risk quantiﬁcation is a useful mechanism for mitigating potential issues (discussed\\nfurther in the Methods section).\\nLevel 5 - Machine Learning “Capability” At this stage the technology is more than an isolated model or algorithm,\\nit is a speciﬁc capability . For instance, producing depth images from stereo vision sensors on a mobile robot is a\\nreal-world capability beyond the isolated ML technique of self-supervised learning for RGB stereo disparity estimation.\\nIn many organizations this represents a technology transition or handoff from R&D to productization. MLTRL\\nmakes this transition explicit, evolving the requisite work, guiding documentation, objectives and metrics, and team;\\nindeed, without MLTRL it is common for this stage to be erroneously leaped completely, as shown in Figure 2. An\\ninterdisciplinary working group is deﬁned, as we start developing the technology in the context of a larger real-world\\nprocess – i.e., transitioning the model or algorithm from an isolated solution to a module of a larger application. Just as\\nthe ML technology should no longer be owned entirely by ML experts, steps have been taken to share the technology\\nwith others in the organization via demos, example scripts, and/or an API; the knowledge and expertise cannot remain\\nwithin the R&D team, let alone an individual ML developer. Graduation from Level 5 should be difﬁcult, as it signiﬁes\\nthe dedication of resources to push this ML technology through productization. This transition is a common challenge\\nin deep-tech, sometimes referred to as “the valley of death” because project managers and decision-makers struggle\\nto allocate resources and align technology roadmaps to effectively move to Level 6, 7 and onward. MLTRL directly\\naddresses this challenge by stepping through the technology transition or handoff explicitly.\\nLevel 5 data – For the most part consistent with Level 4. However, considerations need to be taken for scaling of data\\npipelines: there will soon be more engineers accessing the existing data and adding more, and the data will be getting\\nmuch more use, including automated testing in later levels. With this scaling can come challenges with data governance.\\nThe data pipelines likely do not mirror the structure of the teams or broader organization. This can result in data silos,\\nduplications, unclear responsibilities, and missing control of data over its entire lifecycle. These challenges and several\\napproaches to data governance (planning and control, organizational, and risk-based) are detailed in Janssen et al. [9].\\nLevel 5 review – The veriﬁcation and validation (V&V) measures and steps deﬁned in earlier R&D stages (namely\\nLevel 2) must all be completed by now, and the product-driven requirements (and corresponding V&V) are drafted at\\nthis stage. We thoroughly review them here, and make sure there is stakeholder alignment (at the ﬁrst possible step of\\nproductization, well ahead of deployment).\\nLevel 6 - Application Development The main work here is signiﬁcant software engineering to bring the code up to\\nproduct-caliber : This code will be deployed to users and thus needs to follow precise speciﬁcations, have comprehensive\\ntest coverage, well-deﬁned APIs, etc. The resulting ML modules should be robustiﬁed towards one or more target\\nuse-cases. If those target use-cases call for model explanations, the methods need to be built and validated alongside\\nthe ML model, and tested for their efﬁcacy in faithfully interpreting the model’s decisions – crucially, this needs to be\\nin the context of downstream tasks and the end-users, as there is often a gap between ML explainability that serves\\nML engineers rather than external stakeholders[ 12]. Similarly, we need to develop the ML modules with known data\\nchallenges in mind, speciﬁcally to check the robustness of the model (and broader pipeline) to changes in the data\\ndistribution between development and deployment.\\nThe deployment setting(s) should be addressed thoroughly in the product requirements document, as ML serving (or\\ndeploying) is an overloaded term that needs careful consideration. First, there are two main types: internal, as APIs\\nfor experiments and other usage mainly by data science and ML teams, and external, meaning an ML model that\\nis embedded or consumed within a real application with real users. The serving constraints vary signiﬁcantly when\\nconsidering cloud deployment vs on-premise or hybrid, batch or streaming, open-source solution or containerized\\nexecutable, etc. Even more, the data at deployment may be limited due to compliance, or we may only have access to\\nencrypted data sources, some of which may only be accessible locally – these scenarios may call for advanced ML\\napproaches such as federated learning[ 13] and other privacy-oriented ML[ 14]. And depending on the application, an\\nML model may not be deployable without restrictions; this typically means being embedded in a rules engine workﬂow\\nwhere the ML model acts like an advisor that discovers edge cases in rules. These deployment factors are hardly\\nconsidered in model and algorithm development despite signiﬁcant inﬂuence on modeling and algorithmic choices;\\nthat said, hardware choices typically are considered early on, such as GPU versus edge devices. It is crucial to make\\nthese systems decisions at Level 6–not too early that serving scenarios and requirements are uncertain, and not too late\\n5'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='that corresponding changes to model or application development risk deployment delays or failures. This marks a key\\ndecision for the project lifecycle, as this expensive ML deployment risk is common without MLTRL (see Figure 2).\\nLevel 6 data – Additional data should be collected and operationalized at this stage towards robustifying the ML\\nmodels, algorithms, and surrounding components. These include adversarial examples to check local robustness [ 15],\\nsemantically-equivalent perturbations to check consistency of the model with respect to domain assumptions [16, 17],\\nand collecting data from different sources and checking how well the trained model generalizes to them. These\\nconsiderations are even more vital in the challenging deployment domains mentioned above with limited data access.\\nLevel 6 review – Focus is on the code quality, the set of newly deﬁned product requirements, system SLA and SLO\\nrequirements, data pipelines spec, and an AI ethics revisit now that we are closer to a real-world use-case. In particular,\\nregulatory compliance is mandated for this gated review; the data privacy and security laws are changing rapidly, and\\nmissteps with compliance can make or break the project.\\nLevel 7 - Integrations For integrating the technology into existing production systems, we recommend the working\\ngroup has a balance of infrastructure engineers andapplied AI engineers – this stage of development is vulnerable\\nto latent model assumptions and failure modes, and as such cannot be safely developed solely by software engineers.\\nImportant tools for them to build together include:\\n•Tests that run use-case speciﬁc critical scenarios and data-slices – a proper risk-quantiﬁcation table will\\nhighlight these.\\n•A “golden dataset” should be deﬁned to baseline the performance of each model and succession of models –see\\nthe computer vision app example in Figure 4–for use in the continuous integration and deployment (CI/CD)\\ntests.\\n•Metamorphic testing : a software engineering methodology for testing a speciﬁc set of relations between the\\noutputs of multiple inputs. When integrating ML modules into larger systems, a codiﬁed list of metamorphic\\nrelations[18] can provide valuable veriﬁcation and validation measures and steps.\\n•Data intervention tests that seek data bugs at various points in the pipelines, downstream to measure the\\npotential effects of data processing and ML on consumers or users of that data, as well as upstream at data\\ningestion or creation. Rather than using model performance as a proxy for data quality, it is crucial to use\\nintervention tests that instead catch data errors with mechanisms speciﬁc to data validation.\\nThese tests in particular help mitigate underspeciﬁcation in ML pipelines, a key obstacle to reliably training models that\\nbehave as expected in deployment[ 19]. On the note of reliability, it is important that quality assurance engineers (QA)\\nplay a key role here and through Level 9, overseeing data processes to ensure privacy and security, and covering audits\\nfor downstream accountability of AI methods.\\nLevel 7 data – In addition to the data for test suites discussed above, this level calls for QA to prioritize data governance :\\nhow data is obtained, managed, used, and secured by the organization. This was earlier suggested in level 5 (in order to\\npreempt related technical debt), and essential here at the main junction for integration, which may create additional\\ngovernance challenges in light of downstream effects and consumers.\\nLevel 7 review – The review should focus on the data pipelines and test suites; a scorecard like the ML Testing\\nRubric[ 20] is useful. The group should also emphasize ethical considerations at this stage, as they may be more\\nadequately addressed now (where there are many test suites put into place) rather than close to shipping later.\\nLevel 8 - Flight-ready The technology is demonstrated to work in its ﬁnal form and under expected conditions.\\nThere should be additional tests implemented at this stage covering deployment aspects, notably A/B tests, blue/green\\ndeployment tests, shadow testing, and canary testing, which enable proactive and gradual testing for changing ML\\nmethods and data. Ahead of deployment, the CI/CD system should be ready to regularly stress test the overall system\\nand ML components. In practice, problems stemming from real-world data are impossible to anticipate and design for –\\nan upstream data provider could change formats unexpectedly or a physical event could cause the customer behavior to\\nchange. Running models in shadow mode for a period of time would help stress test the infrastructure and evaluate how\\nsusceptible the ML model(s) will be to performance regressions caused by data. We observe that ML systems with\\ndata-oriented architectures are more readily tested in this manner, and better surface data quality issues, data drifts, and\\nconcept drifts – this is discussed later in the Beyond Software Engineering section. To close this stage, the key decision\\nis go or no-go for deployment, and when.\\nLevel 8 data – If not already in place, there absolutely needs to be mechanisms for automatically logging data\\ndistributions alongside model performance once deployed.\\n6'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='Level 8 review – A diligent walkthrough of every technical and product requirement, showing the corresponding\\nvalidations, and the review panel is representative of the full slate of stakeholders.\\nLevel 9 - Deployment In deploying AI and ML technologies, there is signiﬁcant need to monitor the current version,\\nand explicit considerations towards improving the next version. For instance, performance degradation can be hidden\\nand critical, and feature improvements often bring unintended consequences and constraints. Thus at this level, the\\nfocus is on maintenance engineering–i.e., methods and pipelines for ML monitoring and updating. Monitoring for data\\nquality, concept drift, and data drift is crucial; no AI system without thorough tests for these can reliably be deployed.\\nBy the same token there must be automated evaluation and reporting – if actuals[ 21] are available, continuous evaluation\\nshould be enabled, but in many cases actuals come with a delay, so it is essential to record model outputs to allow for\\nefﬁcient evaluation after the fact. To these ends, the ML pipeline should be instrumented to log system metadata, model\\nmetadata, and data itself.\\nMonitoring for data quality issues and data drifts is crucial to catch deviations in model behavior, particularly those that\\nare non-obvious in the model or product end-performance. Data logging is unique in the context of ML systems: data\\nlogs should capture statistical properties of input features and model predictions, and capture their anomalies. With\\nmonitoring for data, concept, and model drifts, the logs are to be sent to the relevant systems, applied, and research\\nengineers. The latter is often non-trivial, as the model server is not ideal for model “observability” because it does not\\nnecessarily have the right data points to link the complex layers needed to analyze and debug models. To this end,\\nMLTRL requires the drift tests to be implemented at stages well ahead of deployment, earlier than is standard practice.\\nAgain we advocate for data-ﬁrst architectures rather than the software industry-standard design by services (discussed\\nlater), which aids in surfacing and logging the relevant data types and slices when monitoring AI systems. For retraining\\nand improving models, monitoring must be enabled to catch training-serving skew and let the team know when to\\nretrain. Towards model improvements, adding or modifying features can often have unintended consequences, such as\\nintroducing latencies or even bias. To mitigate these risks, MLTRL has an embedded switchback here: any component\\nor module changes to the deployed version must cycle back to Level 7 (integrations stage) or earlier. Additionally,\\nfor quality ML products, we stress a deﬁned communication path for user feedback without roadblocks to R&D; we\\nencourage real-world feedback all the way to research, providing valuable problem constraints and perspectives.\\nLevel 9 data – Proper mechanisms for logging and inspecting data (alongside models) is critical for deploying reliable\\nAI and ML – systems that learn on data have unique monitoring requirements (detailed above). In addition to the\\ninfrastructure and test suites covering data and environment shifts, it’s important for product managers and other owners\\nto be on top of data policy shifts in domains such as ﬁnance and healthcare.\\nLevel 9 review – The review at this stage is unique, as it also helps in lifecycle management: at a regular cadence\\nthat depends on the deployed system and domain of use, owners and other stakeholders are to revisit this review and\\nrecommend switchbacks if needed (discussed in the Methods section). This additional oversight at deployment is\\nshown to help deﬁne regimented release cycles of updated versions, and provide another “eye” check for stale model\\nperformance or other system abnormalities.\\nNotice MLTRL is deﬁned as stages or levels, yet much of the value in practice is realized in the transitions: MLTRL\\nenables teams to move from one level to the next reliably and efﬁciently, and provides a guide for how teams and\\nobjectives evolve with the progressing technology.\\nDiscussion\\nMLTRL is designed to apply to many real-world use-cases involving data and ML, from simple regression models\\nused for predictive modeling energy demand or anomaly detection in datacenters, to real-time modeling in rideshare\\napplications and motion planning in warehouse robotics. For simple use-cases MLTRL may be overkill, and a subset\\nmay sufﬁce – for instance, model cards as demonstrated by Google for basic image classiﬁcation. Yet this is a ﬁne line,\\nas the same cards-only approach in the popular “Huggingface” codebases are too simplistic for the language models\\nthey represent, deployed in domains that carry signiﬁcant consequences. MLTRL becomes more valuable with more\\ncomplex, larger systems and environments, especially in risk averse domains. We thoroughly discuss this through\\nseveral real uses of MLTRL below.\\n7'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='Figure 2: Most ML and AI projects live in these sections of MLTRL, not concerned with fundamental R&D – that is,\\ncompletely using existing methods and implementations, and even pretrained models. In the left diagram, the arrows\\nshow a common development pattern with MLTRL in industry: projects go back to the ML toolbox to develop new\\nfeatures (dashed line), and frequent, incremental improvements are often a practice of jumping back a couple levels to\\nLevel 7 (which is the main systems integrations stage). At Levels 7 and 8 we stress the need for tests that run use-case\\nspeciﬁc critical scenarios and data-slices, which are highlighted by a proper risk-quantiﬁcation matrix [ 22]. Cycling\\nback to previous lower levels is not just a late-stage mechanism in MLTRL, but rather “switchbacks” occur throughout\\nthe process (as discussed in the Methods section and throughout the text). In the right diagram we show the more\\ncommon approach in industry ( without using our framework), which skips essential technology transition stages – ML\\nEngineers push straight through to deployment, ignoring important productization and systems integration factors. This\\nwill be discussed in more detail in the Methods section.\\nEXAMPLES\\nHuman-machine visual inspection\\nWhile most ML projects begin with a speciﬁc task and/or dataset, there are many that originate in ML theory without\\nany target application – i.e., projects starting MLTRL at level 0 or 1. These projects nicely demonstrate the utility of\\nMLTRL built-in switchbacks, bifurcating paths, and iteration with domain experts. An example we discuss here is a\\nnovel approach to representing data in generative vision models from Naud & Lavin[ 23], which was then developed into\\nstate-of-the-art unsupervised anomaly detection, and targeted for two human-machine visual inspection applications:\\nFirst, industrial anomaly detection, notably in precision manufacturing, to identify potential errors for human-expert\\nmanual inspection. Second, using the model to improve the accuracy and efﬁciency of neuropathology, the microscopic\\nexamination of neurosurgical specimens for cancerous tissue. In these human-machine teaming use-cases there are\\nspeciﬁc challenges impeding practical, reliable use:\\n•Hidden feedback loops can be common and problematic in real-world systems inﬂuencing their own training\\ndata: over time the behavior of users may evolve to select data inputs they prefer for the speciﬁc AI system,\\nrepresenting some skew from the training data. In this neuropathology case, selecting whole-slide images that\\nare uniquely difﬁcult for manual inspection, or even biased by that individual user. Similarly we see underlying\\nhealthcare processes can act as hidden confounders, resulting in unreliable decision support tools[26].\\n•Model availability can be limited in many deployment settings: for example, on-premises deployments\\n(common in privacy preserving domains like healthcare and banking), edge deployments (common in industrial\\nuse-cases such as manufacturing and agriculture), or from the infrastructure’s inability to scale to the volume\\nof requests. This can severely limit the team’s ability to monitor, debug, and improve deployed models.\\n•Uncertainty estimation is valuable in many AI scenarios, yet not straightforward to implement in practice.\\nThis is further complicated with multiple data sources and users, each injecting generally unknown amounts of\\nnoise and uncertainties. In medical applications it is of critical importance, to provide measures of conﬁdence\\nand sensitivity, and for AI researchers through end-users. In anomaly detection, various uncertainty measures\\ncan help calibrate the false-positive versus false-negative rates, which can be very domain speciﬁc.\\n8'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='Figure 3: The maturity of each ML technology is tracked via TRL Cards , which we describe in the Methods section.\\nHere is an example reﬂecting a neuropathology machine vision use-case[ 23], detailed in the Discussion Section. Note\\nthis is a subset of a full TRL Card, which in reality lives as a full document in an internal wiki. Notice the card\\nclearly communicates the data sources, versions, and assumptions. This helps mitigate invalid assumptions about\\nperformance and generalizability when moving from R&D to production, and promotes the use of real-world data\\nearlier in the project lifecycle. We recommend documenting datasets thoroughly with semantic versioning and tools\\nsuch as datasheets for datasets [24], and following data accountability best-practices as they evolve (see [25]).\\n•Costs of edge cases can be signiﬁcant, sometimes risking expensive machine downtime or medical failures.\\nThis is exacerbated in anomaly detection anomalies are by deﬁnition rare so they can be difﬁcult to train for,\\nespecially for the anomalies that are completely unseen until they arise in the wild.\\n•End-user trust can be difﬁcult to achieve, often preventing the adoption of ML applications, particularly in\\nthe healthcare domain and other highly regulated industries.\\nThese and additional ML challenges such as data privacy and interpretability can inhibit ML adoption in clinical practice\\nand industrial settings, but can be mitigated with MLTRL processes. We’ll describe how in the context of the Naud\\n& Lavin[ 23] example, which began at level 0 with theoretical ML work on manifold geometries, and at level 5 was\\ndirected towards specialized human-machine teaming applications utilizing the same ML method under-the-hood.\\n•Levels 0-1 – From open-ended exploration of data-representation properties in various Riemmanian manifold\\ncurvatures, we derived from ﬁrst principles and empirically identiﬁed a property with hyperbolic manifolds:\\nwhen used as a latent space for embedding data without labels, the geometry organizes the data by it’s implicit\\nhierarchical structure. Unsupervised computer vision was identiﬁed in reviews as a promising direction for\\nproof-of-principle work.\\n•Level 2 – One approach for validating the earlier theoretical developments was to generate synthetic data to\\nisolate very speciﬁc features in data we would expect represented in the latent manifold. The results showed\\npromise for anomaly detection – using the latent representation of data to automatically identify images that\\nare out-of-the-ordinary (anomalous), and also using the manifold to inspect how they are semantically different.\\nFurther, starting with an implicitly probabilistic modeling approach implied uncertainty estimation could be\\na valuable feature downstream. This made the level 2 key decision point clear: proceed with applied ML\\ndevelopment.\\n•Levels 3-5 – Proof-of-concept development and reviews demonstrated promise for several commercial appli-\\ncations relevant to the business, and also highlighted the need for several key features (deﬁned as R&D and\\nproduct requirements): interpretability (towards end-user trust), uncertainty quantiﬁcation (to show conﬁdence\\nscores), and human-in-the-loop (for domain expertise). Without the MLTRL PoC steps and review processes,\\nthese features can often be delayed until beta testing or overlooked completely – for example, the failures of\\napplying IBM Watson in medical applications [ 27]. For this technology, the applications to develop towards\\nare anomaly detection in histopathology and manufacturing, speciﬁcally inspecting whole-slide images of\\nneural tissue, and detecting defects in metallic surfaces, respectively.\\n9'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='From the systems perspective, we suggest quantifying the uncertainties of components and propagating them\\nthrough the system, which can improve safety and trust. Probabilistic ML methods, rooted in Bayesian\\nprobability theory, provide a principled approach to representing and manipulating uncertainty about models\\nand predictions[ 28]. For this reason we advocate strongly for probabilistic models and algorithms in AI\\nsystems. In this machine vision example, the MLTRL technical requirements speciﬁcally called for a\\nprobabilistic generative model to readily quantify various types of uncertainties and propagate them forward to\\nthe visualization component of the pipeline, and the product requirements called for the downstream conﬁdence\\nand sensitivity measures to be exposed to the end-user. Component uncertainties must be assembled in a\\nprincipled way to yield a meaningful measure of overall system uncertainty, based on which safe decisions can\\nbe made[29]. See the Methods section for more on uncertainty in AI systems.\\nThe early checks for data management and governance proved valuable here, as the application areas dealt\\nwith highly sensitive data that would signiﬁcantly inﬂuence the design of data pipelines and test suites. In\\nboth the neuropathology and manufacturing applications, the data management checks also raised concerns\\nabout hidden feedback loops, where users may unintentionally skew the data inputs when using the anomaly\\ndetection models in practice, for instance biasing the data towards speciﬁc subsets they subjectively need help\\nwith. Incorporating domain experts this early in the project lifecycle helped inform veriﬁcation and validation\\nsteps to help be robust to the hidden feedback loops. Not to mention their input guided us towards user-centric\\nmetrics for performance, which can often skew from ML metrics in important ways – for instance, the typical\\nacceptance ratio for false positives versus false negatives doesn’t apply to select edge cases, for which our\\nhierarchical anomaly classiﬁcation scheme was useful [23].\\nFrom prior reviews and TRL card documentation, we also identiﬁed the value of synthetic data generation\\ninto application development: anomalies are by deﬁnition rare so they are hard to come by in real datasets,\\nespecially with evolving environments in deployment settings, so the ability to generate synthetic datasets for\\nanomaly detection can accelerate the level 6-9 pipeline, and help ensure more reliable models in the wild.\\n•Level 6 (medical) – The medical inspection application experienced a bifurcation with product work proceed-\\ning while additional R&D was desired to explore improved data processing methods, while engaging with\\nclinicians and medical researchers for feedback. Proceeding through the levels in a non-linear, non-monotonic\\nway is common in MLTRL and encouraged by various switchback mechanisms (detailed in the Methods\\nsection). These practices – intentional switchbacks, frequent engagement with domain experts and users – can\\nhelp mitigate methodological ﬂaws and underlying biases that are common when applying ML to clinical\\napplications. For instance, recent work by Roberts et al. [ 30] investigated 2,122 studies applying ML to\\nCOVID-19 use-cases, ﬁnding that none of the models are sufﬁcient for clinical use due to methodological ﬂaws\\nand/or underlying biases. They go on to give many recommendations – some we’ve discussed in the context of\\nMLTRL, and more – which should be reviewed for higher quality medical-ML models and documentation.\\n•Level 6-9 (manufacturing) – Overall these stages proceeded regularly and efﬁciently for the defect detection\\nproduct. MLTRL’s embedded switchback from level 9 to 4 proved particularly useful in this lifecycle, both\\nfor incorporating feedback from the ﬁeld and for updating with research progress. On the former, the data\\ndistribution shifts from one deployment setting to another signiﬁcantly affected false-positive versus false-\\nnegative calibrations, so this was added as a feature to the CI/CD pipelines. On the latter, the built-in touch\\npoints for real-world feedback and data into the continued ML research provided valuable constraints to\\nhelp guide research, and product managers could readily understand what capabilities could be available for\\nproduct integration and when (readily communicated with TRL Cards) – for instance, later adding support for\\nvideo-based inspection for defects, and tooling for end-users to reason about uncertainty estimates (which\\nhelps establish trust).\\n•Level 7-9 (medical) – For productization the “neuropathology copilot” was handed off to a partner pharmaceu-\\ntical company to integrate into their existing software systems. The MLTRL documentation and communication\\nstreamlined the technology transfer, which can often by a time-consuming manual process. If not pursuing\\nthis path, the product would’ve likely faced many of the medical-ML deployment challenges with model\\navailability and data access; MLTRL cannot overcome the technical challenges of deploying on-premises, but\\nthe manifestation of those challenges as performance regressions, data shifts, privacy and ethics concerns, etc.\\ncan be mitigated by the system-level checks and strategies MLTRL puts forth.\\nComputer vision with real and synthetic data\\nAdvancements in physics engines and graphics processing have advanced AI environment and data-generation capabili-\\nties, putting increased emphasis on transitioning models across the simulation-to-reality gap [ 31,32,33]. To develop a\\ncomputer vision application for automated recycling, we leveraged the Unity Perception [ 34] package, a toolkit for\\ngenerating large-scale datasets for perception-based ML training and validation. We produced synthetic images to\\n10'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='\\x15A?U?HEJC\\x03?H=OOEBE?=PEKJ\\x03LELAHEJA\\x03\\n\\xa0=¡\\xa0>¡\\n,V\\x03PRGHO\\x03FRQğGHQFH\\x03\\x1f\\x03WKUHVKROG\",V\\x03GHWHFWHG\\x03REMHFW\\x03LQ\\x03WDUJHW\\x03VHW\"3URYLGH\\x03FRUUHVSRQGLQJ\\x03UHF\\\\FOLQJ\\x03LQVWUXFWLRQV,QLWLDWH\\x03KXPDQ\\x10\\x03LQ\\x10WKH\\x10ORRS\\x03SURWRFRO12<(6<(6Figure 4: Computer vision pipeline for an automated recycling application (a), which contains multiple ML models,\\nuser input, and image data from various sources. Complicated logic such as this can mask ML model performance lags\\nand failures, and also emphasized the need for R&D-to-product hand off described in MLTRL. Additional emphasis is\\nplaced on ML tests that consider the mix of real-world data with user annotations (b, right) and synthetic data generated\\nby Unity AI’s Perception tool and structured domain randomization (b, left).\\ncomplement real-world data sources (Figure 4). This application exempliﬁes three important challenges in ML product\\ndevelopment that MLTRL helps overcome:\\n•Multiple and disparate data sources are common in deployed ML pipelines yet often ignored in R&D.\\nFor instance, upstream data providers can change formats unexpectedly, or a physical event could cause the\\ncustomer behavior to change. It is nearly impossible to anticipate and design for all potential problems with\\nreal-world data and deployment. This computer vision system implemented pipelines and extended test suites\\nto cover open-source benchmark data, real user data, and synthetic data.\\n•Hidden performance degradation can be challenging to detect and debug in ML systems because gradual\\nchanges in performance may not be immediately visible. Common reasons for this challenge are that the\\nML component may be one step in a series. Additionally, local/isolated changes to an ML component’s\\nperformance may not directly affect the observed downstream performance. We can see both issues in the\\nillustrated logic diagram for the automated recycling app (Figure 4). A slight degradation in the initial CV\\nmodel may not heavily inﬂuence the following user input. However, when an uncommon input image appears\\nin the future, the app fails altogether.\\n•Model usage requirements can make or break an ML product. For example, the Netﬂix “$1M Prize” solution\\nwas never fully deployed because of signiﬁcant engineering costs in real-world scenariosv. For example,\\nengineering teams must communicate memory usage, compute power requirements, hardware availability,\\nnetwork privacy, and latency to the ML teams. ML teams often only understand the statistics or ML theory\\nbehind a model but not the system requirements or how it scales.\\nWe next elucidate these challenges and how MLTRL helps overcome them in the context of this project’s lifecycle. This\\nproject started at level 4, using largely existing ML methods with a target use-case.\\n•Level 4 – For this project, we validated most of the components in other projects. Speciﬁcally, the computer\\nvision (CV) model for object recognition and classiﬁcation was an off-the-shelf model. The synthetic data\\ngeneration method used Unity Perception, a well-established open-source project. Though this allowed us to\\nvnetﬂixtechblog.com/netﬂix-recommendations-beyond-the-5-stars-part-1\\n11'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='skip the earlier levels, many challenges arise when combining ML elements that were independently validated\\nand developed. The MLTRL prototype-caliber code checkpoint ensures that the existing code components\\nare validated and helps avoid poorly deﬁned borders and abstractions between components. ML pipelines\\noften grow out of glue code, and our regimented code checkpoints motivate well-architected software that\\nminimizes these danger spots.\\n•Level 5 – The problematic “valley of death”, mentioned earlier in the level 5 deﬁnitions, is less prevalent in use-\\ncases like this that start at a higher MLTRL level with a speciﬁc product deliverable. In this case, the product\\ndeliverable was a real-time object recognition and classiﬁcation of trash for a mobile recycling application.\\nStill, this stage is critical for the requirements and V&V transition. This stage mitigates failure risks due to the\\ndisparate data sources integrated at various steps in this CV system and accounted for the end-user compute\\nconstraints for mobile computing. Speciﬁcally, the TRL cards from earlier stages surfaced potential issues\\nwith imbalanced datasets and the need for speciﬁc synthetic images. These considerations are essential for the\\ndata readiness and testing V&V in the productization requirements. Data quality and availability issues often\\npresent huge blockers because teams discover them too late in the game. Data-readiness is one class of many\\nexample issues teams face without MLTRL, as depicted in Fig. 2.\\n•Level 6 – We were re-using a well-understood model and deployment pipeline in this use-case, meaning our\\nprimary challenge was around data reliability. For the problem of recognizing and classifying trash, building a\\nreliable data source using only real data is almost impossible due to diversity, class imbalance, and annotation\\nchallenges. Therefore we chose to develop a synthetic data generator to create training data. At this MLTRL\\nlevel, we needed to ensure that the synthetic data generator created sufﬁciently diverse data and exposed the\\ncontrols needed to alter the data distribution in production. Therefore, we carefully exposed APIs using the\\nUnity Perception package, which allowed us to control lighting, camera parameters, target and non-target\\nobject placements and counts, and background textures. Additionally, we ensured that the object labeling\\nmatched the real-world annotator instructions and that output data formats matched real-world counterparts.\\nLastly, we established a set of statistical tests to compare synthetic and real-world data distributions. The\\nMLTRL checks ensured that we understood, and in this case, adequately designed our data sources to meet\\nin-production requirements.\\n•Level 7 – From the previous level’s R&D TRL cards and observations, we knew relatively early in produc-\\ntization that we would need to assume bias for the real data sources due to class imbalance and imperfect\\nannotations. Therefore we designed tests to monitor these in the deployed application. MLTRL imposes these\\ncritical deployment tests well ahead of deployment, where we can easily overlook ML-speciﬁc failure modes.\\n•Level 8 – As we suggested earlier, problems that stem from real-world data are near impossible to anticipate\\nand design for, implying the need for level 8 ﬂight-readiness preparations. Given that we were generating\\nsynthetic images (with structured domain randomization) to complement the real data, we created tests for\\ndifferent data distribution shifts at multiple points in the classiﬁcation pipeline. We also implemented thorough\\nshadow tests ahead of deployment to evaluate how susceptible the ML model(s) to performance regressions\\ncaused by data. Additionally, we also implemented these as CI/CD tests over various deployment scenarios (or\\nmobile device computing speciﬁcations). Without these fully covered, documented, and automated, it would\\nbe impossible to pass level 8 review and deploy the technology.\\n•Level 9 – Post-deployment, the monitoring tests prescribed at Levels 8 and 9 and the three main code quality\\ncheckpoints in the MLTRL process help surface hidden performance degradation problems, common with\\ncomplex pipelines of data ﬂows and various models. The switchbacks depicted in Fig. 2 are typical in CV\\nuse-cases. For instance, miscalibrations in models pre-trained on synthetic data and ﬁne-tuned on newer real\\ndata can be common yet difﬁcult to catch. However, the level 7 to 4 switchback is designed precisely for these\\nchallenges and product improvements.\\nAccelerating scientiﬁc discovery with massive particle physics simulators\\nComputational models and simulation are key to scientiﬁc advances at all scales, from particle physics, to material\\ndesign and drug discovery, to weather and climate science, and to cosmology[ 35]. Many simulators model the forward\\nevolution of a system (coinciding with the arrow of time), such as the interaction of elementary particles, diffusion of\\ngasses, folding of proteins, or evolution of the universe in the largest scale. The task of inference refers to ﬁnding initial\\nconditions or global parameters of such systems that can lead to some observed data representing the ﬁnal outcome\\nof a simulation. In probabilistic programming[ 36], this inference task is performed by deﬁning prior distributions\\nover any latent quantities of interest, and obtaining posterior distributions over these latent quantities conditioned\\non observed outcomes (for example, experimental data) using Bayes rule. This process, in effect, corresponds to\\ninverting the simulator such that we go from the outcomes towards the inputs that caused the outcomes. In the\\n“Etalumis” project[ 37] (“simulate” spelled backwards), we are using probabilistic programming methods to invert\\n12'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='existing, large-scale simulators via Bayesian inference. The project is as an interdisciplinary collaboration of specialists\\nin probabilistic machine learning, particle physics, and high-performance computing, all essential elements to achieve\\nthe project outcomes. Even more, it is a multi-year project spanning multiple countries, companies, university labs, and\\ngovernment research organizations, bringing signiﬁcant challenges in project management, technology coordination\\nand validation. Aided by MLTRL, there were several key challenges to overcome in this project that are common in\\nscientiﬁc-ML projects:\\n•Integrating with legacy systems is common in scientiﬁc and industrial use-cases, where ML methods are\\napplied with existing sensor networks, infrastructure, and codebases. In this case, particle physics domain\\nexperts at CERN are using the SHERPA simulator[ 38], a 1 million line codebase developed over the last\\ntwo decades. Rewriting the simulator for ML use-cases is infeasible due to the codebase size and buried\\ndomain knowledge, and new ML experts would need signiﬁcant onboarding to gain working knowledge of\\nthe codebase. It is also common to work with legacy data infrastructure, which can be poorly organized for\\nmachine learning (let alone preprocessed and clean) and unlikely to have followed best practices such as\\ndataset versioning.\\n•Coupling hardware and software architectures is non-trivial when deploying ML at scale, as performance\\nconstraints are often considered in deployment tests well after model and algorithm development, not to\\nmention the expertise is often split across disjoint teams. This can be exacerbated in scientiﬁc-ML when\\nscaling to supercomputing infrastructure, and working with massive datasets that can be in the terabytes and\\npetabytes.\\n•Interpretability is often a desired feature yet difﬁcult to deliver and validate in practice. Particularly in\\nscientiﬁc ML applications such as this, mechanisms and tooling for domain experts to interpret predictions\\nand models are key for usability (integrating in workﬂows and building trust).\\nTo this end, we will go through the MLTRL levels one by one, demonstrating how they ensure the above scientiﬁc ML\\nchallenges are diligently addressed.\\n•Level 0 – The theoretical developments leading to Etalumis are immense and well discussed in Baydin et\\nal [37]. In particular the ML theory and methods are in a relatively nascent area of ML and mathematics,\\nprobabilistic programming. New territory can present more challenges compared to well-traveled research\\npaths, for instance in computer vision with neural networks. It is thus helpful to have a guiding framework\\nwhen making a new path in ML research, such as MLTRL where early reviews help theoretical ML projects\\nget legs.\\n•Level 1-2 – Running low-level experiments in simple testbeds is generally straightforward when working\\nwith probabilistic programming and simulation; in a sense, this easy iteration over experiments is what\\nPPL are designed for. It was additionally helpful in this project to have rich data grounded in physical\\nconstraints, allowing us to better isolate model behaviors (rather than data assumptions and noise). The\\nMLTRL requirements documentation is particularly useful for the standard PPL experimentation workﬂow:\\nmodel, infer, criticize, repeat (or Box’s loop) [ 39]. The evaluation step (i.e. criticizing the model) can be\\nmore nuanced than checking summary statistics as in deep learning and similar ML workﬂows. It is thus a\\nuseful practice to write down the criticism methods, metrics, and expected results as veriﬁcations for speciﬁc\\nresearch requirements, rather than iterating over Box’s loop without a priori targets. Further, because this\\nresearch project had a speciﬁc target application early in the process (the SHERPA simulator), the project\\ntimeline beneﬁted from recognizing simulator-integration constraints upfront as requirements, not to mention\\ndata availability concerns, which are often overlooked in early R&D levels. It was additionally useful to have\\nCERN scientists as domain experts in the reviews at these R&D levels.\\n•Level 3 – Systems development can be challenging with probabilistic programming, again because it is\\nrelatively nascent and much of the out-of-the-box tools and infrastructure are not there as in most ML and\\ndeep learning. Here in particular there’s a novel (unproven) approach for systems integration: a probabilistic\\nprogramming execution protocol was developed to reroute random number draws in the stochastic simulator\\ncodebase (SHERPA) to the probabilistic programming system, thus enabling the system to control stochastic\\nchoices in SHERPA and run inference on its execution traces, all while keeping the legacy codebase intact! A\\nmore invasive method that modiﬁes SHERPA would not have been acceptable. If it were not for MLTRL forcing\\nsystems considerations this early in the Etalumis project lifecycle, this could have been an insurmountable\\nhurdle later when multiple codebases and infrastructures come into play. By the same token, systems planning\\nhere helped enable the signiﬁcant HPC scaling later: the team deﬁned the need for HPC support well ahead\\nof actually running HPC, in order to build the prototype code in a way that would readily map to HPC (in\\naddition to local or cloud CPU and GPU). The data engineering challenges in this system’s development\\n13'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='nonetheless persist – that is, data pipelines and APIs that can integrate various sources and infrastructures, and\\nnormalize data from various databases – although MLTRL helps consider these at the an earlier stage that can\\nhelp inform architecture design.\\n•Level 4 – The natural “embedded switchback” from Level 4 to 2 (see the Methods section) provided an efﬁcient\\npath toward developing an improved, amortized inference method–i.e., using a computationally expensive\\ndeep learning based inference algorithm to train only once, in order to then do fast, repeated inference in the\\nSHERPA model. Leveraging cyclic R&D methods, the Etalumis project could iteratively improve inference\\nmethods without stalling the broader system development, ultimately producing the largest scale posterior\\ninference in a Turing-complete probabilistic programming system. Achieving this scale through iterative R&D\\nalong the main project lifecycle was additionally enabled by working with with NERSC engineers and their\\nCori supercomputer to progressively scale smaller R&D tests to the goal supercomputing deployment scenario.\\nTypical ML workﬂows that follow simple linear progressions[ 6,40] would not enable ramping up in this\\nfashion, and can actual prevent scaling R&D to production due to lack of systems engineering processes (like\\nMLTRL) connecting research to deployment.\\n•Level 5 – Multi-org international collaborations can be riddled with communication and teamwork issues,\\nin particular at this pivotal stage where teams transition from R&D to application and product development.\\nFirst, MLTRL as a lingua franca was key to the team effort bringing Etalumis proof-of-concept into the\\nlarger effort of applying it to massive high-energy physics simulators. It was also critical at this stage to\\nclearly communicate end-user requirements across the various teams and organizations, which must be deﬁned\\nin MLTRL requirements docs with V&V measures – the essential science-user requirements were mainly\\nfor model and prediction interpretability, uncertainty estimation, and code usability. If there are concerns\\nover these features, MLTRL switchbacks can help to quickly cycle back and improve modeling choices in a\\ntransparent, efﬁcient way – generally in ML projects, these fundamental issues with usability are caught too\\nlate, even after deployment. In the probabilistic generative model setting we’ve deﬁned in Etalumis, Bayesian\\ninference gives results that are interpretable because they include exact locations and processes in the model\\nthat are associated with each prediction. Working with ML methods that are inherently interpretable, we are\\nwell-positioned to deliver interpretable interfaces for the end-users later in the project lifecycle.\\n•Level 6-9 – The standard MLTRL protocol apply in these application-to-deployment stages, with several\\nEtalumis-speciﬁc highlights. First, given the signiﬁcant research contributions in both probabilistic pro-\\ngramming and scientiﬁc-ML, it’s important to share the code publicly. The development and deployment\\nof the open-source code repository PPXvibranched into a separate MLTRL path from the Etalumis path\\nfor deployment at CERN. It’s useful to have systems engineering enable clean separation of requirements,\\ndeployments, etc. when there are different development and product lifecycles originating from a common\\nparent project. For example, in this case it was useful to employ MLTRL switchbacks in the open-sourcing\\nprocess, isolated from the CERN application paths, in order to add support for additional programming\\nlanguages so PPX can apply to more scientiﬁc simulators – both directions beneﬁted signiﬁcantly the from\\nthe data pipelines considerations brought up levels earlier, where open-sourcing required different data APIs\\nand data transformations to enable broad usability. Second, related to the open-source code deliverable and\\nthe scientiﬁc ML user requirements we noted above, the late stages of MLTRL reviews include higher level\\nstakeholders and speciﬁc end-users, yet again enforcing these scientiﬁc usability requirements are met. An\\nexample result of this in Etalumis is the ability to output human-readable execution traces of the SHERPA\\nruns and inference, enabling never before possible step-by-step interpretability of the black-box simulator.\\nThe scientiﬁc ML perspective additionally brings to forefront an end-to-end data perspective that is pertinent in\\nessentially all ML use-cases: these systems are only useful to the extent they provide comprehensive data analyses that\\nintegrate the data consumed and generated in these workﬂows, from raw domain data to machine-learned models. These\\ndata analyses drive reproducibility, explainability, and experiment data understanding, which are critical requirements\\nin scientiﬁc endeavors and ML broadly.\\nCausal inference & ML in medicine\\nUnderstanding cause and effect relationships is crucial for accurate and actionable decision-making in many settings,\\nfrom healthcare and epidemiology, to economics and government policy development. Unfortunately, standard\\nmachine learning algorithms can only ﬁnd patterns and correlation in data, and as correlation is not causation, their\\npredictions cannot be conﬁdently used for understanding cause and effect. Indeed, relying on correlations extracted\\nfrom observational data to guide decision-making can lead to embarrassing, costly, and even dangerous mistakes,\\nsuch as concluding that asthma reduces pneumonia mortality risk [ 41], and that smoking reduces risk of developing\\nvigithub.com/pyprob/ppx\\n14'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='severe COVID-19 [ 42]. Fortunately, there has been much recent development in a ﬁeld known as causal inference that\\ncan quantitatively make sense of cause and effect from purely observational data[ 43]. The ability of causal inference\\nalgorithms to quantify causal impact rests on a number of important checks and assumptions–beyond those employed\\nin standard machine learning or purely statistical methodology–that must be carefully deliberated over during their\\ndevelopment and training. These speciﬁc checks and assumptions are as follows:\\n•Specifying cause-and-effect relationships between relevant variables– One of the most important assump-\\ntions underlying causal inference is the structure of the causal relations between quantities of interest. The\\ngold standard for determining causal relations is to perform a randomised controlled trial, but in most cases\\nthese cannot be employed due to ethical concerns, technological infeasibility, or prohibitive cost. In these\\nsituations, domain experts have to be consulted to determine the causal relationships. It is important in these\\nsituations to carefully address the manner in which such domain knowledge was extracted from experts, the\\nnumber and diversity of experts involved, the amount of consensus between experts, and so on. The need for\\ncareful documentation of this knowledge and its periodic review is made clear in the MLTRL framework, as\\nwe shall see below.\\n•Identiﬁability– Another vital component of building causal models is whether the causal question of interest\\nisidentiﬁable from the causal structure speciﬁed for the model together with observational (and sometimes\\nexperimental) data.\\n•Adjusting for and monitoring confounding bias– An important aspect of causal model performance, not\\npresent in standard machine learning algorithms, is confounding bias adjustment. The standard approach is to\\nemploy propensity score matching to remove such bias. However, the quality of bias adjustment achieved in\\nany speciﬁc instance with such propensity-based matching methods needs to be checked and documented,\\nwith alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[44].\\n•Sensitivity analysis– As causal estimates are based on generally untestable assumptions, such as observing all\\nrelevant confounders, it is vital to determine how sensitive the resulting predictions are to potential violations\\nof these assumptions.\\n•Consistency– It is crucial to understand if the learned causal estimate provably converges to the true causal\\neffect in the limit of inﬁnite sample size. However, causal models cannot be validated by standard held-out\\ntests, but rather require randomization or special data collection strategies to evaluate their predictions [ 45,46].\\nThe MLTRL framework makes transparent the need to carefully document and defend these assumptions, thus ensuring\\nthe safe and robust creation, deployment, and maintenance of causal models. We elucidate this with recent work by\\nRichens et al.[ 47], developing a causal approach to computer-assisted diagnosis which outperforms previous purely\\nmachine learning based methods. To this end, we will go through the MLTRL levels one by one, demonstrating how\\nthey ensure the above speciﬁc checks and assumptions are naturally accounted for. This should provide a blueprint for\\nhow to employ the MLTRL levels in other causal inference applications.\\n•Level 0 – When initially faced with a causal inference task, the ﬁrst step is always to understand the causal\\nrelationships between relevant variables. For instance, in Richens et al. [ 47], the ﬁrst step toward building\\nthe diagnostic model was specifying the causal relationships between the diverse set risk factors, diseases,\\nand symptoms included in the model. To learn these relations, doctors and healthcare professionals were\\nconsulted to employ their expansive medical domain knowledge which was robustly evaluated by additional\\nindependent groups of healthcare professionals. The MLTRL framework ensured this issue is dealt with and\\ndocumented correctly, as such knowledge is required to progress from Level 0; failure to do this has plagued\\nsimilar healthcare AI projects [48].\\nThe next step of any causal analysis is to understand whether the causal question of interest is uniquely\\nidentiﬁable from the causal structure speciﬁed for the model together with observational and experimental data.\\nIn this medical diagnosis example, identiﬁcation was crucial to establish, as the causal question of interest,\\n“would the observed symptoms not be present had a speciﬁc disease been cured?”, was highly non-trivial.\\nAgain, MLTRL ensures this vital aspect of model building is carefully considered, as a mathematical proof of\\nidentiﬁability would be required to graduate from Level 0.\\nWith both the causal structure and identiﬁability result in hand, one can progress to Level 1.\\n•Level 1 – At this level, the goal is to take the estimand for the identiﬁed causal question of interest and\\ndevise a way to estimate it from data. To do this one will need efﬁcient ways to adjust for confounﬁng bias.\\nThe standard approach is to employ propensity score-based methods to remove such bias when the target\\ndecision is binary, and use multi-stage ML models adhering to the assumed causal structure[ 49] for continuous\\ntarget decisions (and high-dimensional data in general). However, the quality of bias adjustment achieved in\\nany speciﬁc instance with propensity-based matching methods needs to be checked and documented, with\\n15'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[ 44]. As\\nabove, MLTRL ensures transparency and adherence to this important aspect of causal model development, as\\nwithout it a project cannot graduate from Level 1. Even more, MLTRL ensures tests for confounding bias\\nare developed early-on and maintained throughout later stages to deployment. Still, in many cases, it is not\\npossible to completely remove confounding in the observed data. TRL Cards offer a transparent way to declare\\nspeciﬁc limitations of a causal ML method.\\n•Level 2 – PoC-level tests for causal models must go beyond that of typical ML models. As discussed above,\\nto ensure the estimated causal effects are robust to the assumptions required for their derivation, sensitivity\\nto these assumptions must be analysed. Such sensitivity analysis is often limited to R&D experiments or\\na post-hoc feature of ML products. MLTRL on the other hand requires this throughout the lifecycle as\\ncomponents of ML test suites and gated reviews. In the case of causal ML, best practice is to employ sensitivity\\nanalysis for this robustness check[ 50]. MLTRL ensures this check is highlighted and adhered to, and no model\\nwill end up graduating Level 2–let alone being deployed–unless it is passed.\\n•Level 3 – Coding best practices, as in general ML applications.\\n•Level 4-5 – There are additional tests to consider when taking causal models from research to production,\\nin particular at Level 4–proof of concept demonstration in a real scenario. Consistency , for example, is an\\nimportant property of causal methods that informs us whether the method provably converges to the true\\ncausal graph in the limit of inﬁnite sample size. Quantifying consistency in the test suite is critical when\\ndatasets change from controlled laboratory settings to open-world, and when the application scales. And\\nPoC validation steps are more efﬁcient with MLTRL because the process facilitates early speciﬁcation of the\\nevaluation metric for a causal model in Level 2. Causal models cannot be validated by standard held-out tests,\\nbut rather require randomization or special data collection strategies to evaluate their predictions[ 45,46]. Any\\ndifﬁculty in evaluating the model’s predictions will be caught early and remedied.\\n•Level 6-9 – With the the causal ML components of this technology developed reliably in the previous levels,\\nthe rest of the levels developing this technology focused on general medical-ML deployment challenges. For\\nthe most part, data governance, privacy, and management that was detailed earlier in the neuropathology\\nMLTRL use-case, as well as on-premises deployment.\\nAI for open-source space sciences\\nThe CAMS (Cameras for Allsky Meteor Surveillance) project [ 51], established in 2010 by NASA, uses hundreds of\\noff-the-shelf CCTV cameras to capture the meteor activity in the night sky. Initially, resident scientists would retrieve\\nhard-disks containing video data captured each night and perform manual triangulation of tracks or streaks of light\\nin the night sky, and compute a meteor’s trajectory, orbit, and lightcurve. Each solution was manually classiﬁed as a\\nmeteor or not (i.e., planes, birds, clouds, etc). In 2017, a project run by the Frontier Development Labvii[52], the AI\\naccelerator for NASA and ESA, aimed to automate the data processing pipeline and replicate the scientists thought\\nprocess to build an ML model that identiﬁes meteors in the CAMS project [ 53,54]. The data automation led to\\norders of magnitude improvements in operational efﬁciency of the system, and allowed new contributors and amateur\\nastronomers to start contributing to meteor sightings. Additionally, a novel web tool allowed anybody anywhere to\\nview the meteors detected in the previous night. The CAMS camera system has had six-fold global expansion of the\\ndata capture network, discovered ten new meteor showers, contributed towards instrumental evidence of previously\\npredicted comets, and helped calculate parent bodies of various meteor showers. CAMS utilized the MLTRL framework\\nto progress as described:\\n•Level 1 – Understanding the domain and data is a prerequisite for any ML development. Extensive data\\nexploration elucidated visual differences between objects in the night sky such as meteors, satellites, clouds,\\ntail lights of planes, light from the eyes of cats peering into cameras, trees, and other tall objects visible in\\nthe moonlight. This step helped (1) understand visual properties of meteors that later deﬁned the ML model\\narchitecture, and (2) mitigate impact of data imbalance by proactively developing domain-oriented strategies.\\nThe results are well-documented on a datasheet associated with the TRL card, and discussed at the stage\\nreview. This MLTRL documentation forced us to consider data sharing and other privacy concerns at this early\\nconceptualization stage, which is certainly relevant considering CAMS is for open-source and gathering data\\nfrom myriad sources.\\n•Level 2-3 – The agile and non-monotonic (or non-linear) development prescribed by MLTRL allowed the\\nteam to ﬁrst develop an approximate end-to-end pipeline that offered a path to ML model deployment and\\nquick turnaround time to incorporate feedback from the regular gated reviews. Then, with relatively quicker\\nviiThe NASA Frontier Development Lab and partners open-source the code and data via the SpaceML platform: spaceml.org\\n16'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='experimentation, the team could improve on the quality of not just the ML model, but also scale up the systems\\ndevelopment simultaneously in a non-monotonic development cycle.\\n•Level 4 – With the initial pipeline in place, scalable training of baselines and initial models on real challenging\\ndatasets ensued. Throughout the levels, the MLTRL gated reviews were essential for making efﬁcient progress\\nwhile ensuring robustness and functionality that meets stakeholder needs. At this stage we highlight speciﬁc\\nadvantages of the MLTRL review processes that had instrumental effect on the project success: With the\\nrequired panel of mixed ML researchers and engineers, domain scientists, and product managers, the stage 4\\nreviews stressed the signiﬁcance of numerical improvements and comparison to existing baselines, and helped\\nidentify and overcome issues with data imbalance. The team likely would have overlooked these approaches\\nwithout the review from peers in diverse roles and teams. In general, the evolving panel of reviewers at\\ndifferent stages of the project was essential for covering a variety of veriﬁcation and validation measures –\\nfrom helping mitigate data challenges, to open-source code quality.\\n•Level 5 – To complete this R&D-to-productization level, a novel web tool called the NASA CAMS Meteor\\nShower Portalviiiwas created that allowed users to view meteor shower activity from the previous night and\\nverify meteor predictions generated by the ML model. This app development was valuable for A/B testing,\\nvalidating detected meteors and classiﬁed new meteor showers with human-AI interaction, and demonstrating\\nreal-world utility to stakeholders in review. ML processes without MLTRL miss out on these valuable\\ndevelopment by overlooking the need for such a demo tool.\\n•Level 6 – Application development was naturally driven by end-user feedback from the web app in level 5 –\\nwithout MLTRL it’s unlikely the team would be able to work with early productization feedback. With almost\\nreal time feedback coming in daily, newer methods for improving robustness of meteor identiﬁcation led to\\nresearching and developing a unique augmentation technique, resulting in the state of the art performance of\\nthe ML model. Further application development led to incorporating features that were in demand by users of\\nthe NASA CAMS Meteor Shower Portal: include celestial reference points through constellations, add ability\\nto zoom in/out and (un)cluster showers, and provide tooling for scientiﬁc communication. The coordination of\\nthese features into product-caliber codebase resulted in the release of the NASA CAMS Meteor Shower Portal\\n2.0 that was built by a team of citizen scientists – again we found the speciﬁc checkpoints in the MLTRL\\nreview were crucial for achieving these goals.\\n•Level 7 – Integration was particularly challenging in two ways. First, integrating the ML and data engineering\\ndeliverables with the existing infrastructure and tools of the larger CAMS system, which had started devel-\\nopment years earlier with other teams in partner organizations, required quantiﬁable progress for verifying\\nthe tech-readiness of ML models and modules. The use of technology readiness levels provided a clear and\\nconsistent metric for the maturity of the ML and data technologies, making for clear communication and\\nefﬁcient project integration. Without MLTRL it is difﬁcult to have a conversation, let alone make progress, to-\\nwards integrating AI/ML and data subsystems and components. Second, integrating open-source contributions\\ninto the main ML subsystem was a signiﬁcant challenge alleviated with diligent veriﬁcation and validation\\nmeasures from MLTRL, as well as quantifying robustness with ML testing suites (using scoring measures like\\nthat of the ML Testing Rubric[20], and devising a checklist based on metamorphic testing[18]).\\n•Level 8 – CAMS, like many datasets in practice, consisted of a smaller labeled subset and a much larger\\nunlabeled set. In an attempt to additionally increase robustness of the ML subsystem ahead of “ﬂight readiness”,\\nwe looked to active learning [ 55,56] techniques to leverage the unlabeled data. Models using an initial version\\nof this approach, where results of the active learning provided “weak” labels, resulted in consumption of the\\nentire decade long unlabelled data collected by CAMS and slightly higher scores on deployment tests. Active\\nlearning showed to be a promising feature and was switched back to level 7 for further development towards\\nthe next deployment version, so as not to delay the rest of the project.\\n•Level 9 – The ML components in CAMS require continual monitoring for model and data drifts, such as\\nchanges in weather, smoke, and cloud patterns that affect the view of the night sky. The data drifts may also be\\nspeciﬁc to locations, such as ﬁreﬂies and bugs in CAMS Australia and New Zealand stations which appear as\\nfalse positives. The ML pipeline is largely automated with CI/CD, runs regular regression tests, and production\\nof benchmarks. Manual intervention can be triggered when needed, such as sending low conﬁdence meteors for\\nveriﬁcation to scientists in the CAMS project. The team also regularly releases the code, models, and web tools\\non the open-source space sciences and exploration ML toolbox, SpaceMLix. Through the SpaceML community\\nand partner organizations, CAMS continually improves with feature requests, debugging, and improving data\\npractices, while tracking progress with standard software release cycles and MLTRL documentation.\\nviiimeteorshowers.seti.org\\nixspaceml.org\\n17'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='BEYOND SOFTWARE ENGINEERING\\nSoftware engineering (SWE) practices vary signiﬁcantly across domains and industries. Some domains, such as medical\\napplications, aerospace, or autonomous vehicles rely on a highly rigorous development process which is required\\nby regulations. Other domains, for example advertising and e-commerce are not regulated and can employ a lenient\\napproach to development. ML development should at minimum inherit the acceptable software engineering practices of\\nthe domain. There are, however, several key areas where ML development stands out from SWE, adding its own unique\\nchallenges which even most rigorous SWE practices are not able to overcome.\\nFor instance, the behavior of ML systems is learned from data, not speciﬁed directly in code. The data requirements\\naround ML (i.e., data discovery, management, and monitoring) adds signiﬁcant complexity not seen in other types\\nof SWE. There are many beneﬁts to using a data-oriented architecture (DOA) [48] with the data-ﬁrst workﬂows and\\nmanagement practices prescribed in MLTRL. DOA aims to make the data ﬂowing between elements of business logic\\nmore explicit and accessible with a streaming-based architecture rather than the micro-service architectures that are\\nstandard in software systems. One speciﬁc beneﬁt of DOA is making data available and traceable by design, which\\nhelps signiﬁcantly in the ML logging challenges and data governance needs we discussed in Levels 7-9. Moreover,\\nMLTRL highlights data-related requirements along every step to ensure that the development process considers data\\nreadiness and availability.\\nNot to mention an array of ML-speciﬁc failure modes; for example, models that become miscalibrated due to subtle\\ndata distributional shifts in the deployment setting, resulting in models that are more conﬁdent in predictions than they\\nshould be. MLTRL helps deﬁne ML-speciﬁc testing considerations (levels 5 and 7) to help surface these failure-modes\\nearly. ML opens up new threat vectors across the whole deployment workﬂow that otherwise aren’t risks in software\\nsystems: for example, a poisoning attack to contaminate the training phase of ML systems, or membership inference\\nto see if a given data record was part of the model’s training. MLTRL consider these threat vectors and suggests\\nrelevant risk-identiﬁcation during prototyping and productization phases. More generally, ML codebases have all the\\nproblems for regular code, plus ML-speciﬁc issues at the system level, mainly as a consequence of added complexity\\nand dynamism. The resulting entanglement, for instance, implies that the SWE practice of making isolated changes is\\noften not feasible – Scully et al.[ 57] refer to this as the “changing anything changes everything” principle. Given this\\nconsideration, typical SWE change-management is insufﬁcient. Furthermore, ML systems almost necessarily increase\\nthe technical debt; package-level refactoring is generally sufﬁcient for removing technical debt in software systems, but\\nthis is not the case in ML systems.\\nThese factors and others suggest that inherited software engineering and management practices of a given domain are\\ninsufﬁcient for the successful development of robust and reliable ML systems. But it is not trading off one for the other:\\nMLTRL can be used in synergy with the existing, industry-standard software engineering practices such as agile [ 58]\\nand waterfall [ 59] to handle unique challenges of ML development. Because ML applications are a category of software,\\nall best practices of building and operating software should be extended when possible to the ML application. Practices\\nlike version control, comprehensive testing, continuous integration and continuous deployment are all applicable to ML\\ndevelopment. MLTRL provides a framework that helps extend SWE building and operating practices that are acceptable\\nin a given domain to tackle the unique challenges of ML development.\\nRELATED WORKS\\nA recent case study from Microsoft Research [ 40] similarly identiﬁes a few themes describing how ML is not equal to\\nsoftware engineering, and recommends a linear ML workﬂow with steps for data preparation through modeling and\\ndeploying. They deﬁne an effective workﬂow for isolated development of an ML model, but this approach does not\\nensure the technology is actually improving in quality and robustness. Their process should be repeated at progressive\\nstages of development in the broader ML and data technology lifecycle. If applied in the MLTRL framework, the\\nspeciﬁc ingredients of the ML model workﬂow – that is, people, software, tests, objectives, etc. – evolve over time and\\nsubsequent stages as the technologies mature.\\nThere exist many recommended workﬂows for speciﬁc ML methods and areas of pipelines. For instance, a more\\niterative process for Bayesian ML [ 60] and even more speciﬁcally for probabilistic programming [ 39], a data mining\\nprocess deﬁned in 2000 that remains widely used [ 61], others for describing data iterations [ 62], and human-computer\\ninteraction cycles [ 63]. In these recommended workﬂows and others, there’s an important distinction between their\\ncycles and “switchback” mechanisms in MLTRL. Their cycles suggest to generically iterate over a data-modeling-\\nevaluation-deployment process. Switchbacks, on the other hand, are speciﬁc, purpose-driven workﬂows for dialing\\npart(s) of a project to an earlier stage – this doesn’t simply mean go back and train the model on more data, but rather\\nswitching back regresses the technology’s maturity level (e.g. from level 5 to level 3) such that it must again fulﬁll the\\nlevel-by-level requirements, evaluations and reviews. See the Methods section for more details on MLTRL switchbacks.\\n18'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='In general, iteration is an important part of data, ML, and software processes. MLTRL is unique from the other\\nrecommended processes in many ways, and perhaps most importantly because it considers data ﬂows and ML models\\nin the context of larger systems. These isolated processes (that are speciﬁc to e.g. modeling in prototype development\\nor data wrangling in application development) are synergistic with MLTRL because they can be used within each level\\nof the larger lifecycle or framework. For example, the Bayesian modeling processes [ 39,60] we mentioned above\\nare really useful to guide developers of probabilistic ML approaches. But there are important distinctions between\\nexecuting these modeling steps and cycles in a well-deﬁned prototyping environment with curated data and minimal\\nresponsibilities, versus a production environment riddled with sparse and noisy data, that interacts with the physical\\nworld in non-obvious ways, and can carry expensive (even hidden) consequences. MLTRL provides the necessary,\\nholistic context and structure to use these and other development processes reliably and responsibly.\\nAlso related to our work, Google teams have proposed ML testing recommendations [ 20] and validating the data fed\\ninto ML systems [ 64]. For NLP applications, typical ML testing practices struggle to translate to real-world settings,\\noften overestimating performance capabilities. An effective way to address this is devising a checklist of linguistic\\ncapabilities and test types, as in Ribeiro et al.[ 17]–interestingly their test suite was inspired by metamorphic testing,\\nwhich we suggested earlier in Level 7 for testing systems AI integrations. A survey by Paleyes et al. [ 48] go over\\nnumerous case studies to discuss challenges in ML deployment. They similarly pay special attention to the need for\\nethical considerations, end-user trust, and extra security in ML deployments. On the latter point, Kumar et al. [ 65]\\nprovide a table thoroughly breaking down new threat vectors across the whole ML deployment workﬂow (some of\\nwhich we mentioned above). These works, notably the ML security measures and the quantiﬁcation of an ML test suite\\nin a principled way – i.e., that does not use misguided heuristics such as code coverage – are valuable to include in any\\nML workﬂow including MLTRL, and are synergistic with the framework we’ve described in this paper. These analyses\\nprovide useful insights, but they do not provide a holistic, regimented process for the full ML lifecycle from R&D\\nthrough deployment. An end-to-end approach is suggested by Raji et al.[ 66], but only for the speciﬁc task of auditing\\nalgorithms; components of AI auditing are mentioned in Level 7, and covered throughout in the review processes.\\nSculley et al.[ 57] go into more ML debt topics such as undeclared consumers and data dependencies, and go on to\\nrecommend an ML Testing Rubric as a production checklist [ 20]. For example, testing models by a canary process\\nbefore serving them into production. This, along with similar shadow testing we mentioned earlier, are common in\\nautonomous ML systems, notably robotics and autonomous vehicles. They explicitly call out tests in four main areas\\n(ML infrastructure, model development, features and data, and monitoring of running ML systems), some of which we\\ndiscussed earlier. For example, tests that the training and serving features compute the same values; a model may train\\non logged processes or user input, but is then served on a live feed with different inputs. In addition to the Google ML\\nTesting Rubric, we advocate metamorphic testing : a SWE methodology for testing a speciﬁc set of relations between\\nthe outputs of multiple inputs. True to the checklists in the Google ML Testing Rubric and in MLTRL, metamorphic\\ntesting for ML can have a codiﬁed list of metamorphic relations[18].\\nIn domains such as healthcare there have been the introduction of similar checklists for data readiness – for example,\\nto ensure regulatory-grade real-world-evidence (RWE) data quality [ 67] – yet these are nascent and not yet widely\\naccepted. Applying AI in healthcare has led to developing guidance for regulatory protocol, which is still a work in\\nprogress. Larson et al.[ 68] provide a comprehensive analysis for medical imaging and AI, arriving at several regulatory\\nframework recommendations that mirror what we outline as important measures in MLTRL: e.g., detailed task elements\\nsuch as pitfalls and limitations (surfaced on TRL Cards), clear deﬁnition of an algorithm relative to the downstream\\ntask, deﬁning the algorithm “capability” (Level 5), real-world monitoring, and more.\\nD’amour et al.[ 19] dive into the problem we noted earlier about model miscalibration. They point to the trend in machine\\nlearning to develop models relatively isolated from the downstream use and larger system, resulting in underspeciﬁcation\\nthat handicaps practical ML pipelines. This is largely problematic in deep learning pipelines, but we’ve also noted this\\nrisk in the case of causal inference applications. Suggested remedies include stress tests –empirical evaluations that\\nprobe the model’s inductive biases on practically relevant dimensions–and in general the methods we deﬁne in Level 7.\\nLIMITATIONS, RESPONSIBILITIES, and ETHICS\\nMLTRL has been developed, deployed, iterated, and validated in myriad environments, as demonstrated by the previous\\nexamples and many others. Nonetheless we strongly suggest that MLTRL not be viewed as a cure-all for machine\\nlearning systems engineering. Rather, MLTRL provides mechanisms to better enable ML practitioners, teams, and\\nstakeholders to be diligent and responsible with these technologies and data. That is, one cannot implement MLTRL in\\nan organization and turn a blind eye to the many data, ML, and integration challenges we’ve discussed here. MLTRL is\\nanalogous to a pilot’s checklist, not autopilot.\\n19'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='MLTRL is intended to be complimentary to existing software development methodologies, not replace or alter them.\\nSpeciﬁcally, whether the team uses agile or waterfall methods, MLTRL can be adopted to help deﬁne and structure\\nphases of the project, as well as the success criteria of each stage. In context of the software development process, the\\npurpose of MLTRL is to help the team minimize the technical dept and risk associated with the delivery of an ML\\napplication by helping the development team ask necessary questions.\\nWe discussed many data challenges and approaches in the context of MLTRL, and should highlight again the importance\\nof data considerations in any ML initiative. The data availability and quality can severely limit the ability to develop and\\ndeploy ML, whether MLTRL is used or not. It is again the responsibility of the ML practitioners, teams, and stakeholders\\nto gather, use, and distribute data in safe, legal, ethical ways. MLTRL helps do so with rigor and transparency, but\\nagain is not a solution for data bias. We recommend these recent works on data bias in ML: [ 69,70,71,72,73].\\nFurther, AI/ML ethics is a continuously evolving, multidisciplinary space – see [ 5]. MLTRL aims to prioritize ethics\\nconsiderations at each level of the framework, and would do well to also evolve over time with the broader AI/ML\\nethics developments.\\nCONCLUSION\\nWe’ve described Machine Learning Technology Readiness Levels (MLTRL) , an industry-hardened systems engineering\\nframework for robust, reliable, and responsible machine learning. MLTRL is derived from the processes and testing\\nstandards of spacecraft development, yet lean and efﬁcient for ML, data, and software workﬂows. Examples from\\nseveral organizations across industries demonstrate the efﬁcacy of MLTRL for AI and ML technologies, from research\\nand development through productization and deployment, in important domains such as healthcare and physics, with\\nemphasis on data readiness amongst other critical challenges. Our aim is MLTRL works in synergy with recent\\napproaches in the community focused on diligent data-readiness, privacy and security, and ethics. Even more, MLTRL\\nestablishes a much-needed lingua franca for the AI ecosystem, and broadly for AI in the worlds of science, engineering,\\nand business. Our hope is that our systems framework is adopted broadly in AI and ML organizations, and that\\n“technology readiness levels” becomes common nomenclature across AI stakeholders – from researchers and engineers\\nto sales-people and executive decision-makers.\\nMethods\\nGated reviews\\nAt the end of each stage is a dedicated review period: (1) Present the technical developments along with the requirements\\nand their corresponding veriﬁcation measures and validation steps, (2) make key decisions on path(s) forward (or\\nbackward) and timing, and (3) debrief the processx. As in the gated reviews deﬁned by TRL used by NASA, DARPA, et\\nal., MLTRL stipulates speciﬁc criteria for review at each level, as well as calling out speciﬁc key decision points (noted\\nin the level descriptions above). The designated reviewers will “graduate” the technology to the next level, or provide a\\nlist of speciﬁc tasks that are still needed (ideally with quantitative remarks). After graduation at each level, the working\\ngroup does a brief post-mortem; we ﬁnd that a quick day or two pays dividends in cutting away technical debt and\\nimproving team processes. Regular gated reviews are essential for making efﬁcient progress while ensuring robustness\\nand functionality that meets stakeholder needs. There are several important mechanisms in MLTRL reviews that are\\nspeciﬁcally useful with AI and ML technologies: First, the review panels evolve over a project lifecycle, as noted\\nbelow. Second, MLTRL prescribes that each review runs through an AI ethics checklist deﬁned by the organization; it is\\nimportant to repeat this at each review, as the review panel and stakeholders evolve considerably over a project lifecycle.\\nAs previously described in the levels deﬁnitions, including ethics reviews as an integral part of early system development\\nis essential for informing model speciﬁcations and avoiding unintended biases or harm[74] after deployment.\\nTRL “Cards”\\nIn Figure 3 we succinctly showcase a key deliverable: TRL Cards . The model cards proposed by Google [ 75] are a useful\\ndevelopment for external user-readiness with ML. On the other hand, our TRL Cards aim to be more information-dense,\\nlike datasheets for medical devices and engineering tools – see the open-source TRL Card repo for examples and\\ntemplates (to be released at github.com/alan-turing-institute). These serve as “report cards” that grow and improve upon\\ngraduating levels, and provide a means of inter-team and cross-functional communication. The content of a TRL Card\\nis roughly in two categories: project info, and implicit knowledge. The former clearly states info such as project owners\\nxMLTRL should include regular debriefs and meta-evaluations such that process improvements can be made in a data-driven,\\nefﬁcient way (rather than an annual meta-review). MLTRL is a high-level framework that each organization should operationalize in\\na way that suits their speciﬁc capabilities and resources.\\n20'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='and reviewers, development status, and semantic versioning–not just for code, also for models and data. In the latter\\ncategory are speciﬁc insights that are typically siloed in the ML development team but should be communicated to\\nother stakeholders: modeling assumptions, dataset biases, corner cases, etc. With the spread of AI and ML in critical\\napplication areas, we are seeing domain expert consortiums deﬁning AI reporting guidelines – e.g., Rivera et al.[ 76]\\ncalling for clinical trials reports for interventions involving AI – which will greatly beneﬁt from the use of our TRL\\nreporting cards. We stress that these TRL Cards are key for the progression of projects, rather than documentation\\nafterthoughts. The TRL Cards thus promote transparency and trust, within teams and across organizations. TRL Card\\ntemplates will be open-sourced upon publication of this work, including methods for coordinating use with other\\nreporting tools such as “Datasheets for Datasets” [24].\\nRisk mitigation\\nIdentifying and addressing risks in a software project is not a new practice. However, akin to the MLTRL roots in\\nspacecraft engineering, risk is a “ﬁrst-class citizen” here. In the deﬁnition of technical and product requirements, each\\nentry has a calculation of the form risk =p(failure )×value , where the value of a component is an integer 1−10.\\nBeing diligent about quantifying risks across the technical requirements is a useful mechanism for ﬂagging ML-related\\nvulnerabilities that can sometimes be hidden by layers of other software. MLTRL also speciﬁes that risk quantiﬁcation\\nand testing strategies are required for sim-to-real development. That is, there is nearly always a non-trivial gap in\\ntransferring a model or algorithm from a simulation testbed to the real world. Requiring explicit sim-to-real testing\\nsteps in the workﬂow helps mitigate unforeseen (and often hazardous) failures. Additionally, comprehensive ML test\\ncoverage that we mention throughout this paper is a critical strategy for mitigating risks anduncertainties: ML-based\\nsystem behavior is not easily speciﬁed in advance, but rather depends on dynamic qualities of the data and on various\\nmodel conﬁguration choices[20].\\nNon-monotonic, non-linear paths\\nWe observe many projects beneﬁt from cyclic paths, dialing components of a technology back to a lower level. Our\\nframework not only encourages cycles, we make them explicit with “switchback mechanisms” to regress the maturity\\nof speciﬁc components in an AI system:\\n1.Discovery switchbacks occur as a natural mechanism – new technical gaps are discovered through systems\\nintegration, sparking later rounds of component development[ 77]. These are most common in the R&D levels,\\nfor example moving a component of a proof-of-concept technology (at Level 4) back to proof-of-principle\\ndevelopment (Level 2).\\n2.Review switchbacks result from gated reviews, where speciﬁc components or larger subsystems may be dialed\\nback to earlier levels. This switchback is one of the “key decision points” in the MLTRL project lifecycle\\n(as noted in the Levels deﬁnitions), and is often a decision driven by business-needs and timing rather than\\ntechnical concerns (for instance when mission priorities and funds shift). This mechanism is common from\\nLevel 6/7 to 4, which stresses the importance of this R&D to product transition phase (see Figure 2 (left)).\\n3.Embedded switchbacks are predeﬁned in the MLTRL process. For example, a predeﬁned path from 4 to 2, and\\nfrom 9 to 4. In complex systems, particularly with AI technologies, these built-in loops help mitigate technical\\ndebt and overcome other inefﬁciencies such as noncomprehensive V&V steps.\\nWithout these built-in mechanisms for cyclic development paths, it can be difﬁcult and inefﬁcient to build systems of\\nmodules and components at varying degrees of maturity. Contrary to traditional thought that switchback events should\\nbe suppressed and minimized, in fact they represent a natural and necessary part of the complex technology development\\nprocess – efforts to eliminate them may stiﬂe important innovations without necessarily improving efﬁciency. This is\\na fault of the standard monotonic approaches in AI/ML projects, stage-gate processes, and even the traditional TRL\\nframework.\\nIt is also important to note that most projects do not start at Level 0; very few ML companies engage in this low-level\\ntheoretical research. For example, a team looking to use an off-the-shelf object recognition model could start that\\ntechnology at Level 3, and proceed with thorough V&V for their speciﬁc datasets and use-cases. However, no technology\\ncan skip levels after the MLTRL process has been initiated. The industry default (that is, without implementing MLTRL)\\nis to ignorantly take pretrained models, run ﬁne tuning on their speciﬁc data, and jump to deployment, effectively\\nskipping Levels 5 to 7. Additionally, we ﬁnd it is advantageous to incorporate components from other high-TRL ranking\\nprojects while starting new projects; MLTRL makes the veriﬁcation and validation (V&V) steps straightforward for\\nintegrating previously developed ML components.\\n21'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='Evolving people, objectives, and measures\\nAs suggested earlier, much of the practical value of MLTRL comes at the transition between levels. More precisely,\\nMLTRL manages these oft neglected transitions explicitly as evolving teams, objectives, and deliverables. For instance,\\nthe team (or working group) at Level 3 is mostly AI Research Engineers, but at Level 6 is mixed Applied AI/SW\\nEngineers mixed with product managers and designers. Similarly, the review panels evolve from level to level, to match\\nthe changing technology development objectives. What the reviewers reference similarly evolves: notice in the level\\ndeﬁnitions that technical requirements and V&V guide early stages, but at and after Level 6 the product requirements\\nand V&V takeover – naturally, the risk quantiﬁcation and mitigation strategies evolve in parallel. Regarding the\\ndeliverables, notably TRL Cards and risk matrices[ 22] (to rank and prioritize various science, technical, and project\\nrisks), the information develops and evolves over time as the technology matures.\\nQuantiﬁable progress\\nBy deﬁning technology maturity in a quantitative way, MLTRL enables teams to accurately and consistently deﬁne\\ntheir ML progress metrics. Notably industry-standard “objectives and key results” (OKRs) and “key performance\\nindicators” (KPIs) [ 78] can be deﬁned as achieving certain readiness levels in a given period of time; this is a preferable\\nmetric in essentially all ML systems which consist of much more than a single performance score to measure progress.\\nEven more, meta-review of MLTRL progress over multiple projects can provide useful insights at the organization\\nlevel. For example, analysis of the time-per-level and the most frequent development paths/cycles can bring to light\\noperational bottlenecks. Compared to conventional software engineering metrics based on sprint stories and tickets, or\\ntime-tracking tools, MLTRL provides a more accurate analysis of ML workﬂows.\\nCommunication and explanation\\nA distinct advantage of MLTRL in practice is the nomenclature: an agreed upon grading scheme for the maturity of\\nan AI technology, and a framework for how/when that technology ﬁts within a product or system, enables everyone\\nto communicate effectively and transparently. MLTRL also acts as a gate for interpretability and explainability–at\\nthe granularity of individual models and algorithms, and more crucially from a holistic, systems standpoint. Notably\\nthe DARPA XAIxiprogram advocates for this advance in developing AI technologies; they suggest interpretability\\nand explainability are necessary at various locations in an AI system to be sufﬁcient for deployment as an AI product,\\notherwise leading to issues with ethics and bias.\\nRobustness via uncertainty-aware ML\\nHow to design a reliable system from unreliable components has been a guiding question in the ﬁelds of computing and\\nintelligence [79]. In the case of AI/ML systems, we aim to build reliable systems with myriad unreliable components:\\nnoisy and faulty sensors, human and AI error, and so on. There is thus signiﬁcant value to quantifying the myriad\\nuncertainties, propagating them throughout a system, and arriving at a notion or measure of reliability. For this reason,\\nalthough MLTRL applies generally to AI/ML methods and systems, we advocate for methods in the class of probabilistic\\nML, which naturally represent and manipulate uncertainty about models and predictions[ 28]. These are Bayesian\\nmethods that use probabilities to represent aleatoric uncertainty , measuring the noise inherent in the observations, and\\nepistemic uncertainty , accounting for uncertainty in the model itself (i.e., capturing our ignorance about which model\\ngenerated the data). In the simplest case, an uncertainty aware ML pipeline should quantify uncertainty at the points of\\nsensor inputs or perception, prediction or model output, and decision or end-user action – McAllister et al.[ 29] suggest\\nthis with Bayesian deep learning models for safer autonomous vehicle pipelines. We can achieve this sufﬁciently well\\nin practice for simple systems. However, we do not yet have a principled, theoretically grounded, and generalizable way\\nof propagating errors and uncertainties downstream and throughout more complex AI systems – i.e., how to integrate\\ndifferent software, hardware, data, and human components while considering how errors and uncertainties propagate\\nthrough the system. This is an important direction of our future work.\\nReferences\\n[1]Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning\\nthat matters. In AAAI , 2018.\\nxiDARPA Explainable Artiﬁcial Intelligence (XAI)\\n22'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='[2]Arnaud de la Tour, Massimo Portincaso, Kyle Blank, and Nicolas Goeldel. The dawn of the deep tech ecosystem. Technical\\nreport, The Boston Consulting Group, 2019.\\n[3] NASA. The NASA systems engineering handbook. 2003.\\n[4] United States Department of Defense. Defense acquisition guidebook. Technical report, U.S. Dept. of Defense, 2004.\\n[5] D. Leslie. Understanding artiﬁcial intelligence ethics and safety. ArXiv , abs/1906.05684, 2019.\\n[6]Google. Machine learning workﬂow. https://cloud.google.com/mlengine/docs/tensorflow/\\nml-solutions-overview . Accessed: 2020-12-13.\\n[7]Alexander Lavin and Gregory Renard. Technology readiness levels for AI & ML. ICML Workshop on Challenges Deploying\\nML Systems , 2020.\\n[8] T. Dasu and T. Johnson. Exploratory data mining and data cleaning. 2003.\\n[9]M. Janssen, P. Brous, Elsa Estevez, L. Barbosa, and T. Janowski. Data governance: Organizing data for trustworthy artiﬁcial\\nintelligence. Gov. Inf. Q. , 37:101493, 2020.\\n[10] B. Shahriari, Kevin Swersky, Ziyu Wang, R. Adams, and N. D. Freitas. Taking the human out of the loop: A review of bayesian\\noptimization. Proceedings of the IEEE , 104:148–175, 2016.\\n[11] Goutham Ramakrishnan, A. Nori, Hannah Murfet, and Pashmina Cameron. Towards compliant data management systems for\\nhealthcare ml. ArXiv , abs/2011.07555, 2020.\\n[12] Umang Bhatt, Alice Xiang, S. Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura,\\nand P. Eckersley. Explainable machine learning in deployment. Proceedings of the 2020 Conference on Fairness, Accountability,\\nand Transparency , 2020.\\n[13] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and V . Smith. Federated learning: Challenges, methods, and future directions.\\nIEEE Signal Processing Magazine , 37:50–60, 2020.\\n[14] T. Ryffel, Andrew Trask, M. Dahl, Bobby Wagner, J. Mancuso, D. Rueckert, and J. Passerat-Palmbach. A generic framework\\nfor privacy preserving deep learning. ArXiv , abs/1811.04017, 2018.\\n[15] A. Madry, Aleksandar Makelov, Ludwig Schmidt, D. Tsipras, and Adrian Vladu. Towards deep learning models resistant to\\nadversarial attacks. ArXiv , abs/1706.06083, 2018.\\n[16] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. ArXiv , abs/1710.11342, 2018.\\n[17] Marco Túlio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models\\nwith checklist. In ACL, 2020.\\n[18] Xiaoyuan Xie, Joshua W. K. Ho, C. Murphy, G. Kaiser, B. Xu, and T. Chen. Testing and validating machine learning classiﬁers\\nby metamorphic testing. The Journal of systems and software , 84 4:544–558, 2011.\\n[19] Alexander D’Amour, K. Heller, D. Moldovan, Ben Adlam, B. Alipanahi, Alex Beutel, C. Chen, Jonathan Deaton, Jacob\\nEisenstein, M. Hoffman, Farhad Hormozdiari, N. Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, M. Lucic,\\nY . Ma, Cory Y . McLean, Diana Mincu, Akinori Mitani, A. Montanari, Zachary Nado, V . Natarajan, C. Nielson, Thomas F.\\nOsborne, R. Raman, K. Ramasamy, Rory Sayres, J. Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, V . Veitch,\\nMax Vladymyrov, Xuezhi Wang, K. Webster, S. Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley. Underspeciﬁcation\\npresents challenges for credibility in modern machine learning. ArXiv , abs/2011.03395, 2020.\\n[20] Eric Breck, Shanqing Cai, E. Nielsen, M. Salib, and D. Sculley. The ml test score: A rubric for ml production readiness and\\ntechnical debt reduction. 2017 IEEE International Conference on Big Data (Big Data) , pages 1123–1132, 2017.\\n[21] A. Botchkarev. A new typology design of performance metrics to measure errors in machine learning regression algorithms.\\nInterdisciplinary Journal of Information, Knowledge, and Management , 14:045–076, 2019.\\n[22] N. Duijm. Recommendations on the use and design of risk matrices. Safety Science , 76:21–31, 2015.\\n[23] Louise Naud and Alexander Lavin. Manifolds for unsupervised visual anomaly detection. ArXiv , abs/2006.11364, 2020.\\n[24] Timnit Gebru, J. Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, H. Wallach, Hal Daumé, and K. Crawford.\\nDatasheets for datasets. ArXiv , abs/1803.09010, 2018.\\n[25] B. Hutchinson, A. Smart, A. Hanna, Emily L. Denton, Christina Greer, Oddur Kjartansson, P. Barnes, and Margaret Mitchell.\\nTowards accountability for machine learning datasets: Practices from software engineering and infrastructure. Proceedings of\\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency , 2021.\\n23'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='[26] P. Schulam and S. Saria. Reliable decision support using counterfactual models. In NIPS 2017 , 2017.\\n[27] Towards trustable machine learning. Nature Biomedical Engineering , 2:709–710, 2018.\\n[28] Zoubin Ghahramani. Probabilistic machine learning and artiﬁcial intelligence. Nature , 521:452–459, 2015.\\n[29] Rowan McAllister, Yarin Gal, Alex Kendall, Mark van der Wilk, A. Shah, R. Cipolla, and Adrian Weller. Concrete problems\\nfor autonomous vehicle safety: Advantages of bayesian deep learning. In IJCAI , 2017.\\n[30] Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I. Avilés-Rivero,\\nChristian Etmann, Cathal McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, Effrossyni Gkrania-Klotsas,\\nJames H. F. Rudd, Evis Sala, and Carola-Bibiane Schönlieb. Common pitfalls and recommendations for using machine learning\\nto detect and prognosticate for covid-19 using chest radiographs and ct scans. Nature Machine Intelligence , 3:199–217, 2021.\\n[31] J. Tobin, Rachel H Fong, Alex Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep\\nneural networks from simulation to the real world. 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems\\n(IROS) , pages 23–30, 2017.\\n[32] Arthur Juliani, Vincent-Pierre Berges, Esh Vckay, Yuan Gao, Hunter Henry, M. Mattar, and D. Lange. Unity: A general\\nplatform for intelligent agents. ArXiv , abs/1809.02627, 2018.\\n[33] Stefan Hinterstoißer, Olivier Pauly, Tim Hauke Heibel, Martina Marek, and Martin Bokeloh. An annotation saved is an\\nannotation earned: Using fully synthetic training for object instance detection. ArXiv , abs/1902.09967, 2019.\\n[34] Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan Hogins, You-Cyuan Jhang, Mohsen Kamalzadeh,\\nBowen Li, Steven Leal, Pete Parisi, Cesar Romero, Wesley Smith, Alex Thaman, Samuel Warren, and Nupur Yadav. Unity\\nperception: Generate synthetic data for computer vision. CoRR , abs/2107.04259, 2021.\\n[35] K. Cranmer, J. Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of the National Academy\\nof Sciences , 117:30055 – 30062, 2020.\\n[36] Jan-Willem van de Meent, Brooks Paige, H. Yang, and Frank Wood. An introduction to probabilistic programming. ArXiv ,\\nabs/1809.10756, 2018.\\n[37] Atilim Günes Baydin, Lei Shao, W. Bhimji, L. Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi,\\nBradley Gram-Hansen, Gilles Louppe, Mingfei Ma, X. Zhao, P. Torr, V . Lee, K. Cranmer, Prabhat, and F. Wood. Etalumis:\\nbringing probabilistic programming to scientiﬁc simulators at scale. Proceedings of the International Conference for High\\nPerformance Computing, Networking, Storage and Analysis , 2019.\\n[38] T. Gleisberg, S. Höche, F. Krauss, M. Schönherr, S. Schumann, F. Siegert, and J. Winter. Event generation with sherpa 1.1.\\nJournal of High Energy Physics , 2009:007–007, 2009.\\n[39] David M. Blei. Build, compute, critique, repeat: Data analysis with latent variable models. 2014.\\n[40] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald C. Gall, Ece Kamar, Nachiappan Nagappan, Besmira\\nNushi, and Thomas Zimmermann. Software engineering for machine learning: A case study. 2019 IEEE/ACM 41st International\\nConference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) , 2019.\\n[41] R. Ambrosino, B. Buchanan, G. Cooper, and Marvin J. Fine. The use of misclassiﬁcation costs to learn rule-based decision\\nsupport models for cost-effective hospital admission strategies. Proceedings. Symposium on Computer Applications in Medical\\nCare , pages 304–8, 1995.\\n[42] Gareth J Grifﬁth, Tim T Morris, Matthew J Tudball, Annie Herbert, Giulia Mancano, Lindsey Pike, Gemma C Sharp, Jonathan\\nSterne, Tom M Palmer, George Davey Smith, et al. Collider bias undermines our understanding of covid-19 disease risk and\\nseverity. Nature communications , 11(1):1–12, 2020.\\n[43] J. Pearl. Theoretical impediments to machine learning with seven sparks from the causal revolution. Proceedings of the\\nEleventh ACM International Conference on Web Search and Data Mining , 2018.\\n[44] T. Nguyen, G. Collins, J. Spence, J. Daurès, P. Devereaux, P. Landais, and Y . Le Manach. Double-adjustment in propensity\\nscore matching analysis: choosing a threshold for considering residual imbalance. BMC Medical Research Methodology , 17,\\n2017.\\n[45] D. Eckles and E. Bakshy. Bias and high-dimensional adjustment in observational studies of peer effects. ArXiv , abs/1706.04692,\\n2017.\\n[46] Yanbo Xu, Divyat Mahajan, Liz Manrao, A. Sharma, and E. Kiciman. Split-treatment analysis to rank heterogeneous causal\\neffects for prospective interventions. ArXiv , abs/2011.05877, 2020.\\n24'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='[47] Jonathan G Richens, C. M. Lee, and Saurabh Johri. Improving the accuracy of medical diagnosis with causal machine learning.\\nNature Communications , 11, 2020.\\n[48] Andrei Paleyes, Raoul-Gabriel Urma, and N. Lawrence. Challenges in deploying machine learning: a survey of case studies.\\nArXiv , abs/2011.09926, 2020.\\n[49] V . Chernozhukov, D. Chetverikov, M. Demirer, E. Duﬂo, Christian L. Hansen, Whitney K. Newey, and J. Robins. Dou-\\nble/debiased machine learning for treatment and structural parameters. Econometrics: Econometric & Statistical Methods -\\nSpecial Topics eJournal , 2018.\\n[50] Victor Veitch and Anisha Zaveri. Sense and sensitivity analysis: Simple post-hoc analysis of bias due to unobserved confounding.\\nNeurIPS 2020, arXiv preprint arXiv:2003.01747 , 2020.\\n[51] P. Jenniskens, P.S. Gural, L. Dynneson, B.J. Grigsby, K.E. Newman, M. Borden, M. Koop, and D. Holman. Cams: Cameras for\\nallsky meteor surveillance to establish minor meteor showers. Icarus , 216(1):40 – 61, 2011.\\n[52] Siddha Ganju, Anirudh Koul, Alexander Lavin, J. Veitch-Michaelis, Meher Kasam, and J. Parr. Learnings from frontier\\ndevelopment lab and spaceml - ai accelerators for nasa and esa. ArXiv , abs/2011.04776, 2020.\\n[53] S. Zoghbi, M. Cicco, A. P. Stapper, A. J. Ordonez, J. Collison, P. S. Gural, S. Ganju, J.-L. Galache, and P. Jenniskens. Searching\\nfor long-period comets with deep learning tools. In Deep Learning for Physical Science Workshop, NeurIPS , 2017.\\n[54] Peter Jenniskens, Jack Baggaley, Ian Crumpton, Peter Aldous, Petr Pokorny, Diego Janches, Peter S. Gural, Dave Samuels, Jim\\nAlbers, Andreas Howell, Carl Johannink, Martin Breukers, Mohammad Odeh, Nicholas Moskovitz, Jack Collison, and Siddha\\nGanju. A survey of southern hemisphere meteor showers. Planetary and Space Science , 154:21 – 29, 2018.\\n[55] D. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active learning with statistical models. In NIPS , 1994.\\n[56] Y . Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. ArXiv , abs/1703.02910, 2017.\\n[57] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young,\\nJean-François Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. In NIPS , 2015.\\n[58] P. Abrahamsson, Outi Salo, Jussi Ronkainen, and Juhani Warsta. Agile software development methods: Review and analysis.\\nArXiv , abs/1709.08439, 2017.\\n[59] Marco Kuhrmann, Philipp Diebold, Jürgen Münch, Paolo Tell, Vahid Garousi, Michael Felderer, Kitija Trektere, Fergal\\nMcCaffery, Oliver Linssen, Eckhart Hanser, and Christian R. Prause. Hybrid software and system development in practice:\\nwaterfall, scrum, and beyond. Proceedings of the 2017 International Conference on Software and System Process , 2017.\\n[60] Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry,\\nPaul-Christian Burkner, and Martin Modrak. Bayesian workﬂow. ArXiv , abs/2011.01808, 2020.\\n[61] P. Chapman, J. Clinton, R. Kerber, T. Khabaza, T. Reinartz, C. Shearer, and R. Wirth. Crisp-dm 1.0: Step-by-step data mining\\nguide. 2000.\\n[62] Fred Hohman, Kanit Wongsuphasawat, Mary Beth Kery, and Kayur Patel. Understanding and visualizing data iteration in\\nmachine learning. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems , 2020.\\n[63] Saleema Amershi, M. Cakmak, W. B. Knox, and T. Kulesza. Power to the people: The role of humans in interactive machine\\nlearning. AI Mag. , 35:105–120, 2014.\\n[64] Eric Breck, Marty Zinkevich, Neoklis Polyzotis, Steven Euijong Whang, and Sudip Roy. Data validation for machine learning.\\n2019.\\n[65] R. Kumar, David R. O’Brien, Kendra Albert, Salomé Viljöen, and Jeffrey Snover. Failure modes in machine learning systems.\\nArXiv , abs/1911.11034, 2019.\\n[66] Inioluwa Deborah Raji, Andrew Smart, Rebecca White, M. Mitchell, Timnit Gebru, B. Hutchinson, Jamila Smith-Loud, Daniel\\nTheron, and P. Barnes. Closing the ai accountability gap: deﬁning an end-to-end framework for internal algorithmic auditing.\\nProceedings of the 2020 Conference on Fairness, Accountability, and Transparency , 2020.\\n[67] R. Miksad and A. Abernethy. Harnessing the power of real-world evidence (rwe): A checklist to ensure regulatory-grade data\\nquality. Clinical Pharmacology and Therapeutics , 103:202 – 205, 2018.\\n[68] D. B. Larson, Hugh Harvey, D. Rubin, Neville Irani, J. R. Tse, and C. Langlotz. Regulatory frameworks for development and\\nevaluation of artiﬁcial intelligence–based diagnostic imaging algorithms: Summary and recommendations. Journal of the\\nAmerican College of Radiology , 2020.\\n25'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='[69] Ninareh Mehrabi, Fred Morstatter, N. Saxena, Kristina Lerman, and A. Galstyan. A survey on bias and fairness in machine\\nlearning. ACM Computing Surveys (CSUR) , 54:1 – 35, 2019.\\n[70] Eirini Ntoutsi, P. Fafalios, U. Gadiraju, Vasileios Iosiﬁdis, W. Nejdl, Maria-Esther Vidal, S. Ruggieri, F. Turini, S. Papadopoulos,\\nEmmanouil Krasanakis, I. Kompatsiaris, K. Kinder-Kurlanda, Claudia Wagner, F. Karimi, Miriam Fernández, Harith Alani,\\nB. Berendt, Tina Kruegel, C. Heinze, Klaus Broelemann, Gjergji Kasneci, T. Tiropanis, and Steffen Staab. Bias in data-driven\\nai systems - an introductory survey. ArXiv , abs/2001.09762, 2020.\\n[71] E. Jo and Timnit Gebru. Lessons from archives: strategies for collecting sociocultural data in machine learning. Proceedings of\\nthe 2020 Conference on Fairness, Accountability, and Transparency , 2020.\\n[72] J. Wiens, W. Price, and M. Sjoding. Diagnosing bias in data-driven algorithms for healthcare. Nature Medicine , 26:25–26,\\n2020.\\n[73] R. Challen, J. Denny, M. Pitt, L. Gompels, T. Edwards, and K. Tsaneva-Atanasova. Artiﬁcial intelligence, bias and clinical\\nsafety. BMJ Quality & Safety , 28:231 – 237, 2019.\\n[74] Z. Obermeyer, B. Powers, C. V ogeli, and S. Mullainathan. Dissecting racial bias in an algorithm used to manage the health of\\npopulations. Science , 366:447 – 453, 2019.\\n[75] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, In-\\nioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. Proceedings of the Conference on Fairness,\\nAccountability, and Transparency , 2019.\\n[76] Samantha Cruz Rivera, Xiaoxuan Liu, A. Chan, A. K. Denniston, and M. Calvert. Guidelines for clinical trial protocols for\\ninterventions involving artiﬁcial intelligence: the spirit-ai extension. Nature Medicine , 26:1351 – 1363, 2020.\\n[77] Z. Szajnfarber. Managing innovation in architecturally hierarchical systems: Three switchback mechanisms that impact practice.\\nIEEE Transactions on Engineering Management , 61:633–645, 2014.\\n[78] H. Zhou and Y . He. Comparative study of okr and kpi. DEStech Transactions on Economics, Business and Management , 2018.\\n[79] J. Neumann. Probabilistic logic and the synthesis of reliable organisms from unreliable components. 1956.\\nAcknowledgements\\nThe authors would like to thank Gur Kimchi, Carl Henrik Ek and Neil Lawrence for valuable discussions about this\\nproject.\\nAuthor contributions statement\\nA.L. conceived of the original ideas and framework, with signiﬁcant contributions towards improving the framework\\nfrom all co-authors. A.L. initiated the use of MLTRL in practice, including the neuropathology test case discussed here.\\nC.G-L. contributed insight regarding causal AI, including the section on counterfactual diagnosis. C.G-L. also made\\nsigniﬁcant contributions broadly in the paper, notably in the Methods descriptions and paper revisions. Si.G. contributed\\nthe spacecraft test case, along with early insights in the framework deﬁnitions. A.V . contributed to the deﬁnition of\\nlater stages involving deployment (as did A.G.), and comparison with traditional software workﬂows. Both E.X. and\\nY .G. provided insights regarding AI in academia, and Y .G. additionally contributed to the uncertainty quantiﬁcation\\nmethods. Su.G. and D.L. contributed the computer vision test case. A.G.B. contributed the particle physics test case,\\nand signiﬁcant reviews of the writeup. A.S. contributed insights related to causal ML and AI ethics. D.N. provided\\nvaluable feedback on the overall framework, and contributed signiﬁcantly with the details on “switchback mechanisms”.\\nS.Z. contributed to multiple paper revisions, with emphasis on clarity and applicability to broad ML users and teams.\\nJ.P. contributed to multiple paper revisions, and to deploying the systems ML methods broadly in practice for Earth and\\nspace sciences. –same goes for C.M., with additional feedback overall on the methods. All co-authors discussed the\\ncontent and contributed to editing the manuscript.\\nCompeting interests\\nThe authors declare no competing interests.\\nAdditional information\\nCorrespondence and requests for materials should be addressed to A.L.\\n26')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"random.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text'], template='\\nWrite a concise and short summary of the following:\\n{text}\\n')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Write a concise and short summary of the following:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"text\"], template= template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Write a concise and short summary of the following:\n",
      "that corresponding changes to model or application development risk deployment delays or failures. This marks a key\n",
      "decision for the project lifecycle, as this expensive ML deployment risk is common without MLTRL (see Figure 2).\n",
      "Level 6 data – Additional data should be collected and operationalized at this stage towards robustifying the ML\n",
      "models, algorithms, and surrounding components. These include adversarial examples to check local robustness [ 15],\n",
      "semantically-equivalent perturbations to check consistency of the model with respect to domain assumptions [16, 17],\n",
      "and collecting data from different sources and checking how well the trained model generalizes to them. These\n",
      "considerations are even more vital in the challenging deployment domains mentioned above with limited data access.\n",
      "Level 6 review – Focus is on the code quality, the set of newly deﬁned product requirements, system SLA and SLO\n",
      "requirements, data pipelines spec, and an AI ethics revisit now that we are closer to a real-world use-case. In particular,\n",
      "regulatory compliance is mandated for this gated review; the data privacy and security laws are changing rapidly, and\n",
      "missteps with compliance can make or break the project.\n",
      "Level 7 - Integrations For integrating the technology into existing production systems, we recommend the working\n",
      "group has a balance of infrastructure engineers andapplied AI engineers – this stage of development is vulnerable\n",
      "to latent model assumptions and failure modes, and as such cannot be safely developed solely by software engineers.\n",
      "Important tools for them to build together include:\n",
      "•Tests that run use-case speciﬁc critical scenarios and data-slices – a proper risk-quantiﬁcation table will\n",
      "highlight these.\n",
      "•A “golden dataset” should be deﬁned to baseline the performance of each model and succession of models –see\n",
      "the computer vision app example in Figure 4–for use in the continuous integration and deployment (CI/CD)\n",
      "tests.\n",
      "•Metamorphic testing : a software engineering methodology for testing a speciﬁc set of relations between the\n",
      "outputs of multiple inputs. When integrating ML modules into larger systems, a codiﬁed list of metamorphic\n",
      "relations[18] can provide valuable veriﬁcation and validation measures and steps.\n",
      "•Data intervention tests that seek data bugs at various points in the pipelines, downstream to measure the\n",
      "potential effects of data processing and ML on consumers or users of that data, as well as upstream at data\n",
      "ingestion or creation. Rather than using model performance as a proxy for data quality, it is crucial to use\n",
      "intervention tests that instead catch data errors with mechanisms speciﬁc to data validation.\n",
      "These tests in particular help mitigate underspeciﬁcation in ML pipelines, a key obstacle to reliably training models that\n",
      "behave as expected in deployment[ 19]. On the note of reliability, it is important that quality assurance engineers (QA)\n",
      "play a key role here and through Level 9, overseeing data processes to ensure privacy and security, and covering audits\n",
      "for downstream accountability of AI methods.\n",
      "Level 7 data – In addition to the data for test suites discussed above, this level calls for QA to prioritize data governance :\n",
      "how data is obtained, managed, used, and secured by the organization. This was earlier suggested in level 5 (in order to\n",
      "preempt related technical debt), and essential here at the main junction for integration, which may create additional\n",
      "governance challenges in light of downstream effects and consumers.\n",
      "Level 7 review – The review should focus on the data pipelines and test suites; a scorecard like the ML Testing\n",
      "Rubric[ 20] is useful. The group should also emphasize ethical considerations at this stage, as they may be more\n",
      "adequately addressed now (where there are many test suites put into place) rather than close to shipping later.\n",
      "Level 8 - Flight-ready The technology is demonstrated to work in its ﬁnal form and under expected conditions.\n",
      "There should be additional tests implemented at this stage covering deployment aspects, notably A/B tests, blue/green\n",
      "deployment tests, shadow testing, and canary testing, which enable proactive and gradual testing for changing ML\n",
      "methods and data. Ahead of deployment, the CI/CD system should be ready to regularly stress test the overall system\n",
      "and ML components. In practice, problems stemming from real-world data are impossible to anticipate and design for –\n",
      "an upstream data provider could change formats unexpectedly or a physical event could cause the customer behavior to\n",
      "change. Running models in shadow mode for a period of time would help stress test the infrastructure and evaluate how\n",
      "susceptible the ML model(s) will be to performance regressions caused by data. We observe that ML systems with\n",
      "data-oriented architectures are more readily tested in this manner, and better surface data quality issues, data drifts, and\n",
      "concept drifts – this is discussed later in the Beyond Software Engineering section. To close this stage, the key decision\n",
      "is go or no-go for deployment, and when.\n",
      "Level 8 data – If not already in place, there absolutely needs to be mechanisms for automatically logging data\n",
      "distributions alongside model performance once deployed.\n",
      "6\n",
      "\n",
      "Level 8 review – A diligent walkthrough of every technical and product requirement, showing the corresponding\n",
      "validations, and the review panel is representative of the full slate of stakeholders.\n",
      "Level 9 - Deployment In deploying AI and ML technologies, there is signiﬁcant need to monitor the current version,\n",
      "and explicit considerations towards improving the next version. For instance, performance degradation can be hidden\n",
      "and critical, and feature improvements often bring unintended consequences and constraints. Thus at this level, the\n",
      "focus is on maintenance engineering–i.e., methods and pipelines for ML monitoring and updating. Monitoring for data\n",
      "quality, concept drift, and data drift is crucial; no AI system without thorough tests for these can reliably be deployed.\n",
      "By the same token there must be automated evaluation and reporting – if actuals[ 21] are available, continuous evaluation\n",
      "should be enabled, but in many cases actuals come with a delay, so it is essential to record model outputs to allow for\n",
      "efﬁcient evaluation after the fact. To these ends, the ML pipeline should be instrumented to log system metadata, model\n",
      "metadata, and data itself.\n",
      "Monitoring for data quality issues and data drifts is crucial to catch deviations in model behavior, particularly those that\n",
      "are non-obvious in the model or product end-performance. Data logging is unique in the context of ML systems: data\n",
      "logs should capture statistical properties of input features and model predictions, and capture their anomalies. With\n",
      "monitoring for data, concept, and model drifts, the logs are to be sent to the relevant systems, applied, and research\n",
      "engineers. The latter is often non-trivial, as the model server is not ideal for model “observability” because it does not\n",
      "necessarily have the right data points to link the complex layers needed to analyze and debug models. To this end,\n",
      "MLTRL requires the drift tests to be implemented at stages well ahead of deployment, earlier than is standard practice.\n",
      "Again we advocate for data-ﬁrst architectures rather than the software industry-standard design by services (discussed\n",
      "later), which aids in surfacing and logging the relevant data types and slices when monitoring AI systems. For retraining\n",
      "and improving models, monitoring must be enabled to catch training-serving skew and let the team know when to\n",
      "retrain. Towards model improvements, adding or modifying features can often have unintended consequences, such as\n",
      "introducing latencies or even bias. To mitigate these risks, MLTRL has an embedded switchback here: any component\n",
      "or module changes to the deployed version must cycle back to Level 7 (integrations stage) or earlier. Additionally,\n",
      "for quality ML products, we stress a deﬁned communication path for user feedback without roadblocks to R&D; we\n",
      "encourage real-world feedback all the way to research, providing valuable problem constraints and perspectives.\n",
      "Level 9 data – Proper mechanisms for logging and inspecting data (alongside models) is critical for deploying reliable\n",
      "AI and ML – systems that learn on data have unique monitoring requirements (detailed above). In addition to the\n",
      "infrastructure and test suites covering data and environment shifts, it’s important for product managers and other owners\n",
      "to be on top of data policy shifts in domains such as ﬁnance and healthcare.\n",
      "Level 9 review – The review at this stage is unique, as it also helps in lifecycle management: at a regular cadence\n",
      "that depends on the deployed system and domain of use, owners and other stakeholders are to revisit this review and\n",
      "recommend switchbacks if needed (discussed in the Methods section). This additional oversight at deployment is\n",
      "shown to help deﬁne regimented release cycles of updated versions, and provide another “eye” check for stale model\n",
      "performance or other system abnormalities.\n",
      "Notice MLTRL is deﬁned as stages or levels, yet much of the value in practice is realized in the transitions: MLTRL\n",
      "enables teams to move from one level to the next reliably and efﬁciently, and provides a guide for how teams and\n",
      "objectives evolve with the progressing technology.\n",
      "Discussion\n",
      "MLTRL is designed to apply to many real-world use-cases involving data and ML, from simple regression models\n",
      "used for predictive modeling energy demand or anomaly detection in datacenters, to real-time modeling in rideshare\n",
      "applications and motion planning in warehouse robotics. For simple use-cases MLTRL may be overkill, and a subset\n",
      "may sufﬁce – for instance, model cards as demonstrated by Google for basic image classiﬁcation. Yet this is a ﬁne line,\n",
      "as the same cards-only approach in the popular “Huggingface” codebases are too simplistic for the language models\n",
      "they represent, deployed in domains that carry signiﬁcant consequences. MLTRL becomes more valuable with more\n",
      "complex, larger systems and environments, especially in risk averse domains. We thoroughly discuss this through\n",
      "several real uses of MLTRL below.\n",
      "7\n",
      "\n",
      "Figure 2: Most ML and AI projects live in these sections of MLTRL, not concerned with fundamental R&D – that is,\n",
      "completely using existing methods and implementations, and even pretrained models. In the left diagram, the arrows\n",
      "show a common development pattern with MLTRL in industry: projects go back to the ML toolbox to develop new\n",
      "features (dashed line), and frequent, incremental improvements are often a practice of jumping back a couple levels to\n",
      "Level 7 (which is the main systems integrations stage). At Levels 7 and 8 we stress the need for tests that run use-case\n",
      "speciﬁc critical scenarios and data-slices, which are highlighted by a proper risk-quantiﬁcation matrix [ 22]. Cycling\n",
      "back to previous lower levels is not just a late-stage mechanism in MLTRL, but rather “switchbacks” occur throughout\n",
      "the process (as discussed in the Methods section and throughout the text). In the right diagram we show the more\n",
      "common approach in industry ( without using our framework), which skips essential technology transition stages – ML\n",
      "Engineers push straight through to deployment, ignoring important productization and systems integration factors. This\n",
      "will be discussed in more detail in the Methods section.\n",
      "EXAMPLES\n",
      "Human-machine visual inspection\n",
      "While most ML projects begin with a speciﬁc task and/or dataset, there are many that originate in ML theory without\n",
      "any target application – i.e., projects starting MLTRL at level 0 or 1. These projects nicely demonstrate the utility of\n",
      "MLTRL built-in switchbacks, bifurcating paths, and iteration with domain experts. An example we discuss here is a\n",
      "novel approach to representing data in generative vision models from Naud & Lavin[ 23], which was then developed into\n",
      "state-of-the-art unsupervised anomaly detection, and targeted for two human-machine visual inspection applications:\n",
      "First, industrial anomaly detection, notably in precision manufacturing, to identify potential errors for human-expert\n",
      "manual inspection. Second, using the model to improve the accuracy and efﬁciency of neuropathology, the microscopic\n",
      "examination of neurosurgical specimens for cancerous tissue. In these human-machine teaming use-cases there are\n",
      "speciﬁc challenges impeding practical, reliable use:\n",
      "•Hidden feedback loops can be common and problematic in real-world systems inﬂuencing their own training\n",
      "data: over time the behavior of users may evolve to select data inputs they prefer for the speciﬁc AI system,\n",
      "representing some skew from the training data. In this neuropathology case, selecting whole-slide images that\n",
      "are uniquely difﬁcult for manual inspection, or even biased by that individual user. Similarly we see underlying\n",
      "healthcare processes can act as hidden confounders, resulting in unreliable decision support tools[26].\n",
      "•Model availability can be limited in many deployment settings: for example, on-premises deployments\n",
      "(common in privacy preserving domains like healthcare and banking), edge deployments (common in industrial\n",
      "use-cases such as manufacturing and agriculture), or from the infrastructure’s inability to scale to the volume\n",
      "of requests. This can severely limit the team’s ability to monitor, debug, and improve deployed models.\n",
      "•Uncertainty estimation is valuable in many AI scenarios, yet not straightforward to implement in practice.\n",
      "This is further complicated with multiple data sources and users, each injecting generally unknown amounts of\n",
      "noise and uncertainties. In medical applications it is of critical importance, to provide measures of conﬁdence\n",
      "and sensitivity, and for AI researchers through end-users. In anomaly detection, various uncertainty measures\n",
      "can help calibrate the false-positive versus false-negative rates, which can be very domain speciﬁc.\n",
      "8\n",
      "\n",
      "Figure 3: The maturity of each ML technology is tracked via TRL Cards , which we describe in the Methods section.\n",
      "Here is an example reﬂecting a neuropathology machine vision use-case[ 23], detailed in the Discussion Section. Note\n",
      "this is a subset of a full TRL Card, which in reality lives as a full document in an internal wiki. Notice the card\n",
      "clearly communicates the data sources, versions, and assumptions. This helps mitigate invalid assumptions about\n",
      "performance and generalizability when moving from R&D to production, and promotes the use of real-world data\n",
      "earlier in the project lifecycle. We recommend documenting datasets thoroughly with semantic versioning and tools\n",
      "such as datasheets for datasets [24], and following data accountability best-practices as they evolve (see [25]).\n",
      "•Costs of edge cases can be signiﬁcant, sometimes risking expensive machine downtime or medical failures.\n",
      "This is exacerbated in anomaly detection anomalies are by deﬁnition rare so they can be difﬁcult to train for,\n",
      "especially for the anomalies that are completely unseen until they arise in the wild.\n",
      "•End-user trust can be difﬁcult to achieve, often preventing the adoption of ML applications, particularly in\n",
      "the healthcare domain and other highly regulated industries.\n",
      "These and additional ML challenges such as data privacy and interpretability can inhibit ML adoption in clinical practice\n",
      "and industrial settings, but can be mitigated with MLTRL processes. We’ll describe how in the context of the Naud\n",
      "& Lavin[ 23] example, which began at level 0 with theoretical ML work on manifold geometries, and at level 5 was\n",
      "directed towards specialized human-machine teaming applications utilizing the same ML method under-the-hood.\n",
      "•Levels 0-1 – From open-ended exploration of data-representation properties in various Riemmanian manifold\n",
      "curvatures, we derived from ﬁrst principles and empirically identiﬁed a property with hyperbolic manifolds:\n",
      "when used as a latent space for embedding data without labels, the geometry organizes the data by it’s implicit\n",
      "hierarchical structure. Unsupervised computer vision was identiﬁed in reviews as a promising direction for\n",
      "proof-of-principle work.\n",
      "•Level 2 – One approach for validating the earlier theoretical developments was to generate synthetic data to\n",
      "isolate very speciﬁc features in data we would expect represented in the latent manifold. The results showed\n",
      "promise for anomaly detection – using the latent representation of data to automatically identify images that\n",
      "are out-of-the-ordinary (anomalous), and also using the manifold to inspect how they are semantically different.\n",
      "Further, starting with an implicitly probabilistic modeling approach implied uncertainty estimation could be\n",
      "a valuable feature downstream. This made the level 2 key decision point clear: proceed with applied ML\n",
      "development.\n",
      "•Levels 3-5 – Proof-of-concept development and reviews demonstrated promise for several commercial appli-\n",
      "cations relevant to the business, and also highlighted the need for several key features (deﬁned as R&D and\n",
      "product requirements): interpretability (towards end-user trust), uncertainty quantiﬁcation (to show conﬁdence\n",
      "scores), and human-in-the-loop (for domain expertise). Without the MLTRL PoC steps and review processes,\n",
      "these features can often be delayed until beta testing or overlooked completely – for example, the failures of\n",
      "applying IBM Watson in medical applications [ 27]. For this technology, the applications to develop towards\n",
      "are anomaly detection in histopathology and manufacturing, speciﬁcally inspecting whole-slide images of\n",
      "neural tissue, and detecting defects in metallic surfaces, respectively.\n",
      "9\n",
      "\n",
      "From the systems perspective, we suggest quantifying the uncertainties of components and propagating them\n",
      "through the system, which can improve safety and trust. Probabilistic ML methods, rooted in Bayesian\n",
      "probability theory, provide a principled approach to representing and manipulating uncertainty about models\n",
      "and predictions[ 28]. For this reason we advocate strongly for probabilistic models and algorithms in AI\n",
      "systems. In this machine vision example, the MLTRL technical requirements speciﬁcally called for a\n",
      "probabilistic generative model to readily quantify various types of uncertainties and propagate them forward to\n",
      "the visualization component of the pipeline, and the product requirements called for the downstream conﬁdence\n",
      "and sensitivity measures to be exposed to the end-user. Component uncertainties must be assembled in a\n",
      "principled way to yield a meaningful measure of overall system uncertainty, based on which safe decisions can\n",
      "be made[29]. See the Methods section for more on uncertainty in AI systems.\n",
      "The early checks for data management and governance proved valuable here, as the application areas dealt\n",
      "with highly sensitive data that would signiﬁcantly inﬂuence the design of data pipelines and test suites. In\n",
      "both the neuropathology and manufacturing applications, the data management checks also raised concerns\n",
      "about hidden feedback loops, where users may unintentionally skew the data inputs when using the anomaly\n",
      "detection models in practice, for instance biasing the data towards speciﬁc subsets they subjectively need help\n",
      "with. Incorporating domain experts this early in the project lifecycle helped inform veriﬁcation and validation\n",
      "steps to help be robust to the hidden feedback loops. Not to mention their input guided us towards user-centric\n",
      "metrics for performance, which can often skew from ML metrics in important ways – for instance, the typical\n",
      "acceptance ratio for false positives versus false negatives doesn’t apply to select edge cases, for which our\n",
      "hierarchical anomaly classiﬁcation scheme was useful [23].\n",
      "From prior reviews and TRL card documentation, we also identiﬁed the value of synthetic data generation\n",
      "into application development: anomalies are by deﬁnition rare so they are hard to come by in real datasets,\n",
      "especially with evolving environments in deployment settings, so the ability to generate synthetic datasets for\n",
      "anomaly detection can accelerate the level 6-9 pipeline, and help ensure more reliable models in the wild.\n",
      "•Level 6 (medical) – The medical inspection application experienced a bifurcation with product work proceed-\n",
      "ing while additional R&D was desired to explore improved data processing methods, while engaging with\n",
      "clinicians and medical researchers for feedback. Proceeding through the levels in a non-linear, non-monotonic\n",
      "way is common in MLTRL and encouraged by various switchback mechanisms (detailed in the Methods\n",
      "section). These practices – intentional switchbacks, frequent engagement with domain experts and users – can\n",
      "help mitigate methodological ﬂaws and underlying biases that are common when applying ML to clinical\n",
      "applications. For instance, recent work by Roberts et al. [ 30] investigated 2,122 studies applying ML to\n",
      "COVID-19 use-cases, ﬁnding that none of the models are sufﬁcient for clinical use due to methodological ﬂaws\n",
      "and/or underlying biases. They go on to give many recommendations – some we’ve discussed in the context of\n",
      "MLTRL, and more – which should be reviewed for higher quality medical-ML models and documentation.\n",
      "•Level 6-9 (manufacturing) – Overall these stages proceeded regularly and efﬁciently for the defect detection\n",
      "product. MLTRL’s embedded switchback from level 9 to 4 proved particularly useful in this lifecycle, both\n",
      "for incorporating feedback from the ﬁeld and for updating with research progress. On the former, the data\n",
      "distribution shifts from one deployment setting to another signiﬁcantly affected false-positive versus false-\n",
      "negative calibrations, so this was added as a feature to the CI/CD pipelines. On the latter, the built-in touch\n",
      "points for real-world feedback and data into the continued ML research provided valuable constraints to\n",
      "help guide research, and product managers could readily understand what capabilities could be available for\n",
      "product integration and when (readily communicated with TRL Cards) – for instance, later adding support for\n",
      "video-based inspection for defects, and tooling for end-users to reason about uncertainty estimates (which\n",
      "helps establish trust).\n",
      "•Level 7-9 (medical) – For productization the “neuropathology copilot” was handed off to a partner pharmaceu-\n",
      "tical company to integrate into their existing software systems. The MLTRL documentation and communication\n",
      "streamlined the technology transfer, which can often by a time-consuming manual process. If not pursuing\n",
      "this path, the product would’ve likely faced many of the medical-ML deployment challenges with model\n",
      "availability and data access; MLTRL cannot overcome the technical challenges of deploying on-premises, but\n",
      "the manifestation of those challenges as performance regressions, data shifts, privacy and ethics concerns, etc.\n",
      "can be mitigated by the system-level checks and strategies MLTRL puts forth.\n",
      "Computer vision with real and synthetic data\n",
      "Advancements in physics engines and graphics processing have advanced AI environment and data-generation capabili-\n",
      "ties, putting increased emphasis on transitioning models across the simulation-to-reality gap [ 31,32,33]. To develop a\n",
      "computer vision application for automated recycling, we leveraged the Unity Perception [ 34] package, a toolkit for\n",
      "generating large-scale datasets for perception-based ML training and validation. We produced synthetic images to\n",
      "10\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type='stuff', prompt=prompt, verbose=True)\n",
    "summary = chain.run(docs[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**MLTRL: A Framework for Managing Machine Learning Model Development and Deployment**\\n\\nMLTRL (Machine Learning Testing and Review Loop) is a framework designed to guide the development and deployment of machine learning (ML) models. It aims to address the common challenges faced in ML projects, such as data quality issues, model bias, and deployment risks.\\n\\n**Key Features of MLTRL:**\\n\\n- **Iterative and risk-driven:** MLTRL involves multiple stages (Levels 0-9) that enable teams to identify potential risks and mitigate them through iterative development and review processes.\\n- **Data-centric approach:** Emphasis on data quality and governance from the early stages of development.\\n- **Transparency and communication:** Use of TRL Cards to document model maturity and assumptions, facilitating communication and collaboration.\\n- **Flexibility and adaptability:** Allows for non-linear progress and switchbacks between stages as needed.\\n\\n**Applications of MLTRL:**\\n\\n- Human-machine visual inspection\\n- Medical image analysis\\n- Anomaly detection\\n- Predictive modeling\\n\\n**Benefits of Using MLTRL:**\\n\\n- Improved model quality and reliability.\\n- Reduced deployment risks and costs.\\n- Enhanced communication and collaboration between stakeholders.\\n- Faster time to market for ML-powered products.\\n\\n**Challenges in Applying MLTRL:**\\n\\n- Requires organizational change and cultural shift.\\n- Can be resource-intensive.\\n- May not be suitable for all ML projects.\\n\\n**Examples of MLTRL in Action:**\\n\\n- Development of a machine vision application for automated recycling.\\n- Deployment of an anomaly detection model for medical imaging.\\n\\n**Conclusion:**\\n\\nMLTRL provides a systematic and risk-driven approach for managing ML model development and deployment. By implementing MLTRL, organizations can improve the quality, reliability, and efficiency of their ML-powered products and solutions.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='TECHNOLOGY READINESS LEVELS\\nFOR MACHINE LEARNING SYSTEMS\\nAlexander Lavin∗\\nPasteur LabsCiarán M. Gilligan-Lee\\nSpotifyAlessya Visnjic\\nWhyLabsSiddha Ganju\\nNvidiaDava Newman\\nMIT\\nAtılım Güne¸ s Baydin\\nUniversity of OxfordSujoy Ganguly\\nUnity AIDanny Lange\\nUnity AIAmit Sharma\\nMicrosoft Research\\nStephan Zheng\\nSalesforce ResearchEric P. Xing\\nPetuumAdam Gibson\\nKonduitJames Parr\\nNASA Frontier Development Lab\\nChris Mattmann\\nNASA Jet Propulsion LabYarin Gal\\nAlan Turing Institute\\nABSTRACT\\nThe development and deployment of machine learning (ML) systems can be executed easily with\\nmodern tools, but the process is typically rushed and means-to-an-end. The lack of diligence can\\nlead to technical debt, scope creep and misaligned objectives, model misuse and failures, and\\nexpensive consequences. Engineering systems, on the other hand, follow well-deﬁned processes\\nand testing standards to streamline development for high-quality, reliable results. The extreme is\\nspacecraft systems, where mission critical measures and robustness are ingrained in the development\\nprocess. Drawing on experience in both spacecraft engineering and ML (from research through\\nproduct across domain areas), we have developed a proven systems engineering approach for machine\\nlearning development and deployment. Our Machine Learning Technology Readiness Levels (MLTRL)\\nframework deﬁnes a principled process to ensure robust, reliable, and responsible systems while\\nbeing streamlined for ML workﬂows, including key distinctions from traditional software engineering.\\nEven more, MLTRL deﬁnes a lingua franca for people across teams and organizations to work\\ncollaboratively on artiﬁcial intelligence and machine learning technologies. Here we describe the\\nframework and elucidate it with several real world use-cases of developing ML methods from basic\\nresearch through productization and deployment, in areas such as medical diagnostics, consumer\\ncomputer vision, satellite imagery, and particle physics.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='computer vision, satellite imagery, and particle physics.\\nKeywords: Machine Learning; Systems Engineering; Data Management; Medical AI; Space Sciences\\nIntroduction\\nThe accelerating use of artiﬁcial intelligence (AI) and machine learning (ML) technologies in systems of software,\\nhardware, data, and people introduces vulnerabilities and risks due to dynamic and unreliable behaviors; fundamentally,\\nML systems learn from data, introducing known and unknown challenges in how these systems behave and interact with\\ntheir environment. Currently the approach to building AI technologies is siloed: models and algorithms are developed\\nin testbeds isolated from real-world environments, and without the context of larger systems or broader products they’ll\\nbe integrated within for deployment. A main concern is models are typically trained and tested on only a handful of\\ncurated datasets, without measures and safeguards for future scenarios, and oblivious of the downstream tasks and\\nusers. Even more, models and algorithms are often integrated into a software stack without regard for the inherent\\nstochasticity –for instance, the massive effect random seeds have on deep reinforcement learning model performance\\n[1] – and failure modes of the ML components, which can be dangerously hidden in layers of software and abstraction.\\nOther domains of engineering, such as civil and aerospace, follow well-deﬁned processes and testing standards to\\nstreamline development for high-quality, reliable results. Technology Readiness Level (TRL) is a systems engineering\\nprotocol for deep tech[ 2] and scientiﬁc endeavors at scale, ideal for integrating many interdependent components\\n∗lavin@simulation.science\\nPreprint. Under review.arXiv:2101.03989v2  [cs.LG]  29 Nov 2021'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='andcross-functional teams of people. It is no surprise that TRL is standard process and parlance in NASA[ 3] and\\nDARPA[4].\\nFor a spaceﬂight project there are several deﬁned phases, from pre-concept to prototyping to deployed operations to\\nend-of-life, each with a series of exacting development cycles and reviews. This is in stark contrast to common machine\\nlearning and software workﬂows, which promote quick iteration, rapid deployment, and simple linear progressions. Yet\\nthe NASA technology readiness process for spacecraft systems is overkill; we need robust ML technologies integrated\\nwith larger systems of software, hardware, data, and humans, but not necessarily for missions to Mars. We aim to bring\\nsystems engineering to AI and ML by deﬁning and putting into action a lean Machine Learning Technology Readiness\\nLevels (MLTRL) framework. We draw on decades of AI and ML development, from research through production,\\nacross domains and diverse data scenarios: for example, computer vision in medical diagnostics and consumer apps,\\nautomation in self-driving vehicles and factory robotics, tools for scientiﬁc discovery and causal inference, streaming\\ntime-series in predictive maintenance and ﬁnance.\\nIn this paper we deﬁne our framework for developing and deploying robust, reliable, and responsible ML and data\\nsystems, with several real test cases of advancing models and algorithms from R&D through productization and\\ndeployment, including essential data considerations. Additionally, MLTRL prioritizes the role of AI ethics and\\nfairness, and our systems AI approach can help curb the large societal issues that can result from poorly deployed and\\nmaintained AI and ML technologies, such as the automation of systemic human bias, denial of individual autonomy,\\nand unjustiﬁable outcomes (see the Alan Turing Institute Report on Ethical AI [5]). The adoption and proliferation of\\nMLTRL provides a common nomenclature and metric across teams and industries. The standardization of MLTRL'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='across the AI industry should help teams and organizations develop principled, safe, and trusted technologies.\\nFigure 1: MLTRL spans research through prototyping, productization, and deployment. Most ML workﬂows prescribe\\nan isolated, linear process of data processing, training, testing, and serving a model [ 6]. Those workﬂows fail to deﬁne\\nhow ML development must iterate over that basic process to become more mature and robust, and how to integrate with\\na much larger system of software, hardware, data, and people. Not to mention MLTRL continues beyond deployment:\\nmonitoring and feedback cycles are important for continuous reliability and improvement over the product lifetime.\\nResults\\nMLTRL deﬁnes technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment.\\nA TRL represents the maturity of a model or algorithmii, data pipelines, software module, or composition thereof; a\\ntypical ML system consists of many interconnected subsystems and components, and the TRL of the system is the\\nlowest level of its constituent parts [ 7]. The anatomy of a level is marked by gated reviews, evolving working groups,\\nrequirements documentation with risk calculations, progressive code and testing standards, and deliverables such as\\nTRL Cards (Figure 3) and ethics checklists.iiiThese components—which are crucial for implementing the levels in a\\niiNote we use “model” and “algorithm” somewhat interchangeably when referring to the technology under development. The\\nsame MLTRL process and methods apply for a machine translation model and for an A/B testing algorithm, for example.\\niiiTemplates and examples for MLTRL deliverables will be open-sourced upon publication at github.com/alan-turing-institute.\\n2'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='systematic fashion—as well as MLTRL metrics and methods are concretely described in examples and in the Methods\\nsection. Lastly, to emphasize the importance of data tasks in ML, from data curation [ 8] to data governance [ 9], we\\nstate several important data considerations at each MLTRL level.\\nMACHINE LEARNING TECHNOLOGY READINESS LEVELS\\nThe levels are brieﬂy deﬁned as follows and in Figure 1, and elucidated with real-world examples later.\\nLevel 0 - First Principles This is a stage for greenﬁeld AI research, initiated with a novel idea, guiding question, or\\npoking at a problem from new angles. The work mainly consists of literature review, building mathematical foundations,\\nwhite-boarding concepts and algorithms, and building an understanding of the data – for work in theoretical AI and ML,\\nhowever, there will not yet be data to work with (for example, a novel algorithm for Bayesian optimization[ 10], which\\ncould eventually be used for many domains and datasets). The outcome of Level 0 is a set of concrete ideas with sound\\nmathematical formulation, to pursue through low-level experimentation in the next stage. When relevant, this level\\nexpects conclusions about data readiness, including strategies for getting the data to be suitable for the speciﬁc ML task.\\nTo graduate, the basic principles, hypotheses, data readiness, and research plans need to be stated, referencing relevant\\nliterature. With graduation, a TRL Card should be started to succinctly document the methods and insights thus far –\\nthis key MLTRL deliverable is detailed in the Methods section and Figure 3.\\nLevel 0 data – Not a hard requirement at this stage because this is largely theoretical machine learning. That being said,\\ndata availability needs to be considered for deﬁning any research project to move past theory.\\nLevel 0 review – The reviewer here is solely the lead of the research lab or team, for instance a PhD supervisor. We'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='assess hypotheses and explorations for mathematical validity and potential novelty or utility, not necessarily code nor\\nend-to-end experiment results.\\nLevel 1 - Goal-Oriented Research To progress from basic principles to practical use, we design and run low-level\\nexperiments to analyze speciﬁc model or algorithm properties (rather than end-to-end runs for a performance benchmark\\nscore). This involves collection and processing of sample data to train and evaluate the model. This sample data need\\nnot be the full data; it may be a smaller sample that is currently available or more convenient to collect. In some\\ncases it may sufﬁce to use synthetic data as the representative sample – in the medical domain, for example, acquiring\\ndatasets can take many months due to security and privacy constraints, so generating sample data can mitigate this\\nblocker from early ML development. Further, working with the sample data provides a blueprint for the data collection\\nand processing pipeline (including answering whether it is even possible to collect all necessary data), that can be\\nscaled up for the for the next steps. The experiments, good results or not, and mathematical foundations need to pass a\\nreview process with fellow researchers before graduating to Level 2. The application is still speculative, but through\\ncomparison studies and analyses we start to understand if/how/where the technology offers potential improvements and\\nutility. Code is research-caliber : The aim here is to be quick and dirty, moving fast through iterations of experiments.\\nHacky code is okay, and full test coverage is actually discouraged, as long as the overall codebase is organized and\\nmaintainable. It is important to start semantic versioning practices early in the project lifecycle, which should cover\\ncode, models, anddatasets. This is crucial for retrospectives and reproducibility, issues with which can be costly and'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='severe at later stages. This versioning information and additional progress should be reported on the TRL Card (see for\\nexample Figure 3).\\nLevel 1 data – At minimum we work with sample data that is representative of downstream real datasets, which can be\\na subset of real data, synthetic data, or both. Beyond driving low-level ML experiments, the sample data forces us to\\nconsider data acquisition and processing strategies at an early stage before it becomes a blocker later.\\nLevel 1 review – The panel for this gated review is entirely members of the research team, reviewing for scientiﬁc rigor\\nin early experimentation, and pointing to important concepts and prior work from their respective areas of expertise.\\nThere may be several iterations of feedback and additional experiments.\\nLevel 2 - Proof of Principle (PoP) Development Active R&D is initiated, mainly by developing and running in\\ntestbeds : simulated environments and/or simulated data that closely matches the conditions and data of real scenarios –\\nnote these are driven by model-speciﬁc technical goals, not necessarily application or product goals (yet). An important\\ndeliverable at this stage is the formal research requirements document (with well-speciﬁed veriﬁcation and validation\\n(V&V) steps)iv. Here is one of several key decision points in the broader process: The R&D team considers several\\npaths forward and sets the course: (A) prototype development towards Level 3, (B) continued R&D for longer-term\\nivArequirement is a singular documented physical or functional need that a particular design, product, or process aims to satisfy.\\nRequirements aim to specify all stakeholders’ needs while not specifying a speciﬁc solution. Deﬁnitions are incomplete without\\n3'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='research initiatives and/or publications, or some combination of A and B. We ﬁnd the culmination of this stage is often\\na bifurcation: some work moves to applied AI, while some circles back for more research. This common MLTRL cycle\\nis an instance of the non-monotonic discovery switchback mechanism (detailed in the Methods section).\\nLevel 2 data – Datasets at this stage may include publicly available benchmark datasets, semi-simulated data based\\non the data sample in Level 1, or fully simulated data based on certain assumptions about the potential deployment\\nenvironments. The data should allow researchers to characterize model properties, and highlight corner cases or\\nboundary conditions, in order to justify the utility of continuing R&D on the model.\\nLevel 2 review – To graduate from the PoP stage, the technology needs to satisfy research claims made in previous\\nstages (brought to be bare by the aforementioned PoP data in both quantitative and qualitative ways) with the analyses\\nwell-documented and reproducible.\\nLevel 3 - System Development Here we have checkpoints that push code development towards interoperability,\\nreliability, maintainability, extensibility, and scalability. Code becomes prototype-caliber : A signiﬁcant step up from\\nresearch code in robustness and cleanliness. This needs to be well-designed, well-architected for dataﬂow and interfaces,\\ngenerally covered by unit and integration tests, meet team style standards, and sufﬁciently-documented. Note the\\nprogrammers’ mentality remains that this code will someday be refactored/scrapped for productization; prototype code\\nis relatively primitive with regard to efﬁciency and reliability of the eventual system. With the transition to Level 4 and\\nproof-of-concept mode, the working group should evolve to include product engineering to help deﬁne service-level\\nagreements and objectives (SLAs and SLOs) of the eventual production system.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='agreements and objectives (SLAs and SLOs) of the eventual production system.\\nLevel 3 data – For the most part consistent with Level 2; in general, the previous level review can elucidate potential\\ngaps in data coverage and robustness to be addressed in the subsequent level. However, for test suites developed at this\\nstage, it is useful to deﬁne dedicated subsets of the experiment data as default testing sources, as well as setup mock\\ndata for speciﬁc functionalities and scenarios to be tested.\\nLevel 3 review – Teammates from applied AI and engineering are brought into the review to focus on sound software\\npractices, interfaces and documentation for future development, and version control for models and datasets. There are\\nlikely domain- or organization-speciﬁc data management considerations going forward that this review should point out\\n– e.g. standards for data tracking and compliance in healthcare [11].\\nLevel 4 - Proof of Concept (PoC) Development This stage is the seed of application-driven development; for many\\norganizations this is the ﬁrst touch-point with product managers and stakeholders beyond the R&D group. Thus TRL\\nCards and requirements documentation are instrumental in communicating the project status and onboarding new\\npeople. The aim is to demonstrate the technology in a real scenario: quick proof-of-concept examples are developed to\\nexplore candidate application areas and communicate the quantitative and qualitative results. It is essential to use real\\nand representative data for these potential applications. Thus data engineering for the PoC largely involves scaling up\\nthe data collection and processing from Level 1, which may include collecting new data or processing all available data\\nusing scaled experiment pipelines from Level 3. In some scenarios there will new datasets brought in for the PoC, for\\nexample, from an external research partner as a means of validation. Hand-in-hand with the evolution from sample to'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='real data, the experiment metrics should evolve from ML research to the applied setting: proof-of-concept evaluations\\nshould quantify model and algorithm performance (e.g., precision and recall and various data splits), computational\\ncosts (e.g., CPU vs GPU runtimes), and also metrics that are more relevant to the eventual end-user (e.g., number\\nof false positives in the top-N predictions of a recommender system). We ﬁnd this PoC exploration reveals speciﬁc\\ndifferences between clean and controlled research data versus noisy and stochastic real-world data. The issues can\\nbe readily identiﬁed because of the well-deﬁned distinctions between those development stages in MLTRL, and then\\ntargeted for further development.\\nAI ethics processes vary across organizations, but all should engage in ethics conversations at this stage, including ethics\\nof data collection, and potential of any harm or discriminatory impacts due to the model (as the AI capabilities and\\ndatasets are known). MLTRL requires ethics considerations to be reported on TRL Cards at all stages, which generally\\nlink to an extended ethics checklist. The key decision point here is to push onward with application development or not.\\nIt is common to pause projects that pass Level 4 review, waiting for a better time to dedicate resources, and/or pull the\\ntechnology into a different project.\\nLevel 4 data – Unlike the previous stages, having real-world and representative data is critical for the PoC; even with\\nmethods for verifying that data distributions in synthetic data reliably mirror those of real data [], sufﬁcient conﬁdence\\nin the technology must be achieved with real-world data of the use-case. Further, one must consider how to obtain\\ncorresponding measures for veriﬁcation and validation (V&V). Veriﬁcation: Are we building the product right? Validation: Are we\\nbuilding the right product?\\n4'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='high-quality and consistent data required for the future model inference: generation of the data pipeline PoC that will\\nresemble the future inference pipeline that will take data from intended sources, transform it into features, and send it to\\nthe model for inference.\\nLevel 4 review – Demonstrate the utility towards one or more practical applications (each with multiple datasets), taking\\ncare to communicate assumptions and limitations, and again reviewing data-readiness: evaluating the real-world data\\nfor quality, validity, and availability. The review also evaluates security and privacy considerations – deﬁning these in\\nthe requirements document with risk quantiﬁcation is a useful mechanism for mitigating potential issues (discussed\\nfurther in the Methods section).\\nLevel 5 - Machine Learning “Capability” At this stage the technology is more than an isolated model or algorithm,\\nit is a speciﬁc capability . For instance, producing depth images from stereo vision sensors on a mobile robot is a\\nreal-world capability beyond the isolated ML technique of self-supervised learning for RGB stereo disparity estimation.\\nIn many organizations this represents a technology transition or handoff from R&D to productization. MLTRL\\nmakes this transition explicit, evolving the requisite work, guiding documentation, objectives and metrics, and team;\\nindeed, without MLTRL it is common for this stage to be erroneously leaped completely, as shown in Figure 2. An\\ninterdisciplinary working group is deﬁned, as we start developing the technology in the context of a larger real-world\\nprocess – i.e., transitioning the model or algorithm from an isolated solution to a module of a larger application. Just as\\nthe ML technology should no longer be owned entirely by ML experts, steps have been taken to share the technology\\nwith others in the organization via demos, example scripts, and/or an API; the knowledge and expertise cannot remain'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='within the R&D team, let alone an individual ML developer. Graduation from Level 5 should be difﬁcult, as it signiﬁes\\nthe dedication of resources to push this ML technology through productization. This transition is a common challenge\\nin deep-tech, sometimes referred to as “the valley of death” because project managers and decision-makers struggle\\nto allocate resources and align technology roadmaps to effectively move to Level 6, 7 and onward. MLTRL directly\\naddresses this challenge by stepping through the technology transition or handoff explicitly.\\nLevel 5 data – For the most part consistent with Level 4. However, considerations need to be taken for scaling of data\\npipelines: there will soon be more engineers accessing the existing data and adding more, and the data will be getting\\nmuch more use, including automated testing in later levels. With this scaling can come challenges with data governance.\\nThe data pipelines likely do not mirror the structure of the teams or broader organization. This can result in data silos,\\nduplications, unclear responsibilities, and missing control of data over its entire lifecycle. These challenges and several\\napproaches to data governance (planning and control, organizational, and risk-based) are detailed in Janssen et al. [9].\\nLevel 5 review – The veriﬁcation and validation (V&V) measures and steps deﬁned in earlier R&D stages (namely\\nLevel 2) must all be completed by now, and the product-driven requirements (and corresponding V&V) are drafted at\\nthis stage. We thoroughly review them here, and make sure there is stakeholder alignment (at the ﬁrst possible step of\\nproductization, well ahead of deployment).\\nLevel 6 - Application Development The main work here is signiﬁcant software engineering to bring the code up to\\nproduct-caliber : This code will be deployed to users and thus needs to follow precise speciﬁcations, have comprehensive'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='test coverage, well-deﬁned APIs, etc. The resulting ML modules should be robustiﬁed towards one or more target\\nuse-cases. If those target use-cases call for model explanations, the methods need to be built and validated alongside\\nthe ML model, and tested for their efﬁcacy in faithfully interpreting the model’s decisions – crucially, this needs to be\\nin the context of downstream tasks and the end-users, as there is often a gap between ML explainability that serves\\nML engineers rather than external stakeholders[ 12]. Similarly, we need to develop the ML modules with known data\\nchallenges in mind, speciﬁcally to check the robustness of the model (and broader pipeline) to changes in the data\\ndistribution between development and deployment.\\nThe deployment setting(s) should be addressed thoroughly in the product requirements document, as ML serving (or\\ndeploying) is an overloaded term that needs careful consideration. First, there are two main types: internal, as APIs\\nfor experiments and other usage mainly by data science and ML teams, and external, meaning an ML model that\\nis embedded or consumed within a real application with real users. The serving constraints vary signiﬁcantly when\\nconsidering cloud deployment vs on-premise or hybrid, batch or streaming, open-source solution or containerized\\nexecutable, etc. Even more, the data at deployment may be limited due to compliance, or we may only have access to\\nencrypted data sources, some of which may only be accessible locally – these scenarios may call for advanced ML\\napproaches such as federated learning[ 13] and other privacy-oriented ML[ 14]. And depending on the application, an\\nML model may not be deployable without restrictions; this typically means being embedded in a rules engine workﬂow\\nwhere the ML model acts like an advisor that discovers edge cases in rules. These deployment factors are hardly\\nconsidered in model and algorithm development despite signiﬁcant inﬂuence on modeling and algorithmic choices;'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='that said, hardware choices typically are considered early on, such as GPU versus edge devices. It is crucial to make\\nthese systems decisions at Level 6–not too early that serving scenarios and requirements are uncertain, and not too late\\n5'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='that corresponding changes to model or application development risk deployment delays or failures. This marks a key\\ndecision for the project lifecycle, as this expensive ML deployment risk is common without MLTRL (see Figure 2).\\nLevel 6 data – Additional data should be collected and operationalized at this stage towards robustifying the ML\\nmodels, algorithms, and surrounding components. These include adversarial examples to check local robustness [ 15],\\nsemantically-equivalent perturbations to check consistency of the model with respect to domain assumptions [16, 17],\\nand collecting data from different sources and checking how well the trained model generalizes to them. These\\nconsiderations are even more vital in the challenging deployment domains mentioned above with limited data access.\\nLevel 6 review – Focus is on the code quality, the set of newly deﬁned product requirements, system SLA and SLO\\nrequirements, data pipelines spec, and an AI ethics revisit now that we are closer to a real-world use-case. In particular,\\nregulatory compliance is mandated for this gated review; the data privacy and security laws are changing rapidly, and\\nmissteps with compliance can make or break the project.\\nLevel 7 - Integrations For integrating the technology into existing production systems, we recommend the working\\ngroup has a balance of infrastructure engineers andapplied AI engineers – this stage of development is vulnerable\\nto latent model assumptions and failure modes, and as such cannot be safely developed solely by software engineers.\\nImportant tools for them to build together include:\\n•Tests that run use-case speciﬁc critical scenarios and data-slices – a proper risk-quantiﬁcation table will\\nhighlight these.\\n•A “golden dataset” should be deﬁned to baseline the performance of each model and succession of models –see\\nthe computer vision app example in Figure 4–for use in the continuous integration and deployment (CI/CD)\\ntests.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='tests.\\n•Metamorphic testing : a software engineering methodology for testing a speciﬁc set of relations between the\\noutputs of multiple inputs. When integrating ML modules into larger systems, a codiﬁed list of metamorphic\\nrelations[18] can provide valuable veriﬁcation and validation measures and steps.\\n•Data intervention tests that seek data bugs at various points in the pipelines, downstream to measure the\\npotential effects of data processing and ML on consumers or users of that data, as well as upstream at data\\ningestion or creation. Rather than using model performance as a proxy for data quality, it is crucial to use\\nintervention tests that instead catch data errors with mechanisms speciﬁc to data validation.\\nThese tests in particular help mitigate underspeciﬁcation in ML pipelines, a key obstacle to reliably training models that\\nbehave as expected in deployment[ 19]. On the note of reliability, it is important that quality assurance engineers (QA)\\nplay a key role here and through Level 9, overseeing data processes to ensure privacy and security, and covering audits\\nfor downstream accountability of AI methods.\\nLevel 7 data – In addition to the data for test suites discussed above, this level calls for QA to prioritize data governance :\\nhow data is obtained, managed, used, and secured by the organization. This was earlier suggested in level 5 (in order to\\npreempt related technical debt), and essential here at the main junction for integration, which may create additional\\ngovernance challenges in light of downstream effects and consumers.\\nLevel 7 review – The review should focus on the data pipelines and test suites; a scorecard like the ML Testing\\nRubric[ 20] is useful. The group should also emphasize ethical considerations at this stage, as they may be more\\nadequately addressed now (where there are many test suites put into place) rather than close to shipping later.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='Level 8 - Flight-ready The technology is demonstrated to work in its ﬁnal form and under expected conditions.\\nThere should be additional tests implemented at this stage covering deployment aspects, notably A/B tests, blue/green\\ndeployment tests, shadow testing, and canary testing, which enable proactive and gradual testing for changing ML\\nmethods and data. Ahead of deployment, the CI/CD system should be ready to regularly stress test the overall system\\nand ML components. In practice, problems stemming from real-world data are impossible to anticipate and design for –\\nan upstream data provider could change formats unexpectedly or a physical event could cause the customer behavior to\\nchange. Running models in shadow mode for a period of time would help stress test the infrastructure and evaluate how\\nsusceptible the ML model(s) will be to performance regressions caused by data. We observe that ML systems with\\ndata-oriented architectures are more readily tested in this manner, and better surface data quality issues, data drifts, and\\nconcept drifts – this is discussed later in the Beyond Software Engineering section. To close this stage, the key decision\\nis go or no-go for deployment, and when.\\nLevel 8 data – If not already in place, there absolutely needs to be mechanisms for automatically logging data\\ndistributions alongside model performance once deployed.\\n6'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='Level 8 review – A diligent walkthrough of every technical and product requirement, showing the corresponding\\nvalidations, and the review panel is representative of the full slate of stakeholders.\\nLevel 9 - Deployment In deploying AI and ML technologies, there is signiﬁcant need to monitor the current version,\\nand explicit considerations towards improving the next version. For instance, performance degradation can be hidden\\nand critical, and feature improvements often bring unintended consequences and constraints. Thus at this level, the\\nfocus is on maintenance engineering–i.e., methods and pipelines for ML monitoring and updating. Monitoring for data\\nquality, concept drift, and data drift is crucial; no AI system without thorough tests for these can reliably be deployed.\\nBy the same token there must be automated evaluation and reporting – if actuals[ 21] are available, continuous evaluation\\nshould be enabled, but in many cases actuals come with a delay, so it is essential to record model outputs to allow for\\nefﬁcient evaluation after the fact. To these ends, the ML pipeline should be instrumented to log system metadata, model\\nmetadata, and data itself.\\nMonitoring for data quality issues and data drifts is crucial to catch deviations in model behavior, particularly those that\\nare non-obvious in the model or product end-performance. Data logging is unique in the context of ML systems: data\\nlogs should capture statistical properties of input features and model predictions, and capture their anomalies. With\\nmonitoring for data, concept, and model drifts, the logs are to be sent to the relevant systems, applied, and research\\nengineers. The latter is often non-trivial, as the model server is not ideal for model “observability” because it does not\\nnecessarily have the right data points to link the complex layers needed to analyze and debug models. To this end,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='MLTRL requires the drift tests to be implemented at stages well ahead of deployment, earlier than is standard practice.\\nAgain we advocate for data-ﬁrst architectures rather than the software industry-standard design by services (discussed\\nlater), which aids in surfacing and logging the relevant data types and slices when monitoring AI systems. For retraining\\nand improving models, monitoring must be enabled to catch training-serving skew and let the team know when to\\nretrain. Towards model improvements, adding or modifying features can often have unintended consequences, such as\\nintroducing latencies or even bias. To mitigate these risks, MLTRL has an embedded switchback here: any component\\nor module changes to the deployed version must cycle back to Level 7 (integrations stage) or earlier. Additionally,\\nfor quality ML products, we stress a deﬁned communication path for user feedback without roadblocks to R&D; we\\nencourage real-world feedback all the way to research, providing valuable problem constraints and perspectives.\\nLevel 9 data – Proper mechanisms for logging and inspecting data (alongside models) is critical for deploying reliable\\nAI and ML – systems that learn on data have unique monitoring requirements (detailed above). In addition to the\\ninfrastructure and test suites covering data and environment shifts, it’s important for product managers and other owners\\nto be on top of data policy shifts in domains such as ﬁnance and healthcare.\\nLevel 9 review – The review at this stage is unique, as it also helps in lifecycle management: at a regular cadence\\nthat depends on the deployed system and domain of use, owners and other stakeholders are to revisit this review and\\nrecommend switchbacks if needed (discussed in the Methods section). This additional oversight at deployment is\\nshown to help deﬁne regimented release cycles of updated versions, and provide another “eye” check for stale model\\nperformance or other system abnormalities.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='performance or other system abnormalities.\\nNotice MLTRL is deﬁned as stages or levels, yet much of the value in practice is realized in the transitions: MLTRL\\nenables teams to move from one level to the next reliably and efﬁciently, and provides a guide for how teams and\\nobjectives evolve with the progressing technology.\\nDiscussion\\nMLTRL is designed to apply to many real-world use-cases involving data and ML, from simple regression models\\nused for predictive modeling energy demand or anomaly detection in datacenters, to real-time modeling in rideshare\\napplications and motion planning in warehouse robotics. For simple use-cases MLTRL may be overkill, and a subset\\nmay sufﬁce – for instance, model cards as demonstrated by Google for basic image classiﬁcation. Yet this is a ﬁne line,\\nas the same cards-only approach in the popular “Huggingface” codebases are too simplistic for the language models\\nthey represent, deployed in domains that carry signiﬁcant consequences. MLTRL becomes more valuable with more\\ncomplex, larger systems and environments, especially in risk averse domains. We thoroughly discuss this through\\nseveral real uses of MLTRL below.\\n7'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='Figure 2: Most ML and AI projects live in these sections of MLTRL, not concerned with fundamental R&D – that is,\\ncompletely using existing methods and implementations, and even pretrained models. In the left diagram, the arrows\\nshow a common development pattern with MLTRL in industry: projects go back to the ML toolbox to develop new\\nfeatures (dashed line), and frequent, incremental improvements are often a practice of jumping back a couple levels to\\nLevel 7 (which is the main systems integrations stage). At Levels 7 and 8 we stress the need for tests that run use-case\\nspeciﬁc critical scenarios and data-slices, which are highlighted by a proper risk-quantiﬁcation matrix [ 22]. Cycling\\nback to previous lower levels is not just a late-stage mechanism in MLTRL, but rather “switchbacks” occur throughout\\nthe process (as discussed in the Methods section and throughout the text). In the right diagram we show the more\\ncommon approach in industry ( without using our framework), which skips essential technology transition stages – ML\\nEngineers push straight through to deployment, ignoring important productization and systems integration factors. This\\nwill be discussed in more detail in the Methods section.\\nEXAMPLES\\nHuman-machine visual inspection\\nWhile most ML projects begin with a speciﬁc task and/or dataset, there are many that originate in ML theory without\\nany target application – i.e., projects starting MLTRL at level 0 or 1. These projects nicely demonstrate the utility of\\nMLTRL built-in switchbacks, bifurcating paths, and iteration with domain experts. An example we discuss here is a\\nnovel approach to representing data in generative vision models from Naud & Lavin[ 23], which was then developed into\\nstate-of-the-art unsupervised anomaly detection, and targeted for two human-machine visual inspection applications:\\nFirst, industrial anomaly detection, notably in precision manufacturing, to identify potential errors for human-expert'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='manual inspection. Second, using the model to improve the accuracy and efﬁciency of neuropathology, the microscopic\\nexamination of neurosurgical specimens for cancerous tissue. In these human-machine teaming use-cases there are\\nspeciﬁc challenges impeding practical, reliable use:\\n•Hidden feedback loops can be common and problematic in real-world systems inﬂuencing their own training\\ndata: over time the behavior of users may evolve to select data inputs they prefer for the speciﬁc AI system,\\nrepresenting some skew from the training data. In this neuropathology case, selecting whole-slide images that\\nare uniquely difﬁcult for manual inspection, or even biased by that individual user. Similarly we see underlying\\nhealthcare processes can act as hidden confounders, resulting in unreliable decision support tools[26].\\n•Model availability can be limited in many deployment settings: for example, on-premises deployments\\n(common in privacy preserving domains like healthcare and banking), edge deployments (common in industrial\\nuse-cases such as manufacturing and agriculture), or from the infrastructure’s inability to scale to the volume\\nof requests. This can severely limit the team’s ability to monitor, debug, and improve deployed models.\\n•Uncertainty estimation is valuable in many AI scenarios, yet not straightforward to implement in practice.\\nThis is further complicated with multiple data sources and users, each injecting generally unknown amounts of\\nnoise and uncertainties. In medical applications it is of critical importance, to provide measures of conﬁdence\\nand sensitivity, and for AI researchers through end-users. In anomaly detection, various uncertainty measures\\ncan help calibrate the false-positive versus false-negative rates, which can be very domain speciﬁc.\\n8'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='Figure 3: The maturity of each ML technology is tracked via TRL Cards , which we describe in the Methods section.\\nHere is an example reﬂecting a neuropathology machine vision use-case[ 23], detailed in the Discussion Section. Note\\nthis is a subset of a full TRL Card, which in reality lives as a full document in an internal wiki. Notice the card\\nclearly communicates the data sources, versions, and assumptions. This helps mitigate invalid assumptions about\\nperformance and generalizability when moving from R&D to production, and promotes the use of real-world data\\nearlier in the project lifecycle. We recommend documenting datasets thoroughly with semantic versioning and tools\\nsuch as datasheets for datasets [24], and following data accountability best-practices as they evolve (see [25]).\\n•Costs of edge cases can be signiﬁcant, sometimes risking expensive machine downtime or medical failures.\\nThis is exacerbated in anomaly detection anomalies are by deﬁnition rare so they can be difﬁcult to train for,\\nespecially for the anomalies that are completely unseen until they arise in the wild.\\n•End-user trust can be difﬁcult to achieve, often preventing the adoption of ML applications, particularly in\\nthe healthcare domain and other highly regulated industries.\\nThese and additional ML challenges such as data privacy and interpretability can inhibit ML adoption in clinical practice\\nand industrial settings, but can be mitigated with MLTRL processes. We’ll describe how in the context of the Naud\\n& Lavin[ 23] example, which began at level 0 with theoretical ML work on manifold geometries, and at level 5 was\\ndirected towards specialized human-machine teaming applications utilizing the same ML method under-the-hood.\\n•Levels 0-1 – From open-ended exploration of data-representation properties in various Riemmanian manifold\\ncurvatures, we derived from ﬁrst principles and empirically identiﬁed a property with hyperbolic manifolds:'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='when used as a latent space for embedding data without labels, the geometry organizes the data by it’s implicit\\nhierarchical structure. Unsupervised computer vision was identiﬁed in reviews as a promising direction for\\nproof-of-principle work.\\n•Level 2 – One approach for validating the earlier theoretical developments was to generate synthetic data to\\nisolate very speciﬁc features in data we would expect represented in the latent manifold. The results showed\\npromise for anomaly detection – using the latent representation of data to automatically identify images that\\nare out-of-the-ordinary (anomalous), and also using the manifold to inspect how they are semantically different.\\nFurther, starting with an implicitly probabilistic modeling approach implied uncertainty estimation could be\\na valuable feature downstream. This made the level 2 key decision point clear: proceed with applied ML\\ndevelopment.\\n•Levels 3-5 – Proof-of-concept development and reviews demonstrated promise for several commercial appli-\\ncations relevant to the business, and also highlighted the need for several key features (deﬁned as R&D and\\nproduct requirements): interpretability (towards end-user trust), uncertainty quantiﬁcation (to show conﬁdence\\nscores), and human-in-the-loop (for domain expertise). Without the MLTRL PoC steps and review processes,\\nthese features can often be delayed until beta testing or overlooked completely – for example, the failures of\\napplying IBM Watson in medical applications [ 27]. For this technology, the applications to develop towards\\nare anomaly detection in histopathology and manufacturing, speciﬁcally inspecting whole-slide images of\\nneural tissue, and detecting defects in metallic surfaces, respectively.\\n9'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='From the systems perspective, we suggest quantifying the uncertainties of components and propagating them\\nthrough the system, which can improve safety and trust. Probabilistic ML methods, rooted in Bayesian\\nprobability theory, provide a principled approach to representing and manipulating uncertainty about models\\nand predictions[ 28]. For this reason we advocate strongly for probabilistic models and algorithms in AI\\nsystems. In this machine vision example, the MLTRL technical requirements speciﬁcally called for a\\nprobabilistic generative model to readily quantify various types of uncertainties and propagate them forward to\\nthe visualization component of the pipeline, and the product requirements called for the downstream conﬁdence\\nand sensitivity measures to be exposed to the end-user. Component uncertainties must be assembled in a\\nprincipled way to yield a meaningful measure of overall system uncertainty, based on which safe decisions can\\nbe made[29]. See the Methods section for more on uncertainty in AI systems.\\nThe early checks for data management and governance proved valuable here, as the application areas dealt\\nwith highly sensitive data that would signiﬁcantly inﬂuence the design of data pipelines and test suites. In\\nboth the neuropathology and manufacturing applications, the data management checks also raised concerns\\nabout hidden feedback loops, where users may unintentionally skew the data inputs when using the anomaly\\ndetection models in practice, for instance biasing the data towards speciﬁc subsets they subjectively need help\\nwith. Incorporating domain experts this early in the project lifecycle helped inform veriﬁcation and validation\\nsteps to help be robust to the hidden feedback loops. Not to mention their input guided us towards user-centric\\nmetrics for performance, which can often skew from ML metrics in important ways – for instance, the typical'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='acceptance ratio for false positives versus false negatives doesn’t apply to select edge cases, for which our\\nhierarchical anomaly classiﬁcation scheme was useful [23].\\nFrom prior reviews and TRL card documentation, we also identiﬁed the value of synthetic data generation\\ninto application development: anomalies are by deﬁnition rare so they are hard to come by in real datasets,\\nespecially with evolving environments in deployment settings, so the ability to generate synthetic datasets for\\nanomaly detection can accelerate the level 6-9 pipeline, and help ensure more reliable models in the wild.\\n•Level 6 (medical) – The medical inspection application experienced a bifurcation with product work proceed-\\ning while additional R&D was desired to explore improved data processing methods, while engaging with\\nclinicians and medical researchers for feedback. Proceeding through the levels in a non-linear, non-monotonic\\nway is common in MLTRL and encouraged by various switchback mechanisms (detailed in the Methods\\nsection). These practices – intentional switchbacks, frequent engagement with domain experts and users – can\\nhelp mitigate methodological ﬂaws and underlying biases that are common when applying ML to clinical\\napplications. For instance, recent work by Roberts et al. [ 30] investigated 2,122 studies applying ML to\\nCOVID-19 use-cases, ﬁnding that none of the models are sufﬁcient for clinical use due to methodological ﬂaws\\nand/or underlying biases. They go on to give many recommendations – some we’ve discussed in the context of\\nMLTRL, and more – which should be reviewed for higher quality medical-ML models and documentation.\\n•Level 6-9 (manufacturing) – Overall these stages proceeded regularly and efﬁciently for the defect detection\\nproduct. MLTRL’s embedded switchback from level 9 to 4 proved particularly useful in this lifecycle, both\\nfor incorporating feedback from the ﬁeld and for updating with research progress. On the former, the data'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='distribution shifts from one deployment setting to another signiﬁcantly affected false-positive versus false-\\nnegative calibrations, so this was added as a feature to the CI/CD pipelines. On the latter, the built-in touch\\npoints for real-world feedback and data into the continued ML research provided valuable constraints to\\nhelp guide research, and product managers could readily understand what capabilities could be available for\\nproduct integration and when (readily communicated with TRL Cards) – for instance, later adding support for\\nvideo-based inspection for defects, and tooling for end-users to reason about uncertainty estimates (which\\nhelps establish trust).\\n•Level 7-9 (medical) – For productization the “neuropathology copilot” was handed off to a partner pharmaceu-\\ntical company to integrate into their existing software systems. The MLTRL documentation and communication\\nstreamlined the technology transfer, which can often by a time-consuming manual process. If not pursuing\\nthis path, the product would’ve likely faced many of the medical-ML deployment challenges with model\\navailability and data access; MLTRL cannot overcome the technical challenges of deploying on-premises, but\\nthe manifestation of those challenges as performance regressions, data shifts, privacy and ethics concerns, etc.\\ncan be mitigated by the system-level checks and strategies MLTRL puts forth.\\nComputer vision with real and synthetic data\\nAdvancements in physics engines and graphics processing have advanced AI environment and data-generation capabili-\\nties, putting increased emphasis on transitioning models across the simulation-to-reality gap [ 31,32,33]. To develop a\\ncomputer vision application for automated recycling, we leveraged the Unity Perception [ 34] package, a toolkit for\\ngenerating large-scale datasets for perception-based ML training and validation. We produced synthetic images to\\n10'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='\\x15A?U?HEJC\\x03?H=OOEBE?=PEKJ\\x03LELAHEJA\\x03\\n\\xa0=¡\\xa0>¡\\n,V\\x03PRGHO\\x03FRQğGHQFH\\x03\\x1f\\x03WKUHVKROG\",V\\x03GHWHFWHG\\x03REMHFW\\x03LQ\\x03WDUJHW\\x03VHW\"3URYLGH\\x03FRUUHVSRQGLQJ\\x03UHF\\\\FOLQJ\\x03LQVWUXFWLRQV,QLWLDWH\\x03KXPDQ\\x10\\x03LQ\\x10WKH\\x10ORRS\\x03SURWRFRO12<(6<(6Figure 4: Computer vision pipeline for an automated recycling application (a), which contains multiple ML models,\\nuser input, and image data from various sources. Complicated logic such as this can mask ML model performance lags\\nand failures, and also emphasized the need for R&D-to-product hand off described in MLTRL. Additional emphasis is\\nplaced on ML tests that consider the mix of real-world data with user annotations (b, right) and synthetic data generated\\nby Unity AI’s Perception tool and structured domain randomization (b, left).\\ncomplement real-world data sources (Figure 4). This application exempliﬁes three important challenges in ML product\\ndevelopment that MLTRL helps overcome:\\n•Multiple and disparate data sources are common in deployed ML pipelines yet often ignored in R&D.\\nFor instance, upstream data providers can change formats unexpectedly, or a physical event could cause the\\ncustomer behavior to change. It is nearly impossible to anticipate and design for all potential problems with\\nreal-world data and deployment. This computer vision system implemented pipelines and extended test suites\\nto cover open-source benchmark data, real user data, and synthetic data.\\n•Hidden performance degradation can be challenging to detect and debug in ML systems because gradual\\nchanges in performance may not be immediately visible. Common reasons for this challenge are that the\\nML component may be one step in a series. Additionally, local/isolated changes to an ML component’s\\nperformance may not directly affect the observed downstream performance. We can see both issues in the\\nillustrated logic diagram for the automated recycling app (Figure 4). A slight degradation in the initial CV'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='model may not heavily inﬂuence the following user input. However, when an uncommon input image appears\\nin the future, the app fails altogether.\\n•Model usage requirements can make or break an ML product. For example, the Netﬂix “$1M Prize” solution\\nwas never fully deployed because of signiﬁcant engineering costs in real-world scenariosv. For example,\\nengineering teams must communicate memory usage, compute power requirements, hardware availability,\\nnetwork privacy, and latency to the ML teams. ML teams often only understand the statistics or ML theory\\nbehind a model but not the system requirements or how it scales.\\nWe next elucidate these challenges and how MLTRL helps overcome them in the context of this project’s lifecycle. This\\nproject started at level 4, using largely existing ML methods with a target use-case.\\n•Level 4 – For this project, we validated most of the components in other projects. Speciﬁcally, the computer\\nvision (CV) model for object recognition and classiﬁcation was an off-the-shelf model. The synthetic data\\ngeneration method used Unity Perception, a well-established open-source project. Though this allowed us to\\nvnetﬂixtechblog.com/netﬂix-recommendations-beyond-the-5-stars-part-1\\n11'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='skip the earlier levels, many challenges arise when combining ML elements that were independently validated\\nand developed. The MLTRL prototype-caliber code checkpoint ensures that the existing code components\\nare validated and helps avoid poorly deﬁned borders and abstractions between components. ML pipelines\\noften grow out of glue code, and our regimented code checkpoints motivate well-architected software that\\nminimizes these danger spots.\\n•Level 5 – The problematic “valley of death”, mentioned earlier in the level 5 deﬁnitions, is less prevalent in use-\\ncases like this that start at a higher MLTRL level with a speciﬁc product deliverable. In this case, the product\\ndeliverable was a real-time object recognition and classiﬁcation of trash for a mobile recycling application.\\nStill, this stage is critical for the requirements and V&V transition. This stage mitigates failure risks due to the\\ndisparate data sources integrated at various steps in this CV system and accounted for the end-user compute\\nconstraints for mobile computing. Speciﬁcally, the TRL cards from earlier stages surfaced potential issues\\nwith imbalanced datasets and the need for speciﬁc synthetic images. These considerations are essential for the\\ndata readiness and testing V&V in the productization requirements. Data quality and availability issues often\\npresent huge blockers because teams discover them too late in the game. Data-readiness is one class of many\\nexample issues teams face without MLTRL, as depicted in Fig. 2.\\n•Level 6 – We were re-using a well-understood model and deployment pipeline in this use-case, meaning our\\nprimary challenge was around data reliability. For the problem of recognizing and classifying trash, building a\\nreliable data source using only real data is almost impossible due to diversity, class imbalance, and annotation\\nchallenges. Therefore we chose to develop a synthetic data generator to create training data. At this MLTRL'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='level, we needed to ensure that the synthetic data generator created sufﬁciently diverse data and exposed the\\ncontrols needed to alter the data distribution in production. Therefore, we carefully exposed APIs using the\\nUnity Perception package, which allowed us to control lighting, camera parameters, target and non-target\\nobject placements and counts, and background textures. Additionally, we ensured that the object labeling\\nmatched the real-world annotator instructions and that output data formats matched real-world counterparts.\\nLastly, we established a set of statistical tests to compare synthetic and real-world data distributions. The\\nMLTRL checks ensured that we understood, and in this case, adequately designed our data sources to meet\\nin-production requirements.\\n•Level 7 – From the previous level’s R&D TRL cards and observations, we knew relatively early in produc-\\ntization that we would need to assume bias for the real data sources due to class imbalance and imperfect\\nannotations. Therefore we designed tests to monitor these in the deployed application. MLTRL imposes these\\ncritical deployment tests well ahead of deployment, where we can easily overlook ML-speciﬁc failure modes.\\n•Level 8 – As we suggested earlier, problems that stem from real-world data are near impossible to anticipate\\nand design for, implying the need for level 8 ﬂight-readiness preparations. Given that we were generating\\nsynthetic images (with structured domain randomization) to complement the real data, we created tests for\\ndifferent data distribution shifts at multiple points in the classiﬁcation pipeline. We also implemented thorough\\nshadow tests ahead of deployment to evaluate how susceptible the ML model(s) to performance regressions\\ncaused by data. Additionally, we also implemented these as CI/CD tests over various deployment scenarios (or\\nmobile device computing speciﬁcations). Without these fully covered, documented, and automated, it would'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='be impossible to pass level 8 review and deploy the technology.\\n•Level 9 – Post-deployment, the monitoring tests prescribed at Levels 8 and 9 and the three main code quality\\ncheckpoints in the MLTRL process help surface hidden performance degradation problems, common with\\ncomplex pipelines of data ﬂows and various models. The switchbacks depicted in Fig. 2 are typical in CV\\nuse-cases. For instance, miscalibrations in models pre-trained on synthetic data and ﬁne-tuned on newer real\\ndata can be common yet difﬁcult to catch. However, the level 7 to 4 switchback is designed precisely for these\\nchallenges and product improvements.\\nAccelerating scientiﬁc discovery with massive particle physics simulators\\nComputational models and simulation are key to scientiﬁc advances at all scales, from particle physics, to material\\ndesign and drug discovery, to weather and climate science, and to cosmology[ 35]. Many simulators model the forward\\nevolution of a system (coinciding with the arrow of time), such as the interaction of elementary particles, diffusion of\\ngasses, folding of proteins, or evolution of the universe in the largest scale. The task of inference refers to ﬁnding initial\\nconditions or global parameters of such systems that can lead to some observed data representing the ﬁnal outcome\\nof a simulation. In probabilistic programming[ 36], this inference task is performed by deﬁning prior distributions\\nover any latent quantities of interest, and obtaining posterior distributions over these latent quantities conditioned\\non observed outcomes (for example, experimental data) using Bayes rule. This process, in effect, corresponds to\\ninverting the simulator such that we go from the outcomes towards the inputs that caused the outcomes. In the\\n“Etalumis” project[ 37] (“simulate” spelled backwards), we are using probabilistic programming methods to invert\\n12'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='existing, large-scale simulators via Bayesian inference. The project is as an interdisciplinary collaboration of specialists\\nin probabilistic machine learning, particle physics, and high-performance computing, all essential elements to achieve\\nthe project outcomes. Even more, it is a multi-year project spanning multiple countries, companies, university labs, and\\ngovernment research organizations, bringing signiﬁcant challenges in project management, technology coordination\\nand validation. Aided by MLTRL, there were several key challenges to overcome in this project that are common in\\nscientiﬁc-ML projects:\\n•Integrating with legacy systems is common in scientiﬁc and industrial use-cases, where ML methods are\\napplied with existing sensor networks, infrastructure, and codebases. In this case, particle physics domain\\nexperts at CERN are using the SHERPA simulator[ 38], a 1 million line codebase developed over the last\\ntwo decades. Rewriting the simulator for ML use-cases is infeasible due to the codebase size and buried\\ndomain knowledge, and new ML experts would need signiﬁcant onboarding to gain working knowledge of\\nthe codebase. It is also common to work with legacy data infrastructure, which can be poorly organized for\\nmachine learning (let alone preprocessed and clean) and unlikely to have followed best practices such as\\ndataset versioning.\\n•Coupling hardware and software architectures is non-trivial when deploying ML at scale, as performance\\nconstraints are often considered in deployment tests well after model and algorithm development, not to\\nmention the expertise is often split across disjoint teams. This can be exacerbated in scientiﬁc-ML when\\nscaling to supercomputing infrastructure, and working with massive datasets that can be in the terabytes and\\npetabytes.\\n•Interpretability is often a desired feature yet difﬁcult to deliver and validate in practice. Particularly in'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='scientiﬁc ML applications such as this, mechanisms and tooling for domain experts to interpret predictions\\nand models are key for usability (integrating in workﬂows and building trust).\\nTo this end, we will go through the MLTRL levels one by one, demonstrating how they ensure the above scientiﬁc ML\\nchallenges are diligently addressed.\\n•Level 0 – The theoretical developments leading to Etalumis are immense and well discussed in Baydin et\\nal [37]. In particular the ML theory and methods are in a relatively nascent area of ML and mathematics,\\nprobabilistic programming. New territory can present more challenges compared to well-traveled research\\npaths, for instance in computer vision with neural networks. It is thus helpful to have a guiding framework\\nwhen making a new path in ML research, such as MLTRL where early reviews help theoretical ML projects\\nget legs.\\n•Level 1-2 – Running low-level experiments in simple testbeds is generally straightforward when working\\nwith probabilistic programming and simulation; in a sense, this easy iteration over experiments is what\\nPPL are designed for. It was additionally helpful in this project to have rich data grounded in physical\\nconstraints, allowing us to better isolate model behaviors (rather than data assumptions and noise). The\\nMLTRL requirements documentation is particularly useful for the standard PPL experimentation workﬂow:\\nmodel, infer, criticize, repeat (or Box’s loop) [ 39]. The evaluation step (i.e. criticizing the model) can be\\nmore nuanced than checking summary statistics as in deep learning and similar ML workﬂows. It is thus a\\nuseful practice to write down the criticism methods, metrics, and expected results as veriﬁcations for speciﬁc\\nresearch requirements, rather than iterating over Box’s loop without a priori targets. Further, because this\\nresearch project had a speciﬁc target application early in the process (the SHERPA simulator), the project'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='timeline beneﬁted from recognizing simulator-integration constraints upfront as requirements, not to mention\\ndata availability concerns, which are often overlooked in early R&D levels. It was additionally useful to have\\nCERN scientists as domain experts in the reviews at these R&D levels.\\n•Level 3 – Systems development can be challenging with probabilistic programming, again because it is\\nrelatively nascent and much of the out-of-the-box tools and infrastructure are not there as in most ML and\\ndeep learning. Here in particular there’s a novel (unproven) approach for systems integration: a probabilistic\\nprogramming execution protocol was developed to reroute random number draws in the stochastic simulator\\ncodebase (SHERPA) to the probabilistic programming system, thus enabling the system to control stochastic\\nchoices in SHERPA and run inference on its execution traces, all while keeping the legacy codebase intact! A\\nmore invasive method that modiﬁes SHERPA would not have been acceptable. If it were not for MLTRL forcing\\nsystems considerations this early in the Etalumis project lifecycle, this could have been an insurmountable\\nhurdle later when multiple codebases and infrastructures come into play. By the same token, systems planning\\nhere helped enable the signiﬁcant HPC scaling later: the team deﬁned the need for HPC support well ahead\\nof actually running HPC, in order to build the prototype code in a way that would readily map to HPC (in\\naddition to local or cloud CPU and GPU). The data engineering challenges in this system’s development\\n13'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='nonetheless persist – that is, data pipelines and APIs that can integrate various sources and infrastructures, and\\nnormalize data from various databases – although MLTRL helps consider these at the an earlier stage that can\\nhelp inform architecture design.\\n•Level 4 – The natural “embedded switchback” from Level 4 to 2 (see the Methods section) provided an efﬁcient\\npath toward developing an improved, amortized inference method–i.e., using a computationally expensive\\ndeep learning based inference algorithm to train only once, in order to then do fast, repeated inference in the\\nSHERPA model. Leveraging cyclic R&D methods, the Etalumis project could iteratively improve inference\\nmethods without stalling the broader system development, ultimately producing the largest scale posterior\\ninference in a Turing-complete probabilistic programming system. Achieving this scale through iterative R&D\\nalong the main project lifecycle was additionally enabled by working with with NERSC engineers and their\\nCori supercomputer to progressively scale smaller R&D tests to the goal supercomputing deployment scenario.\\nTypical ML workﬂows that follow simple linear progressions[ 6,40] would not enable ramping up in this\\nfashion, and can actual prevent scaling R&D to production due to lack of systems engineering processes (like\\nMLTRL) connecting research to deployment.\\n•Level 5 – Multi-org international collaborations can be riddled with communication and teamwork issues,\\nin particular at this pivotal stage where teams transition from R&D to application and product development.\\nFirst, MLTRL as a lingua franca was key to the team effort bringing Etalumis proof-of-concept into the\\nlarger effort of applying it to massive high-energy physics simulators. It was also critical at this stage to\\nclearly communicate end-user requirements across the various teams and organizations, which must be deﬁned\\nin MLTRL requirements docs with V&V measures – the essential science-user requirements were mainly'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='in MLTRL requirements docs with V&V measures – the essential science-user requirements were mainly\\nfor model and prediction interpretability, uncertainty estimation, and code usability. If there are concerns\\nover these features, MLTRL switchbacks can help to quickly cycle back and improve modeling choices in a\\ntransparent, efﬁcient way – generally in ML projects, these fundamental issues with usability are caught too\\nlate, even after deployment. In the probabilistic generative model setting we’ve deﬁned in Etalumis, Bayesian\\ninference gives results that are interpretable because they include exact locations and processes in the model\\nthat are associated with each prediction. Working with ML methods that are inherently interpretable, we are\\nwell-positioned to deliver interpretable interfaces for the end-users later in the project lifecycle.\\n•Level 6-9 – The standard MLTRL protocol apply in these application-to-deployment stages, with several\\nEtalumis-speciﬁc highlights. First, given the signiﬁcant research contributions in both probabilistic pro-\\ngramming and scientiﬁc-ML, it’s important to share the code publicly. The development and deployment\\nof the open-source code repository PPXvibranched into a separate MLTRL path from the Etalumis path\\nfor deployment at CERN. It’s useful to have systems engineering enable clean separation of requirements,\\ndeployments, etc. when there are different development and product lifecycles originating from a common\\nparent project. For example, in this case it was useful to employ MLTRL switchbacks in the open-sourcing\\nprocess, isolated from the CERN application paths, in order to add support for additional programming\\nlanguages so PPX can apply to more scientiﬁc simulators – both directions beneﬁted signiﬁcantly the from\\nthe data pipelines considerations brought up levels earlier, where open-sourcing required different data APIs'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='and data transformations to enable broad usability. Second, related to the open-source code deliverable and\\nthe scientiﬁc ML user requirements we noted above, the late stages of MLTRL reviews include higher level\\nstakeholders and speciﬁc end-users, yet again enforcing these scientiﬁc usability requirements are met. An\\nexample result of this in Etalumis is the ability to output human-readable execution traces of the SHERPA\\nruns and inference, enabling never before possible step-by-step interpretability of the black-box simulator.\\nThe scientiﬁc ML perspective additionally brings to forefront an end-to-end data perspective that is pertinent in\\nessentially all ML use-cases: these systems are only useful to the extent they provide comprehensive data analyses that\\nintegrate the data consumed and generated in these workﬂows, from raw domain data to machine-learned models. These\\ndata analyses drive reproducibility, explainability, and experiment data understanding, which are critical requirements\\nin scientiﬁc endeavors and ML broadly.\\nCausal inference & ML in medicine\\nUnderstanding cause and effect relationships is crucial for accurate and actionable decision-making in many settings,\\nfrom healthcare and epidemiology, to economics and government policy development. Unfortunately, standard\\nmachine learning algorithms can only ﬁnd patterns and correlation in data, and as correlation is not causation, their\\npredictions cannot be conﬁdently used for understanding cause and effect. Indeed, relying on correlations extracted\\nfrom observational data to guide decision-making can lead to embarrassing, costly, and even dangerous mistakes,\\nsuch as concluding that asthma reduces pneumonia mortality risk [ 41], and that smoking reduces risk of developing\\nvigithub.com/pyprob/ppx\\n14'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='severe COVID-19 [ 42]. Fortunately, there has been much recent development in a ﬁeld known as causal inference that\\ncan quantitatively make sense of cause and effect from purely observational data[ 43]. The ability of causal inference\\nalgorithms to quantify causal impact rests on a number of important checks and assumptions–beyond those employed\\nin standard machine learning or purely statistical methodology–that must be carefully deliberated over during their\\ndevelopment and training. These speciﬁc checks and assumptions are as follows:\\n•Specifying cause-and-effect relationships between relevant variables– One of the most important assump-\\ntions underlying causal inference is the structure of the causal relations between quantities of interest. The\\ngold standard for determining causal relations is to perform a randomised controlled trial, but in most cases\\nthese cannot be employed due to ethical concerns, technological infeasibility, or prohibitive cost. In these\\nsituations, domain experts have to be consulted to determine the causal relationships. It is important in these\\nsituations to carefully address the manner in which such domain knowledge was extracted from experts, the\\nnumber and diversity of experts involved, the amount of consensus between experts, and so on. The need for\\ncareful documentation of this knowledge and its periodic review is made clear in the MLTRL framework, as\\nwe shall see below.\\n•Identiﬁability– Another vital component of building causal models is whether the causal question of interest\\nisidentiﬁable from the causal structure speciﬁed for the model together with observational (and sometimes\\nexperimental) data.\\n•Adjusting for and monitoring confounding bias– An important aspect of causal model performance, not\\npresent in standard machine learning algorithms, is confounding bias adjustment. The standard approach is to\\nemploy propensity score matching to remove such bias. However, the quality of bias adjustment achieved in'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='any speciﬁc instance with such propensity-based matching methods needs to be checked and documented,\\nwith alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[44].\\n•Sensitivity analysis– As causal estimates are based on generally untestable assumptions, such as observing all\\nrelevant confounders, it is vital to determine how sensitive the resulting predictions are to potential violations\\nof these assumptions.\\n•Consistency– It is crucial to understand if the learned causal estimate provably converges to the true causal\\neffect in the limit of inﬁnite sample size. However, causal models cannot be validated by standard held-out\\ntests, but rather require randomization or special data collection strategies to evaluate their predictions [ 45,46].\\nThe MLTRL framework makes transparent the need to carefully document and defend these assumptions, thus ensuring\\nthe safe and robust creation, deployment, and maintenance of causal models. We elucidate this with recent work by\\nRichens et al.[ 47], developing a causal approach to computer-assisted diagnosis which outperforms previous purely\\nmachine learning based methods. To this end, we will go through the MLTRL levels one by one, demonstrating how\\nthey ensure the above speciﬁc checks and assumptions are naturally accounted for. This should provide a blueprint for\\nhow to employ the MLTRL levels in other causal inference applications.\\n•Level 0 – When initially faced with a causal inference task, the ﬁrst step is always to understand the causal\\nrelationships between relevant variables. For instance, in Richens et al. [ 47], the ﬁrst step toward building\\nthe diagnostic model was specifying the causal relationships between the diverse set risk factors, diseases,\\nand symptoms included in the model. To learn these relations, doctors and healthcare professionals were\\nconsulted to employ their expansive medical domain knowledge which was robustly evaluated by additional'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='independent groups of healthcare professionals. The MLTRL framework ensured this issue is dealt with and\\ndocumented correctly, as such knowledge is required to progress from Level 0; failure to do this has plagued\\nsimilar healthcare AI projects [48].\\nThe next step of any causal analysis is to understand whether the causal question of interest is uniquely\\nidentiﬁable from the causal structure speciﬁed for the model together with observational and experimental data.\\nIn this medical diagnosis example, identiﬁcation was crucial to establish, as the causal question of interest,\\n“would the observed symptoms not be present had a speciﬁc disease been cured?”, was highly non-trivial.\\nAgain, MLTRL ensures this vital aspect of model building is carefully considered, as a mathematical proof of\\nidentiﬁability would be required to graduate from Level 0.\\nWith both the causal structure and identiﬁability result in hand, one can progress to Level 1.\\n•Level 1 – At this level, the goal is to take the estimand for the identiﬁed causal question of interest and\\ndevise a way to estimate it from data. To do this one will need efﬁcient ways to adjust for confounﬁng bias.\\nThe standard approach is to employ propensity score-based methods to remove such bias when the target\\ndecision is binary, and use multi-stage ML models adhering to the assumed causal structure[ 49] for continuous\\ntarget decisions (and high-dimensional data in general). However, the quality of bias adjustment achieved in\\nany speciﬁc instance with propensity-based matching methods needs to be checked and documented, with\\n15'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[ 44]. As\\nabove, MLTRL ensures transparency and adherence to this important aspect of causal model development, as\\nwithout it a project cannot graduate from Level 1. Even more, MLTRL ensures tests for confounding bias\\nare developed early-on and maintained throughout later stages to deployment. Still, in many cases, it is not\\npossible to completely remove confounding in the observed data. TRL Cards offer a transparent way to declare\\nspeciﬁc limitations of a causal ML method.\\n•Level 2 – PoC-level tests for causal models must go beyond that of typical ML models. As discussed above,\\nto ensure the estimated causal effects are robust to the assumptions required for their derivation, sensitivity\\nto these assumptions must be analysed. Such sensitivity analysis is often limited to R&D experiments or\\na post-hoc feature of ML products. MLTRL on the other hand requires this throughout the lifecycle as\\ncomponents of ML test suites and gated reviews. In the case of causal ML, best practice is to employ sensitivity\\nanalysis for this robustness check[ 50]. MLTRL ensures this check is highlighted and adhered to, and no model\\nwill end up graduating Level 2–let alone being deployed–unless it is passed.\\n•Level 3 – Coding best practices, as in general ML applications.\\n•Level 4-5 – There are additional tests to consider when taking causal models from research to production,\\nin particular at Level 4–proof of concept demonstration in a real scenario. Consistency , for example, is an\\nimportant property of causal methods that informs us whether the method provably converges to the true\\ncausal graph in the limit of inﬁnite sample size. Quantifying consistency in the test suite is critical when\\ndatasets change from controlled laboratory settings to open-world, and when the application scales. And'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='PoC validation steps are more efﬁcient with MLTRL because the process facilitates early speciﬁcation of the\\nevaluation metric for a causal model in Level 2. Causal models cannot be validated by standard held-out tests,\\nbut rather require randomization or special data collection strategies to evaluate their predictions[ 45,46]. Any\\ndifﬁculty in evaluating the model’s predictions will be caught early and remedied.\\n•Level 6-9 – With the the causal ML components of this technology developed reliably in the previous levels,\\nthe rest of the levels developing this technology focused on general medical-ML deployment challenges. For\\nthe most part, data governance, privacy, and management that was detailed earlier in the neuropathology\\nMLTRL use-case, as well as on-premises deployment.\\nAI for open-source space sciences\\nThe CAMS (Cameras for Allsky Meteor Surveillance) project [ 51], established in 2010 by NASA, uses hundreds of\\noff-the-shelf CCTV cameras to capture the meteor activity in the night sky. Initially, resident scientists would retrieve\\nhard-disks containing video data captured each night and perform manual triangulation of tracks or streaks of light\\nin the night sky, and compute a meteor’s trajectory, orbit, and lightcurve. Each solution was manually classiﬁed as a\\nmeteor or not (i.e., planes, birds, clouds, etc). In 2017, a project run by the Frontier Development Labvii[52], the AI\\naccelerator for NASA and ESA, aimed to automate the data processing pipeline and replicate the scientists thought\\nprocess to build an ML model that identiﬁes meteors in the CAMS project [ 53,54]. The data automation led to\\norders of magnitude improvements in operational efﬁciency of the system, and allowed new contributors and amateur\\nastronomers to start contributing to meteor sightings. Additionally, a novel web tool allowed anybody anywhere to\\nview the meteors detected in the previous night. The CAMS camera system has had six-fold global expansion of the'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='data capture network, discovered ten new meteor showers, contributed towards instrumental evidence of previously\\npredicted comets, and helped calculate parent bodies of various meteor showers. CAMS utilized the MLTRL framework\\nto progress as described:\\n•Level 1 – Understanding the domain and data is a prerequisite for any ML development. Extensive data\\nexploration elucidated visual differences between objects in the night sky such as meteors, satellites, clouds,\\ntail lights of planes, light from the eyes of cats peering into cameras, trees, and other tall objects visible in\\nthe moonlight. This step helped (1) understand visual properties of meteors that later deﬁned the ML model\\narchitecture, and (2) mitigate impact of data imbalance by proactively developing domain-oriented strategies.\\nThe results are well-documented on a datasheet associated with the TRL card, and discussed at the stage\\nreview. This MLTRL documentation forced us to consider data sharing and other privacy concerns at this early\\nconceptualization stage, which is certainly relevant considering CAMS is for open-source and gathering data\\nfrom myriad sources.\\n•Level 2-3 – The agile and non-monotonic (or non-linear) development prescribed by MLTRL allowed the\\nteam to ﬁrst develop an approximate end-to-end pipeline that offered a path to ML model deployment and\\nquick turnaround time to incorporate feedback from the regular gated reviews. Then, with relatively quicker\\nviiThe NASA Frontier Development Lab and partners open-source the code and data via the SpaceML platform: spaceml.org\\n16'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='experimentation, the team could improve on the quality of not just the ML model, but also scale up the systems\\ndevelopment simultaneously in a non-monotonic development cycle.\\n•Level 4 – With the initial pipeline in place, scalable training of baselines and initial models on real challenging\\ndatasets ensued. Throughout the levels, the MLTRL gated reviews were essential for making efﬁcient progress\\nwhile ensuring robustness and functionality that meets stakeholder needs. At this stage we highlight speciﬁc\\nadvantages of the MLTRL review processes that had instrumental effect on the project success: With the\\nrequired panel of mixed ML researchers and engineers, domain scientists, and product managers, the stage 4\\nreviews stressed the signiﬁcance of numerical improvements and comparison to existing baselines, and helped\\nidentify and overcome issues with data imbalance. The team likely would have overlooked these approaches\\nwithout the review from peers in diverse roles and teams. In general, the evolving panel of reviewers at\\ndifferent stages of the project was essential for covering a variety of veriﬁcation and validation measures –\\nfrom helping mitigate data challenges, to open-source code quality.\\n•Level 5 – To complete this R&D-to-productization level, a novel web tool called the NASA CAMS Meteor\\nShower Portalviiiwas created that allowed users to view meteor shower activity from the previous night and\\nverify meteor predictions generated by the ML model. This app development was valuable for A/B testing,\\nvalidating detected meteors and classiﬁed new meteor showers with human-AI interaction, and demonstrating\\nreal-world utility to stakeholders in review. ML processes without MLTRL miss out on these valuable\\ndevelopment by overlooking the need for such a demo tool.\\n•Level 6 – Application development was naturally driven by end-user feedback from the web app in level 5 –'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='without MLTRL it’s unlikely the team would be able to work with early productization feedback. With almost\\nreal time feedback coming in daily, newer methods for improving robustness of meteor identiﬁcation led to\\nresearching and developing a unique augmentation technique, resulting in the state of the art performance of\\nthe ML model. Further application development led to incorporating features that were in demand by users of\\nthe NASA CAMS Meteor Shower Portal: include celestial reference points through constellations, add ability\\nto zoom in/out and (un)cluster showers, and provide tooling for scientiﬁc communication. The coordination of\\nthese features into product-caliber codebase resulted in the release of the NASA CAMS Meteor Shower Portal\\n2.0 that was built by a team of citizen scientists – again we found the speciﬁc checkpoints in the MLTRL\\nreview were crucial for achieving these goals.\\n•Level 7 – Integration was particularly challenging in two ways. First, integrating the ML and data engineering\\ndeliverables with the existing infrastructure and tools of the larger CAMS system, which had started devel-\\nopment years earlier with other teams in partner organizations, required quantiﬁable progress for verifying\\nthe tech-readiness of ML models and modules. The use of technology readiness levels provided a clear and\\nconsistent metric for the maturity of the ML and data technologies, making for clear communication and\\nefﬁcient project integration. Without MLTRL it is difﬁcult to have a conversation, let alone make progress, to-\\nwards integrating AI/ML and data subsystems and components. Second, integrating open-source contributions\\ninto the main ML subsystem was a signiﬁcant challenge alleviated with diligent veriﬁcation and validation\\nmeasures from MLTRL, as well as quantifying robustness with ML testing suites (using scoring measures like\\nthat of the ML Testing Rubric[20], and devising a checklist based on metamorphic testing[18]).'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='that of the ML Testing Rubric[20], and devising a checklist based on metamorphic testing[18]).\\n•Level 8 – CAMS, like many datasets in practice, consisted of a smaller labeled subset and a much larger\\nunlabeled set. In an attempt to additionally increase robustness of the ML subsystem ahead of “ﬂight readiness”,\\nwe looked to active learning [ 55,56] techniques to leverage the unlabeled data. Models using an initial version\\nof this approach, where results of the active learning provided “weak” labels, resulted in consumption of the\\nentire decade long unlabelled data collected by CAMS and slightly higher scores on deployment tests. Active\\nlearning showed to be a promising feature and was switched back to level 7 for further development towards\\nthe next deployment version, so as not to delay the rest of the project.\\n•Level 9 – The ML components in CAMS require continual monitoring for model and data drifts, such as\\nchanges in weather, smoke, and cloud patterns that affect the view of the night sky. The data drifts may also be\\nspeciﬁc to locations, such as ﬁreﬂies and bugs in CAMS Australia and New Zealand stations which appear as\\nfalse positives. The ML pipeline is largely automated with CI/CD, runs regular regression tests, and production\\nof benchmarks. Manual intervention can be triggered when needed, such as sending low conﬁdence meteors for\\nveriﬁcation to scientists in the CAMS project. The team also regularly releases the code, models, and web tools\\non the open-source space sciences and exploration ML toolbox, SpaceMLix. Through the SpaceML community\\nand partner organizations, CAMS continually improves with feature requests, debugging, and improving data\\npractices, while tracking progress with standard software release cycles and MLTRL documentation.\\nviiimeteorshowers.seti.org\\nixspaceml.org\\n17'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='BEYOND SOFTWARE ENGINEERING\\nSoftware engineering (SWE) practices vary signiﬁcantly across domains and industries. Some domains, such as medical\\napplications, aerospace, or autonomous vehicles rely on a highly rigorous development process which is required\\nby regulations. Other domains, for example advertising and e-commerce are not regulated and can employ a lenient\\napproach to development. ML development should at minimum inherit the acceptable software engineering practices of\\nthe domain. There are, however, several key areas where ML development stands out from SWE, adding its own unique\\nchallenges which even most rigorous SWE practices are not able to overcome.\\nFor instance, the behavior of ML systems is learned from data, not speciﬁed directly in code. The data requirements\\naround ML (i.e., data discovery, management, and monitoring) adds signiﬁcant complexity not seen in other types\\nof SWE. There are many beneﬁts to using a data-oriented architecture (DOA) [48] with the data-ﬁrst workﬂows and\\nmanagement practices prescribed in MLTRL. DOA aims to make the data ﬂowing between elements of business logic\\nmore explicit and accessible with a streaming-based architecture rather than the micro-service architectures that are\\nstandard in software systems. One speciﬁc beneﬁt of DOA is making data available and traceable by design, which\\nhelps signiﬁcantly in the ML logging challenges and data governance needs we discussed in Levels 7-9. Moreover,\\nMLTRL highlights data-related requirements along every step to ensure that the development process considers data\\nreadiness and availability.\\nNot to mention an array of ML-speciﬁc failure modes; for example, models that become miscalibrated due to subtle\\ndata distributional shifts in the deployment setting, resulting in models that are more conﬁdent in predictions than they\\nshould be. MLTRL helps deﬁne ML-speciﬁc testing considerations (levels 5 and 7) to help surface these failure-modes'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='early. ML opens up new threat vectors across the whole deployment workﬂow that otherwise aren’t risks in software\\nsystems: for example, a poisoning attack to contaminate the training phase of ML systems, or membership inference\\nto see if a given data record was part of the model’s training. MLTRL consider these threat vectors and suggests\\nrelevant risk-identiﬁcation during prototyping and productization phases. More generally, ML codebases have all the\\nproblems for regular code, plus ML-speciﬁc issues at the system level, mainly as a consequence of added complexity\\nand dynamism. The resulting entanglement, for instance, implies that the SWE practice of making isolated changes is\\noften not feasible – Scully et al.[ 57] refer to this as the “changing anything changes everything” principle. Given this\\nconsideration, typical SWE change-management is insufﬁcient. Furthermore, ML systems almost necessarily increase\\nthe technical debt; package-level refactoring is generally sufﬁcient for removing technical debt in software systems, but\\nthis is not the case in ML systems.\\nThese factors and others suggest that inherited software engineering and management practices of a given domain are\\ninsufﬁcient for the successful development of robust and reliable ML systems. But it is not trading off one for the other:\\nMLTRL can be used in synergy with the existing, industry-standard software engineering practices such as agile [ 58]\\nand waterfall [ 59] to handle unique challenges of ML development. Because ML applications are a category of software,\\nall best practices of building and operating software should be extended when possible to the ML application. Practices\\nlike version control, comprehensive testing, continuous integration and continuous deployment are all applicable to ML\\ndevelopment. MLTRL provides a framework that helps extend SWE building and operating practices that are acceptable\\nin a given domain to tackle the unique challenges of ML development.\\nRELATED WORKS'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='in a given domain to tackle the unique challenges of ML development.\\nRELATED WORKS\\nA recent case study from Microsoft Research [ 40] similarly identiﬁes a few themes describing how ML is not equal to\\nsoftware engineering, and recommends a linear ML workﬂow with steps for data preparation through modeling and\\ndeploying. They deﬁne an effective workﬂow for isolated development of an ML model, but this approach does not\\nensure the technology is actually improving in quality and robustness. Their process should be repeated at progressive\\nstages of development in the broader ML and data technology lifecycle. If applied in the MLTRL framework, the\\nspeciﬁc ingredients of the ML model workﬂow – that is, people, software, tests, objectives, etc. – evolve over time and\\nsubsequent stages as the technologies mature.\\nThere exist many recommended workﬂows for speciﬁc ML methods and areas of pipelines. For instance, a more\\niterative process for Bayesian ML [ 60] and even more speciﬁcally for probabilistic programming [ 39], a data mining\\nprocess deﬁned in 2000 that remains widely used [ 61], others for describing data iterations [ 62], and human-computer\\ninteraction cycles [ 63]. In these recommended workﬂows and others, there’s an important distinction between their\\ncycles and “switchback” mechanisms in MLTRL. Their cycles suggest to generically iterate over a data-modeling-\\nevaluation-deployment process. Switchbacks, on the other hand, are speciﬁc, purpose-driven workﬂows for dialing\\npart(s) of a project to an earlier stage – this doesn’t simply mean go back and train the model on more data, but rather\\nswitching back regresses the technology’s maturity level (e.g. from level 5 to level 3) such that it must again fulﬁll the\\nlevel-by-level requirements, evaluations and reviews. See the Methods section for more details on MLTRL switchbacks.\\n18'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='In general, iteration is an important part of data, ML, and software processes. MLTRL is unique from the other\\nrecommended processes in many ways, and perhaps most importantly because it considers data ﬂows and ML models\\nin the context of larger systems. These isolated processes (that are speciﬁc to e.g. modeling in prototype development\\nor data wrangling in application development) are synergistic with MLTRL because they can be used within each level\\nof the larger lifecycle or framework. For example, the Bayesian modeling processes [ 39,60] we mentioned above\\nare really useful to guide developers of probabilistic ML approaches. But there are important distinctions between\\nexecuting these modeling steps and cycles in a well-deﬁned prototyping environment with curated data and minimal\\nresponsibilities, versus a production environment riddled with sparse and noisy data, that interacts with the physical\\nworld in non-obvious ways, and can carry expensive (even hidden) consequences. MLTRL provides the necessary,\\nholistic context and structure to use these and other development processes reliably and responsibly.\\nAlso related to our work, Google teams have proposed ML testing recommendations [ 20] and validating the data fed\\ninto ML systems [ 64]. For NLP applications, typical ML testing practices struggle to translate to real-world settings,\\noften overestimating performance capabilities. An effective way to address this is devising a checklist of linguistic\\ncapabilities and test types, as in Ribeiro et al.[ 17]–interestingly their test suite was inspired by metamorphic testing,\\nwhich we suggested earlier in Level 7 for testing systems AI integrations. A survey by Paleyes et al. [ 48] go over\\nnumerous case studies to discuss challenges in ML deployment. They similarly pay special attention to the need for\\nethical considerations, end-user trust, and extra security in ML deployments. On the latter point, Kumar et al. [ 65]'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='provide a table thoroughly breaking down new threat vectors across the whole ML deployment workﬂow (some of\\nwhich we mentioned above). These works, notably the ML security measures and the quantiﬁcation of an ML test suite\\nin a principled way – i.e., that does not use misguided heuristics such as code coverage – are valuable to include in any\\nML workﬂow including MLTRL, and are synergistic with the framework we’ve described in this paper. These analyses\\nprovide useful insights, but they do not provide a holistic, regimented process for the full ML lifecycle from R&D\\nthrough deployment. An end-to-end approach is suggested by Raji et al.[ 66], but only for the speciﬁc task of auditing\\nalgorithms; components of AI auditing are mentioned in Level 7, and covered throughout in the review processes.\\nSculley et al.[ 57] go into more ML debt topics such as undeclared consumers and data dependencies, and go on to\\nrecommend an ML Testing Rubric as a production checklist [ 20]. For example, testing models by a canary process\\nbefore serving them into production. This, along with similar shadow testing we mentioned earlier, are common in\\nautonomous ML systems, notably robotics and autonomous vehicles. They explicitly call out tests in four main areas\\n(ML infrastructure, model development, features and data, and monitoring of running ML systems), some of which we\\ndiscussed earlier. For example, tests that the training and serving features compute the same values; a model may train\\non logged processes or user input, but is then served on a live feed with different inputs. In addition to the Google ML\\nTesting Rubric, we advocate metamorphic testing : a SWE methodology for testing a speciﬁc set of relations between\\nthe outputs of multiple inputs. True to the checklists in the Google ML Testing Rubric and in MLTRL, metamorphic\\ntesting for ML can have a codiﬁed list of metamorphic relations[18].'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='testing for ML can have a codiﬁed list of metamorphic relations[18].\\nIn domains such as healthcare there have been the introduction of similar checklists for data readiness – for example,\\nto ensure regulatory-grade real-world-evidence (RWE) data quality [ 67] – yet these are nascent and not yet widely\\naccepted. Applying AI in healthcare has led to developing guidance for regulatory protocol, which is still a work in\\nprogress. Larson et al.[ 68] provide a comprehensive analysis for medical imaging and AI, arriving at several regulatory\\nframework recommendations that mirror what we outline as important measures in MLTRL: e.g., detailed task elements\\nsuch as pitfalls and limitations (surfaced on TRL Cards), clear deﬁnition of an algorithm relative to the downstream\\ntask, deﬁning the algorithm “capability” (Level 5), real-world monitoring, and more.\\nD’amour et al.[ 19] dive into the problem we noted earlier about model miscalibration. They point to the trend in machine\\nlearning to develop models relatively isolated from the downstream use and larger system, resulting in underspeciﬁcation\\nthat handicaps practical ML pipelines. This is largely problematic in deep learning pipelines, but we’ve also noted this\\nrisk in the case of causal inference applications. Suggested remedies include stress tests –empirical evaluations that\\nprobe the model’s inductive biases on practically relevant dimensions–and in general the methods we deﬁne in Level 7.\\nLIMITATIONS, RESPONSIBILITIES, and ETHICS\\nMLTRL has been developed, deployed, iterated, and validated in myriad environments, as demonstrated by the previous\\nexamples and many others. Nonetheless we strongly suggest that MLTRL not be viewed as a cure-all for machine\\nlearning systems engineering. Rather, MLTRL provides mechanisms to better enable ML practitioners, teams, and\\nstakeholders to be diligent and responsible with these technologies and data. That is, one cannot implement MLTRL in'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='an organization and turn a blind eye to the many data, ML, and integration challenges we’ve discussed here. MLTRL is\\nanalogous to a pilot’s checklist, not autopilot.\\n19'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='MLTRL is intended to be complimentary to existing software development methodologies, not replace or alter them.\\nSpeciﬁcally, whether the team uses agile or waterfall methods, MLTRL can be adopted to help deﬁne and structure\\nphases of the project, as well as the success criteria of each stage. In context of the software development process, the\\npurpose of MLTRL is to help the team minimize the technical dept and risk associated with the delivery of an ML\\napplication by helping the development team ask necessary questions.\\nWe discussed many data challenges and approaches in the context of MLTRL, and should highlight again the importance\\nof data considerations in any ML initiative. The data availability and quality can severely limit the ability to develop and\\ndeploy ML, whether MLTRL is used or not. It is again the responsibility of the ML practitioners, teams, and stakeholders\\nto gather, use, and distribute data in safe, legal, ethical ways. MLTRL helps do so with rigor and transparency, but\\nagain is not a solution for data bias. We recommend these recent works on data bias in ML: [ 69,70,71,72,73].\\nFurther, AI/ML ethics is a continuously evolving, multidisciplinary space – see [ 5]. MLTRL aims to prioritize ethics\\nconsiderations at each level of the framework, and would do well to also evolve over time with the broader AI/ML\\nethics developments.\\nCONCLUSION\\nWe’ve described Machine Learning Technology Readiness Levels (MLTRL) , an industry-hardened systems engineering\\nframework for robust, reliable, and responsible machine learning. MLTRL is derived from the processes and testing\\nstandards of spacecraft development, yet lean and efﬁcient for ML, data, and software workﬂows. Examples from\\nseveral organizations across industries demonstrate the efﬁcacy of MLTRL for AI and ML technologies, from research\\nand development through productization and deployment, in important domains such as healthcare and physics, with'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='emphasis on data readiness amongst other critical challenges. Our aim is MLTRL works in synergy with recent\\napproaches in the community focused on diligent data-readiness, privacy and security, and ethics. Even more, MLTRL\\nestablishes a much-needed lingua franca for the AI ecosystem, and broadly for AI in the worlds of science, engineering,\\nand business. Our hope is that our systems framework is adopted broadly in AI and ML organizations, and that\\n“technology readiness levels” becomes common nomenclature across AI stakeholders – from researchers and engineers\\nto sales-people and executive decision-makers.\\nMethods\\nGated reviews\\nAt the end of each stage is a dedicated review period: (1) Present the technical developments along with the requirements\\nand their corresponding veriﬁcation measures and validation steps, (2) make key decisions on path(s) forward (or\\nbackward) and timing, and (3) debrief the processx. As in the gated reviews deﬁned by TRL used by NASA, DARPA, et\\nal., MLTRL stipulates speciﬁc criteria for review at each level, as well as calling out speciﬁc key decision points (noted\\nin the level descriptions above). The designated reviewers will “graduate” the technology to the next level, or provide a\\nlist of speciﬁc tasks that are still needed (ideally with quantitative remarks). After graduation at each level, the working\\ngroup does a brief post-mortem; we ﬁnd that a quick day or two pays dividends in cutting away technical debt and\\nimproving team processes. Regular gated reviews are essential for making efﬁcient progress while ensuring robustness\\nand functionality that meets stakeholder needs. There are several important mechanisms in MLTRL reviews that are\\nspeciﬁcally useful with AI and ML technologies: First, the review panels evolve over a project lifecycle, as noted\\nbelow. Second, MLTRL prescribes that each review runs through an AI ethics checklist deﬁned by the organization; it is'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='important to repeat this at each review, as the review panel and stakeholders evolve considerably over a project lifecycle.\\nAs previously described in the levels deﬁnitions, including ethics reviews as an integral part of early system development\\nis essential for informing model speciﬁcations and avoiding unintended biases or harm[74] after deployment.\\nTRL “Cards”\\nIn Figure 3 we succinctly showcase a key deliverable: TRL Cards . The model cards proposed by Google [ 75] are a useful\\ndevelopment for external user-readiness with ML. On the other hand, our TRL Cards aim to be more information-dense,\\nlike datasheets for medical devices and engineering tools – see the open-source TRL Card repo for examples and\\ntemplates (to be released at github.com/alan-turing-institute). These serve as “report cards” that grow and improve upon\\ngraduating levels, and provide a means of inter-team and cross-functional communication. The content of a TRL Card\\nis roughly in two categories: project info, and implicit knowledge. The former clearly states info such as project owners\\nxMLTRL should include regular debriefs and meta-evaluations such that process improvements can be made in a data-driven,\\nefﬁcient way (rather than an annual meta-review). MLTRL is a high-level framework that each organization should operationalize in\\na way that suits their speciﬁc capabilities and resources.\\n20'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='and reviewers, development status, and semantic versioning–not just for code, also for models and data. In the latter\\ncategory are speciﬁc insights that are typically siloed in the ML development team but should be communicated to\\nother stakeholders: modeling assumptions, dataset biases, corner cases, etc. With the spread of AI and ML in critical\\napplication areas, we are seeing domain expert consortiums deﬁning AI reporting guidelines – e.g., Rivera et al.[ 76]\\ncalling for clinical trials reports for interventions involving AI – which will greatly beneﬁt from the use of our TRL\\nreporting cards. We stress that these TRL Cards are key for the progression of projects, rather than documentation\\nafterthoughts. The TRL Cards thus promote transparency and trust, within teams and across organizations. TRL Card\\ntemplates will be open-sourced upon publication of this work, including methods for coordinating use with other\\nreporting tools such as “Datasheets for Datasets” [24].\\nRisk mitigation\\nIdentifying and addressing risks in a software project is not a new practice. However, akin to the MLTRL roots in\\nspacecraft engineering, risk is a “ﬁrst-class citizen” here. In the deﬁnition of technical and product requirements, each\\nentry has a calculation of the form risk =p(failure )×value , where the value of a component is an integer 1−10.\\nBeing diligent about quantifying risks across the technical requirements is a useful mechanism for ﬂagging ML-related\\nvulnerabilities that can sometimes be hidden by layers of other software. MLTRL also speciﬁes that risk quantiﬁcation\\nand testing strategies are required for sim-to-real development. That is, there is nearly always a non-trivial gap in\\ntransferring a model or algorithm from a simulation testbed to the real world. Requiring explicit sim-to-real testing\\nsteps in the workﬂow helps mitigate unforeseen (and often hazardous) failures. Additionally, comprehensive ML test'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='coverage that we mention throughout this paper is a critical strategy for mitigating risks anduncertainties: ML-based\\nsystem behavior is not easily speciﬁed in advance, but rather depends on dynamic qualities of the data and on various\\nmodel conﬁguration choices[20].\\nNon-monotonic, non-linear paths\\nWe observe many projects beneﬁt from cyclic paths, dialing components of a technology back to a lower level. Our\\nframework not only encourages cycles, we make them explicit with “switchback mechanisms” to regress the maturity\\nof speciﬁc components in an AI system:\\n1.Discovery switchbacks occur as a natural mechanism – new technical gaps are discovered through systems\\nintegration, sparking later rounds of component development[ 77]. These are most common in the R&D levels,\\nfor example moving a component of a proof-of-concept technology (at Level 4) back to proof-of-principle\\ndevelopment (Level 2).\\n2.Review switchbacks result from gated reviews, where speciﬁc components or larger subsystems may be dialed\\nback to earlier levels. This switchback is one of the “key decision points” in the MLTRL project lifecycle\\n(as noted in the Levels deﬁnitions), and is often a decision driven by business-needs and timing rather than\\ntechnical concerns (for instance when mission priorities and funds shift). This mechanism is common from\\nLevel 6/7 to 4, which stresses the importance of this R&D to product transition phase (see Figure 2 (left)).\\n3.Embedded switchbacks are predeﬁned in the MLTRL process. For example, a predeﬁned path from 4 to 2, and\\nfrom 9 to 4. In complex systems, particularly with AI technologies, these built-in loops help mitigate technical\\ndebt and overcome other inefﬁciencies such as noncomprehensive V&V steps.\\nWithout these built-in mechanisms for cyclic development paths, it can be difﬁcult and inefﬁcient to build systems of\\nmodules and components at varying degrees of maturity. Contrary to traditional thought that switchback events should'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='be suppressed and minimized, in fact they represent a natural and necessary part of the complex technology development\\nprocess – efforts to eliminate them may stiﬂe important innovations without necessarily improving efﬁciency. This is\\na fault of the standard monotonic approaches in AI/ML projects, stage-gate processes, and even the traditional TRL\\nframework.\\nIt is also important to note that most projects do not start at Level 0; very few ML companies engage in this low-level\\ntheoretical research. For example, a team looking to use an off-the-shelf object recognition model could start that\\ntechnology at Level 3, and proceed with thorough V&V for their speciﬁc datasets and use-cases. However, no technology\\ncan skip levels after the MLTRL process has been initiated. The industry default (that is, without implementing MLTRL)\\nis to ignorantly take pretrained models, run ﬁne tuning on their speciﬁc data, and jump to deployment, effectively\\nskipping Levels 5 to 7. Additionally, we ﬁnd it is advantageous to incorporate components from other high-TRL ranking\\nprojects while starting new projects; MLTRL makes the veriﬁcation and validation (V&V) steps straightforward for\\nintegrating previously developed ML components.\\n21'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='Evolving people, objectives, and measures\\nAs suggested earlier, much of the practical value of MLTRL comes at the transition between levels. More precisely,\\nMLTRL manages these oft neglected transitions explicitly as evolving teams, objectives, and deliverables. For instance,\\nthe team (or working group) at Level 3 is mostly AI Research Engineers, but at Level 6 is mixed Applied AI/SW\\nEngineers mixed with product managers and designers. Similarly, the review panels evolve from level to level, to match\\nthe changing technology development objectives. What the reviewers reference similarly evolves: notice in the level\\ndeﬁnitions that technical requirements and V&V guide early stages, but at and after Level 6 the product requirements\\nand V&V takeover – naturally, the risk quantiﬁcation and mitigation strategies evolve in parallel. Regarding the\\ndeliverables, notably TRL Cards and risk matrices[ 22] (to rank and prioritize various science, technical, and project\\nrisks), the information develops and evolves over time as the technology matures.\\nQuantiﬁable progress\\nBy deﬁning technology maturity in a quantitative way, MLTRL enables teams to accurately and consistently deﬁne\\ntheir ML progress metrics. Notably industry-standard “objectives and key results” (OKRs) and “key performance\\nindicators” (KPIs) [ 78] can be deﬁned as achieving certain readiness levels in a given period of time; this is a preferable\\nmetric in essentially all ML systems which consist of much more than a single performance score to measure progress.\\nEven more, meta-review of MLTRL progress over multiple projects can provide useful insights at the organization\\nlevel. For example, analysis of the time-per-level and the most frequent development paths/cycles can bring to light\\noperational bottlenecks. Compared to conventional software engineering metrics based on sprint stories and tickets, or\\ntime-tracking tools, MLTRL provides a more accurate analysis of ML workﬂows.\\nCommunication and explanation'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='Communication and explanation\\nA distinct advantage of MLTRL in practice is the nomenclature: an agreed upon grading scheme for the maturity of\\nan AI technology, and a framework for how/when that technology ﬁts within a product or system, enables everyone\\nto communicate effectively and transparently. MLTRL also acts as a gate for interpretability and explainability–at\\nthe granularity of individual models and algorithms, and more crucially from a holistic, systems standpoint. Notably\\nthe DARPA XAIxiprogram advocates for this advance in developing AI technologies; they suggest interpretability\\nand explainability are necessary at various locations in an AI system to be sufﬁcient for deployment as an AI product,\\notherwise leading to issues with ethics and bias.\\nRobustness via uncertainty-aware ML\\nHow to design a reliable system from unreliable components has been a guiding question in the ﬁelds of computing and\\nintelligence [79]. In the case of AI/ML systems, we aim to build reliable systems with myriad unreliable components:\\nnoisy and faulty sensors, human and AI error, and so on. There is thus signiﬁcant value to quantifying the myriad\\nuncertainties, propagating them throughout a system, and arriving at a notion or measure of reliability. For this reason,\\nalthough MLTRL applies generally to AI/ML methods and systems, we advocate for methods in the class of probabilistic\\nML, which naturally represent and manipulate uncertainty about models and predictions[ 28]. These are Bayesian\\nmethods that use probabilities to represent aleatoric uncertainty , measuring the noise inherent in the observations, and\\nepistemic uncertainty , accounting for uncertainty in the model itself (i.e., capturing our ignorance about which model\\ngenerated the data). In the simplest case, an uncertainty aware ML pipeline should quantify uncertainty at the points of\\nsensor inputs or perception, prediction or model output, and decision or end-user action – McAllister et al.[ 29] suggest'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='this with Bayesian deep learning models for safer autonomous vehicle pipelines. We can achieve this sufﬁciently well\\nin practice for simple systems. However, we do not yet have a principled, theoretically grounded, and generalizable way\\nof propagating errors and uncertainties downstream and throughout more complex AI systems – i.e., how to integrate\\ndifferent software, hardware, data, and human components while considering how errors and uncertainties propagate\\nthrough the system. This is an important direction of our future work.\\nReferences\\n[1]Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning\\nthat matters. In AAAI , 2018.\\nxiDARPA Explainable Artiﬁcial Intelligence (XAI)\\n22'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='[2]Arnaud de la Tour, Massimo Portincaso, Kyle Blank, and Nicolas Goeldel. The dawn of the deep tech ecosystem. Technical\\nreport, The Boston Consulting Group, 2019.\\n[3] NASA. The NASA systems engineering handbook. 2003.\\n[4] United States Department of Defense. Defense acquisition guidebook. Technical report, U.S. Dept. of Defense, 2004.\\n[5] D. Leslie. Understanding artiﬁcial intelligence ethics and safety. ArXiv , abs/1906.05684, 2019.\\n[6]Google. Machine learning workﬂow. https://cloud.google.com/mlengine/docs/tensorflow/\\nml-solutions-overview . Accessed: 2020-12-13.\\n[7]Alexander Lavin and Gregory Renard. Technology readiness levels for AI & ML. ICML Workshop on Challenges Deploying\\nML Systems , 2020.\\n[8] T. Dasu and T. Johnson. Exploratory data mining and data cleaning. 2003.\\n[9]M. Janssen, P. Brous, Elsa Estevez, L. Barbosa, and T. Janowski. Data governance: Organizing data for trustworthy artiﬁcial\\nintelligence. Gov. Inf. Q. , 37:101493, 2020.\\n[10] B. Shahriari, Kevin Swersky, Ziyu Wang, R. Adams, and N. D. Freitas. Taking the human out of the loop: A review of bayesian\\noptimization. Proceedings of the IEEE , 104:148–175, 2016.\\n[11] Goutham Ramakrishnan, A. Nori, Hannah Murfet, and Pashmina Cameron. Towards compliant data management systems for\\nhealthcare ml. ArXiv , abs/2011.07555, 2020.\\n[12] Umang Bhatt, Alice Xiang, S. Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura,\\nand P. Eckersley. Explainable machine learning in deployment. Proceedings of the 2020 Conference on Fairness, Accountability,\\nand Transparency , 2020.\\n[13] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and V . Smith. Federated learning: Challenges, methods, and future directions.\\nIEEE Signal Processing Magazine , 37:50–60, 2020.\\n[14] T. Ryffel, Andrew Trask, M. Dahl, Bobby Wagner, J. Mancuso, D. Rueckert, and J. Passerat-Palmbach. A generic framework\\nfor privacy preserving deep learning. ArXiv , abs/1811.04017, 2018.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='for privacy preserving deep learning. ArXiv , abs/1811.04017, 2018.\\n[15] A. Madry, Aleksandar Makelov, Ludwig Schmidt, D. Tsipras, and Adrian Vladu. Towards deep learning models resistant to\\nadversarial attacks. ArXiv , abs/1706.06083, 2018.\\n[16] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. ArXiv , abs/1710.11342, 2018.\\n[17] Marco Túlio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models\\nwith checklist. In ACL, 2020.\\n[18] Xiaoyuan Xie, Joshua W. K. Ho, C. Murphy, G. Kaiser, B. Xu, and T. Chen. Testing and validating machine learning classiﬁers\\nby metamorphic testing. The Journal of systems and software , 84 4:544–558, 2011.\\n[19] Alexander D’Amour, K. Heller, D. Moldovan, Ben Adlam, B. Alipanahi, Alex Beutel, C. Chen, Jonathan Deaton, Jacob\\nEisenstein, M. Hoffman, Farhad Hormozdiari, N. Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, M. Lucic,\\nY . Ma, Cory Y . McLean, Diana Mincu, Akinori Mitani, A. Montanari, Zachary Nado, V . Natarajan, C. Nielson, Thomas F.\\nOsborne, R. Raman, K. Ramasamy, Rory Sayres, J. Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, V . Veitch,\\nMax Vladymyrov, Xuezhi Wang, K. Webster, S. Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley. Underspeciﬁcation\\npresents challenges for credibility in modern machine learning. ArXiv , abs/2011.03395, 2020.\\n[20] Eric Breck, Shanqing Cai, E. Nielsen, M. Salib, and D. Sculley. The ml test score: A rubric for ml production readiness and\\ntechnical debt reduction. 2017 IEEE International Conference on Big Data (Big Data) , pages 1123–1132, 2017.\\n[21] A. Botchkarev. A new typology design of performance metrics to measure errors in machine learning regression algorithms.\\nInterdisciplinary Journal of Information, Knowledge, and Management , 14:045–076, 2019.\\n[22] N. Duijm. Recommendations on the use and design of risk matrices. Safety Science , 76:21–31, 2015.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='[23] Louise Naud and Alexander Lavin. Manifolds for unsupervised visual anomaly detection. ArXiv , abs/2006.11364, 2020.\\n[24] Timnit Gebru, J. Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, H. Wallach, Hal Daumé, and K. Crawford.\\nDatasheets for datasets. ArXiv , abs/1803.09010, 2018.\\n[25] B. Hutchinson, A. Smart, A. Hanna, Emily L. Denton, Christina Greer, Oddur Kjartansson, P. Barnes, and Margaret Mitchell.\\nTowards accountability for machine learning datasets: Practices from software engineering and infrastructure. Proceedings of\\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency , 2021.\\n23'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='[26] P. Schulam and S. Saria. Reliable decision support using counterfactual models. In NIPS 2017 , 2017.\\n[27] Towards trustable machine learning. Nature Biomedical Engineering , 2:709–710, 2018.\\n[28] Zoubin Ghahramani. Probabilistic machine learning and artiﬁcial intelligence. Nature , 521:452–459, 2015.\\n[29] Rowan McAllister, Yarin Gal, Alex Kendall, Mark van der Wilk, A. Shah, R. Cipolla, and Adrian Weller. Concrete problems\\nfor autonomous vehicle safety: Advantages of bayesian deep learning. In IJCAI , 2017.\\n[30] Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I. Avilés-Rivero,\\nChristian Etmann, Cathal McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, Effrossyni Gkrania-Klotsas,\\nJames H. F. Rudd, Evis Sala, and Carola-Bibiane Schönlieb. Common pitfalls and recommendations for using machine learning\\nto detect and prognosticate for covid-19 using chest radiographs and ct scans. Nature Machine Intelligence , 3:199–217, 2021.\\n[31] J. Tobin, Rachel H Fong, Alex Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep\\nneural networks from simulation to the real world. 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems\\n(IROS) , pages 23–30, 2017.\\n[32] Arthur Juliani, Vincent-Pierre Berges, Esh Vckay, Yuan Gao, Hunter Henry, M. Mattar, and D. Lange. Unity: A general\\nplatform for intelligent agents. ArXiv , abs/1809.02627, 2018.\\n[33] Stefan Hinterstoißer, Olivier Pauly, Tim Hauke Heibel, Martina Marek, and Martin Bokeloh. An annotation saved is an\\nannotation earned: Using fully synthetic training for object instance detection. ArXiv , abs/1902.09967, 2019.\\n[34] Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan Hogins, You-Cyuan Jhang, Mohsen Kamalzadeh,\\nBowen Li, Steven Leal, Pete Parisi, Cesar Romero, Wesley Smith, Alex Thaman, Samuel Warren, and Nupur Yadav. Unity'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='perception: Generate synthetic data for computer vision. CoRR , abs/2107.04259, 2021.\\n[35] K. Cranmer, J. Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of the National Academy\\nof Sciences , 117:30055 – 30062, 2020.\\n[36] Jan-Willem van de Meent, Brooks Paige, H. Yang, and Frank Wood. An introduction to probabilistic programming. ArXiv ,\\nabs/1809.10756, 2018.\\n[37] Atilim Günes Baydin, Lei Shao, W. Bhimji, L. Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi,\\nBradley Gram-Hansen, Gilles Louppe, Mingfei Ma, X. Zhao, P. Torr, V . Lee, K. Cranmer, Prabhat, and F. Wood. Etalumis:\\nbringing probabilistic programming to scientiﬁc simulators at scale. Proceedings of the International Conference for High\\nPerformance Computing, Networking, Storage and Analysis , 2019.\\n[38] T. Gleisberg, S. Höche, F. Krauss, M. Schönherr, S. Schumann, F. Siegert, and J. Winter. Event generation with sherpa 1.1.\\nJournal of High Energy Physics , 2009:007–007, 2009.\\n[39] David M. Blei. Build, compute, critique, repeat: Data analysis with latent variable models. 2014.\\n[40] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald C. Gall, Ece Kamar, Nachiappan Nagappan, Besmira\\nNushi, and Thomas Zimmermann. Software engineering for machine learning: A case study. 2019 IEEE/ACM 41st International\\nConference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) , 2019.\\n[41] R. Ambrosino, B. Buchanan, G. Cooper, and Marvin J. Fine. The use of misclassiﬁcation costs to learn rule-based decision\\nsupport models for cost-effective hospital admission strategies. Proceedings. Symposium on Computer Applications in Medical\\nCare , pages 304–8, 1995.\\n[42] Gareth J Grifﬁth, Tim T Morris, Matthew J Tudball, Annie Herbert, Giulia Mancano, Lindsey Pike, Gemma C Sharp, Jonathan\\nSterne, Tom M Palmer, George Davey Smith, et al. Collider bias undermines our understanding of covid-19 disease risk and'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='severity. Nature communications , 11(1):1–12, 2020.\\n[43] J. Pearl. Theoretical impediments to machine learning with seven sparks from the causal revolution. Proceedings of the\\nEleventh ACM International Conference on Web Search and Data Mining , 2018.\\n[44] T. Nguyen, G. Collins, J. Spence, J. Daurès, P. Devereaux, P. Landais, and Y . Le Manach. Double-adjustment in propensity\\nscore matching analysis: choosing a threshold for considering residual imbalance. BMC Medical Research Methodology , 17,\\n2017.\\n[45] D. Eckles and E. Bakshy. Bias and high-dimensional adjustment in observational studies of peer effects. ArXiv , abs/1706.04692,\\n2017.\\n[46] Yanbo Xu, Divyat Mahajan, Liz Manrao, A. Sharma, and E. Kiciman. Split-treatment analysis to rank heterogeneous causal\\neffects for prospective interventions. ArXiv , abs/2011.05877, 2020.\\n24'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='[47] Jonathan G Richens, C. M. Lee, and Saurabh Johri. Improving the accuracy of medical diagnosis with causal machine learning.\\nNature Communications , 11, 2020.\\n[48] Andrei Paleyes, Raoul-Gabriel Urma, and N. Lawrence. Challenges in deploying machine learning: a survey of case studies.\\nArXiv , abs/2011.09926, 2020.\\n[49] V . Chernozhukov, D. Chetverikov, M. Demirer, E. Duﬂo, Christian L. Hansen, Whitney K. Newey, and J. Robins. Dou-\\nble/debiased machine learning for treatment and structural parameters. Econometrics: Econometric & Statistical Methods -\\nSpecial Topics eJournal , 2018.\\n[50] Victor Veitch and Anisha Zaveri. Sense and sensitivity analysis: Simple post-hoc analysis of bias due to unobserved confounding.\\nNeurIPS 2020, arXiv preprint arXiv:2003.01747 , 2020.\\n[51] P. Jenniskens, P.S. Gural, L. Dynneson, B.J. Grigsby, K.E. Newman, M. Borden, M. Koop, and D. Holman. Cams: Cameras for\\nallsky meteor surveillance to establish minor meteor showers. Icarus , 216(1):40 – 61, 2011.\\n[52] Siddha Ganju, Anirudh Koul, Alexander Lavin, J. Veitch-Michaelis, Meher Kasam, and J. Parr. Learnings from frontier\\ndevelopment lab and spaceml - ai accelerators for nasa and esa. ArXiv , abs/2011.04776, 2020.\\n[53] S. Zoghbi, M. Cicco, A. P. Stapper, A. J. Ordonez, J. Collison, P. S. Gural, S. Ganju, J.-L. Galache, and P. Jenniskens. Searching\\nfor long-period comets with deep learning tools. In Deep Learning for Physical Science Workshop, NeurIPS , 2017.\\n[54] Peter Jenniskens, Jack Baggaley, Ian Crumpton, Peter Aldous, Petr Pokorny, Diego Janches, Peter S. Gural, Dave Samuels, Jim\\nAlbers, Andreas Howell, Carl Johannink, Martin Breukers, Mohammad Odeh, Nicholas Moskovitz, Jack Collison, and Siddha\\nGanju. A survey of southern hemisphere meteor showers. Planetary and Space Science , 154:21 – 29, 2018.\\n[55] D. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active learning with statistical models. In NIPS , 1994.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='[56] Y . Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. ArXiv , abs/1703.02910, 2017.\\n[57] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young,\\nJean-François Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. In NIPS , 2015.\\n[58] P. Abrahamsson, Outi Salo, Jussi Ronkainen, and Juhani Warsta. Agile software development methods: Review and analysis.\\nArXiv , abs/1709.08439, 2017.\\n[59] Marco Kuhrmann, Philipp Diebold, Jürgen Münch, Paolo Tell, Vahid Garousi, Michael Felderer, Kitija Trektere, Fergal\\nMcCaffery, Oliver Linssen, Eckhart Hanser, and Christian R. Prause. Hybrid software and system development in practice:\\nwaterfall, scrum, and beyond. Proceedings of the 2017 International Conference on Software and System Process , 2017.\\n[60] Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry,\\nPaul-Christian Burkner, and Martin Modrak. Bayesian workﬂow. ArXiv , abs/2011.01808, 2020.\\n[61] P. Chapman, J. Clinton, R. Kerber, T. Khabaza, T. Reinartz, C. Shearer, and R. Wirth. Crisp-dm 1.0: Step-by-step data mining\\nguide. 2000.\\n[62] Fred Hohman, Kanit Wongsuphasawat, Mary Beth Kery, and Kayur Patel. Understanding and visualizing data iteration in\\nmachine learning. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems , 2020.\\n[63] Saleema Amershi, M. Cakmak, W. B. Knox, and T. Kulesza. Power to the people: The role of humans in interactive machine\\nlearning. AI Mag. , 35:105–120, 2014.\\n[64] Eric Breck, Marty Zinkevich, Neoklis Polyzotis, Steven Euijong Whang, and Sudip Roy. Data validation for machine learning.\\n2019.\\n[65] R. Kumar, David R. O’Brien, Kendra Albert, Salomé Viljöen, and Jeffrey Snover. Failure modes in machine learning systems.\\nArXiv , abs/1911.11034, 2019.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='ArXiv , abs/1911.11034, 2019.\\n[66] Inioluwa Deborah Raji, Andrew Smart, Rebecca White, M. Mitchell, Timnit Gebru, B. Hutchinson, Jamila Smith-Loud, Daniel\\nTheron, and P. Barnes. Closing the ai accountability gap: deﬁning an end-to-end framework for internal algorithmic auditing.\\nProceedings of the 2020 Conference on Fairness, Accountability, and Transparency , 2020.\\n[67] R. Miksad and A. Abernethy. Harnessing the power of real-world evidence (rwe): A checklist to ensure regulatory-grade data\\nquality. Clinical Pharmacology and Therapeutics , 103:202 – 205, 2018.\\n[68] D. B. Larson, Hugh Harvey, D. Rubin, Neville Irani, J. R. Tse, and C. Langlotz. Regulatory frameworks for development and\\nevaluation of artiﬁcial intelligence–based diagnostic imaging algorithms: Summary and recommendations. Journal of the\\nAmerican College of Radiology , 2020.\\n25'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='[69] Ninareh Mehrabi, Fred Morstatter, N. Saxena, Kristina Lerman, and A. Galstyan. A survey on bias and fairness in machine\\nlearning. ACM Computing Surveys (CSUR) , 54:1 – 35, 2019.\\n[70] Eirini Ntoutsi, P. Fafalios, U. Gadiraju, Vasileios Iosiﬁdis, W. Nejdl, Maria-Esther Vidal, S. Ruggieri, F. Turini, S. Papadopoulos,\\nEmmanouil Krasanakis, I. Kompatsiaris, K. Kinder-Kurlanda, Claudia Wagner, F. Karimi, Miriam Fernández, Harith Alani,\\nB. Berendt, Tina Kruegel, C. Heinze, Klaus Broelemann, Gjergji Kasneci, T. Tiropanis, and Steffen Staab. Bias in data-driven\\nai systems - an introductory survey. ArXiv , abs/2001.09762, 2020.\\n[71] E. Jo and Timnit Gebru. Lessons from archives: strategies for collecting sociocultural data in machine learning. Proceedings of\\nthe 2020 Conference on Fairness, Accountability, and Transparency , 2020.\\n[72] J. Wiens, W. Price, and M. Sjoding. Diagnosing bias in data-driven algorithms for healthcare. Nature Medicine , 26:25–26,\\n2020.\\n[73] R. Challen, J. Denny, M. Pitt, L. Gompels, T. Edwards, and K. Tsaneva-Atanasova. Artiﬁcial intelligence, bias and clinical\\nsafety. BMJ Quality & Safety , 28:231 – 237, 2019.\\n[74] Z. Obermeyer, B. Powers, C. V ogeli, and S. Mullainathan. Dissecting racial bias in an algorithm used to manage the health of\\npopulations. Science , 366:447 – 453, 2019.\\n[75] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, In-\\nioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. Proceedings of the Conference on Fairness,\\nAccountability, and Transparency , 2019.\\n[76] Samantha Cruz Rivera, Xiaoxuan Liu, A. Chan, A. K. Denniston, and M. Calvert. Guidelines for clinical trial protocols for\\ninterventions involving artiﬁcial intelligence: the spirit-ai extension. Nature Medicine , 26:1351 – 1363, 2020.\\n[77] Z. Szajnfarber. Managing innovation in architecturally hierarchical systems: Three switchback mechanisms that impact practice.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='IEEE Transactions on Engineering Management , 61:633–645, 2014.\\n[78] H. Zhou and Y . He. Comparative study of okr and kpi. DEStech Transactions on Economics, Business and Management , 2018.\\n[79] J. Neumann. Probabilistic logic and the synthesis of reliable organisms from unreliable components. 1956.\\nAcknowledgements\\nThe authors would like to thank Gur Kimchi, Carl Henrik Ek and Neil Lawrence for valuable discussions about this\\nproject.\\nAuthor contributions statement\\nA.L. conceived of the original ideas and framework, with signiﬁcant contributions towards improving the framework\\nfrom all co-authors. A.L. initiated the use of MLTRL in practice, including the neuropathology test case discussed here.\\nC.G-L. contributed insight regarding causal AI, including the section on counterfactual diagnosis. C.G-L. also made\\nsigniﬁcant contributions broadly in the paper, notably in the Methods descriptions and paper revisions. Si.G. contributed\\nthe spacecraft test case, along with early insights in the framework deﬁnitions. A.V . contributed to the deﬁnition of\\nlater stages involving deployment (as did A.G.), and comparison with traditional software workﬂows. Both E.X. and\\nY .G. provided insights regarding AI in academia, and Y .G. additionally contributed to the uncertainty quantiﬁcation\\nmethods. Su.G. and D.L. contributed the computer vision test case. A.G.B. contributed the particle physics test case,\\nand signiﬁcant reviews of the writeup. A.S. contributed insights related to causal ML and AI ethics. D.N. provided\\nvaluable feedback on the overall framework, and contributed signiﬁcantly with the details on “switchback mechanisms”.\\nS.Z. contributed to multiple paper revisions, with emphasis on clarity and applicability to broad ML users and teams.\\nJ.P. contributed to multiple paper revisions, and to deploying the systems ML methods broadly in practice for Earth and\\nspace sciences. –same goes for C.M., with additional feedback overall on the methods. All co-authors discussed the'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='content and contributed to editing the manuscript.\\nCompeting interests\\nThe authors declare no competing interests.\\nAdditional information\\nCorrespondence and requests for materials should be addressed to A.L.\\n26')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "TECHNOLOGY READINESS LEVELS\n",
      "FOR MACHINE LEARNING SYSTEMS\n",
      "Alexander Lavin∗\n",
      "Pasteur LabsCiarán M. Gilligan-Lee\n",
      "SpotifyAlessya Visnjic\n",
      "WhyLabsSiddha Ganju\n",
      "NvidiaDava Newman\n",
      "MIT\n",
      "Atılım Güne¸ s Baydin\n",
      "University of OxfordSujoy Ganguly\n",
      "Unity AIDanny Lange\n",
      "Unity AIAmit Sharma\n",
      "Microsoft Research\n",
      "Stephan Zheng\n",
      "Salesforce ResearchEric P. Xing\n",
      "PetuumAdam Gibson\n",
      "KonduitJames Parr\n",
      "NASA Frontier Development Lab\n",
      "Chris Mattmann\n",
      "NASA Jet Propulsion LabYarin Gal\n",
      "Alan Turing Institute\n",
      "ABSTRACT\n",
      "The development and deployment of machine learning (ML) systems can be executed easily with\n",
      "modern tools, but the process is typically rushed and means-to-an-end. The lack of diligence can\n",
      "lead to technical debt, scope creep and misaligned objectives, model misuse and failures, and\n",
      "expensive consequences. Engineering systems, on the other hand, follow well-deﬁned processes\n",
      "and testing standards to streamline development for high-quality, reliable results. The extreme is\n",
      "spacecraft systems, where mission critical measures and robustness are ingrained in the development\n",
      "process. Drawing on experience in both spacecraft engineering and ML (from research through\n",
      "product across domain areas), we have developed a proven systems engineering approach for machine\n",
      "learning development and deployment. Our Machine Learning Technology Readiness Levels (MLTRL)\n",
      "framework deﬁnes a principled process to ensure robust, reliable, and responsible systems while\n",
      "being streamlined for ML workﬂows, including key distinctions from traditional software engineering.\n",
      "Even more, MLTRL deﬁnes a lingua franca for people across teams and organizations to work\n",
      "collaboratively on artiﬁcial intelligence and machine learning technologies. Here we describe the\n",
      "framework and elucidate it with several real world use-cases of developing ML methods from basic\n",
      "research through productization and deployment, in areas such as medical diagnostics, consumer\n",
      "computer vision, satellite imagery, and particle physics.\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "computer vision, satellite imagery, and particle physics.\n",
      "Keywords: Machine Learning; Systems Engineering; Data Management; Medical AI; Space Sciences\n",
      "Introduction\n",
      "The accelerating use of artiﬁcial intelligence (AI) and machine learning (ML) technologies in systems of software,\n",
      "hardware, data, and people introduces vulnerabilities and risks due to dynamic and unreliable behaviors; fundamentally,\n",
      "ML systems learn from data, introducing known and unknown challenges in how these systems behave and interact with\n",
      "their environment. Currently the approach to building AI technologies is siloed: models and algorithms are developed\n",
      "in testbeds isolated from real-world environments, and without the context of larger systems or broader products they’ll\n",
      "be integrated within for deployment. A main concern is models are typically trained and tested on only a handful of\n",
      "curated datasets, without measures and safeguards for future scenarios, and oblivious of the downstream tasks and\n",
      "users. Even more, models and algorithms are often integrated into a software stack without regard for the inherent\n",
      "stochasticity –for instance, the massive effect random seeds have on deep reinforcement learning model performance\n",
      "[1] – and failure modes of the ML components, which can be dangerously hidden in layers of software and abstraction.\n",
      "Other domains of engineering, such as civil and aerospace, follow well-deﬁned processes and testing standards to\n",
      "streamline development for high-quality, reliable results. Technology Readiness Level (TRL) is a systems engineering\n",
      "protocol for deep tech[ 2] and scientiﬁc endeavors at scale, ideal for integrating many interdependent components\n",
      "∗lavin@simulation.science\n",
      "Preprint. Under review.arXiv:2101.03989v2  [cs.LG]  29 Nov 2021\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "andcross-functional teams of people. It is no surprise that TRL is standard process and parlance in NASA[ 3] and\n",
      "DARPA[4].\n",
      "For a spaceﬂight project there are several deﬁned phases, from pre-concept to prototyping to deployed operations to\n",
      "end-of-life, each with a series of exacting development cycles and reviews. This is in stark contrast to common machine\n",
      "learning and software workﬂows, which promote quick iteration, rapid deployment, and simple linear progressions. Yet\n",
      "the NASA technology readiness process for spacecraft systems is overkill; we need robust ML technologies integrated\n",
      "with larger systems of software, hardware, data, and humans, but not necessarily for missions to Mars. We aim to bring\n",
      "systems engineering to AI and ML by deﬁning and putting into action a lean Machine Learning Technology Readiness\n",
      "Levels (MLTRL) framework. We draw on decades of AI and ML development, from research through production,\n",
      "across domains and diverse data scenarios: for example, computer vision in medical diagnostics and consumer apps,\n",
      "automation in self-driving vehicles and factory robotics, tools for scientiﬁc discovery and causal inference, streaming\n",
      "time-series in predictive maintenance and ﬁnance.\n",
      "In this paper we deﬁne our framework for developing and deploying robust, reliable, and responsible ML and data\n",
      "systems, with several real test cases of advancing models and algorithms from R&D through productization and\n",
      "deployment, including essential data considerations. Additionally, MLTRL prioritizes the role of AI ethics and\n",
      "fairness, and our systems AI approach can help curb the large societal issues that can result from poorly deployed and\n",
      "maintained AI and ML technologies, such as the automation of systemic human bias, denial of individual autonomy,\n",
      "and unjustiﬁable outcomes (see the Alan Turing Institute Report on Ethical AI [5]). The adoption and proliferation of\n",
      "MLTRL provides a common nomenclature and metric across teams and industries. The standardization of MLTRL\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "across the AI industry should help teams and organizations develop principled, safe, and trusted technologies.\n",
      "Figure 1: MLTRL spans research through prototyping, productization, and deployment. Most ML workﬂows prescribe\n",
      "an isolated, linear process of data processing, training, testing, and serving a model [ 6]. Those workﬂows fail to deﬁne\n",
      "how ML development must iterate over that basic process to become more mature and robust, and how to integrate with\n",
      "a much larger system of software, hardware, data, and people. Not to mention MLTRL continues beyond deployment:\n",
      "monitoring and feedback cycles are important for continuous reliability and improvement over the product lifetime.\n",
      "Results\n",
      "MLTRL deﬁnes technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment.\n",
      "A TRL represents the maturity of a model or algorithmii, data pipelines, software module, or composition thereof; a\n",
      "typical ML system consists of many interconnected subsystems and components, and the TRL of the system is the\n",
      "lowest level of its constituent parts [ 7]. The anatomy of a level is marked by gated reviews, evolving working groups,\n",
      "requirements documentation with risk calculations, progressive code and testing standards, and deliverables such as\n",
      "TRL Cards (Figure 3) and ethics checklists.iiiThese components—which are crucial for implementing the levels in a\n",
      "iiNote we use “model” and “algorithm” somewhat interchangeably when referring to the technology under development. The\n",
      "same MLTRL process and methods apply for a machine translation model and for an A/B testing algorithm, for example.\n",
      "iiiTemplates and examples for MLTRL deliverables will be open-sourced upon publication at github.com/alan-turing-institute.\n",
      "2\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "systematic fashion—as well as MLTRL metrics and methods are concretely described in examples and in the Methods\n",
      "section. Lastly, to emphasize the importance of data tasks in ML, from data curation [ 8] to data governance [ 9], we\n",
      "state several important data considerations at each MLTRL level.\n",
      "MACHINE LEARNING TECHNOLOGY READINESS LEVELS\n",
      "The levels are brieﬂy deﬁned as follows and in Figure 1, and elucidated with real-world examples later.\n",
      "Level 0 - First Principles This is a stage for greenﬁeld AI research, initiated with a novel idea, guiding question, or\n",
      "poking at a problem from new angles. The work mainly consists of literature review, building mathematical foundations,\n",
      "white-boarding concepts and algorithms, and building an understanding of the data – for work in theoretical AI and ML,\n",
      "however, there will not yet be data to work with (for example, a novel algorithm for Bayesian optimization[ 10], which\n",
      "could eventually be used for many domains and datasets). The outcome of Level 0 is a set of concrete ideas with sound\n",
      "mathematical formulation, to pursue through low-level experimentation in the next stage. When relevant, this level\n",
      "expects conclusions about data readiness, including strategies for getting the data to be suitable for the speciﬁc ML task.\n",
      "To graduate, the basic principles, hypotheses, data readiness, and research plans need to be stated, referencing relevant\n",
      "literature. With graduation, a TRL Card should be started to succinctly document the methods and insights thus far –\n",
      "this key MLTRL deliverable is detailed in the Methods section and Figure 3.\n",
      "Level 0 data – Not a hard requirement at this stage because this is largely theoretical machine learning. That being said,\n",
      "data availability needs to be considered for deﬁning any research project to move past theory.\n",
      "Level 0 review – The reviewer here is solely the lead of the research lab or team, for instance a PhD supervisor. We\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "assess hypotheses and explorations for mathematical validity and potential novelty or utility, not necessarily code nor\n",
      "end-to-end experiment results.\n",
      "Level 1 - Goal-Oriented Research To progress from basic principles to practical use, we design and run low-level\n",
      "experiments to analyze speciﬁc model or algorithm properties (rather than end-to-end runs for a performance benchmark\n",
      "score). This involves collection and processing of sample data to train and evaluate the model. This sample data need\n",
      "not be the full data; it may be a smaller sample that is currently available or more convenient to collect. In some\n",
      "cases it may sufﬁce to use synthetic data as the representative sample – in the medical domain, for example, acquiring\n",
      "datasets can take many months due to security and privacy constraints, so generating sample data can mitigate this\n",
      "blocker from early ML development. Further, working with the sample data provides a blueprint for the data collection\n",
      "and processing pipeline (including answering whether it is even possible to collect all necessary data), that can be\n",
      "scaled up for the for the next steps. The experiments, good results or not, and mathematical foundations need to pass a\n",
      "review process with fellow researchers before graduating to Level 2. The application is still speculative, but through\n",
      "comparison studies and analyses we start to understand if/how/where the technology offers potential improvements and\n",
      "utility. Code is research-caliber : The aim here is to be quick and dirty, moving fast through iterations of experiments.\n",
      "Hacky code is okay, and full test coverage is actually discouraged, as long as the overall codebase is organized and\n",
      "maintainable. It is important to start semantic versioning practices early in the project lifecycle, which should cover\n",
      "code, models, anddatasets. This is crucial for retrospectives and reproducibility, issues with which can be costly and\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "severe at later stages. This versioning information and additional progress should be reported on the TRL Card (see for\n",
      "example Figure 3).\n",
      "Level 1 data – At minimum we work with sample data that is representative of downstream real datasets, which can be\n",
      "a subset of real data, synthetic data, or both. Beyond driving low-level ML experiments, the sample data forces us to\n",
      "consider data acquisition and processing strategies at an early stage before it becomes a blocker later.\n",
      "Level 1 review – The panel for this gated review is entirely members of the research team, reviewing for scientiﬁc rigor\n",
      "in early experimentation, and pointing to important concepts and prior work from their respective areas of expertise.\n",
      "There may be several iterations of feedback and additional experiments.\n",
      "Level 2 - Proof of Principle (PoP) Development Active R&D is initiated, mainly by developing and running in\n",
      "testbeds : simulated environments and/or simulated data that closely matches the conditions and data of real scenarios –\n",
      "note these are driven by model-speciﬁc technical goals, not necessarily application or product goals (yet). An important\n",
      "deliverable at this stage is the formal research requirements document (with well-speciﬁed veriﬁcation and validation\n",
      "(V&V) steps)iv. Here is one of several key decision points in the broader process: The R&D team considers several\n",
      "paths forward and sets the course: (A) prototype development towards Level 3, (B) continued R&D for longer-term\n",
      "ivArequirement is a singular documented physical or functional need that a particular design, product, or process aims to satisfy.\n",
      "Requirements aim to specify all stakeholders’ needs while not specifying a speciﬁc solution. Deﬁnitions are incomplete without\n",
      "3\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "research initiatives and/or publications, or some combination of A and B. We ﬁnd the culmination of this stage is often\n",
      "a bifurcation: some work moves to applied AI, while some circles back for more research. This common MLTRL cycle\n",
      "is an instance of the non-monotonic discovery switchback mechanism (detailed in the Methods section).\n",
      "Level 2 data – Datasets at this stage may include publicly available benchmark datasets, semi-simulated data based\n",
      "on the data sample in Level 1, or fully simulated data based on certain assumptions about the potential deployment\n",
      "environments. The data should allow researchers to characterize model properties, and highlight corner cases or\n",
      "boundary conditions, in order to justify the utility of continuing R&D on the model.\n",
      "Level 2 review – To graduate from the PoP stage, the technology needs to satisfy research claims made in previous\n",
      "stages (brought to be bare by the aforementioned PoP data in both quantitative and qualitative ways) with the analyses\n",
      "well-documented and reproducible.\n",
      "Level 3 - System Development Here we have checkpoints that push code development towards interoperability,\n",
      "reliability, maintainability, extensibility, and scalability. Code becomes prototype-caliber : A signiﬁcant step up from\n",
      "research code in robustness and cleanliness. This needs to be well-designed, well-architected for dataﬂow and interfaces,\n",
      "generally covered by unit and integration tests, meet team style standards, and sufﬁciently-documented. Note the\n",
      "programmers’ mentality remains that this code will someday be refactored/scrapped for productization; prototype code\n",
      "is relatively primitive with regard to efﬁciency and reliability of the eventual system. With the transition to Level 4 and\n",
      "proof-of-concept mode, the working group should evolve to include product engineering to help deﬁne service-level\n",
      "agreements and objectives (SLAs and SLOs) of the eventual production system.\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "agreements and objectives (SLAs and SLOs) of the eventual production system.\n",
      "Level 3 data – For the most part consistent with Level 2; in general, the previous level review can elucidate potential\n",
      "gaps in data coverage and robustness to be addressed in the subsequent level. However, for test suites developed at this\n",
      "stage, it is useful to deﬁne dedicated subsets of the experiment data as default testing sources, as well as setup mock\n",
      "data for speciﬁc functionalities and scenarios to be tested.\n",
      "Level 3 review – Teammates from applied AI and engineering are brought into the review to focus on sound software\n",
      "practices, interfaces and documentation for future development, and version control for models and datasets. There are\n",
      "likely domain- or organization-speciﬁc data management considerations going forward that this review should point out\n",
      "– e.g. standards for data tracking and compliance in healthcare [11].\n",
      "Level 4 - Proof of Concept (PoC) Development This stage is the seed of application-driven development; for many\n",
      "organizations this is the ﬁrst touch-point with product managers and stakeholders beyond the R&D group. Thus TRL\n",
      "Cards and requirements documentation are instrumental in communicating the project status and onboarding new\n",
      "people. The aim is to demonstrate the technology in a real scenario: quick proof-of-concept examples are developed to\n",
      "explore candidate application areas and communicate the quantitative and qualitative results. It is essential to use real\n",
      "and representative data for these potential applications. Thus data engineering for the PoC largely involves scaling up\n",
      "the data collection and processing from Level 1, which may include collecting new data or processing all available data\n",
      "using scaled experiment pipelines from Level 3. In some scenarios there will new datasets brought in for the PoC, for\n",
      "example, from an external research partner as a means of validation. Hand-in-hand with the evolution from sample to\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "real data, the experiment metrics should evolve from ML research to the applied setting: proof-of-concept evaluations\n",
      "should quantify model and algorithm performance (e.g., precision and recall and various data splits), computational\n",
      "costs (e.g., CPU vs GPU runtimes), and also metrics that are more relevant to the eventual end-user (e.g., number\n",
      "of false positives in the top-N predictions of a recommender system). We ﬁnd this PoC exploration reveals speciﬁc\n",
      "differences between clean and controlled research data versus noisy and stochastic real-world data. The issues can\n",
      "be readily identiﬁed because of the well-deﬁned distinctions between those development stages in MLTRL, and then\n",
      "targeted for further development.\n",
      "AI ethics processes vary across organizations, but all should engage in ethics conversations at this stage, including ethics\n",
      "of data collection, and potential of any harm or discriminatory impacts due to the model (as the AI capabilities and\n",
      "datasets are known). MLTRL requires ethics considerations to be reported on TRL Cards at all stages, which generally\n",
      "link to an extended ethics checklist. The key decision point here is to push onward with application development or not.\n",
      "It is common to pause projects that pass Level 4 review, waiting for a better time to dedicate resources, and/or pull the\n",
      "technology into a different project.\n",
      "Level 4 data – Unlike the previous stages, having real-world and representative data is critical for the PoC; even with\n",
      "methods for verifying that data distributions in synthetic data reliably mirror those of real data [], sufﬁcient conﬁdence\n",
      "in the technology must be achieved with real-world data of the use-case. Further, one must consider how to obtain\n",
      "corresponding measures for veriﬁcation and validation (V&V). Veriﬁcation: Are we building the product right? Validation: Are we\n",
      "building the right product?\n",
      "4\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "high-quality and consistent data required for the future model inference: generation of the data pipeline PoC that will\n",
      "resemble the future inference pipeline that will take data from intended sources, transform it into features, and send it to\n",
      "the model for inference.\n",
      "Level 4 review – Demonstrate the utility towards one or more practical applications (each with multiple datasets), taking\n",
      "care to communicate assumptions and limitations, and again reviewing data-readiness: evaluating the real-world data\n",
      "for quality, validity, and availability. The review also evaluates security and privacy considerations – deﬁning these in\n",
      "the requirements document with risk quantiﬁcation is a useful mechanism for mitigating potential issues (discussed\n",
      "further in the Methods section).\n",
      "Level 5 - Machine Learning “Capability” At this stage the technology is more than an isolated model or algorithm,\n",
      "it is a speciﬁc capability . For instance, producing depth images from stereo vision sensors on a mobile robot is a\n",
      "real-world capability beyond the isolated ML technique of self-supervised learning for RGB stereo disparity estimation.\n",
      "In many organizations this represents a technology transition or handoff from R&D to productization. MLTRL\n",
      "makes this transition explicit, evolving the requisite work, guiding documentation, objectives and metrics, and team;\n",
      "indeed, without MLTRL it is common for this stage to be erroneously leaped completely, as shown in Figure 2. An\n",
      "interdisciplinary working group is deﬁned, as we start developing the technology in the context of a larger real-world\n",
      "process – i.e., transitioning the model or algorithm from an isolated solution to a module of a larger application. Just as\n",
      "the ML technology should no longer be owned entirely by ML experts, steps have been taken to share the technology\n",
      "with others in the organization via demos, example scripts, and/or an API; the knowledge and expertise cannot remain\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "within the R&D team, let alone an individual ML developer. Graduation from Level 5 should be difﬁcult, as it signiﬁes\n",
      "the dedication of resources to push this ML technology through productization. This transition is a common challenge\n",
      "in deep-tech, sometimes referred to as “the valley of death” because project managers and decision-makers struggle\n",
      "to allocate resources and align technology roadmaps to effectively move to Level 6, 7 and onward. MLTRL directly\n",
      "addresses this challenge by stepping through the technology transition or handoff explicitly.\n",
      "Level 5 data – For the most part consistent with Level 4. However, considerations need to be taken for scaling of data\n",
      "pipelines: there will soon be more engineers accessing the existing data and adding more, and the data will be getting\n",
      "much more use, including automated testing in later levels. With this scaling can come challenges with data governance.\n",
      "The data pipelines likely do not mirror the structure of the teams or broader organization. This can result in data silos,\n",
      "duplications, unclear responsibilities, and missing control of data over its entire lifecycle. These challenges and several\n",
      "approaches to data governance (planning and control, organizational, and risk-based) are detailed in Janssen et al. [9].\n",
      "Level 5 review – The veriﬁcation and validation (V&V) measures and steps deﬁned in earlier R&D stages (namely\n",
      "Level 2) must all be completed by now, and the product-driven requirements (and corresponding V&V) are drafted at\n",
      "this stage. We thoroughly review them here, and make sure there is stakeholder alignment (at the ﬁrst possible step of\n",
      "productization, well ahead of deployment).\n",
      "Level 6 - Application Development The main work here is signiﬁcant software engineering to bring the code up to\n",
      "product-caliber : This code will be deployed to users and thus needs to follow precise speciﬁcations, have comprehensive\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "test coverage, well-deﬁned APIs, etc. The resulting ML modules should be robustiﬁed towards one or more target\n",
      "use-cases. If those target use-cases call for model explanations, the methods need to be built and validated alongside\n",
      "the ML model, and tested for their efﬁcacy in faithfully interpreting the model’s decisions – crucially, this needs to be\n",
      "in the context of downstream tasks and the end-users, as there is often a gap between ML explainability that serves\n",
      "ML engineers rather than external stakeholders[ 12]. Similarly, we need to develop the ML modules with known data\n",
      "challenges in mind, speciﬁcally to check the robustness of the model (and broader pipeline) to changes in the data\n",
      "distribution between development and deployment.\n",
      "The deployment setting(s) should be addressed thoroughly in the product requirements document, as ML serving (or\n",
      "deploying) is an overloaded term that needs careful consideration. First, there are two main types: internal, as APIs\n",
      "for experiments and other usage mainly by data science and ML teams, and external, meaning an ML model that\n",
      "is embedded or consumed within a real application with real users. The serving constraints vary signiﬁcantly when\n",
      "considering cloud deployment vs on-premise or hybrid, batch or streaming, open-source solution or containerized\n",
      "executable, etc. Even more, the data at deployment may be limited due to compliance, or we may only have access to\n",
      "encrypted data sources, some of which may only be accessible locally – these scenarios may call for advanced ML\n",
      "approaches such as federated learning[ 13] and other privacy-oriented ML[ 14]. And depending on the application, an\n",
      "ML model may not be deployable without restrictions; this typically means being embedded in a rules engine workﬂow\n",
      "where the ML model acts like an advisor that discovers edge cases in rules. These deployment factors are hardly\n",
      "considered in model and algorithm development despite signiﬁcant inﬂuence on modeling and algorithmic choices;\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "that said, hardware choices typically are considered early on, such as GPU versus edge devices. It is crucial to make\n",
      "these systems decisions at Level 6–not too early that serving scenarios and requirements are uncertain, and not too late\n",
      "5\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "that corresponding changes to model or application development risk deployment delays or failures. This marks a key\n",
      "decision for the project lifecycle, as this expensive ML deployment risk is common without MLTRL (see Figure 2).\n",
      "Level 6 data – Additional data should be collected and operationalized at this stage towards robustifying the ML\n",
      "models, algorithms, and surrounding components. These include adversarial examples to check local robustness [ 15],\n",
      "semantically-equivalent perturbations to check consistency of the model with respect to domain assumptions [16, 17],\n",
      "and collecting data from different sources and checking how well the trained model generalizes to them. These\n",
      "considerations are even more vital in the challenging deployment domains mentioned above with limited data access.\n",
      "Level 6 review – Focus is on the code quality, the set of newly deﬁned product requirements, system SLA and SLO\n",
      "requirements, data pipelines spec, and an AI ethics revisit now that we are closer to a real-world use-case. In particular,\n",
      "regulatory compliance is mandated for this gated review; the data privacy and security laws are changing rapidly, and\n",
      "missteps with compliance can make or break the project.\n",
      "Level 7 - Integrations For integrating the technology into existing production systems, we recommend the working\n",
      "group has a balance of infrastructure engineers andapplied AI engineers – this stage of development is vulnerable\n",
      "to latent model assumptions and failure modes, and as such cannot be safely developed solely by software engineers.\n",
      "Important tools for them to build together include:\n",
      "•Tests that run use-case speciﬁc critical scenarios and data-slices – a proper risk-quantiﬁcation table will\n",
      "highlight these.\n",
      "•A “golden dataset” should be deﬁned to baseline the performance of each model and succession of models –see\n",
      "the computer vision app example in Figure 4–for use in the continuous integration and deployment (CI/CD)\n",
      "tests.\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "tests.\n",
      "•Metamorphic testing : a software engineering methodology for testing a speciﬁc set of relations between the\n",
      "outputs of multiple inputs. When integrating ML modules into larger systems, a codiﬁed list of metamorphic\n",
      "relations[18] can provide valuable veriﬁcation and validation measures and steps.\n",
      "•Data intervention tests that seek data bugs at various points in the pipelines, downstream to measure the\n",
      "potential effects of data processing and ML on consumers or users of that data, as well as upstream at data\n",
      "ingestion or creation. Rather than using model performance as a proxy for data quality, it is crucial to use\n",
      "intervention tests that instead catch data errors with mechanisms speciﬁc to data validation.\n",
      "These tests in particular help mitigate underspeciﬁcation in ML pipelines, a key obstacle to reliably training models that\n",
      "behave as expected in deployment[ 19]. On the note of reliability, it is important that quality assurance engineers (QA)\n",
      "play a key role here and through Level 9, overseeing data processes to ensure privacy and security, and covering audits\n",
      "for downstream accountability of AI methods.\n",
      "Level 7 data – In addition to the data for test suites discussed above, this level calls for QA to prioritize data governance :\n",
      "how data is obtained, managed, used, and secured by the organization. This was earlier suggested in level 5 (in order to\n",
      "preempt related technical debt), and essential here at the main junction for integration, which may create additional\n",
      "governance challenges in light of downstream effects and consumers.\n",
      "Level 7 review – The review should focus on the data pipelines and test suites; a scorecard like the ML Testing\n",
      "Rubric[ 20] is useful. The group should also emphasize ethical considerations at this stage, as they may be more\n",
      "adequately addressed now (where there are many test suites put into place) rather than close to shipping later.\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "Level 8 - Flight-ready The technology is demonstrated to work in its ﬁnal form and under expected conditions.\n",
      "There should be additional tests implemented at this stage covering deployment aspects, notably A/B tests, blue/green\n",
      "deployment tests, shadow testing, and canary testing, which enable proactive and gradual testing for changing ML\n",
      "methods and data. Ahead of deployment, the CI/CD system should be ready to regularly stress test the overall system\n",
      "and ML components. In practice, problems stemming from real-world data are impossible to anticipate and design for –\n",
      "an upstream data provider could change formats unexpectedly or a physical event could cause the customer behavior to\n",
      "change. Running models in shadow mode for a period of time would help stress test the infrastructure and evaluate how\n",
      "susceptible the ML model(s) will be to performance regressions caused by data. We observe that ML systems with\n",
      "data-oriented architectures are more readily tested in this manner, and better surface data quality issues, data drifts, and\n",
      "concept drifts – this is discussed later in the Beyond Software Engineering section. To close this stage, the key decision\n",
      "is go or no-go for deployment, and when.\n",
      "Level 8 data – If not already in place, there absolutely needs to be mechanisms for automatically logging data\n",
      "distributions alongside model performance once deployed.\n",
      "6\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "Level 8 review – A diligent walkthrough of every technical and product requirement, showing the corresponding\n",
      "validations, and the review panel is representative of the full slate of stakeholders.\n",
      "Level 9 - Deployment In deploying AI and ML technologies, there is signiﬁcant need to monitor the current version,\n",
      "and explicit considerations towards improving the next version. For instance, performance degradation can be hidden\n",
      "and critical, and feature improvements often bring unintended consequences and constraints. Thus at this level, the\n",
      "focus is on maintenance engineering–i.e., methods and pipelines for ML monitoring and updating. Monitoring for data\n",
      "quality, concept drift, and data drift is crucial; no AI system without thorough tests for these can reliably be deployed.\n",
      "By the same token there must be automated evaluation and reporting – if actuals[ 21] are available, continuous evaluation\n",
      "should be enabled, but in many cases actuals come with a delay, so it is essential to record model outputs to allow for\n",
      "efﬁcient evaluation after the fact. To these ends, the ML pipeline should be instrumented to log system metadata, model\n",
      "metadata, and data itself.\n",
      "Monitoring for data quality issues and data drifts is crucial to catch deviations in model behavior, particularly those that\n",
      "are non-obvious in the model or product end-performance. Data logging is unique in the context of ML systems: data\n",
      "logs should capture statistical properties of input features and model predictions, and capture their anomalies. With\n",
      "monitoring for data, concept, and model drifts, the logs are to be sent to the relevant systems, applied, and research\n",
      "engineers. The latter is often non-trivial, as the model server is not ideal for model “observability” because it does not\n",
      "necessarily have the right data points to link the complex layers needed to analyze and debug models. To this end,\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "MLTRL requires the drift tests to be implemented at stages well ahead of deployment, earlier than is standard practice.\n",
      "Again we advocate for data-ﬁrst architectures rather than the software industry-standard design by services (discussed\n",
      "later), which aids in surfacing and logging the relevant data types and slices when monitoring AI systems. For retraining\n",
      "and improving models, monitoring must be enabled to catch training-serving skew and let the team know when to\n",
      "retrain. Towards model improvements, adding or modifying features can often have unintended consequences, such as\n",
      "introducing latencies or even bias. To mitigate these risks, MLTRL has an embedded switchback here: any component\n",
      "or module changes to the deployed version must cycle back to Level 7 (integrations stage) or earlier. Additionally,\n",
      "for quality ML products, we stress a deﬁned communication path for user feedback without roadblocks to R&D; we\n",
      "encourage real-world feedback all the way to research, providing valuable problem constraints and perspectives.\n",
      "Level 9 data – Proper mechanisms for logging and inspecting data (alongside models) is critical for deploying reliable\n",
      "AI and ML – systems that learn on data have unique monitoring requirements (detailed above). In addition to the\n",
      "infrastructure and test suites covering data and environment shifts, it’s important for product managers and other owners\n",
      "to be on top of data policy shifts in domains such as ﬁnance and healthcare.\n",
      "Level 9 review – The review at this stage is unique, as it also helps in lifecycle management: at a regular cadence\n",
      "that depends on the deployed system and domain of use, owners and other stakeholders are to revisit this review and\n",
      "recommend switchbacks if needed (discussed in the Methods section). This additional oversight at deployment is\n",
      "shown to help deﬁne regimented release cycles of updated versions, and provide another “eye” check for stale model\n",
      "performance or other system abnormalities.\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "performance or other system abnormalities.\n",
      "Notice MLTRL is deﬁned as stages or levels, yet much of the value in practice is realized in the transitions: MLTRL\n",
      "enables teams to move from one level to the next reliably and efﬁciently, and provides a guide for how teams and\n",
      "objectives evolve with the progressing technology.\n",
      "Discussion\n",
      "MLTRL is designed to apply to many real-world use-cases involving data and ML, from simple regression models\n",
      "used for predictive modeling energy demand or anomaly detection in datacenters, to real-time modeling in rideshare\n",
      "applications and motion planning in warehouse robotics. For simple use-cases MLTRL may be overkill, and a subset\n",
      "may sufﬁce – for instance, model cards as demonstrated by Google for basic image classiﬁcation. Yet this is a ﬁne line,\n",
      "as the same cards-only approach in the popular “Huggingface” codebases are too simplistic for the language models\n",
      "they represent, deployed in domains that carry signiﬁcant consequences. MLTRL becomes more valuable with more\n",
      "complex, larger systems and environments, especially in risk averse domains. We thoroughly discuss this through\n",
      "several real uses of MLTRL below.\n",
      "7\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "Figure 2: Most ML and AI projects live in these sections of MLTRL, not concerned with fundamental R&D – that is,\n",
      "completely using existing methods and implementations, and even pretrained models. In the left diagram, the arrows\n",
      "show a common development pattern with MLTRL in industry: projects go back to the ML toolbox to develop new\n",
      "features (dashed line), and frequent, incremental improvements are often a practice of jumping back a couple levels to\n",
      "Level 7 (which is the main systems integrations stage). At Levels 7 and 8 we stress the need for tests that run use-case\n",
      "speciﬁc critical scenarios and data-slices, which are highlighted by a proper risk-quantiﬁcation matrix [ 22]. Cycling\n",
      "back to previous lower levels is not just a late-stage mechanism in MLTRL, but rather “switchbacks” occur throughout\n",
      "the process (as discussed in the Methods section and throughout the text). In the right diagram we show the more\n",
      "common approach in industry ( without using our framework), which skips essential technology transition stages – ML\n",
      "Engineers push straight through to deployment, ignoring important productization and systems integration factors. This\n",
      "will be discussed in more detail in the Methods section.\n",
      "EXAMPLES\n",
      "Human-machine visual inspection\n",
      "While most ML projects begin with a speciﬁc task and/or dataset, there are many that originate in ML theory without\n",
      "any target application – i.e., projects starting MLTRL at level 0 or 1. These projects nicely demonstrate the utility of\n",
      "MLTRL built-in switchbacks, bifurcating paths, and iteration with domain experts. An example we discuss here is a\n",
      "novel approach to representing data in generative vision models from Naud & Lavin[ 23], which was then developed into\n",
      "state-of-the-art unsupervised anomaly detection, and targeted for two human-machine visual inspection applications:\n",
      "First, industrial anomaly detection, notably in precision manufacturing, to identify potential errors for human-expert\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "manual inspection. Second, using the model to improve the accuracy and efﬁciency of neuropathology, the microscopic\n",
      "examination of neurosurgical specimens for cancerous tissue. In these human-machine teaming use-cases there are\n",
      "speciﬁc challenges impeding practical, reliable use:\n",
      "•Hidden feedback loops can be common and problematic in real-world systems inﬂuencing their own training\n",
      "data: over time the behavior of users may evolve to select data inputs they prefer for the speciﬁc AI system,\n",
      "representing some skew from the training data. In this neuropathology case, selecting whole-slide images that\n",
      "are uniquely difﬁcult for manual inspection, or even biased by that individual user. Similarly we see underlying\n",
      "healthcare processes can act as hidden confounders, resulting in unreliable decision support tools[26].\n",
      "•Model availability can be limited in many deployment settings: for example, on-premises deployments\n",
      "(common in privacy preserving domains like healthcare and banking), edge deployments (common in industrial\n",
      "use-cases such as manufacturing and agriculture), or from the infrastructure’s inability to scale to the volume\n",
      "of requests. This can severely limit the team’s ability to monitor, debug, and improve deployed models.\n",
      "•Uncertainty estimation is valuable in many AI scenarios, yet not straightforward to implement in practice.\n",
      "This is further complicated with multiple data sources and users, each injecting generally unknown amounts of\n",
      "noise and uncertainties. In medical applications it is of critical importance, to provide measures of conﬁdence\n",
      "and sensitivity, and for AI researchers through end-users. In anomaly detection, various uncertainty measures\n",
      "can help calibrate the false-positive versus false-negative rates, which can be very domain speciﬁc.\n",
      "8\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "Figure 3: The maturity of each ML technology is tracked via TRL Cards , which we describe in the Methods section.\n",
      "Here is an example reﬂecting a neuropathology machine vision use-case[ 23], detailed in the Discussion Section. Note\n",
      "this is a subset of a full TRL Card, which in reality lives as a full document in an internal wiki. Notice the card\n",
      "clearly communicates the data sources, versions, and assumptions. This helps mitigate invalid assumptions about\n",
      "performance and generalizability when moving from R&D to production, and promotes the use of real-world data\n",
      "earlier in the project lifecycle. We recommend documenting datasets thoroughly with semantic versioning and tools\n",
      "such as datasheets for datasets [24], and following data accountability best-practices as they evolve (see [25]).\n",
      "•Costs of edge cases can be signiﬁcant, sometimes risking expensive machine downtime or medical failures.\n",
      "This is exacerbated in anomaly detection anomalies are by deﬁnition rare so they can be difﬁcult to train for,\n",
      "especially for the anomalies that are completely unseen until they arise in the wild.\n",
      "•End-user trust can be difﬁcult to achieve, often preventing the adoption of ML applications, particularly in\n",
      "the healthcare domain and other highly regulated industries.\n",
      "These and additional ML challenges such as data privacy and interpretability can inhibit ML adoption in clinical practice\n",
      "and industrial settings, but can be mitigated with MLTRL processes. We’ll describe how in the context of the Naud\n",
      "& Lavin[ 23] example, which began at level 0 with theoretical ML work on manifold geometries, and at level 5 was\n",
      "directed towards specialized human-machine teaming applications utilizing the same ML method under-the-hood.\n",
      "•Levels 0-1 – From open-ended exploration of data-representation properties in various Riemmanian manifold\n",
      "curvatures, we derived from ﬁrst principles and empirically identiﬁed a property with hyperbolic manifolds:\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "when used as a latent space for embedding data without labels, the geometry organizes the data by it’s implicit\n",
      "hierarchical structure. Unsupervised computer vision was identiﬁed in reviews as a promising direction for\n",
      "proof-of-principle work.\n",
      "•Level 2 – One approach for validating the earlier theoretical developments was to generate synthetic data to\n",
      "isolate very speciﬁc features in data we would expect represented in the latent manifold. The results showed\n",
      "promise for anomaly detection – using the latent representation of data to automatically identify images that\n",
      "are out-of-the-ordinary (anomalous), and also using the manifold to inspect how they are semantically different.\n",
      "Further, starting with an implicitly probabilistic modeling approach implied uncertainty estimation could be\n",
      "a valuable feature downstream. This made the level 2 key decision point clear: proceed with applied ML\n",
      "development.\n",
      "•Levels 3-5 – Proof-of-concept development and reviews demonstrated promise for several commercial appli-\n",
      "cations relevant to the business, and also highlighted the need for several key features (deﬁned as R&D and\n",
      "product requirements): interpretability (towards end-user trust), uncertainty quantiﬁcation (to show conﬁdence\n",
      "scores), and human-in-the-loop (for domain expertise). Without the MLTRL PoC steps and review processes,\n",
      "these features can often be delayed until beta testing or overlooked completely – for example, the failures of\n",
      "applying IBM Watson in medical applications [ 27]. For this technology, the applications to develop towards\n",
      "are anomaly detection in histopathology and manufacturing, speciﬁcally inspecting whole-slide images of\n",
      "neural tissue, and detecting defects in metallic surfaces, respectively.\n",
      "9\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "From the systems perspective, we suggest quantifying the uncertainties of components and propagating them\n",
      "through the system, which can improve safety and trust. Probabilistic ML methods, rooted in Bayesian\n",
      "probability theory, provide a principled approach to representing and manipulating uncertainty about models\n",
      "and predictions[ 28]. For this reason we advocate strongly for probabilistic models and algorithms in AI\n",
      "systems. In this machine vision example, the MLTRL technical requirements speciﬁcally called for a\n",
      "probabilistic generative model to readily quantify various types of uncertainties and propagate them forward to\n",
      "the visualization component of the pipeline, and the product requirements called for the downstream conﬁdence\n",
      "and sensitivity measures to be exposed to the end-user. Component uncertainties must be assembled in a\n",
      "principled way to yield a meaningful measure of overall system uncertainty, based on which safe decisions can\n",
      "be made[29]. See the Methods section for more on uncertainty in AI systems.\n",
      "The early checks for data management and governance proved valuable here, as the application areas dealt\n",
      "with highly sensitive data that would signiﬁcantly inﬂuence the design of data pipelines and test suites. In\n",
      "both the neuropathology and manufacturing applications, the data management checks also raised concerns\n",
      "about hidden feedback loops, where users may unintentionally skew the data inputs when using the anomaly\n",
      "detection models in practice, for instance biasing the data towards speciﬁc subsets they subjectively need help\n",
      "with. Incorporating domain experts this early in the project lifecycle helped inform veriﬁcation and validation\n",
      "steps to help be robust to the hidden feedback loops. Not to mention their input guided us towards user-centric\n",
      "metrics for performance, which can often skew from ML metrics in important ways – for instance, the typical\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "acceptance ratio for false positives versus false negatives doesn’t apply to select edge cases, for which our\n",
      "hierarchical anomaly classiﬁcation scheme was useful [23].\n",
      "From prior reviews and TRL card documentation, we also identiﬁed the value of synthetic data generation\n",
      "into application development: anomalies are by deﬁnition rare so they are hard to come by in real datasets,\n",
      "especially with evolving environments in deployment settings, so the ability to generate synthetic datasets for\n",
      "anomaly detection can accelerate the level 6-9 pipeline, and help ensure more reliable models in the wild.\n",
      "•Level 6 (medical) – The medical inspection application experienced a bifurcation with product work proceed-\n",
      "ing while additional R&D was desired to explore improved data processing methods, while engaging with\n",
      "clinicians and medical researchers for feedback. Proceeding through the levels in a non-linear, non-monotonic\n",
      "way is common in MLTRL and encouraged by various switchback mechanisms (detailed in the Methods\n",
      "section). These practices – intentional switchbacks, frequent engagement with domain experts and users – can\n",
      "help mitigate methodological ﬂaws and underlying biases that are common when applying ML to clinical\n",
      "applications. For instance, recent work by Roberts et al. [ 30] investigated 2,122 studies applying ML to\n",
      "COVID-19 use-cases, ﬁnding that none of the models are sufﬁcient for clinical use due to methodological ﬂaws\n",
      "and/or underlying biases. They go on to give many recommendations – some we’ve discussed in the context of\n",
      "MLTRL, and more – which should be reviewed for higher quality medical-ML models and documentation.\n",
      "•Level 6-9 (manufacturing) – Overall these stages proceeded regularly and efﬁciently for the defect detection\n",
      "product. MLTRL’s embedded switchback from level 9 to 4 proved particularly useful in this lifecycle, both\n",
      "for incorporating feedback from the ﬁeld and for updating with research progress. On the former, the data\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "distribution shifts from one deployment setting to another signiﬁcantly affected false-positive versus false-\n",
      "negative calibrations, so this was added as a feature to the CI/CD pipelines. On the latter, the built-in touch\n",
      "points for real-world feedback and data into the continued ML research provided valuable constraints to\n",
      "help guide research, and product managers could readily understand what capabilities could be available for\n",
      "product integration and when (readily communicated with TRL Cards) – for instance, later adding support for\n",
      "video-based inspection for defects, and tooling for end-users to reason about uncertainty estimates (which\n",
      "helps establish trust).\n",
      "•Level 7-9 (medical) – For productization the “neuropathology copilot” was handed off to a partner pharmaceu-\n",
      "tical company to integrate into their existing software systems. The MLTRL documentation and communication\n",
      "streamlined the technology transfer, which can often by a time-consuming manual process. If not pursuing\n",
      "this path, the product would’ve likely faced many of the medical-ML deployment challenges with model\n",
      "availability and data access; MLTRL cannot overcome the technical challenges of deploying on-premises, but\n",
      "the manifestation of those challenges as performance regressions, data shifts, privacy and ethics concerns, etc.\n",
      "can be mitigated by the system-level checks and strategies MLTRL puts forth.\n",
      "Computer vision with real and synthetic data\n",
      "Advancements in physics engines and graphics processing have advanced AI environment and data-generation capabili-\n",
      "ties, putting increased emphasis on transitioning models across the simulation-to-reality gap [ 31,32,33]. To develop a\n",
      "computer vision application for automated recycling, we leveraged the Unity Perception [ 34] package, a toolkit for\n",
      "generating large-scale datasets for perception-based ML training and validation. We produced synthetic images to\n",
      "10\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "\u0015A?U?HEJC\u0003?H=OOEBE?=PEKJ\u0003LELAHEJA\u0003\n",
      " =¡ >¡\n",
      ",V\u0003PRGHO\u0003FRQğGHQFH\u0003\u001f\u0003WKUHVKROG\",V\u0003GHWHFWHG\u0003REMHFW\u0003LQ\u0003WDUJHW\u0003VHW\"3URYLGH\u0003FRUUHVSRQGLQJ\u0003UHF\\FOLQJ\u0003LQVWUXFWLRQV,QLWLDWH\u0003KXPDQ\u0010\u0003LQ\u0010WKH\u0010ORRS\u0003SURWRFRO12<(6<(6Figure 4: Computer vision pipeline for an automated recycling application (a), which contains multiple ML models,\n",
      "user input, and image data from various sources. Complicated logic such as this can mask ML model performance lags\n",
      "and failures, and also emphasized the need for R&D-to-product hand off described in MLTRL. Additional emphasis is\n",
      "placed on ML tests that consider the mix of real-world data with user annotations (b, right) and synthetic data generated\n",
      "by Unity AI’s Perception tool and structured domain randomization (b, left).\n",
      "complement real-world data sources (Figure 4). This application exempliﬁes three important challenges in ML product\n",
      "development that MLTRL helps overcome:\n",
      "•Multiple and disparate data sources are common in deployed ML pipelines yet often ignored in R&D.\n",
      "For instance, upstream data providers can change formats unexpectedly, or a physical event could cause the\n",
      "customer behavior to change. It is nearly impossible to anticipate and design for all potential problems with\n",
      "real-world data and deployment. This computer vision system implemented pipelines and extended test suites\n",
      "to cover open-source benchmark data, real user data, and synthetic data.\n",
      "•Hidden performance degradation can be challenging to detect and debug in ML systems because gradual\n",
      "changes in performance may not be immediately visible. Common reasons for this challenge are that the\n",
      "ML component may be one step in a series. Additionally, local/isolated changes to an ML component’s\n",
      "performance may not directly affect the observed downstream performance. We can see both issues in the\n",
      "illustrated logic diagram for the automated recycling app (Figure 4). A slight degradation in the initial CV\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "model may not heavily inﬂuence the following user input. However, when an uncommon input image appears\n",
      "in the future, the app fails altogether.\n",
      "•Model usage requirements can make or break an ML product. For example, the Netﬂix “$1M Prize” solution\n",
      "was never fully deployed because of signiﬁcant engineering costs in real-world scenariosv. For example,\n",
      "engineering teams must communicate memory usage, compute power requirements, hardware availability,\n",
      "network privacy, and latency to the ML teams. ML teams often only understand the statistics or ML theory\n",
      "behind a model but not the system requirements or how it scales.\n",
      "We next elucidate these challenges and how MLTRL helps overcome them in the context of this project’s lifecycle. This\n",
      "project started at level 4, using largely existing ML methods with a target use-case.\n",
      "•Level 4 – For this project, we validated most of the components in other projects. Speciﬁcally, the computer\n",
      "vision (CV) model for object recognition and classiﬁcation was an off-the-shelf model. The synthetic data\n",
      "generation method used Unity Perception, a well-established open-source project. Though this allowed us to\n",
      "vnetﬂixtechblog.com/netﬂix-recommendations-beyond-the-5-stars-part-1\n",
      "11\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "skip the earlier levels, many challenges arise when combining ML elements that were independently validated\n",
      "and developed. The MLTRL prototype-caliber code checkpoint ensures that the existing code components\n",
      "are validated and helps avoid poorly deﬁned borders and abstractions between components. ML pipelines\n",
      "often grow out of glue code, and our regimented code checkpoints motivate well-architected software that\n",
      "minimizes these danger spots.\n",
      "•Level 5 – The problematic “valley of death”, mentioned earlier in the level 5 deﬁnitions, is less prevalent in use-\n",
      "cases like this that start at a higher MLTRL level with a speciﬁc product deliverable. In this case, the product\n",
      "deliverable was a real-time object recognition and classiﬁcation of trash for a mobile recycling application.\n",
      "Still, this stage is critical for the requirements and V&V transition. This stage mitigates failure risks due to the\n",
      "disparate data sources integrated at various steps in this CV system and accounted for the end-user compute\n",
      "constraints for mobile computing. Speciﬁcally, the TRL cards from earlier stages surfaced potential issues\n",
      "with imbalanced datasets and the need for speciﬁc synthetic images. These considerations are essential for the\n",
      "data readiness and testing V&V in the productization requirements. Data quality and availability issues often\n",
      "present huge blockers because teams discover them too late in the game. Data-readiness is one class of many\n",
      "example issues teams face without MLTRL, as depicted in Fig. 2.\n",
      "•Level 6 – We were re-using a well-understood model and deployment pipeline in this use-case, meaning our\n",
      "primary challenge was around data reliability. For the problem of recognizing and classifying trash, building a\n",
      "reliable data source using only real data is almost impossible due to diversity, class imbalance, and annotation\n",
      "challenges. Therefore we chose to develop a synthetic data generator to create training data. At this MLTRL\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "level, we needed to ensure that the synthetic data generator created sufﬁciently diverse data and exposed the\n",
      "controls needed to alter the data distribution in production. Therefore, we carefully exposed APIs using the\n",
      "Unity Perception package, which allowed us to control lighting, camera parameters, target and non-target\n",
      "object placements and counts, and background textures. Additionally, we ensured that the object labeling\n",
      "matched the real-world annotator instructions and that output data formats matched real-world counterparts.\n",
      "Lastly, we established a set of statistical tests to compare synthetic and real-world data distributions. The\n",
      "MLTRL checks ensured that we understood, and in this case, adequately designed our data sources to meet\n",
      "in-production requirements.\n",
      "•Level 7 – From the previous level’s R&D TRL cards and observations, we knew relatively early in produc-\n",
      "tization that we would need to assume bias for the real data sources due to class imbalance and imperfect\n",
      "annotations. Therefore we designed tests to monitor these in the deployed application. MLTRL imposes these\n",
      "critical deployment tests well ahead of deployment, where we can easily overlook ML-speciﬁc failure modes.\n",
      "•Level 8 – As we suggested earlier, problems that stem from real-world data are near impossible to anticipate\n",
      "and design for, implying the need for level 8 ﬂight-readiness preparations. Given that we were generating\n",
      "synthetic images (with structured domain randomization) to complement the real data, we created tests for\n",
      "different data distribution shifts at multiple points in the classiﬁcation pipeline. We also implemented thorough\n",
      "shadow tests ahead of deployment to evaluate how susceptible the ML model(s) to performance regressions\n",
      "caused by data. Additionally, we also implemented these as CI/CD tests over various deployment scenarios (or\n",
      "mobile device computing speciﬁcations). Without these fully covered, documented, and automated, it would\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "be impossible to pass level 8 review and deploy the technology.\n",
      "•Level 9 – Post-deployment, the monitoring tests prescribed at Levels 8 and 9 and the three main code quality\n",
      "checkpoints in the MLTRL process help surface hidden performance degradation problems, common with\n",
      "complex pipelines of data ﬂows and various models. The switchbacks depicted in Fig. 2 are typical in CV\n",
      "use-cases. For instance, miscalibrations in models pre-trained on synthetic data and ﬁne-tuned on newer real\n",
      "data can be common yet difﬁcult to catch. However, the level 7 to 4 switchback is designed precisely for these\n",
      "challenges and product improvements.\n",
      "Accelerating scientiﬁc discovery with massive particle physics simulators\n",
      "Computational models and simulation are key to scientiﬁc advances at all scales, from particle physics, to material\n",
      "design and drug discovery, to weather and climate science, and to cosmology[ 35]. Many simulators model the forward\n",
      "evolution of a system (coinciding with the arrow of time), such as the interaction of elementary particles, diffusion of\n",
      "gasses, folding of proteins, or evolution of the universe in the largest scale. The task of inference refers to ﬁnding initial\n",
      "conditions or global parameters of such systems that can lead to some observed data representing the ﬁnal outcome\n",
      "of a simulation. In probabilistic programming[ 36], this inference task is performed by deﬁning prior distributions\n",
      "over any latent quantities of interest, and obtaining posterior distributions over these latent quantities conditioned\n",
      "on observed outcomes (for example, experimental data) using Bayes rule. This process, in effect, corresponds to\n",
      "inverting the simulator such that we go from the outcomes towards the inputs that caused the outcomes. In the\n",
      "“Etalumis” project[ 37] (“simulate” spelled backwards), we are using probabilistic programming methods to invert\n",
      "12\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "existing, large-scale simulators via Bayesian inference. The project is as an interdisciplinary collaboration of specialists\n",
      "in probabilistic machine learning, particle physics, and high-performance computing, all essential elements to achieve\n",
      "the project outcomes. Even more, it is a multi-year project spanning multiple countries, companies, university labs, and\n",
      "government research organizations, bringing signiﬁcant challenges in project management, technology coordination\n",
      "and validation. Aided by MLTRL, there were several key challenges to overcome in this project that are common in\n",
      "scientiﬁc-ML projects:\n",
      "•Integrating with legacy systems is common in scientiﬁc and industrial use-cases, where ML methods are\n",
      "applied with existing sensor networks, infrastructure, and codebases. In this case, particle physics domain\n",
      "experts at CERN are using the SHERPA simulator[ 38], a 1 million line codebase developed over the last\n",
      "two decades. Rewriting the simulator for ML use-cases is infeasible due to the codebase size and buried\n",
      "domain knowledge, and new ML experts would need signiﬁcant onboarding to gain working knowledge of\n",
      "the codebase. It is also common to work with legacy data infrastructure, which can be poorly organized for\n",
      "machine learning (let alone preprocessed and clean) and unlikely to have followed best practices such as\n",
      "dataset versioning.\n",
      "•Coupling hardware and software architectures is non-trivial when deploying ML at scale, as performance\n",
      "constraints are often considered in deployment tests well after model and algorithm development, not to\n",
      "mention the expertise is often split across disjoint teams. This can be exacerbated in scientiﬁc-ML when\n",
      "scaling to supercomputing infrastructure, and working with massive datasets that can be in the terabytes and\n",
      "petabytes.\n",
      "•Interpretability is often a desired feature yet difﬁcult to deliver and validate in practice. Particularly in\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "scientiﬁc ML applications such as this, mechanisms and tooling for domain experts to interpret predictions\n",
      "and models are key for usability (integrating in workﬂows and building trust).\n",
      "To this end, we will go through the MLTRL levels one by one, demonstrating how they ensure the above scientiﬁc ML\n",
      "challenges are diligently addressed.\n",
      "•Level 0 – The theoretical developments leading to Etalumis are immense and well discussed in Baydin et\n",
      "al [37]. In particular the ML theory and methods are in a relatively nascent area of ML and mathematics,\n",
      "probabilistic programming. New territory can present more challenges compared to well-traveled research\n",
      "paths, for instance in computer vision with neural networks. It is thus helpful to have a guiding framework\n",
      "when making a new path in ML research, such as MLTRL where early reviews help theoretical ML projects\n",
      "get legs.\n",
      "•Level 1-2 – Running low-level experiments in simple testbeds is generally straightforward when working\n",
      "with probabilistic programming and simulation; in a sense, this easy iteration over experiments is what\n",
      "PPL are designed for. It was additionally helpful in this project to have rich data grounded in physical\n",
      "constraints, allowing us to better isolate model behaviors (rather than data assumptions and noise). The\n",
      "MLTRL requirements documentation is particularly useful for the standard PPL experimentation workﬂow:\n",
      "model, infer, criticize, repeat (or Box’s loop) [ 39]. The evaluation step (i.e. criticizing the model) can be\n",
      "more nuanced than checking summary statistics as in deep learning and similar ML workﬂows. It is thus a\n",
      "useful practice to write down the criticism methods, metrics, and expected results as veriﬁcations for speciﬁc\n",
      "research requirements, rather than iterating over Box’s loop without a priori targets. Further, because this\n",
      "research project had a speciﬁc target application early in the process (the SHERPA simulator), the project\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "timeline beneﬁted from recognizing simulator-integration constraints upfront as requirements, not to mention\n",
      "data availability concerns, which are often overlooked in early R&D levels. It was additionally useful to have\n",
      "CERN scientists as domain experts in the reviews at these R&D levels.\n",
      "•Level 3 – Systems development can be challenging with probabilistic programming, again because it is\n",
      "relatively nascent and much of the out-of-the-box tools and infrastructure are not there as in most ML and\n",
      "deep learning. Here in particular there’s a novel (unproven) approach for systems integration: a probabilistic\n",
      "programming execution protocol was developed to reroute random number draws in the stochastic simulator\n",
      "codebase (SHERPA) to the probabilistic programming system, thus enabling the system to control stochastic\n",
      "choices in SHERPA and run inference on its execution traces, all while keeping the legacy codebase intact! A\n",
      "more invasive method that modiﬁes SHERPA would not have been acceptable. If it were not for MLTRL forcing\n",
      "systems considerations this early in the Etalumis project lifecycle, this could have been an insurmountable\n",
      "hurdle later when multiple codebases and infrastructures come into play. By the same token, systems planning\n",
      "here helped enable the signiﬁcant HPC scaling later: the team deﬁned the need for HPC support well ahead\n",
      "of actually running HPC, in order to build the prototype code in a way that would readily map to HPC (in\n",
      "addition to local or cloud CPU and GPU). The data engineering challenges in this system’s development\n",
      "13\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "nonetheless persist – that is, data pipelines and APIs that can integrate various sources and infrastructures, and\n",
      "normalize data from various databases – although MLTRL helps consider these at the an earlier stage that can\n",
      "help inform architecture design.\n",
      "•Level 4 – The natural “embedded switchback” from Level 4 to 2 (see the Methods section) provided an efﬁcient\n",
      "path toward developing an improved, amortized inference method–i.e., using a computationally expensive\n",
      "deep learning based inference algorithm to train only once, in order to then do fast, repeated inference in the\n",
      "SHERPA model. Leveraging cyclic R&D methods, the Etalumis project could iteratively improve inference\n",
      "methods without stalling the broader system development, ultimately producing the largest scale posterior\n",
      "inference in a Turing-complete probabilistic programming system. Achieving this scale through iterative R&D\n",
      "along the main project lifecycle was additionally enabled by working with with NERSC engineers and their\n",
      "Cori supercomputer to progressively scale smaller R&D tests to the goal supercomputing deployment scenario.\n",
      "Typical ML workﬂows that follow simple linear progressions[ 6,40] would not enable ramping up in this\n",
      "fashion, and can actual prevent scaling R&D to production due to lack of systems engineering processes (like\n",
      "MLTRL) connecting research to deployment.\n",
      "•Level 5 – Multi-org international collaborations can be riddled with communication and teamwork issues,\n",
      "in particular at this pivotal stage where teams transition from R&D to application and product development.\n",
      "First, MLTRL as a lingua franca was key to the team effort bringing Etalumis proof-of-concept into the\n",
      "larger effort of applying it to massive high-energy physics simulators. It was also critical at this stage to\n",
      "clearly communicate end-user requirements across the various teams and organizations, which must be deﬁned\n",
      "in MLTRL requirements docs with V&V measures – the essential science-user requirements were mainly\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "in MLTRL requirements docs with V&V measures – the essential science-user requirements were mainly\n",
      "for model and prediction interpretability, uncertainty estimation, and code usability. If there are concerns\n",
      "over these features, MLTRL switchbacks can help to quickly cycle back and improve modeling choices in a\n",
      "transparent, efﬁcient way – generally in ML projects, these fundamental issues with usability are caught too\n",
      "late, even after deployment. In the probabilistic generative model setting we’ve deﬁned in Etalumis, Bayesian\n",
      "inference gives results that are interpretable because they include exact locations and processes in the model\n",
      "that are associated with each prediction. Working with ML methods that are inherently interpretable, we are\n",
      "well-positioned to deliver interpretable interfaces for the end-users later in the project lifecycle.\n",
      "•Level 6-9 – The standard MLTRL protocol apply in these application-to-deployment stages, with several\n",
      "Etalumis-speciﬁc highlights. First, given the signiﬁcant research contributions in both probabilistic pro-\n",
      "gramming and scientiﬁc-ML, it’s important to share the code publicly. The development and deployment\n",
      "of the open-source code repository PPXvibranched into a separate MLTRL path from the Etalumis path\n",
      "for deployment at CERN. It’s useful to have systems engineering enable clean separation of requirements,\n",
      "deployments, etc. when there are different development and product lifecycles originating from a common\n",
      "parent project. For example, in this case it was useful to employ MLTRL switchbacks in the open-sourcing\n",
      "process, isolated from the CERN application paths, in order to add support for additional programming\n",
      "languages so PPX can apply to more scientiﬁc simulators – both directions beneﬁted signiﬁcantly the from\n",
      "the data pipelines considerations brought up levels earlier, where open-sourcing required different data APIs\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "and data transformations to enable broad usability. Second, related to the open-source code deliverable and\n",
      "the scientiﬁc ML user requirements we noted above, the late stages of MLTRL reviews include higher level\n",
      "stakeholders and speciﬁc end-users, yet again enforcing these scientiﬁc usability requirements are met. An\n",
      "example result of this in Etalumis is the ability to output human-readable execution traces of the SHERPA\n",
      "runs and inference, enabling never before possible step-by-step interpretability of the black-box simulator.\n",
      "The scientiﬁc ML perspective additionally brings to forefront an end-to-end data perspective that is pertinent in\n",
      "essentially all ML use-cases: these systems are only useful to the extent they provide comprehensive data analyses that\n",
      "integrate the data consumed and generated in these workﬂows, from raw domain data to machine-learned models. These\n",
      "data analyses drive reproducibility, explainability, and experiment data understanding, which are critical requirements\n",
      "in scientiﬁc endeavors and ML broadly.\n",
      "Causal inference & ML in medicine\n",
      "Understanding cause and effect relationships is crucial for accurate and actionable decision-making in many settings,\n",
      "from healthcare and epidemiology, to economics and government policy development. Unfortunately, standard\n",
      "machine learning algorithms can only ﬁnd patterns and correlation in data, and as correlation is not causation, their\n",
      "predictions cannot be conﬁdently used for understanding cause and effect. Indeed, relying on correlations extracted\n",
      "from observational data to guide decision-making can lead to embarrassing, costly, and even dangerous mistakes,\n",
      "such as concluding that asthma reduces pneumonia mortality risk [ 41], and that smoking reduces risk of developing\n",
      "vigithub.com/pyprob/ppx\n",
      "14\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "severe COVID-19 [ 42]. Fortunately, there has been much recent development in a ﬁeld known as causal inference that\n",
      "can quantitatively make sense of cause and effect from purely observational data[ 43]. The ability of causal inference\n",
      "algorithms to quantify causal impact rests on a number of important checks and assumptions–beyond those employed\n",
      "in standard machine learning or purely statistical methodology–that must be carefully deliberated over during their\n",
      "development and training. These speciﬁc checks and assumptions are as follows:\n",
      "•Specifying cause-and-effect relationships between relevant variables– One of the most important assump-\n",
      "tions underlying causal inference is the structure of the causal relations between quantities of interest. The\n",
      "gold standard for determining causal relations is to perform a randomised controlled trial, but in most cases\n",
      "these cannot be employed due to ethical concerns, technological infeasibility, or prohibitive cost. In these\n",
      "situations, domain experts have to be consulted to determine the causal relationships. It is important in these\n",
      "situations to carefully address the manner in which such domain knowledge was extracted from experts, the\n",
      "number and diversity of experts involved, the amount of consensus between experts, and so on. The need for\n",
      "careful documentation of this knowledge and its periodic review is made clear in the MLTRL framework, as\n",
      "we shall see below.\n",
      "•Identiﬁability– Another vital component of building causal models is whether the causal question of interest\n",
      "isidentiﬁable from the causal structure speciﬁed for the model together with observational (and sometimes\n",
      "experimental) data.\n",
      "•Adjusting for and monitoring confounding bias– An important aspect of causal model performance, not\n",
      "present in standard machine learning algorithms, is confounding bias adjustment. The standard approach is to\n",
      "employ propensity score matching to remove such bias. However, the quality of bias adjustment achieved in\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "any speciﬁc instance with such propensity-based matching methods needs to be checked and documented,\n",
      "with alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[44].\n",
      "•Sensitivity analysis– As causal estimates are based on generally untestable assumptions, such as observing all\n",
      "relevant confounders, it is vital to determine how sensitive the resulting predictions are to potential violations\n",
      "of these assumptions.\n",
      "•Consistency– It is crucial to understand if the learned causal estimate provably converges to the true causal\n",
      "effect in the limit of inﬁnite sample size. However, causal models cannot be validated by standard held-out\n",
      "tests, but rather require randomization or special data collection strategies to evaluate their predictions [ 45,46].\n",
      "The MLTRL framework makes transparent the need to carefully document and defend these assumptions, thus ensuring\n",
      "the safe and robust creation, deployment, and maintenance of causal models. We elucidate this with recent work by\n",
      "Richens et al.[ 47], developing a causal approach to computer-assisted diagnosis which outperforms previous purely\n",
      "machine learning based methods. To this end, we will go through the MLTRL levels one by one, demonstrating how\n",
      "they ensure the above speciﬁc checks and assumptions are naturally accounted for. This should provide a blueprint for\n",
      "how to employ the MLTRL levels in other causal inference applications.\n",
      "•Level 0 – When initially faced with a causal inference task, the ﬁrst step is always to understand the causal\n",
      "relationships between relevant variables. For instance, in Richens et al. [ 47], the ﬁrst step toward building\n",
      "the diagnostic model was specifying the causal relationships between the diverse set risk factors, diseases,\n",
      "and symptoms included in the model. To learn these relations, doctors and healthcare professionals were\n",
      "consulted to employ their expansive medical domain knowledge which was robustly evaluated by additional\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "independent groups of healthcare professionals. The MLTRL framework ensured this issue is dealt with and\n",
      "documented correctly, as such knowledge is required to progress from Level 0; failure to do this has plagued\n",
      "similar healthcare AI projects [48].\n",
      "The next step of any causal analysis is to understand whether the causal question of interest is uniquely\n",
      "identiﬁable from the causal structure speciﬁed for the model together with observational and experimental data.\n",
      "In this medical diagnosis example, identiﬁcation was crucial to establish, as the causal question of interest,\n",
      "“would the observed symptoms not be present had a speciﬁc disease been cured?”, was highly non-trivial.\n",
      "Again, MLTRL ensures this vital aspect of model building is carefully considered, as a mathematical proof of\n",
      "identiﬁability would be required to graduate from Level 0.\n",
      "With both the causal structure and identiﬁability result in hand, one can progress to Level 1.\n",
      "•Level 1 – At this level, the goal is to take the estimand for the identiﬁed causal question of interest and\n",
      "devise a way to estimate it from data. To do this one will need efﬁcient ways to adjust for confounﬁng bias.\n",
      "The standard approach is to employ propensity score-based methods to remove such bias when the target\n",
      "decision is binary, and use multi-stage ML models adhering to the assumed causal structure[ 49] for continuous\n",
      "target decisions (and high-dimensional data in general). However, the quality of bias adjustment achieved in\n",
      "any speciﬁc instance with propensity-based matching methods needs to be checked and documented, with\n",
      "15\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[ 44]. As\n",
      "above, MLTRL ensures transparency and adherence to this important aspect of causal model development, as\n",
      "without it a project cannot graduate from Level 1. Even more, MLTRL ensures tests for confounding bias\n",
      "are developed early-on and maintained throughout later stages to deployment. Still, in many cases, it is not\n",
      "possible to completely remove confounding in the observed data. TRL Cards offer a transparent way to declare\n",
      "speciﬁc limitations of a causal ML method.\n",
      "•Level 2 – PoC-level tests for causal models must go beyond that of typical ML models. As discussed above,\n",
      "to ensure the estimated causal effects are robust to the assumptions required for their derivation, sensitivity\n",
      "to these assumptions must be analysed. Such sensitivity analysis is often limited to R&D experiments or\n",
      "a post-hoc feature of ML products. MLTRL on the other hand requires this throughout the lifecycle as\n",
      "components of ML test suites and gated reviews. In the case of causal ML, best practice is to employ sensitivity\n",
      "analysis for this robustness check[ 50]. MLTRL ensures this check is highlighted and adhered to, and no model\n",
      "will end up graduating Level 2–let alone being deployed–unless it is passed.\n",
      "•Level 3 – Coding best practices, as in general ML applications.\n",
      "•Level 4-5 – There are additional tests to consider when taking causal models from research to production,\n",
      "in particular at Level 4–proof of concept demonstration in a real scenario. Consistency , for example, is an\n",
      "important property of causal methods that informs us whether the method provably converges to the true\n",
      "causal graph in the limit of inﬁnite sample size. Quantifying consistency in the test suite is critical when\n",
      "datasets change from controlled laboratory settings to open-world, and when the application scales. And\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "PoC validation steps are more efﬁcient with MLTRL because the process facilitates early speciﬁcation of the\n",
      "evaluation metric for a causal model in Level 2. Causal models cannot be validated by standard held-out tests,\n",
      "but rather require randomization or special data collection strategies to evaluate their predictions[ 45,46]. Any\n",
      "difﬁculty in evaluating the model’s predictions will be caught early and remedied.\n",
      "•Level 6-9 – With the the causal ML components of this technology developed reliably in the previous levels,\n",
      "the rest of the levels developing this technology focused on general medical-ML deployment challenges. For\n",
      "the most part, data governance, privacy, and management that was detailed earlier in the neuropathology\n",
      "MLTRL use-case, as well as on-premises deployment.\n",
      "AI for open-source space sciences\n",
      "The CAMS (Cameras for Allsky Meteor Surveillance) project [ 51], established in 2010 by NASA, uses hundreds of\n",
      "off-the-shelf CCTV cameras to capture the meteor activity in the night sky. Initially, resident scientists would retrieve\n",
      "hard-disks containing video data captured each night and perform manual triangulation of tracks or streaks of light\n",
      "in the night sky, and compute a meteor’s trajectory, orbit, and lightcurve. Each solution was manually classiﬁed as a\n",
      "meteor or not (i.e., planes, birds, clouds, etc). In 2017, a project run by the Frontier Development Labvii[52], the AI\n",
      "accelerator for NASA and ESA, aimed to automate the data processing pipeline and replicate the scientists thought\n",
      "process to build an ML model that identiﬁes meteors in the CAMS project [ 53,54]. The data automation led to\n",
      "orders of magnitude improvements in operational efﬁciency of the system, and allowed new contributors and amateur\n",
      "astronomers to start contributing to meteor sightings. Additionally, a novel web tool allowed anybody anywhere to\n",
      "view the meteors detected in the previous night. The CAMS camera system has had six-fold global expansion of the\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "data capture network, discovered ten new meteor showers, contributed towards instrumental evidence of previously\n",
      "predicted comets, and helped calculate parent bodies of various meteor showers. CAMS utilized the MLTRL framework\n",
      "to progress as described:\n",
      "•Level 1 – Understanding the domain and data is a prerequisite for any ML development. Extensive data\n",
      "exploration elucidated visual differences between objects in the night sky such as meteors, satellites, clouds,\n",
      "tail lights of planes, light from the eyes of cats peering into cameras, trees, and other tall objects visible in\n",
      "the moonlight. This step helped (1) understand visual properties of meteors that later deﬁned the ML model\n",
      "architecture, and (2) mitigate impact of data imbalance by proactively developing domain-oriented strategies.\n",
      "The results are well-documented on a datasheet associated with the TRL card, and discussed at the stage\n",
      "review. This MLTRL documentation forced us to consider data sharing and other privacy concerns at this early\n",
      "conceptualization stage, which is certainly relevant considering CAMS is for open-source and gathering data\n",
      "from myriad sources.\n",
      "•Level 2-3 – The agile and non-monotonic (or non-linear) development prescribed by MLTRL allowed the\n",
      "team to ﬁrst develop an approximate end-to-end pipeline that offered a path to ML model deployment and\n",
      "quick turnaround time to incorporate feedback from the regular gated reviews. Then, with relatively quicker\n",
      "viiThe NASA Frontier Development Lab and partners open-source the code and data via the SpaceML platform: spaceml.org\n",
      "16\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "experimentation, the team could improve on the quality of not just the ML model, but also scale up the systems\n",
      "development simultaneously in a non-monotonic development cycle.\n",
      "•Level 4 – With the initial pipeline in place, scalable training of baselines and initial models on real challenging\n",
      "datasets ensued. Throughout the levels, the MLTRL gated reviews were essential for making efﬁcient progress\n",
      "while ensuring robustness and functionality that meets stakeholder needs. At this stage we highlight speciﬁc\n",
      "advantages of the MLTRL review processes that had instrumental effect on the project success: With the\n",
      "required panel of mixed ML researchers and engineers, domain scientists, and product managers, the stage 4\n",
      "reviews stressed the signiﬁcance of numerical improvements and comparison to existing baselines, and helped\n",
      "identify and overcome issues with data imbalance. The team likely would have overlooked these approaches\n",
      "without the review from peers in diverse roles and teams. In general, the evolving panel of reviewers at\n",
      "different stages of the project was essential for covering a variety of veriﬁcation and validation measures –\n",
      "from helping mitigate data challenges, to open-source code quality.\n",
      "•Level 5 – To complete this R&D-to-productization level, a novel web tool called the NASA CAMS Meteor\n",
      "Shower Portalviiiwas created that allowed users to view meteor shower activity from the previous night and\n",
      "verify meteor predictions generated by the ML model. This app development was valuable for A/B testing,\n",
      "validating detected meteors and classiﬁed new meteor showers with human-AI interaction, and demonstrating\n",
      "real-world utility to stakeholders in review. ML processes without MLTRL miss out on these valuable\n",
      "development by overlooking the need for such a demo tool.\n",
      "•Level 6 – Application development was naturally driven by end-user feedback from the web app in level 5 –\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "without MLTRL it’s unlikely the team would be able to work with early productization feedback. With almost\n",
      "real time feedback coming in daily, newer methods for improving robustness of meteor identiﬁcation led to\n",
      "researching and developing a unique augmentation technique, resulting in the state of the art performance of\n",
      "the ML model. Further application development led to incorporating features that were in demand by users of\n",
      "the NASA CAMS Meteor Shower Portal: include celestial reference points through constellations, add ability\n",
      "to zoom in/out and (un)cluster showers, and provide tooling for scientiﬁc communication. The coordination of\n",
      "these features into product-caliber codebase resulted in the release of the NASA CAMS Meteor Shower Portal\n",
      "2.0 that was built by a team of citizen scientists – again we found the speciﬁc checkpoints in the MLTRL\n",
      "review were crucial for achieving these goals.\n",
      "•Level 7 – Integration was particularly challenging in two ways. First, integrating the ML and data engineering\n",
      "deliverables with the existing infrastructure and tools of the larger CAMS system, which had started devel-\n",
      "opment years earlier with other teams in partner organizations, required quantiﬁable progress for verifying\n",
      "the tech-readiness of ML models and modules. The use of technology readiness levels provided a clear and\n",
      "consistent metric for the maturity of the ML and data technologies, making for clear communication and\n",
      "efﬁcient project integration. Without MLTRL it is difﬁcult to have a conversation, let alone make progress, to-\n",
      "wards integrating AI/ML and data subsystems and components. Second, integrating open-source contributions\n",
      "into the main ML subsystem was a signiﬁcant challenge alleviated with diligent veriﬁcation and validation\n",
      "measures from MLTRL, as well as quantifying robustness with ML testing suites (using scoring measures like\n",
      "that of the ML Testing Rubric[20], and devising a checklist based on metamorphic testing[18]).\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "that of the ML Testing Rubric[20], and devising a checklist based on metamorphic testing[18]).\n",
      "•Level 8 – CAMS, like many datasets in practice, consisted of a smaller labeled subset and a much larger\n",
      "unlabeled set. In an attempt to additionally increase robustness of the ML subsystem ahead of “ﬂight readiness”,\n",
      "we looked to active learning [ 55,56] techniques to leverage the unlabeled data. Models using an initial version\n",
      "of this approach, where results of the active learning provided “weak” labels, resulted in consumption of the\n",
      "entire decade long unlabelled data collected by CAMS and slightly higher scores on deployment tests. Active\n",
      "learning showed to be a promising feature and was switched back to level 7 for further development towards\n",
      "the next deployment version, so as not to delay the rest of the project.\n",
      "•Level 9 – The ML components in CAMS require continual monitoring for model and data drifts, such as\n",
      "changes in weather, smoke, and cloud patterns that affect the view of the night sky. The data drifts may also be\n",
      "speciﬁc to locations, such as ﬁreﬂies and bugs in CAMS Australia and New Zealand stations which appear as\n",
      "false positives. The ML pipeline is largely automated with CI/CD, runs regular regression tests, and production\n",
      "of benchmarks. Manual intervention can be triggered when needed, such as sending low conﬁdence meteors for\n",
      "veriﬁcation to scientists in the CAMS project. The team also regularly releases the code, models, and web tools\n",
      "on the open-source space sciences and exploration ML toolbox, SpaceMLix. Through the SpaceML community\n",
      "and partner organizations, CAMS continually improves with feature requests, debugging, and improving data\n",
      "practices, while tracking progress with standard software release cycles and MLTRL documentation.\n",
      "viiimeteorshowers.seti.org\n",
      "ixspaceml.org\n",
      "17\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "BEYOND SOFTWARE ENGINEERING\n",
      "Software engineering (SWE) practices vary signiﬁcantly across domains and industries. Some domains, such as medical\n",
      "applications, aerospace, or autonomous vehicles rely on a highly rigorous development process which is required\n",
      "by regulations. Other domains, for example advertising and e-commerce are not regulated and can employ a lenient\n",
      "approach to development. ML development should at minimum inherit the acceptable software engineering practices of\n",
      "the domain. There are, however, several key areas where ML development stands out from SWE, adding its own unique\n",
      "challenges which even most rigorous SWE practices are not able to overcome.\n",
      "For instance, the behavior of ML systems is learned from data, not speciﬁed directly in code. The data requirements\n",
      "around ML (i.e., data discovery, management, and monitoring) adds signiﬁcant complexity not seen in other types\n",
      "of SWE. There are many beneﬁts to using a data-oriented architecture (DOA) [48] with the data-ﬁrst workﬂows and\n",
      "management practices prescribed in MLTRL. DOA aims to make the data ﬂowing between elements of business logic\n",
      "more explicit and accessible with a streaming-based architecture rather than the micro-service architectures that are\n",
      "standard in software systems. One speciﬁc beneﬁt of DOA is making data available and traceable by design, which\n",
      "helps signiﬁcantly in the ML logging challenges and data governance needs we discussed in Levels 7-9. Moreover,\n",
      "MLTRL highlights data-related requirements along every step to ensure that the development process considers data\n",
      "readiness and availability.\n",
      "Not to mention an array of ML-speciﬁc failure modes; for example, models that become miscalibrated due to subtle\n",
      "data distributional shifts in the deployment setting, resulting in models that are more conﬁdent in predictions than they\n",
      "should be. MLTRL helps deﬁne ML-speciﬁc testing considerations (levels 5 and 7) to help surface these failure-modes\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "early. ML opens up new threat vectors across the whole deployment workﬂow that otherwise aren’t risks in software\n",
      "systems: for example, a poisoning attack to contaminate the training phase of ML systems, or membership inference\n",
      "to see if a given data record was part of the model’s training. MLTRL consider these threat vectors and suggests\n",
      "relevant risk-identiﬁcation during prototyping and productization phases. More generally, ML codebases have all the\n",
      "problems for regular code, plus ML-speciﬁc issues at the system level, mainly as a consequence of added complexity\n",
      "and dynamism. The resulting entanglement, for instance, implies that the SWE practice of making isolated changes is\n",
      "often not feasible – Scully et al.[ 57] refer to this as the “changing anything changes everything” principle. Given this\n",
      "consideration, typical SWE change-management is insufﬁcient. Furthermore, ML systems almost necessarily increase\n",
      "the technical debt; package-level refactoring is generally sufﬁcient for removing technical debt in software systems, but\n",
      "this is not the case in ML systems.\n",
      "These factors and others suggest that inherited software engineering and management practices of a given domain are\n",
      "insufﬁcient for the successful development of robust and reliable ML systems. But it is not trading off one for the other:\n",
      "MLTRL can be used in synergy with the existing, industry-standard software engineering practices such as agile [ 58]\n",
      "and waterfall [ 59] to handle unique challenges of ML development. Because ML applications are a category of software,\n",
      "all best practices of building and operating software should be extended when possible to the ML application. Practices\n",
      "like version control, comprehensive testing, continuous integration and continuous deployment are all applicable to ML\n",
      "development. MLTRL provides a framework that helps extend SWE building and operating practices that are acceptable\n",
      "in a given domain to tackle the unique challenges of ML development.\n",
      "RELATED WORKS\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "in a given domain to tackle the unique challenges of ML development.\n",
      "RELATED WORKS\n",
      "A recent case study from Microsoft Research [ 40] similarly identiﬁes a few themes describing how ML is not equal to\n",
      "software engineering, and recommends a linear ML workﬂow with steps for data preparation through modeling and\n",
      "deploying. They deﬁne an effective workﬂow for isolated development of an ML model, but this approach does not\n",
      "ensure the technology is actually improving in quality and robustness. Their process should be repeated at progressive\n",
      "stages of development in the broader ML and data technology lifecycle. If applied in the MLTRL framework, the\n",
      "speciﬁc ingredients of the ML model workﬂow – that is, people, software, tests, objectives, etc. – evolve over time and\n",
      "subsequent stages as the technologies mature.\n",
      "There exist many recommended workﬂows for speciﬁc ML methods and areas of pipelines. For instance, a more\n",
      "iterative process for Bayesian ML [ 60] and even more speciﬁcally for probabilistic programming [ 39], a data mining\n",
      "process deﬁned in 2000 that remains widely used [ 61], others for describing data iterations [ 62], and human-computer\n",
      "interaction cycles [ 63]. In these recommended workﬂows and others, there’s an important distinction between their\n",
      "cycles and “switchback” mechanisms in MLTRL. Their cycles suggest to generically iterate over a data-modeling-\n",
      "evaluation-deployment process. Switchbacks, on the other hand, are speciﬁc, purpose-driven workﬂows for dialing\n",
      "part(s) of a project to an earlier stage – this doesn’t simply mean go back and train the model on more data, but rather\n",
      "switching back regresses the technology’s maturity level (e.g. from level 5 to level 3) such that it must again fulﬁll the\n",
      "level-by-level requirements, evaluations and reviews. See the Methods section for more details on MLTRL switchbacks.\n",
      "18\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "In general, iteration is an important part of data, ML, and software processes. MLTRL is unique from the other\n",
      "recommended processes in many ways, and perhaps most importantly because it considers data ﬂows and ML models\n",
      "in the context of larger systems. These isolated processes (that are speciﬁc to e.g. modeling in prototype development\n",
      "or data wrangling in application development) are synergistic with MLTRL because they can be used within each level\n",
      "of the larger lifecycle or framework. For example, the Bayesian modeling processes [ 39,60] we mentioned above\n",
      "are really useful to guide developers of probabilistic ML approaches. But there are important distinctions between\n",
      "executing these modeling steps and cycles in a well-deﬁned prototyping environment with curated data and minimal\n",
      "responsibilities, versus a production environment riddled with sparse and noisy data, that interacts with the physical\n",
      "world in non-obvious ways, and can carry expensive (even hidden) consequences. MLTRL provides the necessary,\n",
      "holistic context and structure to use these and other development processes reliably and responsibly.\n",
      "Also related to our work, Google teams have proposed ML testing recommendations [ 20] and validating the data fed\n",
      "into ML systems [ 64]. For NLP applications, typical ML testing practices struggle to translate to real-world settings,\n",
      "often overestimating performance capabilities. An effective way to address this is devising a checklist of linguistic\n",
      "capabilities and test types, as in Ribeiro et al.[ 17]–interestingly their test suite was inspired by metamorphic testing,\n",
      "which we suggested earlier in Level 7 for testing systems AI integrations. A survey by Paleyes et al. [ 48] go over\n",
      "numerous case studies to discuss challenges in ML deployment. They similarly pay special attention to the need for\n",
      "ethical considerations, end-user trust, and extra security in ML deployments. On the latter point, Kumar et al. [ 65]\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "provide a table thoroughly breaking down new threat vectors across the whole ML deployment workﬂow (some of\n",
      "which we mentioned above). These works, notably the ML security measures and the quantiﬁcation of an ML test suite\n",
      "in a principled way – i.e., that does not use misguided heuristics such as code coverage – are valuable to include in any\n",
      "ML workﬂow including MLTRL, and are synergistic with the framework we’ve described in this paper. These analyses\n",
      "provide useful insights, but they do not provide a holistic, regimented process for the full ML lifecycle from R&D\n",
      "through deployment. An end-to-end approach is suggested by Raji et al.[ 66], but only for the speciﬁc task of auditing\n",
      "algorithms; components of AI auditing are mentioned in Level 7, and covered throughout in the review processes.\n",
      "Sculley et al.[ 57] go into more ML debt topics such as undeclared consumers and data dependencies, and go on to\n",
      "recommend an ML Testing Rubric as a production checklist [ 20]. For example, testing models by a canary process\n",
      "before serving them into production. This, along with similar shadow testing we mentioned earlier, are common in\n",
      "autonomous ML systems, notably robotics and autonomous vehicles. They explicitly call out tests in four main areas\n",
      "(ML infrastructure, model development, features and data, and monitoring of running ML systems), some of which we\n",
      "discussed earlier. For example, tests that the training and serving features compute the same values; a model may train\n",
      "on logged processes or user input, but is then served on a live feed with different inputs. In addition to the Google ML\n",
      "Testing Rubric, we advocate metamorphic testing : a SWE methodology for testing a speciﬁc set of relations between\n",
      "the outputs of multiple inputs. True to the checklists in the Google ML Testing Rubric and in MLTRL, metamorphic\n",
      "testing for ML can have a codiﬁed list of metamorphic relations[18].\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "testing for ML can have a codiﬁed list of metamorphic relations[18].\n",
      "In domains such as healthcare there have been the introduction of similar checklists for data readiness – for example,\n",
      "to ensure regulatory-grade real-world-evidence (RWE) data quality [ 67] – yet these are nascent and not yet widely\n",
      "accepted. Applying AI in healthcare has led to developing guidance for regulatory protocol, which is still a work in\n",
      "progress. Larson et al.[ 68] provide a comprehensive analysis for medical imaging and AI, arriving at several regulatory\n",
      "framework recommendations that mirror what we outline as important measures in MLTRL: e.g., detailed task elements\n",
      "such as pitfalls and limitations (surfaced on TRL Cards), clear deﬁnition of an algorithm relative to the downstream\n",
      "task, deﬁning the algorithm “capability” (Level 5), real-world monitoring, and more.\n",
      "D’amour et al.[ 19] dive into the problem we noted earlier about model miscalibration. They point to the trend in machine\n",
      "learning to develop models relatively isolated from the downstream use and larger system, resulting in underspeciﬁcation\n",
      "that handicaps practical ML pipelines. This is largely problematic in deep learning pipelines, but we’ve also noted this\n",
      "risk in the case of causal inference applications. Suggested remedies include stress tests –empirical evaluations that\n",
      "probe the model’s inductive biases on practically relevant dimensions–and in general the methods we deﬁne in Level 7.\n",
      "LIMITATIONS, RESPONSIBILITIES, and ETHICS\n",
      "MLTRL has been developed, deployed, iterated, and validated in myriad environments, as demonstrated by the previous\n",
      "examples and many others. Nonetheless we strongly suggest that MLTRL not be viewed as a cure-all for machine\n",
      "learning systems engineering. Rather, MLTRL provides mechanisms to better enable ML practitioners, teams, and\n",
      "stakeholders to be diligent and responsible with these technologies and data. That is, one cannot implement MLTRL in\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "an organization and turn a blind eye to the many data, ML, and integration challenges we’ve discussed here. MLTRL is\n",
      "analogous to a pilot’s checklist, not autopilot.\n",
      "19\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "MLTRL is intended to be complimentary to existing software development methodologies, not replace or alter them.\n",
      "Speciﬁcally, whether the team uses agile or waterfall methods, MLTRL can be adopted to help deﬁne and structure\n",
      "phases of the project, as well as the success criteria of each stage. In context of the software development process, the\n",
      "purpose of MLTRL is to help the team minimize the technical dept and risk associated with the delivery of an ML\n",
      "application by helping the development team ask necessary questions.\n",
      "We discussed many data challenges and approaches in the context of MLTRL, and should highlight again the importance\n",
      "of data considerations in any ML initiative. The data availability and quality can severely limit the ability to develop and\n",
      "deploy ML, whether MLTRL is used or not. It is again the responsibility of the ML practitioners, teams, and stakeholders\n",
      "to gather, use, and distribute data in safe, legal, ethical ways. MLTRL helps do so with rigor and transparency, but\n",
      "again is not a solution for data bias. We recommend these recent works on data bias in ML: [ 69,70,71,72,73].\n",
      "Further, AI/ML ethics is a continuously evolving, multidisciplinary space – see [ 5]. MLTRL aims to prioritize ethics\n",
      "considerations at each level of the framework, and would do well to also evolve over time with the broader AI/ML\n",
      "ethics developments.\n",
      "CONCLUSION\n",
      "We’ve described Machine Learning Technology Readiness Levels (MLTRL) , an industry-hardened systems engineering\n",
      "framework for robust, reliable, and responsible machine learning. MLTRL is derived from the processes and testing\n",
      "standards of spacecraft development, yet lean and efﬁcient for ML, data, and software workﬂows. Examples from\n",
      "several organizations across industries demonstrate the efﬁcacy of MLTRL for AI and ML technologies, from research\n",
      "and development through productization and deployment, in important domains such as healthcare and physics, with\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "emphasis on data readiness amongst other critical challenges. Our aim is MLTRL works in synergy with recent\n",
      "approaches in the community focused on diligent data-readiness, privacy and security, and ethics. Even more, MLTRL\n",
      "establishes a much-needed lingua franca for the AI ecosystem, and broadly for AI in the worlds of science, engineering,\n",
      "and business. Our hope is that our systems framework is adopted broadly in AI and ML organizations, and that\n",
      "“technology readiness levels” becomes common nomenclature across AI stakeholders – from researchers and engineers\n",
      "to sales-people and executive decision-makers.\n",
      "Methods\n",
      "Gated reviews\n",
      "At the end of each stage is a dedicated review period: (1) Present the technical developments along with the requirements\n",
      "and their corresponding veriﬁcation measures and validation steps, (2) make key decisions on path(s) forward (or\n",
      "backward) and timing, and (3) debrief the processx. As in the gated reviews deﬁned by TRL used by NASA, DARPA, et\n",
      "al., MLTRL stipulates speciﬁc criteria for review at each level, as well as calling out speciﬁc key decision points (noted\n",
      "in the level descriptions above). The designated reviewers will “graduate” the technology to the next level, or provide a\n",
      "list of speciﬁc tasks that are still needed (ideally with quantitative remarks). After graduation at each level, the working\n",
      "group does a brief post-mortem; we ﬁnd that a quick day or two pays dividends in cutting away technical debt and\n",
      "improving team processes. Regular gated reviews are essential for making efﬁcient progress while ensuring robustness\n",
      "and functionality that meets stakeholder needs. There are several important mechanisms in MLTRL reviews that are\n",
      "speciﬁcally useful with AI and ML technologies: First, the review panels evolve over a project lifecycle, as noted\n",
      "below. Second, MLTRL prescribes that each review runs through an AI ethics checklist deﬁned by the organization; it is\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "important to repeat this at each review, as the review panel and stakeholders evolve considerably over a project lifecycle.\n",
      "As previously described in the levels deﬁnitions, including ethics reviews as an integral part of early system development\n",
      "is essential for informing model speciﬁcations and avoiding unintended biases or harm[74] after deployment.\n",
      "TRL “Cards”\n",
      "In Figure 3 we succinctly showcase a key deliverable: TRL Cards . The model cards proposed by Google [ 75] are a useful\n",
      "development for external user-readiness with ML. On the other hand, our TRL Cards aim to be more information-dense,\n",
      "like datasheets for medical devices and engineering tools – see the open-source TRL Card repo for examples and\n",
      "templates (to be released at github.com/alan-turing-institute). These serve as “report cards” that grow and improve upon\n",
      "graduating levels, and provide a means of inter-team and cross-functional communication. The content of a TRL Card\n",
      "is roughly in two categories: project info, and implicit knowledge. The former clearly states info such as project owners\n",
      "xMLTRL should include regular debriefs and meta-evaluations such that process improvements can be made in a data-driven,\n",
      "efﬁcient way (rather than an annual meta-review). MLTRL is a high-level framework that each organization should operationalize in\n",
      "a way that suits their speciﬁc capabilities and resources.\n",
      "20\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "and reviewers, development status, and semantic versioning–not just for code, also for models and data. In the latter\n",
      "category are speciﬁc insights that are typically siloed in the ML development team but should be communicated to\n",
      "other stakeholders: modeling assumptions, dataset biases, corner cases, etc. With the spread of AI and ML in critical\n",
      "application areas, we are seeing domain expert consortiums deﬁning AI reporting guidelines – e.g., Rivera et al.[ 76]\n",
      "calling for clinical trials reports for interventions involving AI – which will greatly beneﬁt from the use of our TRL\n",
      "reporting cards. We stress that these TRL Cards are key for the progression of projects, rather than documentation\n",
      "afterthoughts. The TRL Cards thus promote transparency and trust, within teams and across organizations. TRL Card\n",
      "templates will be open-sourced upon publication of this work, including methods for coordinating use with other\n",
      "reporting tools such as “Datasheets for Datasets” [24].\n",
      "Risk mitigation\n",
      "Identifying and addressing risks in a software project is not a new practice. However, akin to the MLTRL roots in\n",
      "spacecraft engineering, risk is a “ﬁrst-class citizen” here. In the deﬁnition of technical and product requirements, each\n",
      "entry has a calculation of the form risk =p(failure )×value , where the value of a component is an integer 1−10.\n",
      "Being diligent about quantifying risks across the technical requirements is a useful mechanism for ﬂagging ML-related\n",
      "vulnerabilities that can sometimes be hidden by layers of other software. MLTRL also speciﬁes that risk quantiﬁcation\n",
      "and testing strategies are required for sim-to-real development. That is, there is nearly always a non-trivial gap in\n",
      "transferring a model or algorithm from a simulation testbed to the real world. Requiring explicit sim-to-real testing\n",
      "steps in the workﬂow helps mitigate unforeseen (and often hazardous) failures. Additionally, comprehensive ML test\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "coverage that we mention throughout this paper is a critical strategy for mitigating risks anduncertainties: ML-based\n",
      "system behavior is not easily speciﬁed in advance, but rather depends on dynamic qualities of the data and on various\n",
      "model conﬁguration choices[20].\n",
      "Non-monotonic, non-linear paths\n",
      "We observe many projects beneﬁt from cyclic paths, dialing components of a technology back to a lower level. Our\n",
      "framework not only encourages cycles, we make them explicit with “switchback mechanisms” to regress the maturity\n",
      "of speciﬁc components in an AI system:\n",
      "1.Discovery switchbacks occur as a natural mechanism – new technical gaps are discovered through systems\n",
      "integration, sparking later rounds of component development[ 77]. These are most common in the R&D levels,\n",
      "for example moving a component of a proof-of-concept technology (at Level 4) back to proof-of-principle\n",
      "development (Level 2).\n",
      "2.Review switchbacks result from gated reviews, where speciﬁc components or larger subsystems may be dialed\n",
      "back to earlier levels. This switchback is one of the “key decision points” in the MLTRL project lifecycle\n",
      "(as noted in the Levels deﬁnitions), and is often a decision driven by business-needs and timing rather than\n",
      "technical concerns (for instance when mission priorities and funds shift). This mechanism is common from\n",
      "Level 6/7 to 4, which stresses the importance of this R&D to product transition phase (see Figure 2 (left)).\n",
      "3.Embedded switchbacks are predeﬁned in the MLTRL process. For example, a predeﬁned path from 4 to 2, and\n",
      "from 9 to 4. In complex systems, particularly with AI technologies, these built-in loops help mitigate technical\n",
      "debt and overcome other inefﬁciencies such as noncomprehensive V&V steps.\n",
      "Without these built-in mechanisms for cyclic development paths, it can be difﬁcult and inefﬁcient to build systems of\n",
      "modules and components at varying degrees of maturity. Contrary to traditional thought that switchback events should\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "be suppressed and minimized, in fact they represent a natural and necessary part of the complex technology development\n",
      "process – efforts to eliminate them may stiﬂe important innovations without necessarily improving efﬁciency. This is\n",
      "a fault of the standard monotonic approaches in AI/ML projects, stage-gate processes, and even the traditional TRL\n",
      "framework.\n",
      "It is also important to note that most projects do not start at Level 0; very few ML companies engage in this low-level\n",
      "theoretical research. For example, a team looking to use an off-the-shelf object recognition model could start that\n",
      "technology at Level 3, and proceed with thorough V&V for their speciﬁc datasets and use-cases. However, no technology\n",
      "can skip levels after the MLTRL process has been initiated. The industry default (that is, without implementing MLTRL)\n",
      "is to ignorantly take pretrained models, run ﬁne tuning on their speciﬁc data, and jump to deployment, effectively\n",
      "skipping Levels 5 to 7. Additionally, we ﬁnd it is advantageous to incorporate components from other high-TRL ranking\n",
      "projects while starting new projects; MLTRL makes the veriﬁcation and validation (V&V) steps straightforward for\n",
      "integrating previously developed ML components.\n",
      "21\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "Evolving people, objectives, and measures\n",
      "As suggested earlier, much of the practical value of MLTRL comes at the transition between levels. More precisely,\n",
      "MLTRL manages these oft neglected transitions explicitly as evolving teams, objectives, and deliverables. For instance,\n",
      "the team (or working group) at Level 3 is mostly AI Research Engineers, but at Level 6 is mixed Applied AI/SW\n",
      "Engineers mixed with product managers and designers. Similarly, the review panels evolve from level to level, to match\n",
      "the changing technology development objectives. What the reviewers reference similarly evolves: notice in the level\n",
      "deﬁnitions that technical requirements and V&V guide early stages, but at and after Level 6 the product requirements\n",
      "and V&V takeover – naturally, the risk quantiﬁcation and mitigation strategies evolve in parallel. Regarding the\n",
      "deliverables, notably TRL Cards and risk matrices[ 22] (to rank and prioritize various science, technical, and project\n",
      "risks), the information develops and evolves over time as the technology matures.\n",
      "Quantiﬁable progress\n",
      "By deﬁning technology maturity in a quantitative way, MLTRL enables teams to accurately and consistently deﬁne\n",
      "their ML progress metrics. Notably industry-standard “objectives and key results” (OKRs) and “key performance\n",
      "indicators” (KPIs) [ 78] can be deﬁned as achieving certain readiness levels in a given period of time; this is a preferable\n",
      "metric in essentially all ML systems which consist of much more than a single performance score to measure progress.\n",
      "Even more, meta-review of MLTRL progress over multiple projects can provide useful insights at the organization\n",
      "level. For example, analysis of the time-per-level and the most frequent development paths/cycles can bring to light\n",
      "operational bottlenecks. Compared to conventional software engineering metrics based on sprint stories and tickets, or\n",
      "time-tracking tools, MLTRL provides a more accurate analysis of ML workﬂows.\n",
      "Communication and explanation\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "Communication and explanation\n",
      "A distinct advantage of MLTRL in practice is the nomenclature: an agreed upon grading scheme for the maturity of\n",
      "an AI technology, and a framework for how/when that technology ﬁts within a product or system, enables everyone\n",
      "to communicate effectively and transparently. MLTRL also acts as a gate for interpretability and explainability–at\n",
      "the granularity of individual models and algorithms, and more crucially from a holistic, systems standpoint. Notably\n",
      "the DARPA XAIxiprogram advocates for this advance in developing AI technologies; they suggest interpretability\n",
      "and explainability are necessary at various locations in an AI system to be sufﬁcient for deployment as an AI product,\n",
      "otherwise leading to issues with ethics and bias.\n",
      "Robustness via uncertainty-aware ML\n",
      "How to design a reliable system from unreliable components has been a guiding question in the ﬁelds of computing and\n",
      "intelligence [79]. In the case of AI/ML systems, we aim to build reliable systems with myriad unreliable components:\n",
      "noisy and faulty sensors, human and AI error, and so on. There is thus signiﬁcant value to quantifying the myriad\n",
      "uncertainties, propagating them throughout a system, and arriving at a notion or measure of reliability. For this reason,\n",
      "although MLTRL applies generally to AI/ML methods and systems, we advocate for methods in the class of probabilistic\n",
      "ML, which naturally represent and manipulate uncertainty about models and predictions[ 28]. These are Bayesian\n",
      "methods that use probabilities to represent aleatoric uncertainty , measuring the noise inherent in the observations, and\n",
      "epistemic uncertainty , accounting for uncertainty in the model itself (i.e., capturing our ignorance about which model\n",
      "generated the data). In the simplest case, an uncertainty aware ML pipeline should quantify uncertainty at the points of\n",
      "sensor inputs or perception, prediction or model output, and decision or end-user action – McAllister et al.[ 29] suggest\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "this with Bayesian deep learning models for safer autonomous vehicle pipelines. We can achieve this sufﬁciently well\n",
      "in practice for simple systems. However, we do not yet have a principled, theoretically grounded, and generalizable way\n",
      "of propagating errors and uncertainties downstream and throughout more complex AI systems – i.e., how to integrate\n",
      "different software, hardware, data, and human components while considering how errors and uncertainties propagate\n",
      "through the system. This is an important direction of our future work.\n",
      "References\n",
      "[1]Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning\n",
      "that matters. In AAAI , 2018.\n",
      "xiDARPA Explainable Artiﬁcial Intelligence (XAI)\n",
      "22\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "[2]Arnaud de la Tour, Massimo Portincaso, Kyle Blank, and Nicolas Goeldel. The dawn of the deep tech ecosystem. Technical\n",
      "report, The Boston Consulting Group, 2019.\n",
      "[3] NASA. The NASA systems engineering handbook. 2003.\n",
      "[4] United States Department of Defense. Defense acquisition guidebook. Technical report, U.S. Dept. of Defense, 2004.\n",
      "[5] D. Leslie. Understanding artiﬁcial intelligence ethics and safety. ArXiv , abs/1906.05684, 2019.\n",
      "[6]Google. Machine learning workﬂow. https://cloud.google.com/mlengine/docs/tensorflow/\n",
      "ml-solutions-overview . Accessed: 2020-12-13.\n",
      "[7]Alexander Lavin and Gregory Renard. Technology readiness levels for AI & ML. ICML Workshop on Challenges Deploying\n",
      "ML Systems , 2020.\n",
      "[8] T. Dasu and T. Johnson. Exploratory data mining and data cleaning. 2003.\n",
      "[9]M. Janssen, P. Brous, Elsa Estevez, L. Barbosa, and T. Janowski. Data governance: Organizing data for trustworthy artiﬁcial\n",
      "intelligence. Gov. Inf. Q. , 37:101493, 2020.\n",
      "[10] B. Shahriari, Kevin Swersky, Ziyu Wang, R. Adams, and N. D. Freitas. Taking the human out of the loop: A review of bayesian\n",
      "optimization. Proceedings of the IEEE , 104:148–175, 2016.\n",
      "[11] Goutham Ramakrishnan, A. Nori, Hannah Murfet, and Pashmina Cameron. Towards compliant data management systems for\n",
      "healthcare ml. ArXiv , abs/2011.07555, 2020.\n",
      "[12] Umang Bhatt, Alice Xiang, S. Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura,\n",
      "and P. Eckersley. Explainable machine learning in deployment. Proceedings of the 2020 Conference on Fairness, Accountability,\n",
      "and Transparency , 2020.\n",
      "[13] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and V . Smith. Federated learning: Challenges, methods, and future directions.\n",
      "IEEE Signal Processing Magazine , 37:50–60, 2020.\n",
      "[14] T. Ryffel, Andrew Trask, M. Dahl, Bobby Wagner, J. Mancuso, D. Rueckert, and J. Passerat-Palmbach. A generic framework\n",
      "for privacy preserving deep learning. ArXiv , abs/1811.04017, 2018.\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "for privacy preserving deep learning. ArXiv , abs/1811.04017, 2018.\n",
      "[15] A. Madry, Aleksandar Makelov, Ludwig Schmidt, D. Tsipras, and Adrian Vladu. Towards deep learning models resistant to\n",
      "adversarial attacks. ArXiv , abs/1706.06083, 2018.\n",
      "[16] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. ArXiv , abs/1710.11342, 2018.\n",
      "[17] Marco Túlio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models\n",
      "with checklist. In ACL, 2020.\n",
      "[18] Xiaoyuan Xie, Joshua W. K. Ho, C. Murphy, G. Kaiser, B. Xu, and T. Chen. Testing and validating machine learning classiﬁers\n",
      "by metamorphic testing. The Journal of systems and software , 84 4:544–558, 2011.\n",
      "[19] Alexander D’Amour, K. Heller, D. Moldovan, Ben Adlam, B. Alipanahi, Alex Beutel, C. Chen, Jonathan Deaton, Jacob\n",
      "Eisenstein, M. Hoffman, Farhad Hormozdiari, N. Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, M. Lucic,\n",
      "Y . Ma, Cory Y . McLean, Diana Mincu, Akinori Mitani, A. Montanari, Zachary Nado, V . Natarajan, C. Nielson, Thomas F.\n",
      "Osborne, R. Raman, K. Ramasamy, Rory Sayres, J. Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, V . Veitch,\n",
      "Max Vladymyrov, Xuezhi Wang, K. Webster, S. Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley. Underspeciﬁcation\n",
      "presents challenges for credibility in modern machine learning. ArXiv , abs/2011.03395, 2020.\n",
      "[20] Eric Breck, Shanqing Cai, E. Nielsen, M. Salib, and D. Sculley. The ml test score: A rubric for ml production readiness and\n",
      "technical debt reduction. 2017 IEEE International Conference on Big Data (Big Data) , pages 1123–1132, 2017.\n",
      "[21] A. Botchkarev. A new typology design of performance metrics to measure errors in machine learning regression algorithms.\n",
      "Interdisciplinary Journal of Information, Knowledge, and Management , 14:045–076, 2019.\n",
      "[22] N. Duijm. Recommendations on the use and design of risk matrices. Safety Science , 76:21–31, 2015.\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "[23] Louise Naud and Alexander Lavin. Manifolds for unsupervised visual anomaly detection. ArXiv , abs/2006.11364, 2020.\n",
      "[24] Timnit Gebru, J. Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, H. Wallach, Hal Daumé, and K. Crawford.\n",
      "Datasheets for datasets. ArXiv , abs/1803.09010, 2018.\n",
      "[25] B. Hutchinson, A. Smart, A. Hanna, Emily L. Denton, Christina Greer, Oddur Kjartansson, P. Barnes, and Margaret Mitchell.\n",
      "Towards accountability for machine learning datasets: Practices from software engineering and infrastructure. Proceedings of\n",
      "the 2021 ACM Conference on Fairness, Accountability, and Transparency , 2021.\n",
      "23\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "[26] P. Schulam and S. Saria. Reliable decision support using counterfactual models. In NIPS 2017 , 2017.\n",
      "[27] Towards trustable machine learning. Nature Biomedical Engineering , 2:709–710, 2018.\n",
      "[28] Zoubin Ghahramani. Probabilistic machine learning and artiﬁcial intelligence. Nature , 521:452–459, 2015.\n",
      "[29] Rowan McAllister, Yarin Gal, Alex Kendall, Mark van der Wilk, A. Shah, R. Cipolla, and Adrian Weller. Concrete problems\n",
      "for autonomous vehicle safety: Advantages of bayesian deep learning. In IJCAI , 2017.\n",
      "[30] Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I. Avilés-Rivero,\n",
      "Christian Etmann, Cathal McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, Effrossyni Gkrania-Klotsas,\n",
      "James H. F. Rudd, Evis Sala, and Carola-Bibiane Schönlieb. Common pitfalls and recommendations for using machine learning\n",
      "to detect and prognosticate for covid-19 using chest radiographs and ct scans. Nature Machine Intelligence , 3:199–217, 2021.\n",
      "[31] J. Tobin, Rachel H Fong, Alex Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep\n",
      "neural networks from simulation to the real world. 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems\n",
      "(IROS) , pages 23–30, 2017.\n",
      "[32] Arthur Juliani, Vincent-Pierre Berges, Esh Vckay, Yuan Gao, Hunter Henry, M. Mattar, and D. Lange. Unity: A general\n",
      "platform for intelligent agents. ArXiv , abs/1809.02627, 2018.\n",
      "[33] Stefan Hinterstoißer, Olivier Pauly, Tim Hauke Heibel, Martina Marek, and Martin Bokeloh. An annotation saved is an\n",
      "annotation earned: Using fully synthetic training for object instance detection. ArXiv , abs/1902.09967, 2019.\n",
      "[34] Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan Hogins, You-Cyuan Jhang, Mohsen Kamalzadeh,\n",
      "Bowen Li, Steven Leal, Pete Parisi, Cesar Romero, Wesley Smith, Alex Thaman, Samuel Warren, and Nupur Yadav. Unity\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "perception: Generate synthetic data for computer vision. CoRR , abs/2107.04259, 2021.\n",
      "[35] K. Cranmer, J. Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of the National Academy\n",
      "of Sciences , 117:30055 – 30062, 2020.\n",
      "[36] Jan-Willem van de Meent, Brooks Paige, H. Yang, and Frank Wood. An introduction to probabilistic programming. ArXiv ,\n",
      "abs/1809.10756, 2018.\n",
      "[37] Atilim Günes Baydin, Lei Shao, W. Bhimji, L. Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi,\n",
      "Bradley Gram-Hansen, Gilles Louppe, Mingfei Ma, X. Zhao, P. Torr, V . Lee, K. Cranmer, Prabhat, and F. Wood. Etalumis:\n",
      "bringing probabilistic programming to scientiﬁc simulators at scale. Proceedings of the International Conference for High\n",
      "Performance Computing, Networking, Storage and Analysis , 2019.\n",
      "[38] T. Gleisberg, S. Höche, F. Krauss, M. Schönherr, S. Schumann, F. Siegert, and J. Winter. Event generation with sherpa 1.1.\n",
      "Journal of High Energy Physics , 2009:007–007, 2009.\n",
      "[39] David M. Blei. Build, compute, critique, repeat: Data analysis with latent variable models. 2014.\n",
      "[40] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald C. Gall, Ece Kamar, Nachiappan Nagappan, Besmira\n",
      "Nushi, and Thomas Zimmermann. Software engineering for machine learning: A case study. 2019 IEEE/ACM 41st International\n",
      "Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) , 2019.\n",
      "[41] R. Ambrosino, B. Buchanan, G. Cooper, and Marvin J. Fine. The use of misclassiﬁcation costs to learn rule-based decision\n",
      "support models for cost-effective hospital admission strategies. Proceedings. Symposium on Computer Applications in Medical\n",
      "Care , pages 304–8, 1995.\n",
      "[42] Gareth J Grifﬁth, Tim T Morris, Matthew J Tudball, Annie Herbert, Giulia Mancano, Lindsey Pike, Gemma C Sharp, Jonathan\n",
      "Sterne, Tom M Palmer, George Davey Smith, et al. Collider bias undermines our understanding of covid-19 disease risk and\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "severity. Nature communications , 11(1):1–12, 2020.\n",
      "[43] J. Pearl. Theoretical impediments to machine learning with seven sparks from the causal revolution. Proceedings of the\n",
      "Eleventh ACM International Conference on Web Search and Data Mining , 2018.\n",
      "[44] T. Nguyen, G. Collins, J. Spence, J. Daurès, P. Devereaux, P. Landais, and Y . Le Manach. Double-adjustment in propensity\n",
      "score matching analysis: choosing a threshold for considering residual imbalance. BMC Medical Research Methodology , 17,\n",
      "2017.\n",
      "[45] D. Eckles and E. Bakshy. Bias and high-dimensional adjustment in observational studies of peer effects. ArXiv , abs/1706.04692,\n",
      "2017.\n",
      "[46] Yanbo Xu, Divyat Mahajan, Liz Manrao, A. Sharma, and E. Kiciman. Split-treatment analysis to rank heterogeneous causal\n",
      "effects for prospective interventions. ArXiv , abs/2011.05877, 2020.\n",
      "24\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "[47] Jonathan G Richens, C. M. Lee, and Saurabh Johri. Improving the accuracy of medical diagnosis with causal machine learning.\n",
      "Nature Communications , 11, 2020.\n",
      "[48] Andrei Paleyes, Raoul-Gabriel Urma, and N. Lawrence. Challenges in deploying machine learning: a survey of case studies.\n",
      "ArXiv , abs/2011.09926, 2020.\n",
      "[49] V . Chernozhukov, D. Chetverikov, M. Demirer, E. Duﬂo, Christian L. Hansen, Whitney K. Newey, and J. Robins. Dou-\n",
      "ble/debiased machine learning for treatment and structural parameters. Econometrics: Econometric & Statistical Methods -\n",
      "Special Topics eJournal , 2018.\n",
      "[50] Victor Veitch and Anisha Zaveri. Sense and sensitivity analysis: Simple post-hoc analysis of bias due to unobserved confounding.\n",
      "NeurIPS 2020, arXiv preprint arXiv:2003.01747 , 2020.\n",
      "[51] P. Jenniskens, P.S. Gural, L. Dynneson, B.J. Grigsby, K.E. Newman, M. Borden, M. Koop, and D. Holman. Cams: Cameras for\n",
      "allsky meteor surveillance to establish minor meteor showers. Icarus , 216(1):40 – 61, 2011.\n",
      "[52] Siddha Ganju, Anirudh Koul, Alexander Lavin, J. Veitch-Michaelis, Meher Kasam, and J. Parr. Learnings from frontier\n",
      "development lab and spaceml - ai accelerators for nasa and esa. ArXiv , abs/2011.04776, 2020.\n",
      "[53] S. Zoghbi, M. Cicco, A. P. Stapper, A. J. Ordonez, J. Collison, P. S. Gural, S. Ganju, J.-L. Galache, and P. Jenniskens. Searching\n",
      "for long-period comets with deep learning tools. In Deep Learning for Physical Science Workshop, NeurIPS , 2017.\n",
      "[54] Peter Jenniskens, Jack Baggaley, Ian Crumpton, Peter Aldous, Petr Pokorny, Diego Janches, Peter S. Gural, Dave Samuels, Jim\n",
      "Albers, Andreas Howell, Carl Johannink, Martin Breukers, Mohammad Odeh, Nicholas Moskovitz, Jack Collison, and Siddha\n",
      "Ganju. A survey of southern hemisphere meteor showers. Planetary and Space Science , 154:21 – 29, 2018.\n",
      "[55] D. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active learning with statistical models. In NIPS , 1994.\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "[56] Y . Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. ArXiv , abs/1703.02910, 2017.\n",
      "[57] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young,\n",
      "Jean-François Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. In NIPS , 2015.\n",
      "[58] P. Abrahamsson, Outi Salo, Jussi Ronkainen, and Juhani Warsta. Agile software development methods: Review and analysis.\n",
      "ArXiv , abs/1709.08439, 2017.\n",
      "[59] Marco Kuhrmann, Philipp Diebold, Jürgen Münch, Paolo Tell, Vahid Garousi, Michael Felderer, Kitija Trektere, Fergal\n",
      "McCaffery, Oliver Linssen, Eckhart Hanser, and Christian R. Prause. Hybrid software and system development in practice:\n",
      "waterfall, scrum, and beyond. Proceedings of the 2017 International Conference on Software and System Process , 2017.\n",
      "[60] Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry,\n",
      "Paul-Christian Burkner, and Martin Modrak. Bayesian workﬂow. ArXiv , abs/2011.01808, 2020.\n",
      "[61] P. Chapman, J. Clinton, R. Kerber, T. Khabaza, T. Reinartz, C. Shearer, and R. Wirth. Crisp-dm 1.0: Step-by-step data mining\n",
      "guide. 2000.\n",
      "[62] Fred Hohman, Kanit Wongsuphasawat, Mary Beth Kery, and Kayur Patel. Understanding and visualizing data iteration in\n",
      "machine learning. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems , 2020.\n",
      "[63] Saleema Amershi, M. Cakmak, W. B. Knox, and T. Kulesza. Power to the people: The role of humans in interactive machine\n",
      "learning. AI Mag. , 35:105–120, 2014.\n",
      "[64] Eric Breck, Marty Zinkevich, Neoklis Polyzotis, Steven Euijong Whang, and Sudip Roy. Data validation for machine learning.\n",
      "2019.\n",
      "[65] R. Kumar, David R. O’Brien, Kendra Albert, Salomé Viljöen, and Jeffrey Snover. Failure modes in machine learning systems.\n",
      "ArXiv , abs/1911.11034, 2019.\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "ArXiv , abs/1911.11034, 2019.\n",
      "[66] Inioluwa Deborah Raji, Andrew Smart, Rebecca White, M. Mitchell, Timnit Gebru, B. Hutchinson, Jamila Smith-Loud, Daniel\n",
      "Theron, and P. Barnes. Closing the ai accountability gap: deﬁning an end-to-end framework for internal algorithmic auditing.\n",
      "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency , 2020.\n",
      "[67] R. Miksad and A. Abernethy. Harnessing the power of real-world evidence (rwe): A checklist to ensure regulatory-grade data\n",
      "quality. Clinical Pharmacology and Therapeutics , 103:202 – 205, 2018.\n",
      "[68] D. B. Larson, Hugh Harvey, D. Rubin, Neville Irani, J. R. Tse, and C. Langlotz. Regulatory frameworks for development and\n",
      "evaluation of artiﬁcial intelligence–based diagnostic imaging algorithms: Summary and recommendations. Journal of the\n",
      "American College of Radiology , 2020.\n",
      "25\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "[69] Ninareh Mehrabi, Fred Morstatter, N. Saxena, Kristina Lerman, and A. Galstyan. A survey on bias and fairness in machine\n",
      "learning. ACM Computing Surveys (CSUR) , 54:1 – 35, 2019.\n",
      "[70] Eirini Ntoutsi, P. Fafalios, U. Gadiraju, Vasileios Iosiﬁdis, W. Nejdl, Maria-Esther Vidal, S. Ruggieri, F. Turini, S. Papadopoulos,\n",
      "Emmanouil Krasanakis, I. Kompatsiaris, K. Kinder-Kurlanda, Claudia Wagner, F. Karimi, Miriam Fernández, Harith Alani,\n",
      "B. Berendt, Tina Kruegel, C. Heinze, Klaus Broelemann, Gjergji Kasneci, T. Tiropanis, and Steffen Staab. Bias in data-driven\n",
      "ai systems - an introductory survey. ArXiv , abs/2001.09762, 2020.\n",
      "[71] E. Jo and Timnit Gebru. Lessons from archives: strategies for collecting sociocultural data in machine learning. Proceedings of\n",
      "the 2020 Conference on Fairness, Accountability, and Transparency , 2020.\n",
      "[72] J. Wiens, W. Price, and M. Sjoding. Diagnosing bias in data-driven algorithms for healthcare. Nature Medicine , 26:25–26,\n",
      "2020.\n",
      "[73] R. Challen, J. Denny, M. Pitt, L. Gompels, T. Edwards, and K. Tsaneva-Atanasova. Artiﬁcial intelligence, bias and clinical\n",
      "safety. BMJ Quality & Safety , 28:231 – 237, 2019.\n",
      "[74] Z. Obermeyer, B. Powers, C. V ogeli, and S. Mullainathan. Dissecting racial bias in an algorithm used to manage the health of\n",
      "populations. Science , 366:447 – 453, 2019.\n",
      "[75] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, In-\n",
      "ioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. Proceedings of the Conference on Fairness,\n",
      "Accountability, and Transparency , 2019.\n",
      "[76] Samantha Cruz Rivera, Xiaoxuan Liu, A. Chan, A. K. Denniston, and M. Calvert. Guidelines for clinical trial protocols for\n",
      "interventions involving artiﬁcial intelligence: the spirit-ai extension. Nature Medicine , 26:1351 – 1363, 2020.\n",
      "[77] Z. Szajnfarber. Managing innovation in architecturally hierarchical systems: Three switchback mechanisms that impact practice.\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "IEEE Transactions on Engineering Management , 61:633–645, 2014.\n",
      "[78] H. Zhou and Y . He. Comparative study of okr and kpi. DEStech Transactions on Economics, Business and Management , 2018.\n",
      "[79] J. Neumann. Probabilistic logic and the synthesis of reliable organisms from unreliable components. 1956.\n",
      "Acknowledgements\n",
      "The authors would like to thank Gur Kimchi, Carl Henrik Ek and Neil Lawrence for valuable discussions about this\n",
      "project.\n",
      "Author contributions statement\n",
      "A.L. conceived of the original ideas and framework, with signiﬁcant contributions towards improving the framework\n",
      "from all co-authors. A.L. initiated the use of MLTRL in practice, including the neuropathology test case discussed here.\n",
      "C.G-L. contributed insight regarding causal AI, including the section on counterfactual diagnosis. C.G-L. also made\n",
      "signiﬁcant contributions broadly in the paper, notably in the Methods descriptions and paper revisions. Si.G. contributed\n",
      "the spacecraft test case, along with early insights in the framework deﬁnitions. A.V . contributed to the deﬁnition of\n",
      "later stages involving deployment (as did A.G.), and comparison with traditional software workﬂows. Both E.X. and\n",
      "Y .G. provided insights regarding AI in academia, and Y .G. additionally contributed to the uncertainty quantiﬁcation\n",
      "methods. Su.G. and D.L. contributed the computer vision test case. A.G.B. contributed the particle physics test case,\n",
      "and signiﬁcant reviews of the writeup. A.S. contributed insights related to causal ML and AI ethics. D.N. provided\n",
      "valuable feedback on the overall framework, and contributed signiﬁcantly with the details on “switchback mechanisms”.\n",
      "S.Z. contributed to multiple paper revisions, with emphasis on clarity and applicability to broad ML users and teams.\n",
      "J.P. contributed to multiple paper revisions, and to deploying the systems ML methods broadly in practice for Earth and\n",
      "space sciences. –same goes for C.M., with additional feedback overall on the methods. All co-authors discussed the\n",
      "Summary:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Please summarize the following:\n",
      "content and contributed to editing the manuscript.\n",
      "Competing interests\n",
      "The authors declare no competing interests.\n",
      "Additional information\n",
      "Correspondence and requests for materials should be addressed to A.L.\n",
      "26\n",
      "Summary:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (14812 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the complete summary of the following summaries:\n",
      "## Summary:\n",
      "\n",
      "**Problem:**\n",
      "\n",
      "- Rapid deployment of Machine Learning (ML) systems can lead to technical debt, failures, and expensive consequences.\n",
      "\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "- The authors propose the **Machine Learning Technology Readiness Levels (MLTRL)** framework to ensure robust, reliable, and responsible ML development and deployment.\n",
      "\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "- **Systematic Approach:** MLTRL defines a principled process for ML development, from research through productization and deployment.\n",
      "- **Domain-Agnostic:** Applies to various domains, including medical diagnostics, consumer computer vision, satellite imagery, and particle physics.\n",
      "- **Collaborative Framework:** Provides a lingua franca for cross-team collaboration on AI and ML technologies.\n",
      "\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "- Improved quality and reliability of ML systems.\n",
      "- Reduced risk of technical debt and failures.\n",
      "- Enhanced collaboration and communication across teams.\n",
      "\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "The MLTRL framework has been used in various real-world projects, including:\n",
      "\n",
      "* Medical diagnostics\n",
      "* Consumer computer vision\n",
      "* Satellite imagery\n",
      "* Particle physics\n",
      "\n",
      "## Summary:\n",
      "\n",
      "**Title:** Machine Learning Systems Engineering: Challenges and Opportunities\n",
      "\n",
      "**Main Focus:**\n",
      "\n",
      "- The paper explores the challenges of using Machine Learning (ML) and Artificial Intelligence (AI) technologies, highlighting their vulnerabilities and risks.\n",
      "- It argues that current ML development is siloed and lacks proper testing and safeguards, leading to potential failures.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- **ML Systems are Vulnerable:** ML models are trained on limited datasets and are susceptible to unseen scenarios, leading to unreliable behavior.\n",
      "- **Lack of Robustness:** ML systems are not adequately tested for various conditions and fail to handle stochasticity, leading to potential failures.\n",
      "- **Need for Systems Engineering:** Traditional engineering disciplines prioritize testing and standardization, which is lacking in ML development.\n",
      "- **Technology Readiness Level (TRL):** This framework can be applied to ML to improve integration and ensure reliability.\n",
      "\n",
      "**Keywords:**\n",
      "\n",
      "- Machine Learning\n",
      "- Systems Engineering\n",
      "- Data Management\n",
      "- Medical AI\n",
      "- Space Sciences\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The paper calls for a shift in ML development towards a more rigorous and systematic approach, borrowing principles from traditional engineering disciplines like systems engineering and applying frameworks like Technology Readiness Level (TRL) for improved reliability and robustness.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The article highlights the gap between traditional spacecraft development processes and the rapid iteration cycles of machine learning (ML) and software workflows. It proposes the Machine Learning Technology Readiness Levels (MLTRL) framework to bridge this gap by:\n",
      "\n",
      "- Providing a standardized process for developing and deploying robust, reliable, and responsible ML and data systems.\n",
      "- Drawing on decades of experience in AI and ML development across diverse domains.\n",
      "- Addressing ethical considerations, such as AI bias, fairness, and societal impact.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- Traditional spacecraft development processes are overly rigorous for ML applications.\n",
      "- MLTRL framework is inspired by NASA's Technology Readiness Levels (TRL) for spacecraft systems.\n",
      "- The framework promotes collaboration and standardization across teams and industries.\n",
      "- MLTRL emphasizes ethical considerations and fairness in ML systems.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "The paper proposes MLTRL (Machine Learning Technology Readiness Levels) to guide the development and deployment of AI and ML technologies. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Traditional ML workflows are linear and isolated:** They fail to address iteration, system integration, and continuous improvement.\n",
      "* **MLTRL defines technology readiness levels:** This allows for clear communication and understanding of the maturity of ML models, data pipelines, software, and systems.\n",
      "* **MLTRL levels are based on:**\n",
      "    * Gated reviews\n",
      "    * Evolving working groups\n",
      "    * Requirements documentation with risk calculations\n",
      "    * Progressive code and testing standards\n",
      "    * Deliverables such as TRL Cards and ethics checklists\n",
      "\n",
      "**Overall, MLTRL provides a systematic approach to measure and improve the maturity of AI and ML technologies across the entire development and deployment lifecycle.**\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The provided text describes a framework called Machine Learning Technology Readiness Levels (MLTRL) for assessing the maturity of machine learning projects. The framework consists of multiple levels, each with specific characteristics and requirements.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Systematic fashion:** The framework includes concrete examples and methods for systematic fashion.\n",
      "* **Data considerations:** Data tasks from curation to governance are highlighted at different MLTRL levels.\n",
      "* **Levels:**\n",
      "    - **Level 0 - First Principles:** Focuses on developing theoretical concepts, building mathematical foundations, and understanding data.\n",
      "    - **Level 0 Data:** Not a formal requirement at this stage, but data availability needs to be considered.\n",
      "    - **Review:** The reviewer is typically the research lab or team lead.\n",
      "\n",
      "**Purpose:**\n",
      "\n",
      "The MLTRL framework aims to:\n",
      "\n",
      "* Provide a clear and consistent way to assess the readiness of machine learning projects.\n",
      "* Guide researchers through different stages of development.\n",
      "* Encourage transparency and accountability in machine learning development.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "**Phase 1: Goal-Oriented Research**\n",
      "\n",
      "- Focuses on analyzing specific model/algorithm properties rather than end-to-end performance.\n",
      "- Uses smaller or synthetic data sets due to privacy/security constraints.\n",
      "- Emphasis on rapid experimentation and iterative analysis.\n",
      "- Code is prioritized for readability and maintainability, rather than extensive testing.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- Emphasis on mathematical validation and potential application.\n",
      "- Justification for using sample data rather than full datasets.\n",
      "- Tolerance for hacky code for rapid experimentation.\n",
      "- Importance of semantic versioning for reproducibility.\n",
      "\n",
      "**Overall Goal:**\n",
      "\n",
      "- To assess hypotheses and explore mathematical validity through low-level experiments.\n",
      "- To identify potential applications and understand their impact.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The document outlines a research and development process for developing machine learning models and associated technologies. It involves multiple stages of review and development, with key decision points at various stages.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Data and Review:**\n",
      "    - Data is sourced from real datasets, synthetic data, or both.\n",
      "    - Data acquisition and processing strategies are considered at an early stage.\n",
      "    - Early experimentation is reviewed by the research team to ensure scientific rigor.\n",
      "\n",
      "\n",
      "* **Proof of Principle:**\n",
      "    - Active development of the technology in simulated environments.\n",
      "    - Formal research requirements document is created, outlining verification and validation steps.\n",
      "\n",
      "\n",
      "* **Decision Point:**\n",
      "    - Choice between continuing R&D or proceeding to prototype development.\n",
      "\n",
      "\n",
      "* **Requirements:**\n",
      "    - Defined as documented physical or functional needs.\n",
      "    - Specifies stakeholders' needs without prescribing a specific solution.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "**Research Cycle in Machine Learning:**\n",
      "\n",
      "The passage describes a four-stage research cycle in machine learning:\n",
      "\n",
      "**1. PoP (Proof of Potential)**\n",
      "\n",
      "- Research initiatives and publications are evaluated.\n",
      "- Some projects move to applied AI, while others return to further research.\n",
      "- Data is collected and analyzed to assess model performance and identify potential issues.\n",
      "\n",
      "\n",
      "**2. Level 2 Review**\n",
      "\n",
      "- Results from Level 1 are evaluated against research claims.\n",
      "- Data quality and analysis are verified for reproducibility.\n",
      "\n",
      "\n",
      "**3. Level 3 - System Development**\n",
      "\n",
      "- Code development focuses on interoperability, reliability, maintainability, and scalability.\n",
      "- Code quality is improved, with emphasis on robustness and documentation.\n",
      "\n",
      "\n",
      "**4. Proof-of-Concept**\n",
      "\n",
      "- Collaboration with product engineering team to establish service-level agreements (SLAs) and objectives (SLOs) for production system.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "**Agreement and Objectives:**\n",
      "\n",
      "* The document outlines the agreements and objectives (SLAs and SLOs) for the eventual production system.\n",
      "* This includes data consistency and coverage, software practices, interfaces, documentation, and data management standards.\n",
      "\n",
      "**Data Collection:**\n",
      "\n",
      "* Level 3 data collection is mostly consistent with previous levels, but dedicated subsets and mock data are needed for specific functionalities.\n",
      "* Level 4 involves scaling up data collection and processing from previous levels. \n",
      "\n",
      "**Review Process:**\n",
      "\n",
      "* Level 3 review involves AI and engineering teams to assess software practices, interfaces, documentation, and data management.\n",
      "* Potential gaps in data coverage and robustness are identified for subsequent levels.\n",
      "\n",
      "**Proof of Concept:**\n",
      "\n",
      "* The Proof of Concept (PoC) stage demonstrates the technology in a real scenario.\n",
      "* Quantifiable and qualitative results are communicated using TRL Cards and requirements documentation.\n",
      "* Real and representative data is crucial for exploring potential application areas.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "**1. Experiment Metrics:**\n",
      "\n",
      "- Real-world experiments require metrics beyond traditional ML research metrics like precision and recall.\n",
      "- Metrics should quantify model performance, computational costs, and user-relevant metrics.\n",
      "\n",
      "\n",
      "**2. AI Ethics:**\n",
      "\n",
      "- Organizations should discuss ethics during the experiment stage, covering data collection and potential harms.\n",
      "- Detailed ethics considerations are documented on TRL Cards.\n",
      "\n",
      "\n",
      "**3. Level 4 Review:**\n",
      "\n",
      "- This level assesses the technology's real-world applicability.\n",
      "- Projects may be paused or halted if ethical concerns or technical limitations are identified.\n",
      "\n",
      "\n",
      "**4. Real-World Data:**\n",
      "\n",
      "- Having real-world data is crucial for proof-of-concept validation.\n",
      "- Confidence in the technology must be established using real-world data.\n",
      "\n",
      "\n",
      "**5. Verification and Validation:**\n",
      "\n",
      "- Verification ensures that the technology is built correctly.\n",
      "- Validation confirms that the technology addresses the intended problem.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The text discusses the importance of high-quality data and a well-designed data pipeline for successful model inference. It outlines a phased approach to data review and validation, encompassing:\n",
      "\n",
      "**Level 4 Review:**\n",
      "- Data quality assessment for real-world applications.\n",
      "- Validation of data readiness.\n",
      "- Evaluation of security and privacy considerations.\n",
      "\n",
      "\n",
      "**Level 5 - Machine Learning “Capability”:**\n",
      "- Recognition that machine learning is a capability, not just a model or algorithm.\n",
      "- Emphasis on the need for a transition from R&D to productization.\n",
      "- Description of the MLTRL process for guiding documentation, objectives, and team collaboration.\n",
      "\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- Data quality and pipeline design are crucial for effective model inference.\n",
      "- Data review and validation are essential for real-world application.\n",
      "- Machine learning is a capability that requires a holistic approach involving multiple disciplines.\n",
      "- The MLTRL process helps facilitate the transition from R&D to productization.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The passage discusses the challenges associated with transitioning ML technology from research and development (R&D) to productization. It highlights the importance of data governance and review processes in this transition.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Graduation from Level 5:** Difficulty in transitioning from research to productization, requiring dedicated resources and alignment.\n",
      "* **Level 5 Data:** Considerations for scaling data pipelines to handle increased access and usage.\n",
      "* **Data Governance Challenges:** Data silos, duplication, unclear responsibilities, and data lifecycle control issues.\n",
      "* **Level 5 Review:** Thorough verification, validation, and stakeholder alignment on product-driven requirements.\n",
      "* **Level 6 - Application Development:** Emphasis on software engineering to develop production-quality code.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "**1. Model Robustness and Explainability:**\n",
      "\n",
      "- ML modules should be robust and able to handle target use-cases.\n",
      "- Model explanations need to be built and validated alongside the model and evaluated in the context of downstream tasks.\n",
      "\n",
      "\n",
      "**2. Data Challenges:**\n",
      "\n",
      "- Data challenges during deployment must be anticipated and addressed.\n",
      "- Data distribution shifts between development and deployment require robust models.\n",
      "\n",
      "\n",
      "**3. Deployment Considerations:**\n",
      "\n",
      "- Two main deployment types: internal APIs for ML teams and external integration with applications.\n",
      "- Deployment settings vary widely, including cloud, on-premise, hybrid, and privacy-constrained environments.\n",
      "\n",
      "\n",
      "**4. Deployment Factors:**\n",
      "\n",
      "- Data limitations and accessibility constraints are common.\n",
      "- Advanced ML approaches like federated learning are needed for privacy-oriented deployments.\n",
      "- ML models often require integration with rules engines.\n",
      "\n",
      "**Additional Notes:**\n",
      "\n",
      "- The text emphasizes the importance of considering deployment settings and factors during ML model development.\n",
      "- Privacy-oriented ML approaches are mentioned as potential solutions for data accessibility challenges.\n",
      "- The connection between model explainability and stakeholder needs is highlighted.\n",
      "\n",
      "Hardware selection is typically made early in the development process, considering factors such as GPU versus edge devices. It is important to make these decisions at the appropriate stage (Level 6) when serving scenarios and requirements are well understood.\n",
      "Keep in mind the important points and also give a title.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the complete summary of the following summaries:\n",
      "**Summary:**\n",
      "\n",
      "The text emphasizes the importance of rigorous testing and validation in the ML deployment process to mitigate risks and ensure successful deployment. It outlines a seven-level framework (Level 6-7) for ML development with specific recommendations for:\n",
      "\n",
      "**Level 6:**\n",
      "- Collect and operationalize additional data for model robustness.\n",
      "- Perform adversarial testing and semantic perturbations.\n",
      "- Collect data from diverse sources to assess generalization.\n",
      "\n",
      "**Level 6 Review:**\n",
      "- Focus on code quality, requirements, data pipelines, and AI ethics.\n",
      "- Ensure regulatory compliance with data privacy and security laws.\n",
      "\n",
      "**Level 7:**\n",
      "- Integrate ML models into production systems with involvement from both infrastructure and applied AI engineers.\n",
      "- Implement tests for critical scenarios and data slices.\n",
      "- Establish a \"golden dataset\" for performance comparison.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "**Testing in ML Pipelines:**\n",
      "\n",
      "* **Metamorphic testing** helps verify ML models by testing relationships between inputs and outputs.\n",
      "\n",
      "\n",
      "* **Data intervention tests** catch data errors throughout the pipeline, rather than relying on model performance.\n",
      "\n",
      "\n",
      "* **Level 7 data** emphasizes prioritizing data governance, including data acquisition, management, and security.\n",
      "\n",
      "\n",
      "* **Level 7 review** focuses on reviewing data pipelines, test suites, and ethical considerations.\n",
      "\n",
      "**Level 8 - Flight-ready:**\n",
      "\n",
      "- Technology is fully developed and tested under expected conditions.\n",
      "\n",
      "\n",
      "- Comprehensive testing is crucial:\n",
      "    - A/B tests and blue/green deployment to assess changes.\n",
      "    - Shadow testing and canary testing for gradual evaluation and feedback.\n",
      "\n",
      "\n",
      "- Continuous integration and continuous delivery (CI/CD) system is vital for stress testing the overall system.\n",
      "\n",
      "\n",
      "- Real-world data issues are inevitable, so shadow mode deployment helps identify potential performance regressions.\n",
      "\n",
      "\n",
      "- Data logging and performance metrics are essential for ongoing monitoring and optimization.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "**Level 8 Review:**\n",
      "\n",
      "- Comprehensive review of technical and product requirements with corresponding validations.\n",
      "- Involves representation from all relevant stakeholders.\n",
      "\n",
      "\n",
      "**Level 9 - Deployment:**\n",
      "\n",
      "- Emphasis on monitoring and maintaining AI/ML models.\n",
      "- Focus on methods for:\n",
      "    - Monitoring data quality, concept drift, and data drift.\n",
      "    - Automated evaluation and reporting.\n",
      "    - Data logging for model metadata, features, and predictions.\n",
      "\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- Monitoring is crucial for identifying and addressing issues with AI/ML models.\n",
      "- Continuous evaluation is important but may be delayed, so logging model outputs is essential for post-hoc analysis.\n",
      "- Data logging in ML systems requires capturing both input features and model predictions.\n",
      "- Monitoring data quality, concept drift, and model behavior identifies issues that might not be evident from end-performance.\n",
      "- Proper data logging and monitoring infrastructure is required for effective debugging and optimization of ML models.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "MLTRL emphasizes the importance of rigorous monitoring and feedback loops for deploying reliable AI and ML systems. Key points include:\n",
      "\n",
      "* **Early drift detection:** Drift tests must be implemented before deployment to detect changes in data patterns.\n",
      "* **Data-driven architecture:** Data-first architectures are preferred over the traditional design-by-services approach.\n",
      "* **Retraining and feature changes:** Monitoring is crucial to identify training-serving skew and enable retraining.\n",
      "* **Embedded switchback:** Any changes to deployed models or features require cycling back to an earlier stage for review.\n",
      "* **Data logging:** Comprehensive logging and inspection of data are essential for quality AI/ML systems.\n",
      "* **Regular reviews:** Regular review cycles allow stakeholders to assess system performance and recommend updates or switchbacks.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "MLTRL (Machine Learning Transition Levels) is a framework designed to address performance issues and abnormalities in ML systems. It provides a structured approach for teams to move between different levels of maturity, enabling reliable and efficient progress.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- MLTRL consists of stages or levels, but its value lies in the transitions between them.\n",
      "- The framework is applicable to diverse ML applications, from energy forecasting to robotics.\n",
      "- For simple models, a simpler approach may suffice, but more complex systems benefit significantly from MLTRL.\n",
      "- MLTRL is particularly valuable for high-risk domains where performance and reliability are critical.\n",
      "\n",
      "**Benefits of MLTRL:**\n",
      "\n",
      "- Provides a guide for team evolution and objective setting in line with technological advancements.\n",
      "- Enables reliable and efficient transition between different levels of maturity.\n",
      "- Helps identify potential performance issues and abnormalities.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The figure highlights two approaches to Machine Learning and AI project development:\n",
      "\n",
      "**1. MLTRL-based approach:**\n",
      "\n",
      "- Focuses on systematic development stages from basic research to deployment.\n",
      "- Involves cycling back to earlier stages for feature development and incremental improvements.\n",
      "- Emphasizes the importance of testing with specific scenarios and quantifying risks.\n",
      "\n",
      "\n",
      "**2. Common industry approach:**\n",
      "\n",
      "- Skips crucial technology transition stages and goes directly from development to deployment.\n",
      "- Ignores productization and systems integration considerations.\n",
      "\n",
      "\n",
      "**Example:**\n",
      "\n",
      "- A project on representing data in generative vision models led to state-of-the-art unsupervised anomaly detection for industrial applications like precision manufacturing.\n",
      "\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- MLTRL encourages iterative development and systematic risk management.\n",
      "- Skipping stages in the MLTRL process can lead to problems in deployment.\n",
      "- The example demonstrates the adaptability of the MLTRL framework to various tasks and datasets.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The use of AI models in neuropathology faces challenges related to:\n",
      "\n",
      "* **Hidden feedback loops:** AI systems can inadvertently influence their training data, leading to biases.\n",
      "* **Model availability:** Limited availability of models in deployment settings can hamper monitoring, debugging, and improvement.\n",
      "* **Uncertainty estimation:** Difficulty in measuring uncertainty in multi-source data environments, crucial for providing confidence and sensitivity measures.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The provided text discusses the challenges and mitigation strategies for Machine Learning (ML) technology maturity.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **TRL Cards:** A tool to track the maturity of ML technologies, with an example reflecting a neuropathology machine vision use case.\n",
      "* **Data Quality and Accountability:** The importance of thorough data documentation and best practices for data accountability.\n",
      "* **Edge Cases and Anomalies:** Challenges in handling edge cases and anomalies due to their rarity and unpredictability.\n",
      "* **Trust and Adoption:** The need to establish end-user trust and address concerns related to data privacy and interpretability.\n",
      "\n",
      "**Mitigation Strategies:**\n",
      "\n",
      "* MLTRL processes can help address the challenges associated with ML technology maturity.\n",
      "* Documenting datasets and following data accountability practices.\n",
      "* Considering specialized human-machine teaming applications.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The passage discusses the validation and application of a latent space model for unsupervised computer vision.\n",
      "\n",
      "**Key points:**\n",
      "\n",
      "- The model automatically organizes data by its implicit hierarchical structure, enabling anomaly detection and semantic analysis.\n",
      "\n",
      "\n",
      "- Validation involved generating synthetic data to isolate specific features and confirmed potential for anomaly detection and semantic differentiation.\n",
      "\n",
      "\n",
      "- The model's probabilistic approach allows for uncertainty estimation, a crucial feature for robust decision-making.\n",
      "\n",
      "\n",
      "- Reviews identified interpretability, uncertainty quantification, and human-in-the-loop capabilities as essential features for practical applications.\n",
      "\n",
      "\n",
      "- Potential applications include anomaly detection in histopathology, manufacturing, and defect detection in metallic surfaces.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The text emphasizes the importance of quantifying and propagating uncertainty in AI systems to enhance safety and trust. It advocates the use of probabilistic models and algorithms to represent and manipulate uncertainty. The authors highlight the value of early checks on data management and governance to address potential biases and hidden feedback loops. Additionally, incorporating domain experts in the process was crucial for robust veriﬁcation and validation, as well as identifying user-centric metrics that better align with practical applications.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "**1. Anomaly Detection:**\n",
      "\n",
      "- Traditional anomaly detection methods struggle to handle certain edge cases, where a hierarchical classification approach proved more effective.\n",
      "- Synthetic data generation is crucial for anomaly detection as real datasets are often limited and evolving.\n",
      "\n",
      "\n",
      "**2. Medical Application:**\n",
      "\n",
      "- The medical application encountered a dilemma between proceeding with existing work or further developing data processing methods.\n",
      "- Flexible progression through different stages was encouraged to address methodological flaws and biases in medical ML models.\n",
      "\n",
      "\n",
      "**3. Manufacturing Application:**\n",
      "\n",
      "- Defect detection product development proceeded smoothly.\n",
      "- The embedded switchback from level 9 to 4 enabled feedback incorporation and research updates.\n",
      "\n",
      "\n",
      "**Overall:**\n",
      "\n",
      "- MLTRL emphasizes flexible and iterative development, incorporating feedback and adapting to research progress.\n",
      "- Synthetic data generation and hierarchical anomaly detection are valuable tools for improving ML model reliability.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "**1. Feedback Loop for ML Research:**\n",
      "\n",
      "* Continuous feedback is incorporated into the ML research process through real-world data and user feedback from the CI/CD pipelines.\n",
      "* This feedback helps guide research direction and product development.\n",
      "\n",
      "\n",
      "**2. Medical ML Deployment:**\n",
      "\n",
      "* The \"neuropathology copilot\" was transferred to a pharmaceutical partner for productization.\n",
      "* MLTRL documentation and communication facilitated a smooth technology transfer, mitigating potential deployment challenges.\n",
      "\n",
      "\n",
      "**3. Computer Vision Application:**\n",
      "\n",
      "* Synthetic data generation and advancements in physics engines were used to develop a computer vision application for automated recycling.\n",
      "* The Unity Perception package was utilized for dataset generation and validation.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "* MLTRL addresses challenges associated with model availability, data access, and deployment in real-world settings.\n",
      "* Feedback loops and documentation streamline the technology transfer process.\n",
      "* Synthetic data and physics engine advancements aid in developing computer vision applications.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The provided text discusses the challenges of developing and deploying machine learning (ML) products, specifically highlighting the issues of dealing with multiple data sources, detecting hidden performance degradation, and the handoff from research to production.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Multiple data sources:** ML pipelines often involve various data sources, which can pose challenges in terms of format changes and unexpected behavior.\n",
      "\n",
      "\n",
      "* **Hidden performance degradation:** Gradual performance declines can be elusive in ML systems due to their complex nature and the interplay of multiple components.\n",
      "\n",
      "\n",
      "* **R&D to product handoff:** Effective transitions from research to production are crucial for successful ML product development.\n",
      "\n",
      "\n",
      "* **Data-driven testing:** Utilizing a combination of real-world data, user annotations, and synthetic data enhances testing coverage.\n",
      "\n",
      "\n",
      "* **Computer vision application:** The text describes a computer vision pipeline for automated recycling as an example of an ML product facing the aforementioned challenges.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The provided text discusses the challenges and solutions related to deploying machine learning (ML) products, specifically focusing on a project involving object recognition and classification.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Model usage requirements:** ML models can have significant impact on user input, and understanding system requirements is crucial for successful deployment.\n",
      "* **Engineering challenges:** Real-world deployment of ML models can face challenges related to memory usage, compute power, privacy, and latency.\n",
      "* **MLTRL framework:** MLTRL helps overcome these challenges by providing tools for tracking, analyzing, and managing ML models throughout their lifecycle.\n",
      "* **Project overview:** The referenced project utilized existing ML methods for object recognition and classification, using a pre-trained model and Unity Perception for synthetic data generation.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Understanding system requirements, addressing engineering challenges, and leveraging tools like MLTRL are crucial for successful deployment of ML products.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "Combining multiple machine learning (ML) elements poses significant challenges, leading to potential integration issues and instability. To address this, the MLTRL prototype introduces code checkpoints to ensure code quality and architecture.\n",
      "\n",
      "**Key points:**\n",
      "\n",
      "* **Level 5:** The \"valley of death\" issue is less relevant in projects starting at a higher MLTRL level, but data readiness and testing are crucial.\n",
      "* **Level 6:** Data reliability was the primary challenge, requiring the development of a synthetic data generator.\n",
      "* **Challenges in ML pipelines:** ML pipelines often suffer from \"glue code\" issues, leading to potential risks.\n",
      "* **Code checkpoints:** Regular code checkpoints promote well-architected software and minimize potential problems.\n",
      "* **Data readiness:** Data quality and availability issues can significantly impede project progress.\n",
      "Keep in mind the important points and also give a title.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the complete summary of the following summaries:\n",
      "**Summary:**\n",
      "\n",
      "The text describes the process of generating synthetic data to enhance data diversity and mitigate potential issues with real-world data in a production environment. The key steps involved are:\n",
      "\n",
      "**Level 7:**\n",
      "\n",
      "- Design of APIs to control various aspects of synthetic data generation, ensuring control over lighting, camera settings, object placements, and background textures.\n",
      "- Validation of object labeling accuracy and data format compatibility with real-world annotations.\n",
      "- Implementation of statistical tests to compare synthetic and real-world data distributions.\n",
      "\n",
      "\n",
      "**Level 8:**\n",
      "\n",
      "- Preparation for potential biases in real-world data sources through monitoring and testing.\n",
      "- Creation of tests to handle data distribution shifts in the classification pipeline.\n",
      "- Implementation of shadow tests and CI/CD tests to assess model performance under data variations and regressions.\n",
      "\n",
      "\n",
      "**Overall goal:**\n",
      "\n",
      "- Ensure the quality and diversity of synthetic data.\n",
      "- Detect and mitigate potential issues with real-world data before deployment.\n",
      "- Maintain model robustness and resilience to data variations in production.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "- Level 8 review and deployment of technology revealed performance degradation issues in complex ML pipelines.\n",
      "\n",
      "\n",
      "- Monitoring tests and code quality checkpoints at Levels 8 and 9 help identify hidden performance problems.\n",
      "\n",
      "\n",
      "- Switchbacks in the MLTRL process are designed to address challenges associated with model miscalibrations and data discrepancies.\n",
      "\n",
      "\n",
      "- Probabilistic programming is used to infer initial conditions or global parameters of scientific simulations based on observed data.\n",
      "\n",
      "**Project Overview:**\n",
      "\n",
      "- A collaborative project involving specialists in ML, particle physics, and high-performance computing.\n",
      "- Aims to utilize Bayesian inference to enhance large-scale simulators.\n",
      "\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "**1. Integrating with Legacy Systems:**\n",
      "- Difficulty in integrating ML methods with existing sensor networks, infrastructure, and codebases.\n",
      "- Large codebases with deep domain knowledge are impractical to rewrite.\n",
      "- Lack of data infrastructure standardization and quality.\n",
      "\n",
      "\n",
      "**2. Hardware-Software Coupling:**\n",
      "- Complexities in deploying ML models at scale, considering performance constraints and team coordination.\n",
      "- Challenges in scaling ML models to supercomputing environments with massive datasets.\n",
      "\n",
      "\n",
      "**3. Interpretability:**\n",
      "- Difficulty in validating and demonstrating interpretability in practical applications.\n",
      "\n",
      "\n",
      "**Key Solutions:**\n",
      "\n",
      "- Utilization of Bayesian inference for simulator enhancements.\n",
      "- Collaboration between experts in diverse disciplines.\n",
      "- Integration of machine learning with existing infrastructure and codebases.\n",
      "\n",
      "\n",
      "**Outcomes:**\n",
      "\n",
      "- Improved scalability, interpretability, and performance of large-scale simulators.\n",
      "- Enhanced scientific and industrial applications of machine learning in various fields.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The article emphasizes the importance of interpretability and usability in scientific machine learning (ML) applications. It explains how the MLTRL framework addresses challenges related to theoretical foundations, experimental validation, and application-driven development.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **MLTRL levels:** The article discusses the different levels of the MLTRL framework, highlighting how they ensure theoretical grounding, experimental validation, and application-driven development.\n",
      "\n",
      "\n",
      "* **Theoretical foundations:** The importance of a guiding framework in ML research is emphasized, especially when dealing with new and uncharted territory in ML theory.\n",
      "\n",
      "\n",
      "* **Experimental validation:** The ease of running low-level experiments in simple testbeds is highlighted, and the value of rich data grounded in physical constraints for isolating model behaviors is mentioned.\n",
      "\n",
      "\n",
      "* **Usability and application:** The article emphasizes the significance of usability and application-driven development, with the example of the SHERPA simulator.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The MLTRL framework provides a valuable approach for addressing scientific ML challenges by ensuring theoretical foundation, experimental validation, and application-driven development.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The timeline benefited from:\n",
      "\n",
      "* Recognizing simulator-integration constraints from the outset.\n",
      "* Including CERN scientists with domain expertise in the review process during R&D.\n",
      "\n",
      "During systems development, challenges arose with probabilistic programming due to its nascent stage and limited available tools. To address this:\n",
      "\n",
      "* A novel probabilistic programming execution protocol was developed to reroute random number draws to the probabilistic programming system.\n",
      "* This approach enabled control over stochastic choices within the existing codebase without major modifications.\n",
      "\n",
      "Early consideration of systems planning enabled:\n",
      "\n",
      "* Signifcant HPC scaling later on.\n",
      "* Proper mapping of the code to HPC infrastructure.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The provided text highlights the importance of data integration, normalization, and iterative inference method development in MLTRL, a project aimed at applying machine learning to high-energy physics.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Data Integration:**\n",
      "    - Data pipelines and APIs were developed to integrate data from various sources and infrastructures.\n",
      "    - Data normalization was done to ensure consistency across different databases.\n",
      "\n",
      "\n",
      "* **Inference Method Development:**\n",
      "    - An efficient \"embedded switchback\" technique was used to improve the inference method.\n",
      "    - The project leveraged cyclic R&D methods to iteratively enhance the inference process without hindering overall development.\n",
      "\n",
      "\n",
      "* **Communication and Collaboration:**\n",
      "    - MLTRL acted as a common language to facilitate communication and teamwork among organizations.\n",
      "    - Clear communication of end-user requirements was essential and documented in MLTRL requirements docs.\n",
      "\n",
      "\n",
      "**Overall:**\n",
      "\n",
      "MLTRL demonstrates the importance of systems engineering processes in scaling ML research to production, where iterative development, communication, and collaboration are crucial for successful deployment in complex scenarios.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The MLTRL project addresses challenges related to model interpretability, uncertainty estimation, and code usability in ML projects. It provides a protocol for transitioning from application to deployment stages, with specific considerations for open-source code release.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- **Interpretability requirements:** MLTRL focuses on ensuring model interpretability through methods like Bayesian inference.\n",
      "- **Early detection of usability issues:** MLTRL helps identify usability issues early in the project lifecycle, before deployment.\n",
      "- **Open-source code release:** The project promotes open-source code release to enhance accessibility and collaboration.\n",
      "- **Adaptability to different lifecycles:** MLTRL supports different development and deployment lifecycles through switchbacks and data pipeline considerations.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The article emphasizes the importance of the \"scientifical ML\" perspective in machine learning (ML) development, which focuses on usability, interpretability, and comprehensive data analysis. \n",
      "\n",
      "**Key points:**\n",
      "\n",
      "* **Broad usability:** \n",
      "    * Emphasis on making ML models explainable and accessible to various stakeholders, including high-level officials and end-users.\n",
      "    * Example: Ability to output human-readable execution traces of ML models, enabling step-by-step interpretability.\n",
      "\n",
      "\n",
      "* **End-to-end data perspective:**\n",
      "    * Importance of analyzing all data stages, from raw data to generated models, to ensure reproducibility, explainability, and understanding.\n",
      "\n",
      "\n",
      "* **Causal inference:**\n",
      "    * Standard ML algorithms can only find correlations, not causal relationships.\n",
      "    * Cautionary tale about relying on correlations leading to misleading conclusions.\n",
      "\n",
      "\n",
      "**Overall, the article argues that a scientific ML approach is crucial for developing reliable and trustworthy ML models that can be effectively used in various applications.**\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The author highlights the significance of causal inference in understanding cause-and-effect relationships from observational data. They emphasize the following key aspects of causal inference:\n",
      "\n",
      "- **Specifying causal relationships:** Establishing the causal structure between variables is crucial, and domain expertise is often required when randomized controlled trials are not feasible.\n",
      "\n",
      "\n",
      "- **Identifiability:** The causal question must be identifiable from the causal structure and available data.\n",
      "\n",
      "\n",
      "- **Adjusting for confounding bias:** Causal models must address confounding bias through techniques such as propensity score matching.\n",
      "\n",
      "**Additional Points:**\n",
      "\n",
      "- The MLTRL framework emphasizes the importance of documenting and reviewing domain knowledge used in causal inference models.\n",
      "- Causal inference algorithms require careful consideration of specific checks and assumptions to ensure accurate quantification of causal impact.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "Building and deploying causal models requires careful consideration of potential biases, assumptions, and limitations. The MLTRL framework outlines a systematic approach to address these challenges by:\n",
      "\n",
      "* **Documenting assumptions and biases:** Specific instances using propensity-based matching methods require careful documentation and potential bias adjustments.\n",
      "* **Conducting sensitivity analysis:** As causal estimates rely on untestable assumptions, it's crucial to assess their sensitivity to violations of those assumptions.\n",
      "* **Ensuring consistency:** The model's causal effect should converge to the true causal effect with sufficient data. Validation through standard held-out tests is not feasible, requiring alternative randomization or data collection strategies.\n",
      "\n",
      "The MLTRL framework highlights the importance of carefully documenting assumptions and biases, citing a case study where a causal approach outperformed traditional machine learning methods.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The MLTRL framework addresses challenges faced in healthcare AI projects by ensuring proper documentation and tackling issues with independent groups of healthcare professionals. It involves:\n",
      "\n",
      "**Step 1: Causal Structure Identification**\n",
      "\n",
      "- Evaluates if the causal question of interest is uniquely identifiable from the causal structure and observational/experimental data.\n",
      "- Ensures this crucial aspect is considered, as a mathematical proof of identifiability is required to progress beyond Level 0.\n",
      "\n",
      "**Step 2: Causal Estimate**\n",
      "\n",
      "- Develops ways to estimate the causal effect of interest from data, addressing confounding bias.\n",
      "- Employs propensity score-based methods for binary targets and multi-stage ML models for continuous targets.\n",
      "\n",
      "**Key Highlights:**\n",
      "\n",
      "- MLTRL ensures the proper handling of independent healthcare professional groups.\n",
      "- Identifiability and causal structure are crucial for causal inference.\n",
      "- Propensity score-based methods are used to adjust for confounding bias.\n",
      "- Quality of bias adjustment needs to be evaluated and documented.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The text emphasizes the importance of thorough testing and validation for causal machine learning (ML) models developed using the MLTRL framework. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Bias Adjustment:**\n",
      "    - Certain bias adjustments may not be sufficient.\n",
      "    - MLTRL ensures transparency and adheres to bias adjustment procedures.\n",
      "\n",
      "\n",
      "* **Sensitivity Analysis:**\n",
      "    - MLTRL requires sensitivity analysis to assess the robustness of causal effects to model assumptions.\n",
      "    - This analysis is crucial for ensuring reliable causal inferences.\n",
      "\n",
      "\n",
      "* **Coding Best Practices:**\n",
      "    - Standard ML coding best practices are required at Level 3.\n",
      "\n",
      "\n",
      "* **Additional Tests for Production:**\n",
      "    - Levels 4-5 involve tests to ensure consistency and other critical properties in real-world scenarios.\n",
      "\n",
      "\n",
      "* **Transparency and Adherence:**\n",
      "    - MLTRL promotes transparency by requiring documentation of bias adjustments and sensitivity analysis.\n",
      "    - Models must pass these tests before graduating from various levels of the MLTRL framework.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "**1. Machine Learning for Causal Model Validation:**\n",
      "\n",
      "- MLTRL technology allows for efficient validation of causal models by enabling early specification of evaluation metrics.\n",
      "- This is crucial as causal models cannot be validated using standard held-out tests and require specific randomization or data collection methods.\n",
      "\n",
      "\n",
      "**2. Application in Space Science:**\n",
      "\n",
      "- The CAMS project uses hundreds of cameras to track meteors in the night sky.\n",
      "- Initial analysis involved manual identification and classification of meteors from video data.\n",
      "- An ML model was developed to automate this process, leading to significant improvements in efficiency and accessibility.\n",
      "\n",
      "\n",
      "**Key Benefits:**\n",
      "\n",
      "- Automated analysis of large datasets.\n",
      "- Increased efficiency and accessibility of scientific research.\n",
      "- Global expansion of citizen science participation in meteor observation.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The Cosmic Aerodynamic Meteor Surveillance (CAMS) project utilized machine learning to significantly advance the study of meteor showers.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "- Discovery of 10 new meteor showers.\n",
      "- Confirmation of previously predicted comets through instrumental evidence.\n",
      "- Identification of parent bodies for various meteor showers.\n",
      "\n",
      "**Methodology:**\n",
      "\n",
      "- **Data Capture Network:** Extensive data collection from various sources.\n",
      "- **MLTRL Framework:** Agile and non-monotonic development process with multiple review stages.\n",
      "- **Data Exploration:** Visual analysis to understand characteristics of meteors and mitigate data imbalance issues.\n",
      "\n",
      "**Outcomes:**\n",
      "\n",
      "- Improved understanding of meteoroid populations.\n",
      "- Development of open-source code and data accessible through the SpaceML platform.\n",
      "\n",
      "**Significance:**\n",
      "\n",
      "- Advancements in meteor research.\n",
      "- Contribution to the understanding of celestial objects and their origins.\n",
      "- Open-source approach promoting collaboration and innovation.\n",
      "Keep in mind the important points and also give a title.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the complete summary of the following summaries:\n",
      "**Summary:**\n",
      "\n",
      "The team experimented with machine learning (ML) model development and system scalability in a non-monotonic development cycle. The MLTRL review process played a crucial role in ensuring model robustness, functionality, and addressing data challenges.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "- Early review stages focused on model quality and scalability.\n",
      "- Reviews at later stages emphasized numerical improvements, baseline comparisons, and addressing data imbalance.\n",
      "- The diverse panel of reviewers provided valuable verification and validation measures.\n",
      "- Development of a web tool (NASA CAMS Meteor Shower Portalviii) validated predictions and facilitated human-AI interaction.\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "- Continue leveraging the MLTRL review process to enhance model quality and address data challenges.\n",
      "- Consider developing similar web-based tools to demonstrate the real-world impact of ML models.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The NASA CAMS Meteor Shower Portal 2.0 was developed using Machine Learning (ML) technology with the help of citizen scientists. The MLTRL framework was crucial for:\n",
      "\n",
      "* Providing real-time feedback for model improvement.\n",
      "* Incorporating user feedback and integrating celestial reference points into the portal.\n",
      "* Verifying and validating the ML models and modules for integration with existing infrastructure.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "* Integrating the ML models with existing infrastructure and tools.\n",
      "* Integrating open-source contributions into the ML subsystem.\n",
      "\n",
      "**Results:**\n",
      "\n",
      "* Enhanced robustness and performance of the ML model.\n",
      "* Release of the NASA CAMS Meteor Shower Portal 2.0 with features requested by users.\n",
      "* Improved communication and project efficiency through the use of technology readiness levels (TRLs).\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The provided text describes the ML testing and deployment approach used in the CAMS project, which aims to track meteors using machine learning.\n",
      "\n",
      "**Key points:**\n",
      "\n",
      "* **Level 8:** Active learning techniques were used to leverage unlabeled data and improve model performance.\n",
      "* **Level 9:** Continuous monitoring and automated testing are employed to address data drifts and model changes.\n",
      "* **Monitoring and Adaptation:** Mechanisms are in place to handle drifts in weather patterns, location-specific issues, and false positives.\n",
      "* **Open Source:** Code, models, and tools are released publicly for community involvement and continuous improvement.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "Software engineering (SWE) practices differ across industries, with some domains requiring rigorous development processes due to regulations. ML development faces unique challenges due to its data-driven nature, resulting in data-related complexities not found in traditional SWE. MLTRL proposes a Data-Oriented Architecture (DOA) to address these challenges, emphasizing data availability, traceability, and data-related requirements throughout the development process. Additionally, MLTRL suggests specific testing considerations to uncover potential model failures caused by data shifts or other ML-specific issues.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The paper highlights the challenges of traditional software engineering (SWE) practices in the context of machine learning (ML) development. It argues that ML systems introduce new threat vectors and complexities that necessitate new approaches to risk management and code management.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- **ML-speciﬁc threat vectors:** ML systems are vulnerable to poisoning attacks and membership inference, which can compromise model integrity.\n",
      "\n",
      "\n",
      "- **Entanglement and complexity:** ML codebases are more entangled and dynamic than traditional software, making isolated changes impractical.\n",
      "\n",
      "\n",
      "- **Technical debt:** ML systems accumulate technical debt due to their complex nature, requiring different debt management strategies.\n",
      "\n",
      "\n",
      "- **Synergistic approach:** MLTRL can be combined with existing SWE practices like agile and waterfall methodologies to address ML-speciﬁc challenges.\n",
      "\n",
      "\n",
      "- **Applicability of SWE practices:** Conventional SWE practices like version control, testing, and deployment can be applied to ML development with the help of MLTRL.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "MLTRL provides a framework to extend existing SWE practices and enhance the robustness and reliability of ML systems.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The provided text discusses the unique challenges of Machine Learning (ML) development and explores various ML workflows and recommendations.\n",
      "\n",
      "**Key points:**\n",
      "\n",
      "* Existing ML workflows often focus on isolated model development, neglecting the broader ML and data technology lifecycle.\n",
      "* A linear ML workflow is recommended, but it doesn't guarantee quality and robustness improvements.\n",
      "* Different workflows exist for specific ML methods, but they don't address the evolving nature of the technology.\n",
      "* The MLTRL framework emphasizes the need for \"switchback\" mechanisms, allowing for regression to earlier stages of development when necessary.\n",
      "* The text suggests that successful ML development requires an iterative approach with continuous evaluation and feedback.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "Iteration plays a crucial role in data, machine learning (ML), and software processes. MLTRL stands out from other recommended processes by considering data flows and ML models within broader system contexts. It emphasizes the need for reliable and responsible use of other development processes, such as modeling and data wrangling, in production environments dealing with sparse and noisy data.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- MLTRL provides a holistic framework for using other ML development processes responsibly.\n",
      "- Traditional ML testing practices often fail to translate effectively to real-world settings.\n",
      "- Checklists and testing methodologies inspired by metamorphic testing can improve NLP application performance.\n",
      "- Ethical considerations, end-user trust, and enhanced security are crucial in ML deployments.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "The provided text discusses new threat vectors throughout the ML deployment workflow and proposes testing methods to address them.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* Existing works on ML security and quantifying test suites are valuable, but lack a holistic approach for the entire ML lifecycle.\n",
      "* An end-to-end approach for ML auditing is suggested by Raji et al., but specifically focuses on algorithm auditing.\n",
      "* Sculley et al. recommend an ML Testing Rubric as a production checklist, encompassing testing of infrastructure, model development, features & data, and monitoring.\n",
      "* Specific testing methods mentioned include:\n",
      "    * **Canary deployment:** Testing models before production release.\n",
      "    * **Shadow testing:** Testing models alongside production systems.\n",
      "    * **Metamorphic testing:** Testing relationships between outputs of multiple inputs.\n",
      "\n",
      "\n",
      "**Additional Notes:**\n",
      "\n",
      "* The text mentions the MLTRL framework but doesn't elaborate on its specific features or how it relates to the discussed testing methods.\n",
      "* The connection between the suggested testing methods and the quantifying of an ML test suite remains unclear.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "- Testing methodologies for ML systems are evolving, with checklists and regulatory guidelines emerging in healthcare and other domains.\n",
      "\n",
      "\n",
      "- Existing checklists focus on data readiness and real-world evidence quality but are not widely accepted.\n",
      "\n",
      "\n",
      "- Regulatory frameworks for AI in healthcare are still under development, drawing inspiration from MLTRL recommendations.\n",
      "\n",
      "\n",
      "- Model miscalibration is a significant issue in ML, leading to underspecification and practical challenges in deep learning pipelines.\n",
      "\n",
      "\n",
      "- Stress tests and other methods outlined in MLTRL can be used to mitigate model inaccuracies and biases.\n",
      "\n",
      "\n",
      "- MLTRL promotes responsible and ethical use of ML by empowering practitioners and stakeholders to handle limitations and risks.\n",
      "\n",
      "The provided text suggests that while organizations may be aware of data, ML, and integration challenges, they often fail to address them. MLTRL is likened to a pilot's checklist, providing guidance but not automatic solutions.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "**MLTRL Framework for Machine Learning:**\n",
      "\n",
      "- MLTRL is a framework to assess and improve the quality and reliability of Machine Learning (ML) projects.\n",
      "\n",
      "\n",
      "- **Purpose:**\n",
      "    - Compliments existing software development methodologies.\n",
      "    - Helps define and structure project phases and success criteria.\n",
      "    - Minimizes technical debt and risk associated with ML deployment.\n",
      "\n",
      "\n",
      "- **Data Considerations:**\n",
      "    - Data availability and quality are crucial for successful ML development.\n",
      "    - MLTRL emphasizes data handling with rigor and transparency, but doesn't address data bias.\n",
      "\n",
      "\n",
      "- **Ethical Considerations:**\n",
      "    - MLTRL prioritizes ethics throughout the ML development process.\n",
      "    - Needs to evolve alongside AI/ML ethics advancements.\n",
      "\n",
      "\n",
      "- **Applications:**\n",
      "    - Demonstrated effectiveness in diverse industries, from research to deployment in healthcare and physics.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "MLTRL (Machine Learning Technology Readiness Levels) emphasizes the importance of data readiness alongside other critical challenges in AI development. It establishes a lingua franca for the AI ecosystem and promotes the adoption of \"technology readiness levels\" across stakeholders.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "- MLTRL focuses on data readiness, privacy, security, and ethics.\n",
      "- It establishes a gated review process with specific criteria and key decision points.\n",
      "- Reviews involve technical development review, requirements verification, validation steps, and post-mortem discussions.\n",
      "- The review process includes an AI ethics checklist.\n",
      "- MLTRL promotes continuous improvement through regular reviews and iterative feedback.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* Ethical reviews are crucial throughout the ML project lifecycle to ensure bias mitigation and model effectiveness.\n",
      "\n",
      "\n",
      "* TRL Cards are information-dense documents that serve as \"report cards\" to track model development and communication.\n",
      "\n",
      "\n",
      "* TRL Cards include project information and implicit knowledge, providing a comprehensive overview of model development.\n",
      "\n",
      "\n",
      "* Regular debriefings and meta-evaluations are essential for continuous process improvement in MLTRL.\n",
      "\n",
      "\n",
      "* MLTRL is a flexible framework that organizations can adapt to their specific needs and resources.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The paper proposes **TRL Cards** as a framework for reporting and managing risks associated with Machine Learning (ML) projects. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **Modeling transparency and risk:**\n",
      "    * TRL Cards cover aspects beyond code, including models, data, and assumptions.\n",
      "    * Reporting includes modeling assumptions, dataset biases, and corner cases.\n",
      "    * Cards promote transparency and trust within teams and across organizations.\n",
      "\n",
      "\n",
      "* **Risk mitigation:**\n",
      "    * Risk is treated as a primary concern, with a formula for quantifying risk based on probability of failure and value. \n",
      "    * Quantifying risks helps identify potential vulnerabilities.\n",
      "    * The framework emphasizes the need for sim-to-real testing to bridge the gap between simulation and real-world deployment.\n",
      "\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* Improved transparency and communication across ML project stakeholders.\n",
      "* Increased awareness and mitigation of potential risks.\n",
      "* Enhanced trust in ML models and their deployment.\n",
      "\n",
      "**Additional Information:**\n",
      "\n",
      "* Open-source TRL Card templates will be available upon publication.\n",
      "* The framework is inspired by risk management practices in spacecraft engineering.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "Coverage through cyclic development paths is crucial for mitigating risks and uncertainties in AI system development. The paper proposes three types of switchback mechanisms to encourage cyclical development:\n",
      "\n",
      "* **Discovery switchbacks:** Occur naturally as new technical gaps are discovered during system integration.\n",
      "* **Review switchbacks:** Result from gated reviews, where components are dialed back to earlier levels based on business needs or technical concerns.\n",
      "* **Embedded switchbacks:** Predefined in the development process to mitigate technical debt and improve efficiency.\n",
      "\n",
      "By incorporating these switchback mechanisms, AI systems can handle non-monotonic and non-linear behavior, allowing for continuous refinement and improvement over time.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "* Suppressing and minimizing errors is not ideal in AI/ML development as they are inevitable and can foster innovation.\n",
      "* Traditional approaches like stage-gate processes and TRL framework are inadequate for ML projects.\n",
      "* Most ML projects begin at higher stages, not from the ground up.\n",
      "* The MLTRL process emphasizes verifying and validating pre-trained models, rather than skipping verification steps like traditional approaches.\n",
      "* Incorporating validated components from other projects enhances efficiency and simplifies V&V.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "MLTRL (Machine Learning Technology Readiness Level) addresses the challenges of tracking and demonstrating progress in Machine Learning (ML) development. It explicitly manages the evolving nature of ML projects by:\n",
      "\n",
      "* **Evolving teams and objectives:** MLTRL acknowledges the changing composition of teams and the shifting focus of ML objectives over different stages of development.\n",
      "* **Quantifiable progress:** MLTRL defines metrics based on industry-standard OKRs and KPIs, allowing for precise tracking of ML progress.\n",
      "* **Improved communication:** By capturing and visualizing ML progress, MLTRL enhances communication and understanding of ML development among stakeholders.\n",
      "\n",
      "**Key points:**\n",
      "\n",
      "* MLTRL addresses the transition between levels of ML development, explicitly managing team composition, objectives, and deliverables.\n",
      "* It quantifies ML maturity through established metrics like OKRs and KPIs.\n",
      "* MLTRL provides insights into ML workflows by analyzing development time and common patterns.\n",
      "Keep in mind the important points and also give a title.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the complete summary of the following summaries:\n",
      "## Summary:\n",
      "\n",
      "**1. Communication and Explanation:**\n",
      "\n",
      "* MLTRL offers a clear and consistent grading system for AI technology maturity, enabling effective communication and transparency.\n",
      "* It promotes interpretability and explainability of individual models and the entire AI system.\n",
      "\n",
      "\n",
      "**2. Robustness via Uncertainty-aware ML:**\n",
      "\n",
      "* Building reliable AI systems requires addressing uncertainties associated with data, models, and decisions.\n",
      "* Probabilistic ML methods excel in quantifying these uncertainties, leading to better system reliability.\n",
      "\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* MLTRL improves communication and transparency in AI development.\n",
      "* Uncertainty quantification is crucial for reliable AI systems.\n",
      "* Probabilistic ML offers a solution for quantifying uncertainties.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The author highlights the challenges in propagating errors and uncertainties in Bayesian deep learning models used for autonomous vehicles. While these models can work effectively for simple systems, a principled and generalizable approach for complex AI systems is still lacking. The author emphasizes the need to address this issue in future research.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "The provided references cover various aspects of Deep Tech, including:\n",
      "\n",
      "**1. Ecosystem & Applications:**\n",
      "\n",
      "- The dawn of the deep tech ecosystem (Arnaud de la Tour, Massimo Portincaso, Kyle Blank, Nicolas Goeldel)\n",
      "- Applications of deep tech in various fields like space engineering (NASA) and defense acquisition (U.S. Department of Defense)\n",
      "\n",
      "**2. Machine Learning & Data:**\n",
      "\n",
      "- Understanding the ethics and safety of AI (D. Leslie)\n",
      "- Google's machine learning workflow (Google)\n",
      "- Technology readiness levels for AI & ML (Alexander Lavin and Gregory Renard)\n",
      "- Data governance for trustworthy AI (M. Janssen et al.)\n",
      "\n",
      "**3. Optimization & Privacy:**\n",
      "\n",
      "- Automated optimization methods using Bayesian statistics (B. Shahriari et al.)\n",
      "- Managing data for healthcare ML (Goutham Ramakrishnan et al.)\n",
      "- Explainability and privacy in ML deployment (Umang Bhatt et al.)\n",
      "\n",
      "**4. Federated Learning:**\n",
      "\n",
      "- Challenges and future directions of this decentralized learning approach (Tian Li et al.)\n",
      "\n",
      "**5. Privacy-Preserving Deep Learning:**\n",
      "\n",
      "- Framework for privacy-preserving deep learning models (T. Ryffel et al.)\n",
      "\n",
      "The provided references explore the issue of **privacy-preserving deep learning** and the importance of **testing and validating** such models.\n",
      "\n",
      "**Relevant findings:**\n",
      "\n",
      "* **Adversarial attacks:** Some papers discuss the vulnerability of deep learning models to adversarial attacks, where malicious inputs can manipulate model outputs.\n",
      "* **Model behavior:** Papers [17] and [18] highlight the need to go beyond accuracy and assess model behavior through diverse tests.\n",
      "* **Verification and validation:** Papers [19] and [20] emphasize the importance of verifying and validating models to ensure their trustworthiness and production readiness.\n",
      "* **Metrics and risk assessment:** Papers [21] and [22] discuss the need for appropriate metrics and risk assessment frameworks to measure and mitigate potential privacy risks.\n",
      "\n",
      "**Key recommendations:**\n",
      "\n",
      "* Implement techniques to mitigate adversarial attacks.\n",
      "* Develop comprehensive testing and validation methodologies.\n",
      "* Define relevant metrics and assess privacy risks.\n",
      "* Design risk mitigation strategies.\n",
      "\n",
      "**Overall, the references emphasize the importance of ensuring privacy, trustworthiness, and accountability in the development and deployment of deep learning models.**\n",
      "\n",
      "The provided text does not contain any specific summary, so I am unable to extract the requested data from the given context.\n",
      "\n",
      "## Summary of the Provided References:\n",
      "\n",
      "**1. Reliable Decision Support using Counterfactual Models (2017)**\n",
      "\n",
      "- Introduces a method for generating reliable decision support by utilizing counterfactual models.\n",
      "\n",
      "\n",
      "**2. Towards Trustworthy Machine Learning (2018)**\n",
      "\n",
      "- Discusses the importance of trustworthiness in machine learning models and suggests ways to achieve it.\n",
      "\n",
      "\n",
      "**3. Probabilistic Machine Learning and Artificial Intelligence (2015)**\n",
      "\n",
      "- Reviews the development of probabilistic machine learning and its applications in AI.\n",
      "\n",
      "\n",
      "**4. Concrete Problems for Autonomous Vehicle Safety (2017)**\n",
      "\n",
      "- Highlights the benefits of Bayesian deep learning for addressing safety-critical problems in autonomous vehicles.\n",
      "\n",
      "\n",
      "**5. Common Pitfalls and Recommendations for Using Machine Learning for COVID-19 Detection (2021)**\n",
      "\n",
      "- Analyzes the challenges and potential pitfalls of using machine learning for diagnosing COVID-19 based on chest radiographs and CT scans.\n",
      "\n",
      "\n",
      "**6. Domain Randomization for Transferring Deep Neural Networks (2017)**\n",
      "\n",
      "- Discusses the problem of transferring knowledge from simulated environments to the real world using domain randomization techniques.\n",
      "\n",
      "\n",
      "**7. Unity: A General Platform for Intelligent Agents (2018)**\n",
      "\n",
      "- Presents Unity, a platform for developing and deploying intelligent agents in various environments.\n",
      "\n",
      "\n",
      "**8. An Annotation Saved is an Annotation Earned (2019)**\n",
      "\n",
      "- Introduces a method for training object detection models using synthetic data generated from realistic data.\n",
      "\n",
      "\n",
      "**9. Unity: A General Platform for Intelligent Agents (2018)**\n",
      "\n",
      "- Provides a comprehensive overview of the Unity platform and its capabilities for developing intelligent agents.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "The paper discusses the use of **probabilistic programming** for **synthetic data generation** in **computer vision**. \n",
      "\n",
      "**Background:**\n",
      "\n",
      "* Traditional machine learning models require large amounts of labeled training data, which can be expensive and difficult to obtain.\n",
      "* Simulation-based inference offers an alternative by generating realistic synthetic data from probabilistic models.\n",
      "\n",
      "**Proposed approach:**\n",
      "\n",
      "* The paper proposes using **probabilistic programming** to create a flexible and scalable framework for generating synthetic data in computer vision tasks.\n",
      "* This approach allows researchers to model the uncertainty and variability in real-world data.\n",
      "\n",
      "**Contributions:**\n",
      "\n",
      "* The paper explores the use of **Etalumis**, a probabilistic programming platform, for generating synthetic data for various computer vision tasks.\n",
      "* It discusses the importance of **misclassification costs** in training computer vision models, drawing inspiration from medical diagnosis applications.\n",
      "\n",
      "**Potential applications:**\n",
      "\n",
      "* The proposed framework can be used to address the issue of **data scarcity** in various computer vision applications, such as object detection, image segmentation, and object tracking.\n",
      "* This approach can also be helpful in cases where obtaining real-world data is costly or unethical.\n",
      "\n",
      "**Future directions:**\n",
      "\n",
      "* The paper suggests future research directions, such as exploring the use of **generative adversarial networks (GANs)** and other advanced generative models for synthetic data generation.\n",
      "* It also emphasizes the importance of integrating domain knowledge into the probabilistic models for improved data quality.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The article explores the issue of causal inference in observational studies, specifically focusing on the presence of selection bias and confounding factors.\n",
      "\n",
      "**Key findings:**\n",
      "\n",
      "- **Theoretical limitations:** The paper argues that traditional machine learning algorithms face theoretical challenges in causal inference due to limitations in representing causal relationships.\n",
      "\n",
      "\n",
      "- **Double adjustment:** To address selection bias, the article proposes a \"double adjustment\" technique that involves simultaneously adjusting for confounding factors and selection bias.\n",
      "\n",
      "\n",
      "- **Propensity score matching:** The paper suggests using propensity score matching to reduce selection bias by selecting individuals with similar probabilities of treatment assignment.\n",
      "\n",
      "\n",
      "- **High-dimensional adjustments:** The authors acknowledge the challenges of adjusting for many potential confounding factors in high-dimensional settings.\n",
      "\n",
      "\n",
      "- **Heterogeneous causal effects:** The article discusses the need for split-treatment analysis to rank the strength of causal effects for different subgroups of the population.\n",
      "\n",
      "\n",
      "**Implications:**\n",
      "\n",
      "- The findings have implications for conducting observational studies in various fields, including healthcare, economics, and public policy.\n",
      "\n",
      "\n",
      "- The proposed methods can help researchers obtain more accurate and reliable estimates of causal effects from observational data.\n",
      "\n",
      "\n",
      "- The paper highlights the importance of addressing selection bias and confounding factors to improve the validity of causal inferences.\n",
      "\n",
      "## Summary:\n",
      "\n",
      "The provided references cover various applications of machine learning (ML) and related challenges:\n",
      "\n",
      "**Healthcare:**\n",
      "\n",
      "* **Improving medical diagnosis:** ML algorithms can be used for more accurate diagnoses by identifying causal relationships in medical data (Richens et al., 2020).\n",
      "\n",
      "\n",
      "**Machine Learning Deployment:**\n",
      "\n",
      "* Challenges in deploying ML models are highlighted, with case studies analyzing deployment issues (Paleyes et al., 2020).\n",
      "\n",
      "\n",
      "**Machine Learning for Economic Applications:**\n",
      "\n",
      "* Strategies for \"double/debiased\" ML are proposed to address bias in treatment recommendations and structural parameter estimations (Chernozhukov et al., 2018).\n",
      "\n",
      "\n",
      "**Computer Vision and Space Science:**\n",
      "\n",
      "* Research on using cameras for detecting and classifying meteors (Jenniskens et al., 2011) and developing AI accelerators for space exploration (Ganju et al., 2020).\n",
      "\n",
      "\n",
      "**Active Learning:**\n",
      "\n",
      "* Techniques for active learning are reviewed, where algorithms can selectively choose data points for training to improve model performance (Cohn et al., 1994).\n",
      "\n",
      "## Summary of Research Papers:\n",
      "\n",
      "**1. Deep Bayesian Active Learning with Image Data:**\n",
      "- Presents a framework for active learning using deep Bayesian models to select informative data points for image classification.\n",
      "\n",
      "\n",
      "**2. Hidden Technical Debt in Machine Learning Systems:**\n",
      "- Analyzes the \"hidden technical debt\" in machine learning systems, focusing on factors like data quality, model complexity, and algorithmic choices.\n",
      "\n",
      "\n",
      "**3. Agile Software Development Methods:**\n",
      "- Reviews agile methodologies for software development, emphasizing their strengths and limitations.\n",
      "\n",
      "\n",
      "**4. Hybrid Software and System Development in Practice:**\n",
      "- Discusses hybrid approaches to software development that combine traditional waterfall methods with agile practices.\n",
      "\n",
      "\n",
      "**5. Bayesian Workflow:**\n",
      "- Introduces a workflow management framework based on Bayesian statistics for improved workflow efficiency and quality.\n",
      "\n",
      "\n",
      "**6. Crisp-dm 1.0:**\n",
      "- Provides a step-by-step guide for data mining tasks, focusing on practical data handling and analysis techniques.\n",
      "\n",
      "\n",
      "**7. Understanding and Visualizing Data Iteration in Machine Learning:**\n",
      "- Explores the concept of \"data iteration\" in machine learning, emphasizing the importance of visualization for understanding model behavior.\n",
      "\n",
      "\n",
      "**8. Power to the People:**\n",
      "- Highlights the role of human feedback in interactive machine learning systems, empowering users to guide model development.\n",
      "\n",
      "\n",
      "**9. Data Validation for Machine Learning:**\n",
      "- Discusses data validation techniques for machine learning models, focusing on identifying and mitigating potential issues.\n",
      "\n",
      "\n",
      "**10. Failure Modes in Machine Learning Systems:**\n",
      "- Identifies various failure modes in machine learning systems and proposes methods for robust system design and deployment.\n",
      "\n",
      "The provided text discusses the importance of accountability and regulation in the context of artificial intelligence (AI). The referenced articles explore the following:\n",
      "\n",
      "**1. Closing the AI accountability gap:**\n",
      "- This paper proposes a framework for \"internal algorithmic auditing\" to assess and improve the accountability of AI systems.\n",
      "- The aim is to bridge the gap between the theoretical understanding of AI accountability and its practical implementation.\n",
      "\n",
      "\n",
      "**2. Ensuring regulatory-grade data quality:**\n",
      "- This article outlines a checklist to evaluate the quality of real-world data used to train AI models.\n",
      "- This is crucial for ensuring the reliability and regulatory compliance of AI applications.\n",
      "\n",
      "\n",
      "**3. Regulatory frameworks for AI-based diagnostic imaging:**\n",
      "- This paper reviews regulatory frameworks for AI algorithms used in medical imaging.\n",
      "- It provides recommendations for developing and evaluating such algorithms.\n",
      "\n",
      "**Overall, the text emphasizes the need for robust accountability, data quality, and regulatory frameworks to ensure the responsible and trustworthy deployment of AI technologies.**\n",
      "Keep in mind the important points and also give a title.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the complete summary of the following summaries:\n",
      "## Summary of the provided references:\n",
      "\n",
      "**1. Bias and Fairness in Machine Learning:**\n",
      "\n",
      "- This paper reviews the problem of bias and fairness in machine learning algorithms.\n",
      "- It explores various factors that can introduce bias into the training data and the algorithmic decision-making process.\n",
      "\n",
      "\n",
      "**2. Bias in Data-Driven AI Systems:**\n",
      "\n",
      "- This paper provides a comprehensive survey on the issue of bias in data-driven AI systems.\n",
      "- It discusses different types of bias, their potential impact on AI performance, and various methods for detecting and mitigating bias.\n",
      "\n",
      "\n",
      "**3. Lessons from Archives:**\n",
      "\n",
      "- This paper proposes strategies for collecting and analyzing sociocultural data in machine learning models to better understand their impact on outcomes.\n",
      "\n",
      "\n",
      "**4. Diagnosing Bias in Data-Driven Algorithms:**\n",
      "\n",
      "- This paper explores methods for identifying bias in healthcare algorithms based on data quality and algorithmic transparency.\n",
      "\n",
      "\n",
      "**5. Artiﬁcial Intelligence, Bias and Clinical Safety:**\n",
      "\n",
      "- This paper highlights the potential risks of bias in AI algorithms used in healthcare and outlines measures to ensure clinical safety.\n",
      "\n",
      "\n",
      "**6. Dissecting Racial Bias in an Algorithm:**\n",
      "\n",
      "- This paper analyzes a healthcare algorithm and identifies factors that perpetuate racial bias in its recommendations.\n",
      "\n",
      "\n",
      "**7. Model Cards for Model Reporting:**\n",
      "\n",
      "- This paper proposes a framework for reporting machine learning models and their potential biases to improve transparency and accountability.\n",
      "\n",
      "\n",
      "**8. Guidelines for Clinical Trial Protocols:**\n",
      "\n",
      "- This paper suggests guidelines for incorporating AI algorithms into clinical trials and mitigating potential biases in healthcare interventions.\n",
      "\n",
      "\n",
      "**9. Managing Innovation in Architecturally Hierarchical Systems:**\n",
      "\n",
      "- This paper discusses strategies for managing innovation within complex systems, focusing on the interplay between technological advancements and organizational practices.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The paper proposes a framework for applying machine learning (ML) to perform reliable and trustworthy diagnoses and interventions in various domains. The framework addresses the challenges of deploying ML models in real-world applications, where uncertainties and errors can occur.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "- The paper proposes a novel approach for quantifying and mitigating uncertainties in ML models using a technique called \"switchback mechanisms.\"\n",
      "- The framework incorporates causal reasoning capabilities, allowing for counterfactual diagnoses and interventions.\n",
      "- The paper demonstrates the applicability of the framework through multiple test cases in diverse domains, including healthcare, spacecraft engineering, particle physics, and computer vision.\n",
      "\n",
      "**Methodology:**\n",
      "\n",
      "- The authors developed a comprehensive framework that includes:\n",
      "    - Uncertainty quantification using statistical and probabilistic methods.\n",
      "    - Counterfactual diagnosis and intervention strategies.\n",
      "    - Switchback mechanisms for handling model errors and uncertainties.\n",
      "- The framework was validated using various test cases in different application domains.\n",
      "\n",
      "**Contributions:**\n",
      "\n",
      "- The paper provides a novel and practical framework for deploying ML models in real-world applications.\n",
      "- It addresses the critical issue of uncertainty quantification and mitigation in ML models.\n",
      "- The framework incorporates causal reasoning capabilities, enabling counterfactual diagnoses and interventions.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "The proposed framework can be applied in various domains, including:\n",
      "\n",
      "- Healthcare: Automated diagnosis and treatment recommendations.\n",
      "- Space exploration: Autonomous navigation and decision-making.\n",
      "- Engineering: Quality control and predictive maintenance.\n",
      "- Finance: Fraud detection and risk assessment.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The paper proposes a robust and practical framework for deploying ML models in real-world applications, addressing the challenges of uncertainty quantification, causal reasoning, and error mitigation.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "The authors declare that they have no competing interests related to the manuscript. They have contributed to its content and editing. For inquiries and material requests, please contact A.L.\n",
      "Keep in mind the important points and also give a title.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the complete summary of the following summaries:\n",
      "## Machine Learning Deployment: A Comprehensive Framework for Production-Ready Systems\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "This text details the Machine Learning Technology Readiness Levels (MLTRL) framework, a systematic approach for developing and deploying reliable and responsible machine learning (ML) systems. \n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "- **Machine Learning Technology Readiness Levels (MLTRL):** A standardized process for assessing the maturity of ML projects across different stages of development.\n",
      "- **Data Quality:** Emphasis on the importance of data integrity throughout the ML pipeline.\n",
      "- **Explainability and Robustness:** Need for interpretable models and robust models that can handle real-world scenarios.\n",
      "- **Deployment Considerations:** Challenges and factors to consider when deploying ML models in production environments.\n",
      "\n",
      "**Phases of MLTRL:**\n",
      "\n",
      "The MLTRL framework consists of multiple levels, each with specific requirements and considerations. \n",
      "\n",
      "- **Levels 0-2:** Research-driven, focusing on theoretical foundations and data analysis.\n",
      "- **Levels 3-4:** Development stages, emphasizing code quality, documentation, and data robustness.\n",
      "- **Levels 5-6:** Production-oriented, focusing on scalability, deployment considerations, and real-world application.\n",
      "\n",
      "**Benefits of MLTRL:**\n",
      "\n",
      "- Improved quality and reliability of ML systems.\n",
      "- Reduced risk of technical debt and failures.\n",
      "- Enhanced collaboration and communication across teams.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "The MLTRL framework has been applied in various real-world projects, including medical diagnostics, consumer computer vision, satellite imagery, and particle physics.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The MLTRL framework provides a systematic and comprehensive approach for developing and deploying robust and reliable ML systems. By addressing the challenges of data quality, explainability, and deployment, MLTRL enables organizations to successfully transition from research to production with machine learning technology.\n",
      "\n",
      "## Summary: Machine Learning Transition Levels (MLTRL) Framework\n",
      "\n",
      "**Introduction:**\n",
      "\n",
      "The MLTRL framework provides a systematic approach for developing and deploying reliable AI and ML systems. It emphasizes the importance of rigorous testing, monitoring, and feedback loops throughout the ML lifecycle.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "**1. Importance of Monitoring and Feedback Loops:**\n",
      "\n",
      "- Continuous feedback from testing and deployment stages is crucial for identifying and addressing performance issues.\n",
      "- Regular reviews and updates ensure that models remain effective and adapt to changing conditions.\n",
      "\n",
      "\n",
      "**2. Addressing Technological Maturity Gaps:**\n",
      "\n",
      "- MLTRL addresses challenges related to model availability, data access, and deployment in real-world settings.\n",
      "- Features like synthetic data generation and hierarchical anomaly detection enhance model reliability.\n",
      "\n",
      "\n",
      "**3. Flexible Development Process:**\n",
      "\n",
      "- The framework encourages iterative development, allowing for adjustments and feedback incorporation.\n",
      "- The ability to cycle back to earlier stages ensures continuous improvement and risk mitigation.\n",
      "\n",
      "\n",
      "**4. Applications Across Domains:**\n",
      "\n",
      "- MLTRL has been applied in various domains, including neuropathology, manufacturing, and computer vision.\n",
      "- The framework helps organizations overcome specific challenges within their respective fields.\n",
      "\n",
      "\n",
      "**5. Benefits of Using MLTRL:**\n",
      "\n",
      "- Improved model reliability and performance.\n",
      "- Enhanced data governance and accountability.\n",
      "- Streamlined technology transfer from research to production.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "MLTRL offers a valuable framework for organizations to develop and deploy reliable AI and ML systems. By prioritizing monitoring, feedback, and continuous improvement, MLTRL enables organizations to overcome challenges and achieve successful ML deployment.\n",
      "\n",
      "## Summary: MLTRL Framework for Machine Learning Development\n",
      "\n",
      "**Introduction:**\n",
      "\n",
      "The MLTRL framework addresses challenges in applying machine learning (ML) models by emphasizing interpretability, usability, and systematic development processes. It consists of nine levels, each with specific goals and deliverables.\n",
      "\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "- **Data Integration and Validation:** Development of APIs and workflows for reliable data integration and validation.\n",
      "- **Interpretability Enhancement:** Focus on model interpretability through techniques like Bayesian inference.\n",
      "- **Continuous Validation:** Implementation of various testing stages to ensure model performance and address potential issues.\n",
      "- **Collaboration and Transparency:** Promotion of open-source code and thorough documentation for transparency and reproducibility.\n",
      "\n",
      "\n",
      "**Challenges and Solutions:**\n",
      "\n",
      "- Integrating ML models with existing infrastructure and legacy systems.\n",
      "- Scalability and performance challenges when deploying ML models at scale.\n",
      "- Lack of interpretability leading to trust issues.\n",
      "\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "- Scientific applications in high-energy physics and particle physics.\n",
      "- Healthcare AI projects for causal inference and bias adjustment.\n",
      "- Space science projects for analyzing vast amounts of astronomical data.\n",
      "\n",
      "\n",
      "**Outcomes:**\n",
      "\n",
      "- Improved scalability, interpretability, and performance of ML models.\n",
      "- Enhanced scientific and industrial applications of ML in diverse fields.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The MLTRL framework provides a systematic approach for developing and deploying reliable and explainable ML models. Its emphasis on continuous validation, interpretability, and collaboration fosters trust and promotes successful ML implementation across industries and scientific disciplines.\n",
      "\n",
      "## Summary: MLTRL Framework for Machine Learning Development\n",
      "\n",
      "**Overview:**\n",
      "\n",
      "The MLTRL framework provides a systematic approach to assess and improve the quality and reliability of Machine Learning (ML) projects. It emphasizes the importance of data readiness, ethical considerations, and continuous improvement throughout the ML development lifecycle.\n",
      "\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "- **Machine Learning Technology Readiness Levels (TRL):** A set of criteria to evaluate the maturity of ML projects across different stages.\n",
      "- **Data Readiness:** Ensuring data quality, availability, and lineage.\n",
      "- **Ethical Review:** Addressing bias, fairness, and accountability in ML models.\n",
      "- **Review Process:** Gated reviews with specific criteria and decision points.\n",
      "\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "- Improved communication and understanding of ML development among stakeholders.\n",
      "- Increased awareness and mitigation of potential risks.\n",
      "- Enhanced efficiency and productivity of ML projects.\n",
      "- Continuous improvement through regular feedback and review cycles.\n",
      "\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "- Healthcare: Model validation and risk mitigation in medical diagnosis and treatment.\n",
      "- Space exploration: Data analysis and anomaly detection for space exploration missions.\n",
      "- Finance: Fraud detection and risk assessment in financial transactions.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "MLTRL fosters a data-driven and iterative approach to ML development, enabling organizations to build reliable and effective ML systems. By addressing the challenges of ML deployment, MLTRL promotes the responsible and successful adoption of ML in various industries.\n",
      "\n",
      "## Summary: Accountability, Data Quality, and Regulatory Frameworks for AI\n",
      "\n",
      "**Introduction:**\n",
      "\n",
      "The provided text emphasizes the crucial role of accountability, data quality, and regulatory frameworks in ensuring the responsible and trustworthy deployment of artificial intelligence (AI) technologies.\n",
      "\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "**1. Accountability Gap in AI:**\n",
      "\n",
      "- Existing AI systems lack clear accountability mechanisms, making it difficult to understand and explain their decisions.\n",
      "- The \"internal algorithmic auditing\" framework promotes accountability by enabling systematic assessment of AI algorithms.\n",
      "\n",
      "\n",
      "**2. Data Quality for AI:**\n",
      "\n",
      "- Ensuring regulatory-grade data quality is essential for training reliable and trustworthy AI models.\n",
      "- A checklist is provided to evaluate the quality of real-world data.\n",
      "\n",
      "\n",
      "**3. Regulatory Frameworks for AI-based Medical Imaging:**\n",
      "\n",
      "- Regulatory frameworks are necessary to ensure the safe and effective use of AI algorithms in medical imaging.\n",
      "- Guidelines for developing and evaluating such algorithms are discussed.\n",
      "\n",
      "\n",
      "**4. Importance of Accountability:**\n",
      "\n",
      "- The lack of accountability can undermine public trust in AI systems and hinder their adoption.\n",
      "- Mechanisms for accountability are crucial for addressing bias, mitigating risks, and ensuring fairness in AI applications.\n",
      "\n",
      "\n",
      "**5. Need for Regulatory Frameworks:**\n",
      "\n",
      "- Regulatory frameworks can provide guidelines for the development, deployment, and use of AI technologies.\n",
      "- These frameworks can help establish clear accountability and promote responsible AI development.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The text concludes by highlighting the urgency of addressing accountability, data quality, and regulatory challenges to ensure the responsible and trustworthy deployment of AI systems. By establishing robust accountability mechanisms, ensuring data integrity, and developing regulatory frameworks, we can maximize the benefits of AI while mitigating potential risks.\n",
      "\n",
      "## **Title: Managing Model Uncertainty and Bias in Machine Learning Applications**\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "This paper proposes a framework for deploying reliable and trustworthy machine learning (ML) models in real-world applications. It addresses the challenges of model uncertainties, biases, and errors.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "- The paper proposes a novel technique called \"switchback mechanisms\" for quantifying and mitigating model uncertainties.\n",
      "- It incorporates causal reasoning capabilities for counterfactual diagnoses and interventions.\n",
      "- The framework was validated through various test cases in healthcare, spacecraft engineering, particle physics, and computer vision.\n",
      "\n",
      "**Methodology:**\n",
      "\n",
      "- Comprehensive framework including:\n",
      "    - Uncertainty quantification using statistical and probabilistic methods.\n",
      "    - Counterfactual diagnosis and intervention strategies.\n",
      "    - Switchback mechanisms for handling model errors and uncertainties.\n",
      "- Validation using diverse test cases in different domains.\n",
      "\n",
      "**Contributions:**\n",
      "\n",
      "- Novel and practical framework for deploying ML models.\n",
      "- Addresses uncertainty quantification and mitigation.\n",
      "- Incorporates causal reasoning capabilities.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "- Healthcare: Automated diagnosis and treatment recommendations.\n",
      "- Space exploration: Autonomous navigation and decision-making.\n",
      "- Engineering: Quality control and predictive maintenance.\n",
      "- Finance: Fraud detection and risk assessment.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The paper proposes a robust framework for deploying ML models in real-world applications, addressing key challenges in uncertainty quantification, causal reasoning, and error mitigation.\n",
      "Keep in mind the important points and also give a title.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chunk_prompt = \"\"\"\n",
    "Please summarize the following:\n",
    "{text}\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "map_prompt_template = PromptTemplate(input_variables=[\"text\"], template= chunk_prompt)\n",
    "\n",
    "final_prompt = \"\"\"\n",
    "Provide the complete summary of the following summaries:\n",
    "{text}\n",
    "Keep in mind the important points and also give a title.\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt_template = PromptTemplate(input_variables=[\"text\"], template= final_prompt)\n",
    "\n",
    "chain2= load_summarize_chain(llm, chain_type='map_reduce', map_prompt=map_prompt_template, verbose=True, combine_prompt=reduce_prompt_template)\n",
    "summary2 = chain2.run(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**MLTRL: A Framework for Managing Machine Learning Model Development and Deployment**\\n\\nMLTRL (Machine Learning Testing and Review Loop) is a framework designed to guide the development and deployment of machine learning (ML) models. It aims to address the common challenges faced in ML projects, such as data quality issues, model bias, and deployment risks.\\n\\n**Key Features of MLTRL:**\\n\\n- **Iterative and risk-driven:** MLTRL involves multiple stages (Levels 0-9) that enable teams to identify potential risks and mitigate them through iterative development and review processes.\\n- **Data-centric approach:** Emphasis on data quality and governance from the early stages of development.\\n- **Transparency and communication:** Use of TRL Cards to document model maturity and assumptions, facilitating communication and collaboration.\\n- **Flexibility and adaptability:** Allows for non-linear progress and switchbacks between stages as needed.\\n\\n**Applications of MLTRL:**\\n\\n- Human-machine visual inspection\\n- Medical image analysis\\n- Anomaly detection\\n- Predictive modeling\\n\\n**Benefits of Using MLTRL:**\\n\\n- Improved model quality and reliability.\\n- Reduced deployment risks and costs.\\n- Enhanced communication and collaboration between stakeholders.\\n- Faster time to market for ML-powered products.\\n\\n**Challenges in Applying MLTRL:**\\n\\n- Requires organizational change and cultural shift.\\n- Can be resource-intensive.\\n- May not be suitable for all ML projects.\\n\\n**Examples of MLTRL in Action:**\\n\\n- Development of a machine vision application for automated recycling.\\n- Deployment of an anomaly detection model for medical imaging.\\n\\n**Conclusion:**\\n\\nMLTRL provides a systematic and risk-driven approach for managing ML model development and deployment. By implementing MLTRL, organizations can improve the quality, reliability, and efficiency of their ML-powered products and solutions.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Refine Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"TECHNOLOGY READINESS LEVELS\n",
      "FOR MACHINE LEARNING SYSTEMS\n",
      "Alexander Lavin∗\n",
      "Pasteur LabsCiarán M. Gilligan-Lee\n",
      "SpotifyAlessya Visnjic\n",
      "WhyLabsSiddha Ganju\n",
      "NvidiaDava Newman\n",
      "MIT\n",
      "Atılım Güne¸ s Baydin\n",
      "University of OxfordSujoy Ganguly\n",
      "Unity AIDanny Lange\n",
      "Unity AIAmit Sharma\n",
      "Microsoft Research\n",
      "Stephan Zheng\n",
      "Salesforce ResearchEric P. Xing\n",
      "PetuumAdam Gibson\n",
      "KonduitJames Parr\n",
      "NASA Frontier Development Lab\n",
      "Chris Mattmann\n",
      "NASA Jet Propulsion LabYarin Gal\n",
      "Alan Turing Institute\n",
      "ABSTRACT\n",
      "The development and deployment of machine learning (ML) systems can be executed easily with\n",
      "modern tools, but the process is typically rushed and means-to-an-end. The lack of diligence can\n",
      "lead to technical debt, scope creep and misaligned objectives, model misuse and failures, and\n",
      "expensive consequences. Engineering systems, on the other hand, follow well-deﬁned processes\n",
      "and testing standards to streamline development for high-quality, reliable results. The extreme is\n",
      "spacecraft systems, where mission critical measures and robustness are ingrained in the development\n",
      "process. Drawing on experience in both spacecraft engineering and ML (from research through\n",
      "product across domain areas), we have developed a proven systems engineering approach for machine\n",
      "learning development and deployment. Our Machine Learning Technology Readiness Levels (MLTRL)\n",
      "framework deﬁnes a principled process to ensure robust, reliable, and responsible systems while\n",
      "being streamlined for ML workﬂows, including key distinctions from traditional software engineering.\n",
      "Even more, MLTRL deﬁnes a lingua franca for people across teams and organizations to work\n",
      "collaboratively on artiﬁcial intelligence and machine learning technologies. Here we describe the\n",
      "framework and elucidate it with several real world use-cases of developing ML methods from basic\n",
      "research through productization and deployment, in areas such as medical diagnostics, consumer\n",
      "computer vision, satellite imagery, and particle physics.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Summary:**\n",
      "\n",
      "The article proposes the Machine Learning Technology Readiness Levels (MLTRL) framework, a systematic approach to ensure robust and reliable machine learning system development. Inspired by engineering practices in spacecraft engineering, MLTRL defines a process for ML development, from research to deployment, encompassing key steps like risk assessment, requirements gathering, and testing. The framework promotes collaboration and transparency across teams by establishing a common language for discussing ML readiness. Applications of MLTRL include medical diagnostics, computer vision, satellite imagery, and particle physics.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "computer vision, satellite imagery, and particle physics.\n",
      "Keywords: Machine Learning; Systems Engineering; Data Management; Medical AI; Space Sciences\n",
      "Introduction\n",
      "The accelerating use of artiﬁcial intelligence (AI) and machine learning (ML) technologies in systems of software,\n",
      "hardware, data, and people introduces vulnerabilities and risks due to dynamic and unreliable behaviors; fundamentally,\n",
      "ML systems learn from data, introducing known and unknown challenges in how these systems behave and interact with\n",
      "their environment. Currently the approach to building AI technologies is siloed: models and algorithms are developed\n",
      "in testbeds isolated from real-world environments, and without the context of larger systems or broader products they’ll\n",
      "be integrated within for deployment. A main concern is models are typically trained and tested on only a handful of\n",
      "curated datasets, without measures and safeguards for future scenarios, and oblivious of the downstream tasks and\n",
      "users. Even more, models and algorithms are often integrated into a software stack without regard for the inherent\n",
      "stochasticity –for instance, the massive effect random seeds have on deep reinforcement learning model performance\n",
      "[1] – and failure modes of the ML components, which can be dangerously hidden in layers of software and abstraction.\n",
      "Other domains of engineering, such as civil and aerospace, follow well-deﬁned processes and testing standards to\n",
      "streamline development for high-quality, reliable results. Technology Readiness Level (TRL) is a systems engineering\n",
      "protocol for deep tech[ 2] and scientiﬁc endeavors at scale, ideal for integrating many interdependent components\n",
      "∗lavin@simulation.science\n",
      "Preprint. Under review.arXiv:2101.03989v2  [cs.LG]  29 Nov 2021\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The article presents the Machine Learning Technology Readiness Levels (MLTRL) framework, a systematic approach to ensure robust and reliable machine learning system development. Inspired by engineering practices in spacecraft engineering, MLTRL defines a process for ML development, from research to deployment, encompassing key steps like risk assessment, requirements gathering, and testing. The framework promotes collaboration and transparency across teams by establishing a common language for discussing ML readiness. Applications of MLTRL include computer vision, satellite imagery, and particle physics.\n",
      "\n",
      "**MLTRL addresses the challenges associated with traditional ML development practices:**\n",
      "\n",
      "- Siloed development without consideration of real-world environments.\n",
      "- Limited and curated datasets, ignoring future scenarios.\n",
      "- Lack of attention to stochasticity and model failure modes.\n",
      "\n",
      "Inspired by established engineering processes like the Technology Readiness Level (TRL) framework, MLTRL provides a systematic and comprehensive approach to overcome these challenges.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "andcross-functional teams of people. It is no surprise that TRL is standard process and parlance in NASA[ 3] and\n",
      "DARPA[4].\n",
      "For a spaceﬂight project there are several deﬁned phases, from pre-concept to prototyping to deployed operations to\n",
      "end-of-life, each with a series of exacting development cycles and reviews. This is in stark contrast to common machine\n",
      "learning and software workﬂows, which promote quick iteration, rapid deployment, and simple linear progressions. Yet\n",
      "the NASA technology readiness process for spacecraft systems is overkill; we need robust ML technologies integrated\n",
      "with larger systems of software, hardware, data, and humans, but not necessarily for missions to Mars. We aim to bring\n",
      "systems engineering to AI and ML by deﬁning and putting into action a lean Machine Learning Technology Readiness\n",
      "Levels (MLTRL) framework. We draw on decades of AI and ML development, from research through production,\n",
      "across domains and diverse data scenarios: for example, computer vision in medical diagnostics and consumer apps,\n",
      "automation in self-driving vehicles and factory robotics, tools for scientiﬁc discovery and causal inference, streaming\n",
      "time-series in predictive maintenance and ﬁnance.\n",
      "In this paper we deﬁne our framework for developing and deploying robust, reliable, and responsible ML and data\n",
      "systems, with several real test cases of advancing models and algorithms from R&D through productization and\n",
      "deployment, including essential data considerations. Additionally, MLTRL prioritizes the role of AI ethics and\n",
      "fairness, and our systems AI approach can help curb the large societal issues that can result from poorly deployed and\n",
      "maintained AI and ML technologies, such as the automation of systemic human bias, denial of individual autonomy,\n",
      "and unjustiﬁable outcomes (see the Alan Turing Institute Report on Ethical AI [5]). The adoption and proliferation of\n",
      "MLTRL provides a common nomenclature and metric across teams and industries. The standardization of MLTRL\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to ensure robust and reliable machine learning system development. Inspired by engineering practices in spacecraft engineering, MLTRL defines a process for ML development, encompassing key steps like risk assessment, requirements gathering, and testing.\n",
      "\n",
      "MLTRL addresses the shortcomings of traditional ML development practices, which often suffer from siloed development, limited datasets, and inadequate attention to model failure modes. By drawing inspiration from established engineering processes like the Technology Readiness Level (TRL) framework, MLTRL provides a comprehensive and systematic approach to overcome these challenges.\n",
      "\n",
      "Inspired by decades of AI and ML development across domains, MLTRL prioritizes AI ethics and fairness, addressing concerns related to algorithmic bias, accountability, and societal impact. The framework promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "across the AI industry should help teams and organizations develop principled, safe, and trusted technologies.\n",
      "Figure 1: MLTRL spans research through prototyping, productization, and deployment. Most ML workﬂows prescribe\n",
      "an isolated, linear process of data processing, training, testing, and serving a model [ 6]. Those workﬂows fail to deﬁne\n",
      "how ML development must iterate over that basic process to become more mature and robust, and how to integrate with\n",
      "a much larger system of software, hardware, data, and people. Not to mention MLTRL continues beyond deployment:\n",
      "monitoring and feedback cycles are important for continuous reliability and improvement over the product lifetime.\n",
      "Results\n",
      "MLTRL deﬁnes technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment.\n",
      "A TRL represents the maturity of a model or algorithmii, data pipelines, software module, or composition thereof; a\n",
      "typical ML system consists of many interconnected subsystems and components, and the TRL of the system is the\n",
      "lowest level of its constituent parts [ 7]. The anatomy of a level is marked by gated reviews, evolving working groups,\n",
      "requirements documentation with risk calculations, progressive code and testing standards, and deliverables such as\n",
      "TRL Cards (Figure 3) and ethics checklists.iiiThese components—which are crucial for implementing the levels in a\n",
      "iiNote we use “model” and “algorithm” somewhat interchangeably when referring to the technology under development. The\n",
      "same MLTRL process and methods apply for a machine translation model and for an A/B testing algorithm, for example.\n",
      "iiiTemplates and examples for MLTRL deliverables will be open-sourced upon publication at github.com/alan-turing-institute.\n",
      "2\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to ensure robust and reliable machine learning system development. Inspired by engineering practices in spacecraft engineering, MLTRL defines a process for ML development, encompassing key steps like risk assessment, requirements gathering, and testing.\n",
      "\n",
      "MLTRL addresses the shortcomings of traditional ML development practices by prioritizing AI ethics and fairness, addressing concerns related to algorithmic bias, accountability, and societal impact. The framework promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "Inspired by decades of AI and ML development, MLTRL recognizes the iterative and interconnected nature of ML workflows, extending beyond deployment to include monitoring and feedback cycles for continuous reliability and improvement. The framework defines technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment, ensuring a systematic and accountable approach to ML system development.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "systematic fashion—as well as MLTRL metrics and methods are concretely described in examples and in the Methods\n",
      "section. Lastly, to emphasize the importance of data tasks in ML, from data curation [ 8] to data governance [ 9], we\n",
      "state several important data considerations at each MLTRL level.\n",
      "MACHINE LEARNING TECHNOLOGY READINESS LEVELS\n",
      "The levels are brieﬂy deﬁned as follows and in Figure 1, and elucidated with real-world examples later.\n",
      "Level 0 - First Principles This is a stage for greenﬁeld AI research, initiated with a novel idea, guiding question, or\n",
      "poking at a problem from new angles. The work mainly consists of literature review, building mathematical foundations,\n",
      "white-boarding concepts and algorithms, and building an understanding of the data – for work in theoretical AI and ML,\n",
      "however, there will not yet be data to work with (for example, a novel algorithm for Bayesian optimization[ 10], which\n",
      "could eventually be used for many domains and datasets). The outcome of Level 0 is a set of concrete ideas with sound\n",
      "mathematical formulation, to pursue through low-level experimentation in the next stage. When relevant, this level\n",
      "expects conclusions about data readiness, including strategies for getting the data to be suitable for the speciﬁc ML task.\n",
      "To graduate, the basic principles, hypotheses, data readiness, and research plans need to be stated, referencing relevant\n",
      "literature. With graduation, a TRL Card should be started to succinctly document the methods and insights thus far –\n",
      "this key MLTRL deliverable is detailed in the Methods section and Figure 3.\n",
      "Level 0 data – Not a hard requirement at this stage because this is largely theoretical machine learning. That being said,\n",
      "data availability needs to be considered for deﬁning any research project to move past theory.\n",
      "Level 0 review – The reviewer here is solely the lead of the research lab or team, for instance a PhD supervisor. We\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to ensure robust and reliable machine learning system development. Inspired by engineering practices in spacecraft engineering, MLTRL defines a process for ML development, encompassing key steps like risk assessment, requirements gathering, and testing.\n",
      "\n",
      "MLTRL addresses the shortcomings of traditional ML development practices by prioritizing AI ethics and fairness, addressing concerns related to algorithmic bias, accountability, and societal impact. The framework promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "Inspired by decades of AI and ML development, MLTRL recognizes the iterative and interconnected nature of ML workflows, extending beyond deployment to include monitoring and feedback cycles for continuous reliability and improvement. The framework defines technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment, ensuring a systematic and accountable approach to ML system development.\n",
      "\n",
      "The MLTRL framework provides detailed descriptions of its metrics and methods in the Methods section and includes concrete examples to illustrate its application. Additionally, it addresses data considerations at each MLTRL level, highlighting the importance of data curation and governance in ML development.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "assess hypotheses and explorations for mathematical validity and potential novelty or utility, not necessarily code nor\n",
      "end-to-end experiment results.\n",
      "Level 1 - Goal-Oriented Research To progress from basic principles to practical use, we design and run low-level\n",
      "experiments to analyze speciﬁc model or algorithm properties (rather than end-to-end runs for a performance benchmark\n",
      "score). This involves collection and processing of sample data to train and evaluate the model. This sample data need\n",
      "not be the full data; it may be a smaller sample that is currently available or more convenient to collect. In some\n",
      "cases it may sufﬁce to use synthetic data as the representative sample – in the medical domain, for example, acquiring\n",
      "datasets can take many months due to security and privacy constraints, so generating sample data can mitigate this\n",
      "blocker from early ML development. Further, working with the sample data provides a blueprint for the data collection\n",
      "and processing pipeline (including answering whether it is even possible to collect all necessary data), that can be\n",
      "scaled up for the for the next steps. The experiments, good results or not, and mathematical foundations need to pass a\n",
      "review process with fellow researchers before graduating to Level 2. The application is still speculative, but through\n",
      "comparison studies and analyses we start to understand if/how/where the technology offers potential improvements and\n",
      "utility. Code is research-caliber : The aim here is to be quick and dirty, moving fast through iterations of experiments.\n",
      "Hacky code is okay, and full test coverage is actually discouraged, as long as the overall codebase is organized and\n",
      "maintainable. It is important to start semantic versioning practices early in the project lifecycle, which should cover\n",
      "code, models, anddatasets. This is crucial for retrospectives and reproducibility, issues with which can be costly and\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to ensure robust and reliable machine learning system development. Inspired by engineering practices in spacecraft engineering, MLTRL defines a process for ML development, encompassing key steps like risk assessment, requirements gathering, and testing.\n",
      "\n",
      "MLTRL addresses the shortcomings of traditional ML development practices by prioritizing AI ethics and fairness, addressing concerns related to algorithmic bias, accountability, and societal impact. The framework promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "Inspired by decades of AI and ML development, MLTRL recognizes the iterative and interconnected nature of ML workflows, extending beyond deployment to include monitoring and feedback cycles for continuous reliability and improvement. The framework defines technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment, ensuring a systematic and accountable approach to ML system development.\n",
      "\n",
      "The MLTRL framework provides detailed descriptions of its metrics and methods in the Methods section and includes concrete examples to illustrate its application. Additionally, it addresses data considerations at each MLTRL level, highlighting the importance of data curation and governance in ML development. MLTRL also includes guidance on low-level experimentation, emphasizing the importance of rapid iteration and continuous assessment of model and algorithm properties.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "severe at later stages. This versioning information and additional progress should be reported on the TRL Card (see for\n",
      "example Figure 3).\n",
      "Level 1 data – At minimum we work with sample data that is representative of downstream real datasets, which can be\n",
      "a subset of real data, synthetic data, or both. Beyond driving low-level ML experiments, the sample data forces us to\n",
      "consider data acquisition and processing strategies at an early stage before it becomes a blocker later.\n",
      "Level 1 review – The panel for this gated review is entirely members of the research team, reviewing for scientiﬁc rigor\n",
      "in early experimentation, and pointing to important concepts and prior work from their respective areas of expertise.\n",
      "There may be several iterations of feedback and additional experiments.\n",
      "Level 2 - Proof of Principle (PoP) Development Active R&D is initiated, mainly by developing and running in\n",
      "testbeds : simulated environments and/or simulated data that closely matches the conditions and data of real scenarios –\n",
      "note these are driven by model-speciﬁc technical goals, not necessarily application or product goals (yet). An important\n",
      "deliverable at this stage is the formal research requirements document (with well-speciﬁed veriﬁcation and validation\n",
      "(V&V) steps)iv. Here is one of several key decision points in the broader process: The R&D team considers several\n",
      "paths forward and sets the course: (A) prototype development towards Level 3, (B) continued R&D for longer-term\n",
      "ivArequirement is a singular documented physical or functional need that a particular design, product, or process aims to satisfy.\n",
      "Requirements aim to specify all stakeholders’ needs while not specifying a speciﬁc solution. Deﬁnitions are incomplete without\n",
      "3\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to ensure robust and reliable machine learning system development. Inspired by engineering practices in spacecraft engineering, MLTRL defines a process for ML development, encompassing key steps like risk assessment, requirements gathering, and testing.\n",
      "\n",
      "MLTRL addresses the shortcomings of traditional ML development practices by prioritizing AI ethics and fairness, addressing concerns related to algorithmic bias, accountability, and societal impact. The framework promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "Inspired by decades of AI and ML development, MLTRL recognizes the iterative and interconnected nature of ML workflows, extending beyond deployment to include monitoring and feedback cycles for continuous reliability and improvement. The framework defines technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment, ensuring a systematic and accountable approach to ML system development.\n",
      "\n",
      "**Additional Context:**\n",
      "\n",
      "The MLTRL framework also addresses data considerations at each MLTRL level, highlighting the importance of data curation and governance in ML development. It includes guidance on low-level experimentation, emphasizing the importance of rapid iteration and continuous assessment of model and algorithm properties.\n",
      "\n",
      "Furthermore, the framework provides specific guidance on data handling at different stages of development, including sample data selection, review processes, and the use of simulated environments for testing. This comprehensive approach ensures that ML systems are developed and validated in a systematic and accountable manner, addressing potential challenges encountered at later stages of development.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "research initiatives and/or publications, or some combination of A and B. We ﬁnd the culmination of this stage is often\n",
      "a bifurcation: some work moves to applied AI, while some circles back for more research. This common MLTRL cycle\n",
      "is an instance of the non-monotonic discovery switchback mechanism (detailed in the Methods section).\n",
      "Level 2 data – Datasets at this stage may include publicly available benchmark datasets, semi-simulated data based\n",
      "on the data sample in Level 1, or fully simulated data based on certain assumptions about the potential deployment\n",
      "environments. The data should allow researchers to characterize model properties, and highlight corner cases or\n",
      "boundary conditions, in order to justify the utility of continuing R&D on the model.\n",
      "Level 2 review – To graduate from the PoP stage, the technology needs to satisfy research claims made in previous\n",
      "stages (brought to be bare by the aforementioned PoP data in both quantitative and qualitative ways) with the analyses\n",
      "well-documented and reproducible.\n",
      "Level 3 - System Development Here we have checkpoints that push code development towards interoperability,\n",
      "reliability, maintainability, extensibility, and scalability. Code becomes prototype-caliber : A signiﬁcant step up from\n",
      "research code in robustness and cleanliness. This needs to be well-designed, well-architected for dataﬂow and interfaces,\n",
      "generally covered by unit and integration tests, meet team style standards, and sufﬁciently-documented. Note the\n",
      "programmers’ mentality remains that this code will someday be refactored/scrapped for productization; prototype code\n",
      "is relatively primitive with regard to efﬁciency and reliability of the eventual system. With the transition to Level 4 and\n",
      "proof-of-concept mode, the working group should evolve to include product engineering to help deﬁne service-level\n",
      "agreements and objectives (SLAs and SLOs) of the eventual production system.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to ensure robust and reliable machine learning system development. Inspired by engineering practices in spacecraft engineering, MLTRL defines a process for ML development, encompassing key steps like risk assessment, requirements gathering, and testing.\n",
      "\n",
      "MLTRL addresses the shortcomings of traditional ML development practices by prioritizing AI ethics and fairness, addressing concerns related to algorithmic bias, accountability, and societal impact. The framework promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "Inspired by decades of AI and ML development, MLTRL recognizes the iterative and interconnected nature of ML workflows, extending beyond deployment to include monitoring and feedback cycles for continuous reliability and improvement. The framework defines technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment, ensuring a systematic and accountable approach to ML system development.\n",
      "\n",
      "**Additional Context:**\n",
      "\n",
      "MLTRL also addresses data considerations at each level, highlighting the importance of data curation and governance in ML development. It includes guidance on low-level experimentation, emphasizing the importance of rapid iteration and continuous assessment of model and algorithm properties.\n",
      "\n",
      "The framework outlines further levels of development, including data handling at different stages of development, code development checkpoints, and the evolution of the working group to include product engineering. This comprehensive approach ensures that ML systems are developed and validated in a systematic and accountable manner, addressing potential challenges encountered at later stages of development.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "agreements and objectives (SLAs and SLOs) of the eventual production system.\n",
      "Level 3 data – For the most part consistent with Level 2; in general, the previous level review can elucidate potential\n",
      "gaps in data coverage and robustness to be addressed in the subsequent level. However, for test suites developed at this\n",
      "stage, it is useful to deﬁne dedicated subsets of the experiment data as default testing sources, as well as setup mock\n",
      "data for speciﬁc functionalities and scenarios to be tested.\n",
      "Level 3 review – Teammates from applied AI and engineering are brought into the review to focus on sound software\n",
      "practices, interfaces and documentation for future development, and version control for models and datasets. There are\n",
      "likely domain- or organization-speciﬁc data management considerations going forward that this review should point out\n",
      "– e.g. standards for data tracking and compliance in healthcare [11].\n",
      "Level 4 - Proof of Concept (PoC) Development This stage is the seed of application-driven development; for many\n",
      "organizations this is the ﬁrst touch-point with product managers and stakeholders beyond the R&D group. Thus TRL\n",
      "Cards and requirements documentation are instrumental in communicating the project status and onboarding new\n",
      "people. The aim is to demonstrate the technology in a real scenario: quick proof-of-concept examples are developed to\n",
      "explore candidate application areas and communicate the quantitative and qualitative results. It is essential to use real\n",
      "and representative data for these potential applications. Thus data engineering for the PoC largely involves scaling up\n",
      "the data collection and processing from Level 1, which may include collecting new data or processing all available data\n",
      "using scaled experiment pipelines from Level 3. In some scenarios there will new datasets brought in for the PoC, for\n",
      "example, from an external research partner as a means of validation. Hand-in-hand with the evolution from sample to\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to ensure robust and reliable machine learning system development. Inspired by engineering practices in spacecraft engineering, MLTRL defines a process for ML development, encompassing key steps like risk assessment, requirements gathering, and testing.\n",
      "\n",
      "MLTRL addresses the shortcomings of traditional ML development practices by prioritizing AI ethics and fairness, addressing concerns related to algorithmic bias, accountability, and societal impact. The framework promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "Inspired by decades of AI and ML development, MLTRL recognizes the iterative and interconnected nature of ML workflows, extending beyond deployment to include monitoring and feedback cycles for continuous reliability and improvement. The framework defines technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment, ensuring a systematic and accountable approach to ML system development.\n",
      "\n",
      "MLTRL also addresses data considerations at each level, emphasizing the importance of data curation and governance in ML development. The framework includes guidance on low-level experimentation, emphasizing the importance of rapid iteration and continuous assessment of model and algorithm properties.\n",
      "\n",
      "The framework outlines further levels of development, including data handling at different stages of development, code development checkpoints, and the evolution of the working group to include product engineering. This comprehensive approach ensures that ML systems are developed and validated in a systematic and accountable manner, addressing potential challenges encountered at later stages of development.\n",
      "\n",
      "**Additional Context:**\n",
      "\n",
      "The MLTRL framework also addresses the development and deployment of machine learning models in production environments. It provides guidance on establishing agreements and objectives (SLAs and SLOs) for the eventual production system, and includes considerations for data handling at different stages of development.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "real data, the experiment metrics should evolve from ML research to the applied setting: proof-of-concept evaluations\n",
      "should quantify model and algorithm performance (e.g., precision and recall and various data splits), computational\n",
      "costs (e.g., CPU vs GPU runtimes), and also metrics that are more relevant to the eventual end-user (e.g., number\n",
      "of false positives in the top-N predictions of a recommender system). We ﬁnd this PoC exploration reveals speciﬁc\n",
      "differences between clean and controlled research data versus noisy and stochastic real-world data. The issues can\n",
      "be readily identiﬁed because of the well-deﬁned distinctions between those development stages in MLTRL, and then\n",
      "targeted for further development.\n",
      "AI ethics processes vary across organizations, but all should engage in ethics conversations at this stage, including ethics\n",
      "of data collection, and potential of any harm or discriminatory impacts due to the model (as the AI capabilities and\n",
      "datasets are known). MLTRL requires ethics considerations to be reported on TRL Cards at all stages, which generally\n",
      "link to an extended ethics checklist. The key decision point here is to push onward with application development or not.\n",
      "It is common to pause projects that pass Level 4 review, waiting for a better time to dedicate resources, and/or pull the\n",
      "technology into a different project.\n",
      "Level 4 data – Unlike the previous stages, having real-world and representative data is critical for the PoC; even with\n",
      "methods for verifying that data distributions in synthetic data reliably mirror those of real data [], sufﬁcient conﬁdence\n",
      "in the technology must be achieved with real-world data of the use-case. Further, one must consider how to obtain\n",
      "corresponding measures for veriﬁcation and validation (V&V). Veriﬁcation: Are we building the product right? Validation: Are we\n",
      "building the right product?\n",
      "4\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to ensure robust and reliable machine learning system development. Inspired by engineering practices in spacecraft engineering, MLTRL defines a process for ML development, encompassing key steps like risk assessment, requirements gathering, and testing.\n",
      "\n",
      "MLTRL addresses the shortcomings of traditional ML development practices by prioritizing AI ethics and fairness, addressing concerns related to algorithmic bias, accountability, and societal impact. The framework promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "Inspired by decades of AI and ML development, MLTRL recognizes the iterative and interconnected nature of ML workflows, extending beyond deployment to include monitoring and feedback cycles for continuous reliability and improvement. The framework defines technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment, ensuring a systematic and accountable approach to ML system development.\n",
      "\n",
      "MLTRL also addresses data considerations at each level, emphasizing the importance of data curation and governance in ML development. The framework includes guidance on low-level experimentation, emphasizing the importance of rapid iteration and continuous assessment of model and algorithm properties.\n",
      "\n",
      "The framework outlines further levels of development, including data handling at different stages of development, code development checkpoints, and the evolution of the working group to include product engineering. MLTRL also addresses the development and deployment of machine learning models in production environments, providing guidance on establishing agreements and objectives for the eventual production system.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "high-quality and consistent data required for the future model inference: generation of the data pipeline PoC that will\n",
      "resemble the future inference pipeline that will take data from intended sources, transform it into features, and send it to\n",
      "the model for inference.\n",
      "Level 4 review – Demonstrate the utility towards one or more practical applications (each with multiple datasets), taking\n",
      "care to communicate assumptions and limitations, and again reviewing data-readiness: evaluating the real-world data\n",
      "for quality, validity, and availability. The review also evaluates security and privacy considerations – deﬁning these in\n",
      "the requirements document with risk quantiﬁcation is a useful mechanism for mitigating potential issues (discussed\n",
      "further in the Methods section).\n",
      "Level 5 - Machine Learning “Capability” At this stage the technology is more than an isolated model or algorithm,\n",
      "it is a speciﬁc capability . For instance, producing depth images from stereo vision sensors on a mobile robot is a\n",
      "real-world capability beyond the isolated ML technique of self-supervised learning for RGB stereo disparity estimation.\n",
      "In many organizations this represents a technology transition or handoff from R&D to productization. MLTRL\n",
      "makes this transition explicit, evolving the requisite work, guiding documentation, objectives and metrics, and team;\n",
      "indeed, without MLTRL it is common for this stage to be erroneously leaped completely, as shown in Figure 2. An\n",
      "interdisciplinary working group is deﬁned, as we start developing the technology in the context of a larger real-world\n",
      "process – i.e., transitioning the model or algorithm from an isolated solution to a module of a larger application. Just as\n",
      "the ML technology should no longer be owned entirely by ML experts, steps have been taken to share the technology\n",
      "with others in the organization via demos, example scripts, and/or an API; the knowledge and expertise cannot remain\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to ensure robust and reliable machine learning system development. Inspired by engineering practices in spacecraft engineering, MLTRL defines a process for ML development, encompassing key steps like risk assessment, requirements gathering, and testing.\n",
      "\n",
      "MLTRL addresses the shortcomings of traditional ML development practices by prioritizing AI ethics and fairness, addressing concerns related to algorithmic bias, accountability, and societal impact. The framework promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "Inspired by decades of AI and ML development, MLTRL recognizes the iterative and interconnected nature of ML workflows, extending beyond deployment to include monitoring and feedback cycles for continuous reliability and improvement. The framework defines technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment, ensuring a systematic and accountable approach to ML system development.\n",
      "\n",
      "MLTRL also addresses data considerations at each level, emphasizing the importance of data curation and governance in ML development. The framework includes guidance on low-level experimentation, emphasizing the importance of rapid iteration and continuous assessment of model and algorithm properties.\n",
      "\n",
      "The framework outlines further levels of development, including data handling at different stages of development, code development checkpoints, and the evolution of the working group to include product engineering. MLTRL also addresses the development and deployment of machine learning models in production environments, providing guidance on establishing agreements and objectives for the eventual production system.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "within the R&D team, let alone an individual ML developer. Graduation from Level 5 should be difﬁcult, as it signiﬁes\n",
      "the dedication of resources to push this ML technology through productization. This transition is a common challenge\n",
      "in deep-tech, sometimes referred to as “the valley of death” because project managers and decision-makers struggle\n",
      "to allocate resources and align technology roadmaps to effectively move to Level 6, 7 and onward. MLTRL directly\n",
      "addresses this challenge by stepping through the technology transition or handoff explicitly.\n",
      "Level 5 data – For the most part consistent with Level 4. However, considerations need to be taken for scaling of data\n",
      "pipelines: there will soon be more engineers accessing the existing data and adding more, and the data will be getting\n",
      "much more use, including automated testing in later levels. With this scaling can come challenges with data governance.\n",
      "The data pipelines likely do not mirror the structure of the teams or broader organization. This can result in data silos,\n",
      "duplications, unclear responsibilities, and missing control of data over its entire lifecycle. These challenges and several\n",
      "approaches to data governance (planning and control, organizational, and risk-based) are detailed in Janssen et al. [9].\n",
      "Level 5 review – The veriﬁcation and validation (V&V) measures and steps deﬁned in earlier R&D stages (namely\n",
      "Level 2) must all be completed by now, and the product-driven requirements (and corresponding V&V) are drafted at\n",
      "this stage. We thoroughly review them here, and make sure there is stakeholder alignment (at the ﬁrst possible step of\n",
      "productization, well ahead of deployment).\n",
      "Level 6 - Application Development The main work here is signiﬁcant software engineering to bring the code up to\n",
      "product-caliber : This code will be deployed to users and thus needs to follow precise speciﬁcations, have comprehensive\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to ensure robust and reliable machine learning system development. Inspired by engineering practices in spacecraft engineering, MLTRL defines a process for ML development, encompassing key steps like risk assessment, requirements gathering, and testing.\n",
      "\n",
      "MLTRL addresses the shortcomings of traditional ML development practices by prioritizing AI ethics and fairness, addressing concerns related to algorithmic bias, accountability, and societal impact. The framework promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "Inspired by decades of AI and ML development, MLTRL recognizes the iterative and interconnected nature of ML workflows, extending beyond deployment to include monitoring and feedback cycles for continuous reliability and improvement. The framework defines technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment, ensuring a systematic and accountable approach to ML system development.\n",
      "\n",
      "MLTRL also addresses data considerations at each level, emphasizing the importance of data curation and governance in ML development. The framework includes guidance on low-level experimentation, emphasizing the importance of rapid iteration and continuous assessment of model and algorithm properties.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. The framework provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "test coverage, well-deﬁned APIs, etc. The resulting ML modules should be robustiﬁed towards one or more target\n",
      "use-cases. If those target use-cases call for model explanations, the methods need to be built and validated alongside\n",
      "the ML model, and tested for their efﬁcacy in faithfully interpreting the model’s decisions – crucially, this needs to be\n",
      "in the context of downstream tasks and the end-users, as there is often a gap between ML explainability that serves\n",
      "ML engineers rather than external stakeholders[ 12]. Similarly, we need to develop the ML modules with known data\n",
      "challenges in mind, speciﬁcally to check the robustness of the model (and broader pipeline) to changes in the data\n",
      "distribution between development and deployment.\n",
      "The deployment setting(s) should be addressed thoroughly in the product requirements document, as ML serving (or\n",
      "deploying) is an overloaded term that needs careful consideration. First, there are two main types: internal, as APIs\n",
      "for experiments and other usage mainly by data science and ML teams, and external, meaning an ML model that\n",
      "is embedded or consumed within a real application with real users. The serving constraints vary signiﬁcantly when\n",
      "considering cloud deployment vs on-premise or hybrid, batch or streaming, open-source solution or containerized\n",
      "executable, etc. Even more, the data at deployment may be limited due to compliance, or we may only have access to\n",
      "encrypted data sources, some of which may only be accessible locally – these scenarios may call for advanced ML\n",
      "approaches such as federated learning[ 13] and other privacy-oriented ML[ 14]. And depending on the application, an\n",
      "ML model may not be deployable without restrictions; this typically means being embedded in a rules engine workﬂow\n",
      "where the ML model acts like an advisor that discovers edge cases in rules. These deployment factors are hardly\n",
      "considered in model and algorithm development despite signiﬁcant inﬂuence on modeling and algorithmic choices;\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework provides a systematic approach to developing robust and reliable machine learning systems. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the shortcomings of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias, accountability, and societal impact. The framework promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "MLTRL recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. It also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. The framework provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL considers the broader deployment context, including various deployment settings, data challenges, and privacy concerns. It highlights the importance of considering deployment constraints, such as cloud deployment, on-premise or hybrid setups, and the need for advanced ML approaches in certain scenarios.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "that said, hardware choices typically are considered early on, such as GPU versus edge devices. It is crucial to make\n",
      "these systems decisions at Level 6–not too early that serving scenarios and requirements are uncertain, and not too late\n",
      "5\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to developing robust and reliable machine learning systems. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the shortcomings of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias, accountability, and societal impact. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "MLTRL recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. It also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. The framework provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL considers the broader deployment context, including various deployment settings, data challenges, and privacy concerns. It highlights the importance of considering deployment constraints, such as hardware selection (e.g., GPUs versus edge devices), on ML system performance optimization.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "that corresponding changes to model or application development risk deployment delays or failures. This marks a key\n",
      "decision for the project lifecycle, as this expensive ML deployment risk is common without MLTRL (see Figure 2).\n",
      "Level 6 data – Additional data should be collected and operationalized at this stage towards robustifying the ML\n",
      "models, algorithms, and surrounding components. These include adversarial examples to check local robustness [ 15],\n",
      "semantically-equivalent perturbations to check consistency of the model with respect to domain assumptions [16, 17],\n",
      "and collecting data from different sources and checking how well the trained model generalizes to them. These\n",
      "considerations are even more vital in the challenging deployment domains mentioned above with limited data access.\n",
      "Level 6 review – Focus is on the code quality, the set of newly deﬁned product requirements, system SLA and SLO\n",
      "requirements, data pipelines spec, and an AI ethics revisit now that we are closer to a real-world use-case. In particular,\n",
      "regulatory compliance is mandated for this gated review; the data privacy and security laws are changing rapidly, and\n",
      "missteps with compliance can make or break the project.\n",
      "Level 7 - Integrations For integrating the technology into existing production systems, we recommend the working\n",
      "group has a balance of infrastructure engineers andapplied AI engineers – this stage of development is vulnerable\n",
      "to latent model assumptions and failure modes, and as such cannot be safely developed solely by software engineers.\n",
      "Important tools for them to build together include:\n",
      "•Tests that run use-case speciﬁc critical scenarios and data-slices – a proper risk-quantiﬁcation table will\n",
      "highlight these.\n",
      "•A “golden dataset” should be deﬁned to baseline the performance of each model and succession of models –see\n",
      "the computer vision app example in Figure 4–for use in the continuous integration and deployment (CI/CD)\n",
      "tests.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to developing robust and reliable machine learning systems. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the limitations of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias, accountability, and societal impact. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "MLTRL recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. It also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. The framework provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL considers the broader deployment context, including various deployment settings, data challenges, and privacy concerns. It highlights the importance of considering deployment constraints, such as hardware selection, on ML system performance optimization.\n",
      "\n",
      "**Additional Considerations for Deployment:**\n",
      "\n",
      "MLTRL emphasizes the need for rigorous testing and validation during deployment, including the collection and operationalization of additional data to enhance model robustness. It also underscores the importance of involving infrastructure and applied AI engineers in the integration process to mitigate potential risks and ensure seamless deployment.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "tests.\n",
      "•Metamorphic testing : a software engineering methodology for testing a speciﬁc set of relations between the\n",
      "outputs of multiple inputs. When integrating ML modules into larger systems, a codiﬁed list of metamorphic\n",
      "relations[18] can provide valuable veriﬁcation and validation measures and steps.\n",
      "•Data intervention tests that seek data bugs at various points in the pipelines, downstream to measure the\n",
      "potential effects of data processing and ML on consumers or users of that data, as well as upstream at data\n",
      "ingestion or creation. Rather than using model performance as a proxy for data quality, it is crucial to use\n",
      "intervention tests that instead catch data errors with mechanisms speciﬁc to data validation.\n",
      "These tests in particular help mitigate underspeciﬁcation in ML pipelines, a key obstacle to reliably training models that\n",
      "behave as expected in deployment[ 19]. On the note of reliability, it is important that quality assurance engineers (QA)\n",
      "play a key role here and through Level 9, overseeing data processes to ensure privacy and security, and covering audits\n",
      "for downstream accountability of AI methods.\n",
      "Level 7 data – In addition to the data for test suites discussed above, this level calls for QA to prioritize data governance :\n",
      "how data is obtained, managed, used, and secured by the organization. This was earlier suggested in level 5 (in order to\n",
      "preempt related technical debt), and essential here at the main junction for integration, which may create additional\n",
      "governance challenges in light of downstream effects and consumers.\n",
      "Level 7 review – The review should focus on the data pipelines and test suites; a scorecard like the ML Testing\n",
      "Rubric[ 20] is useful. The group should also emphasize ethical considerations at this stage, as they may be more\n",
      "adequately addressed now (where there are many test suites put into place) rather than close to shipping later.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework provides a systematic approach to developing robust and reliable machine learning systems. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the limitations of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias, accountability, and societal impact. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "MLTRL recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. It also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. The framework provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL also highlights the importance of rigorous testing and validation during deployment, including the collection and operationalization of additional data to enhance model robustness. It underscores the need for involving infrastructure and applied AI engineers in the integration process to mitigate potential risks and ensure seamless deployment.\n",
      "\n",
      "**Additional Considerations:**\n",
      "\n",
      "MLTRL emphasizes the importance of data governance, highlighting the need for QA to prioritize data governance at Level 7. Metamorphic testing and data intervention tests are recommended for verifying model behavior and identifying data quality issues. Ethical considerations are also emphasized, suggesting that they be addressed early in the development process rather than later.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Level 8 - Flight-ready The technology is demonstrated to work in its ﬁnal form and under expected conditions.\n",
      "There should be additional tests implemented at this stage covering deployment aspects, notably A/B tests, blue/green\n",
      "deployment tests, shadow testing, and canary testing, which enable proactive and gradual testing for changing ML\n",
      "methods and data. Ahead of deployment, the CI/CD system should be ready to regularly stress test the overall system\n",
      "and ML components. In practice, problems stemming from real-world data are impossible to anticipate and design for –\n",
      "an upstream data provider could change formats unexpectedly or a physical event could cause the customer behavior to\n",
      "change. Running models in shadow mode for a period of time would help stress test the infrastructure and evaluate how\n",
      "susceptible the ML model(s) will be to performance regressions caused by data. We observe that ML systems with\n",
      "data-oriented architectures are more readily tested in this manner, and better surface data quality issues, data drifts, and\n",
      "concept drifts – this is discussed later in the Beyond Software Engineering section. To close this stage, the key decision\n",
      "is go or no-go for deployment, and when.\n",
      "Level 8 data – If not already in place, there absolutely needs to be mechanisms for automatically logging data\n",
      "distributions alongside model performance once deployed.\n",
      "6\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework provides a systematic approach to developing robust and reliable machine learning systems. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the limitations of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias, accountability, and societal impact. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "MLTRL recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. It also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. The framework provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL also highlights the importance of rigorous testing and validation during deployment, including A/B testing, blue/green deployment tests, shadow testing, and canary testing. It emphasizes the need for involving infrastructure and applied AI engineers in the integration process to mitigate potential risks and ensure seamless deployment. Additionally, MLTRL underscores the importance of logging data distributions alongside model performance once deployed.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Level 8 review – A diligent walkthrough of every technical and product requirement, showing the corresponding\n",
      "validations, and the review panel is representative of the full slate of stakeholders.\n",
      "Level 9 - Deployment In deploying AI and ML technologies, there is signiﬁcant need to monitor the current version,\n",
      "and explicit considerations towards improving the next version. For instance, performance degradation can be hidden\n",
      "and critical, and feature improvements often bring unintended consequences and constraints. Thus at this level, the\n",
      "focus is on maintenance engineering–i.e., methods and pipelines for ML monitoring and updating. Monitoring for data\n",
      "quality, concept drift, and data drift is crucial; no AI system without thorough tests for these can reliably be deployed.\n",
      "By the same token there must be automated evaluation and reporting – if actuals[ 21] are available, continuous evaluation\n",
      "should be enabled, but in many cases actuals come with a delay, so it is essential to record model outputs to allow for\n",
      "efﬁcient evaluation after the fact. To these ends, the ML pipeline should be instrumented to log system metadata, model\n",
      "metadata, and data itself.\n",
      "Monitoring for data quality issues and data drifts is crucial to catch deviations in model behavior, particularly those that\n",
      "are non-obvious in the model or product end-performance. Data logging is unique in the context of ML systems: data\n",
      "logs should capture statistical properties of input features and model predictions, and capture their anomalies. With\n",
      "monitoring for data, concept, and model drifts, the logs are to be sent to the relevant systems, applied, and research\n",
      "engineers. The latter is often non-trivial, as the model server is not ideal for model “observability” because it does not\n",
      "necessarily have the right data points to link the complex layers needed to analyze and debug models. To this end,\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to developing robust and reliable machine learning systems. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the limitations of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias, accountability, and societal impact. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "MLTRL recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. It also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. The framework provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL also highlights the importance of rigorous testing and validation during deployment, including A/B testing, blue/green deployment tests, shadow testing, and canary testing. It emphasizes the need for involving infrastructure and applied AI engineers in the integration process to mitigate potential risks and ensure seamless deployment. Additionally, MLTRL underscores the importance of logging data distributions alongside model performance once deployed.\n",
      "\n",
      "**Context-Specific Insights:**\n",
      "\n",
      "The provided context emphasizes the significance of monitoring and maintaining ML systems. It highlights the need for:\n",
      "\n",
      "- Continuous evaluation and reporting of model performance.\n",
      "- Monitoring data quality, concept drift, and data drift.\n",
      "- Logging data and model metadata for post-deployment analysis.\n",
      "\n",
      "These insights can be incorporated into the MLTRL framework to enhance its guidance on:\n",
      "\n",
      "- Establishing monitoring and feedback cycles.\n",
      "- Addressing data governance issues.\n",
      "- Ensuring seamless deployment and continuous improvement.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "MLTRL requires the drift tests to be implemented at stages well ahead of deployment, earlier than is standard practice.\n",
      "Again we advocate for data-ﬁrst architectures rather than the software industry-standard design by services (discussed\n",
      "later), which aids in surfacing and logging the relevant data types and slices when monitoring AI systems. For retraining\n",
      "and improving models, monitoring must be enabled to catch training-serving skew and let the team know when to\n",
      "retrain. Towards model improvements, adding or modifying features can often have unintended consequences, such as\n",
      "introducing latencies or even bias. To mitigate these risks, MLTRL has an embedded switchback here: any component\n",
      "or module changes to the deployed version must cycle back to Level 7 (integrations stage) or earlier. Additionally,\n",
      "for quality ML products, we stress a deﬁned communication path for user feedback without roadblocks to R&D; we\n",
      "encourage real-world feedback all the way to research, providing valuable problem constraints and perspectives.\n",
      "Level 9 data – Proper mechanisms for logging and inspecting data (alongside models) is critical for deploying reliable\n",
      "AI and ML – systems that learn on data have unique monitoring requirements (detailed above). In addition to the\n",
      "infrastructure and test suites covering data and environment shifts, it’s important for product managers and other owners\n",
      "to be on top of data policy shifts in domains such as ﬁnance and healthcare.\n",
      "Level 9 review – The review at this stage is unique, as it also helps in lifecycle management: at a regular cadence\n",
      "that depends on the deployed system and domain of use, owners and other stakeholders are to revisit this review and\n",
      "recommend switchbacks if needed (discussed in the Methods section). This additional oversight at deployment is\n",
      "shown to help deﬁne regimented release cycles of updated versions, and provide another “eye” check for stale model\n",
      "performance or other system abnormalities.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework provides a systematic approach to developing robust and reliable machine learning systems. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the limitations of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias, accountability, and societal impact. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "MLTRL recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. It also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. The framework provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL also highlights the importance of rigorous testing and validation during deployment, including A/B testing, blue/green deployment tests, shadow testing, and canary testing. It emphasizes the need for involving infrastructure and applied AI engineers in the integration process to mitigate potential risks and ensure seamless deployment. Additionally, MLTRL underscores the importance of logging data distributions alongside model performance once deployed.\n",
      "\n",
      "**Additional Insights:**\n",
      "\n",
      "MLTRL emphasizes the need for:\n",
      "\n",
      "- Continuous evaluation and reporting of model performance.\n",
      "- Monitoring data quality, concept drift, and data drift.\n",
      "- Logging data and model metadata for post-deployment analysis.\n",
      "\n",
      "Furthermore, MLTRL highlights the importance of:\n",
      "\n",
      "- Implementing drift tests well ahead of deployment.\n",
      "- Adopting data-first architectures.\n",
      "- Monitoring data and models for retraining and improvement.\n",
      "- Establishing clear communication paths for user feedback.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "performance or other system abnormalities.\n",
      "Notice MLTRL is deﬁned as stages or levels, yet much of the value in practice is realized in the transitions: MLTRL\n",
      "enables teams to move from one level to the next reliably and efﬁciently, and provides a guide for how teams and\n",
      "objectives evolve with the progressing technology.\n",
      "Discussion\n",
      "MLTRL is designed to apply to many real-world use-cases involving data and ML, from simple regression models\n",
      "used for predictive modeling energy demand or anomaly detection in datacenters, to real-time modeling in rideshare\n",
      "applications and motion planning in warehouse robotics. For simple use-cases MLTRL may be overkill, and a subset\n",
      "may sufﬁce – for instance, model cards as demonstrated by Google for basic image classiﬁcation. Yet this is a ﬁne line,\n",
      "as the same cards-only approach in the popular “Huggingface” codebases are too simplistic for the language models\n",
      "they represent, deployed in domains that carry signiﬁcant consequences. MLTRL becomes more valuable with more\n",
      "complex, larger systems and environments, especially in risk averse domains. We thoroughly discuss this through\n",
      "several real uses of MLTRL below.\n",
      "7\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to developing robust and reliable machine learning systems. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the limitations of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias and accountability. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "The framework recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. MLTRL also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. It provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL emphasizes the importance of rigorous testing and validation during deployment, including various testing methodologies. It also underscores the need for logging data distributions alongside model performance once deployed.\n",
      "\n",
      "MLTRL is designed to apply to a wide range of real-world applications, from simple predictive modeling to complex systems in risk-averse domains.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Figure 2: Most ML and AI projects live in these sections of MLTRL, not concerned with fundamental R&D – that is,\n",
      "completely using existing methods and implementations, and even pretrained models. In the left diagram, the arrows\n",
      "show a common development pattern with MLTRL in industry: projects go back to the ML toolbox to develop new\n",
      "features (dashed line), and frequent, incremental improvements are often a practice of jumping back a couple levels to\n",
      "Level 7 (which is the main systems integrations stage). At Levels 7 and 8 we stress the need for tests that run use-case\n",
      "speciﬁc critical scenarios and data-slices, which are highlighted by a proper risk-quantiﬁcation matrix [ 22]. Cycling\n",
      "back to previous lower levels is not just a late-stage mechanism in MLTRL, but rather “switchbacks” occur throughout\n",
      "the process (as discussed in the Methods section and throughout the text). In the right diagram we show the more\n",
      "common approach in industry ( without using our framework), which skips essential technology transition stages – ML\n",
      "Engineers push straight through to deployment, ignoring important productization and systems integration factors. This\n",
      "will be discussed in more detail in the Methods section.\n",
      "EXAMPLES\n",
      "Human-machine visual inspection\n",
      "While most ML projects begin with a speciﬁc task and/or dataset, there are many that originate in ML theory without\n",
      "any target application – i.e., projects starting MLTRL at level 0 or 1. These projects nicely demonstrate the utility of\n",
      "MLTRL built-in switchbacks, bifurcating paths, and iteration with domain experts. An example we discuss here is a\n",
      "novel approach to representing data in generative vision models from Naud & Lavin[ 23], which was then developed into\n",
      "state-of-the-art unsupervised anomaly detection, and targeted for two human-machine visual inspection applications:\n",
      "First, industrial anomaly detection, notably in precision manufacturing, to identify potential errors for human-expert\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to developing robust and reliable machine learning systems. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the limitations of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias and accountability. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "The framework recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. MLTRL also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. It provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL emphasizes the importance of rigorous testing and validation during deployment, including various testing methodologies. It also underscores the need for logging data distributions alongside model performance once deployed.\n",
      "\n",
      "The framework is designed to apply to a wide range of real-world applications, from simple predictive modeling to complex systems in risk-averse domains. It highlights the iterative and cyclical development patterns commonly observed in industry, emphasizing the importance of switching back to earlier stages of development when necessary.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "manual inspection. Second, using the model to improve the accuracy and efﬁciency of neuropathology, the microscopic\n",
      "examination of neurosurgical specimens for cancerous tissue. In these human-machine teaming use-cases there are\n",
      "speciﬁc challenges impeding practical, reliable use:\n",
      "•Hidden feedback loops can be common and problematic in real-world systems inﬂuencing their own training\n",
      "data: over time the behavior of users may evolve to select data inputs they prefer for the speciﬁc AI system,\n",
      "representing some skew from the training data. In this neuropathology case, selecting whole-slide images that\n",
      "are uniquely difﬁcult for manual inspection, or even biased by that individual user. Similarly we see underlying\n",
      "healthcare processes can act as hidden confounders, resulting in unreliable decision support tools[26].\n",
      "•Model availability can be limited in many deployment settings: for example, on-premises deployments\n",
      "(common in privacy preserving domains like healthcare and banking), edge deployments (common in industrial\n",
      "use-cases such as manufacturing and agriculture), or from the infrastructure’s inability to scale to the volume\n",
      "of requests. This can severely limit the team’s ability to monitor, debug, and improve deployed models.\n",
      "•Uncertainty estimation is valuable in many AI scenarios, yet not straightforward to implement in practice.\n",
      "This is further complicated with multiple data sources and users, each injecting generally unknown amounts of\n",
      "noise and uncertainties. In medical applications it is of critical importance, to provide measures of conﬁdence\n",
      "and sensitivity, and for AI researchers through end-users. In anomaly detection, various uncertainty measures\n",
      "can help calibrate the false-positive versus false-negative rates, which can be very domain speciﬁc.\n",
      "8\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to developing robust and reliable machine learning systems. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the limitations of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias and accountability. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "The framework recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. MLTRL also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. It provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL is designed to apply to a wide range of real-world applications, including medical image analysis, autonomous systems, and risk assessment. It highlights the iterative and cyclical development patterns commonly observed in industry, emphasizing the importance of switching back to earlier stages of development when necessary.\n",
      "\n",
      "**Additional Context:**\n",
      "\n",
      "The provided context highlights specific challenges related to the deployment of machine learning models in real-world applications:\n",
      "\n",
      "- **Hidden feedback loops:** Changes in user behavior can influence training data, leading to biased models.\n",
      "- **Limited model availability:** Constraints in deployment settings can hinder monitoring, debugging, and model improvement.\n",
      "- **Uncertainty estimation:** Estimating model confidence is crucial but can be challenging in practical settings with multiple data sources.\n",
      "\n",
      "These challenges underscore the importance of addressing data quality issues, managing model availability, and providing reliable uncertainty estimates in machine learning applications.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Figure 3: The maturity of each ML technology is tracked via TRL Cards , which we describe in the Methods section.\n",
      "Here is an example reﬂecting a neuropathology machine vision use-case[ 23], detailed in the Discussion Section. Note\n",
      "this is a subset of a full TRL Card, which in reality lives as a full document in an internal wiki. Notice the card\n",
      "clearly communicates the data sources, versions, and assumptions. This helps mitigate invalid assumptions about\n",
      "performance and generalizability when moving from R&D to production, and promotes the use of real-world data\n",
      "earlier in the project lifecycle. We recommend documenting datasets thoroughly with semantic versioning and tools\n",
      "such as datasheets for datasets [24], and following data accountability best-practices as they evolve (see [25]).\n",
      "•Costs of edge cases can be signiﬁcant, sometimes risking expensive machine downtime or medical failures.\n",
      "This is exacerbated in anomaly detection anomalies are by deﬁnition rare so they can be difﬁcult to train for,\n",
      "especially for the anomalies that are completely unseen until they arise in the wild.\n",
      "•End-user trust can be difﬁcult to achieve, often preventing the adoption of ML applications, particularly in\n",
      "the healthcare domain and other highly regulated industries.\n",
      "These and additional ML challenges such as data privacy and interpretability can inhibit ML adoption in clinical practice\n",
      "and industrial settings, but can be mitigated with MLTRL processes. We’ll describe how in the context of the Naud\n",
      "& Lavin[ 23] example, which began at level 0 with theoretical ML work on manifold geometries, and at level 5 was\n",
      "directed towards specialized human-machine teaming applications utilizing the same ML method under-the-hood.\n",
      "•Levels 0-1 – From open-ended exploration of data-representation properties in various Riemmanian manifold\n",
      "curvatures, we derived from ﬁrst principles and empirically identiﬁed a property with hyperbolic manifolds:\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Refined Summary:**\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework offers a systematic approach to developing reliable and robust machine learning systems. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the limitations of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias and accountability. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "The framework recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. MLTRL also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. It provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL is designed to apply to a wide range of real-world applications, including medical image analysis, autonomous systems, and risk assessment. It highlights the iterative and cyclical development patterns commonly observed in industry, emphasizing the importance of switching back to earlier stages of development when necessary.\n",
      "\n",
      "**Additional Context:**\n",
      "\n",
      "The provided context highlights significant challenges in deploying machine learning models in real-world applications, including:\n",
      "\n",
      "- Hidden feedback loops and data quality issues\n",
      "- Limited model availability and uncertainty estimation\n",
      "- Costs associated with edge cases, especially in anomaly detection\n",
      "\n",
      "These challenges underscore the need for robust data management practices, model interpretability, and addressing the trust gap in regulated industries. MLTRL processes can mitigate these challenges and facilitate the successful deployment of ML solutions across various applications.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "when used as a latent space for embedding data without labels, the geometry organizes the data by it’s implicit\n",
      "hierarchical structure. Unsupervised computer vision was identiﬁed in reviews as a promising direction for\n",
      "proof-of-principle work.\n",
      "•Level 2 – One approach for validating the earlier theoretical developments was to generate synthetic data to\n",
      "isolate very speciﬁc features in data we would expect represented in the latent manifold. The results showed\n",
      "promise for anomaly detection – using the latent representation of data to automatically identify images that\n",
      "are out-of-the-ordinary (anomalous), and also using the manifold to inspect how they are semantically different.\n",
      "Further, starting with an implicitly probabilistic modeling approach implied uncertainty estimation could be\n",
      "a valuable feature downstream. This made the level 2 key decision point clear: proceed with applied ML\n",
      "development.\n",
      "•Levels 3-5 – Proof-of-concept development and reviews demonstrated promise for several commercial appli-\n",
      "cations relevant to the business, and also highlighted the need for several key features (deﬁned as R&D and\n",
      "product requirements): interpretability (towards end-user trust), uncertainty quantiﬁcation (to show conﬁdence\n",
      "scores), and human-in-the-loop (for domain expertise). Without the MLTRL PoC steps and review processes,\n",
      "these features can often be delayed until beta testing or overlooked completely – for example, the failures of\n",
      "applying IBM Watson in medical applications [ 27]. For this technology, the applications to develop towards\n",
      "are anomaly detection in histopathology and manufacturing, speciﬁcally inspecting whole-slide images of\n",
      "neural tissue, and detecting defects in metallic surfaces, respectively.\n",
      "9\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the challenges associated with deploying machine learning models in real-world applications. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the limitations of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias and accountability. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "The framework recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. MLTRL also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. It provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL has been applied to various applications, including medical image analysis, autonomous systems, and risk assessment. It highlights the iterative and cyclical development patterns commonly observed in industry, emphasizing the importance of switching back to earlier stages of development when necessary.\n",
      "\n",
      "**Additional Context:**\n",
      "\n",
      "The provided context emphasizes the importance of interpretability, uncertainty estimation, and human-in-the-loop capabilities for successful machine learning deployment. MLTRL's emphasis on these features aligns with the findings from other studies, where these aspects were identified as crucial for real-world applications.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "From the systems perspective, we suggest quantifying the uncertainties of components and propagating them\n",
      "through the system, which can improve safety and trust. Probabilistic ML methods, rooted in Bayesian\n",
      "probability theory, provide a principled approach to representing and manipulating uncertainty about models\n",
      "and predictions[ 28]. For this reason we advocate strongly for probabilistic models and algorithms in AI\n",
      "systems. In this machine vision example, the MLTRL technical requirements speciﬁcally called for a\n",
      "probabilistic generative model to readily quantify various types of uncertainties and propagate them forward to\n",
      "the visualization component of the pipeline, and the product requirements called for the downstream conﬁdence\n",
      "and sensitivity measures to be exposed to the end-user. Component uncertainties must be assembled in a\n",
      "principled way to yield a meaningful measure of overall system uncertainty, based on which safe decisions can\n",
      "be made[29]. See the Methods section for more on uncertainty in AI systems.\n",
      "The early checks for data management and governance proved valuable here, as the application areas dealt\n",
      "with highly sensitive data that would signiﬁcantly inﬂuence the design of data pipelines and test suites. In\n",
      "both the neuropathology and manufacturing applications, the data management checks also raised concerns\n",
      "about hidden feedback loops, where users may unintentionally skew the data inputs when using the anomaly\n",
      "detection models in practice, for instance biasing the data towards speciﬁc subsets they subjectively need help\n",
      "with. Incorporating domain experts this early in the project lifecycle helped inform veriﬁcation and validation\n",
      "steps to help be robust to the hidden feedback loops. Not to mention their input guided us towards user-centric\n",
      "metrics for performance, which can often skew from ML metrics in important ways – for instance, the typical\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the limitations of traditional ML development by prioritizing AI ethics and fairness, mitigating concerns related to algorithmic bias and accountability. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "\n",
      "The framework recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. MLTRL also addresses data considerations, emphasizing data curation and governance in ML development.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering. It provides guidance on aligning technology roadmaps with production objectives and ensuring smooth handoff between R&D and product engineering teams.\n",
      "\n",
      "MLTRL has been applied to various applications, including medical image analysis, autonomous systems, and risk assessment. It highlights the iterative and cyclical development patterns commonly observed in industry, emphasizing the importance of switching back to earlier stages of development when necessary.\n",
      "\n",
      "**From a systems perspective, MLTRL emphasizes quantifying uncertainties through probabilistic models to improve safety and trust in ML systems. Early checks for data management and governance identified potential data bias and unintended feedback loops, leading to adjustments in data pipelines and testing methodologies.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "acceptance ratio for false positives versus false negatives doesn’t apply to select edge cases, for which our\n",
      "hierarchical anomaly classiﬁcation scheme was useful [23].\n",
      "From prior reviews and TRL card documentation, we also identiﬁed the value of synthetic data generation\n",
      "into application development: anomalies are by deﬁnition rare so they are hard to come by in real datasets,\n",
      "especially with evolving environments in deployment settings, so the ability to generate synthetic datasets for\n",
      "anomaly detection can accelerate the level 6-9 pipeline, and help ensure more reliable models in the wild.\n",
      "•Level 6 (medical) – The medical inspection application experienced a bifurcation with product work proceed-\n",
      "ing while additional R&D was desired to explore improved data processing methods, while engaging with\n",
      "clinicians and medical researchers for feedback. Proceeding through the levels in a non-linear, non-monotonic\n",
      "way is common in MLTRL and encouraged by various switchback mechanisms (detailed in the Methods\n",
      "section). These practices – intentional switchbacks, frequent engagement with domain experts and users – can\n",
      "help mitigate methodological ﬂaws and underlying biases that are common when applying ML to clinical\n",
      "applications. For instance, recent work by Roberts et al. [ 30] investigated 2,122 studies applying ML to\n",
      "COVID-19 use-cases, ﬁnding that none of the models are sufﬁcient for clinical use due to methodological ﬂaws\n",
      "and/or underlying biases. They go on to give many recommendations – some we’ve discussed in the context of\n",
      "MLTRL, and more – which should be reviewed for higher quality medical-ML models and documentation.\n",
      "•Level 6-9 (manufacturing) – Overall these stages proceeded regularly and efﬁciently for the defect detection\n",
      "product. MLTRL’s embedded switchback from level 9 to 4 proved particularly useful in this lifecycle, both\n",
      "for incorporating feedback from the ﬁeld and for updating with research progress. On the former, the data\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering practices, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process. It prioritizes AI ethics and fairness, mitigating concerns related to algorithmic bias and accountability.\n",
      "\n",
      "MLTRL recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. It also addresses data considerations, emphasizing data curation and governance in ML development. Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering.\n",
      "\n",
      "Recent applications of MLTRL include medical image analysis, autonomous systems, and risk assessment. The framework emphasizes quantifying uncertainties through probabilistic models to improve safety and trust in ML systems. Early checks for data management and governance identified potential data bias, leading to adjustments in data pipelines and testing methodologies.\n",
      "\n",
      "MLTRL encourages non-linear development patterns, facilitating iterative refinement and mitigating methodological flaws. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "distribution shifts from one deployment setting to another signiﬁcantly affected false-positive versus false-\n",
      "negative calibrations, so this was added as a feature to the CI/CD pipelines. On the latter, the built-in touch\n",
      "points for real-world feedback and data into the continued ML research provided valuable constraints to\n",
      "help guide research, and product managers could readily understand what capabilities could be available for\n",
      "product integration and when (readily communicated with TRL Cards) – for instance, later adding support for\n",
      "video-based inspection for defects, and tooling for end-users to reason about uncertainty estimates (which\n",
      "helps establish trust).\n",
      "•Level 7-9 (medical) – For productization the “neuropathology copilot” was handed off to a partner pharmaceu-\n",
      "tical company to integrate into their existing software systems. The MLTRL documentation and communication\n",
      "streamlined the technology transfer, which can often by a time-consuming manual process. If not pursuing\n",
      "this path, the product would’ve likely faced many of the medical-ML deployment challenges with model\n",
      "availability and data access; MLTRL cannot overcome the technical challenges of deploying on-premises, but\n",
      "the manifestation of those challenges as performance regressions, data shifts, privacy and ethics concerns, etc.\n",
      "can be mitigated by the system-level checks and strategies MLTRL puts forth.\n",
      "Computer vision with real and synthetic data\n",
      "Advancements in physics engines and graphics processing have advanced AI environment and data-generation capabili-\n",
      "ties, putting increased emphasis on transitioning models across the simulation-to-reality gap [ 31,32,33]. To develop a\n",
      "computer vision application for automated recycling, we leveraged the Unity Perception [ 34] package, a toolkit for\n",
      "generating large-scale datasets for perception-based ML training and validation. We produced synthetic images to\n",
      "10\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process. It prioritizes AI ethics and fairness, mitigating algorithmic bias and accountability concerns.\n",
      "\n",
      "MLTRL recognizes the iterative and interconnected nature of ML workflows, encompassing monitoring and feedback cycles for continuous reliability and improvement. It also addresses data considerations, emphasizing data curation and governance in ML development. Furthermore, MLTRL addresses the challenges associated with transitioning from research and development to production, including data governance issues and the need for comprehensive software engineering.\n",
      "\n",
      "Recent applications of MLTRL include medical image analysis, autonomous systems, and risk assessment. The framework emphasizes quantifying uncertainties through probabilistic models to improve safety and trust in ML systems. Early checks for data management and governance identified potential data bias, leading to adjustments in data pipelines and testing methodologies.\n",
      "\n",
      "MLTRL encourages non-linear development patterns, facilitating iterative refinement and mitigating methodological flaws. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness. Notably, MLTRL has facilitated the successful deployment of models in various domains, including medical, computer vision, and robotics, by mitigating technical challenges and streamlining technology transfer processes.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "\u0015A?U?HEJC\u0003?H=OOEBE?=PEKJ\u0003LELAHEJA\u0003\n",
      " =¡ >¡\n",
      ",V\u0003PRGHO\u0003FRQğGHQFH\u0003\u001f\u0003WKUHVKROG\",V\u0003GHWHFWHG\u0003REMHFW\u0003LQ\u0003WDUJHW\u0003VHW\"3URYLGH\u0003FRUUHVSRQGLQJ\u0003UHF\\FOLQJ\u0003LQVWUXFWLRQV,QLWLDWH\u0003KXPDQ\u0010\u0003LQ\u0010WKH\u0010ORRS\u0003SURWRFRO12<(6<(6Figure 4: Computer vision pipeline for an automated recycling application (a), which contains multiple ML models,\n",
      "user input, and image data from various sources. Complicated logic such as this can mask ML model performance lags\n",
      "and failures, and also emphasized the need for R&D-to-product hand off described in MLTRL. Additional emphasis is\n",
      "placed on ML tests that consider the mix of real-world data with user annotations (b, right) and synthetic data generated\n",
      "by Unity AI’s Perception tool and structured domain randomization (b, left).\n",
      "complement real-world data sources (Figure 4). This application exempliﬁes three important challenges in ML product\n",
      "development that MLTRL helps overcome:\n",
      "•Multiple and disparate data sources are common in deployed ML pipelines yet often ignored in R&D.\n",
      "For instance, upstream data providers can change formats unexpectedly, or a physical event could cause the\n",
      "customer behavior to change. It is nearly impossible to anticipate and design for all potential problems with\n",
      "real-world data and deployment. This computer vision system implemented pipelines and extended test suites\n",
      "to cover open-source benchmark data, real user data, and synthetic data.\n",
      "•Hidden performance degradation can be challenging to detect and debug in ML systems because gradual\n",
      "changes in performance may not be immediately visible. Common reasons for this challenge are that the\n",
      "ML component may be one step in a series. Additionally, local/isolated changes to an ML component’s\n",
      "performance may not directly affect the observed downstream performance. We can see both issues in the\n",
      "illustrated logic diagram for the automated recycling app (Figure 4). A slight degradation in the initial CV\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering, MLTRL emphasizes risk assessment, requirements gathering, and testing throughout the ML development process. It prioritizes ethical considerations, mitigating algorithmic bias and fostering accountability.\n",
      "\n",
      "MLTRL recognizes the iterative and interconnected nature of ML workflows, including continuous monitoring and feedback for reliability improvement. It also addresses data considerations, emphasizing data curation and governance in ML development. Additionally, MLTRL tackles the gap between research and production, addressing data governance issues and promoting comprehensive software engineering practices.\n",
      "\n",
      "Recent applications of MLTRL range from medical image analysis to autonomous systems and risk assessment. The framework promotes quantifying uncertainties through probabilistic models to enhance safety and trust in ML systems. Early checks for data management and governance identified potential data bias, leading to adjustments in data pipelines and testing methodologies.\n",
      "\n",
      "MLTRL encourages non-linear development patterns, facilitating iterative refinement and mitigating methodological flaws. It promotes collaboration and transparency across teams, establishing a common language for discussing ML readiness. By mitigating technical challenges and streamlining technology transfer processes, MLTRL has facilitated the successful deployment of models in various domains, including medical, computer vision, and robotics.\n",
      "\n",
      "**Additional Insights:**\n",
      "\n",
      "The provided text highlights the importance of addressing challenges associated with multiple and disparate data sources in ML pipelines. It emphasizes the need for comprehensive testing that incorporates real-world data, user annotations, and synthetic data to ensure robustness and resilience. This aligns with MLTRL's emphasis on addressing real-world deployment challenges through iterative development and comprehensive testing practices.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "model may not heavily inﬂuence the following user input. However, when an uncommon input image appears\n",
      "in the future, the app fails altogether.\n",
      "•Model usage requirements can make or break an ML product. For example, the Netﬂix “$1M Prize” solution\n",
      "was never fully deployed because of signiﬁcant engineering costs in real-world scenariosv. For example,\n",
      "engineering teams must communicate memory usage, compute power requirements, hardware availability,\n",
      "network privacy, and latency to the ML teams. ML teams often only understand the statistics or ML theory\n",
      "behind a model but not the system requirements or how it scales.\n",
      "We next elucidate these challenges and how MLTRL helps overcome them in the context of this project’s lifecycle. This\n",
      "project started at level 4, using largely existing ML methods with a target use-case.\n",
      "•Level 4 – For this project, we validated most of the components in other projects. Speciﬁcally, the computer\n",
      "vision (CV) model for object recognition and classiﬁcation was an off-the-shelf model. The synthetic data\n",
      "generation method used Unity Perception, a well-established open-source project. Though this allowed us to\n",
      "vnetﬂixtechblog.com/netﬂix-recommendations-beyond-the-5-stars-part-1\n",
      "11\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the challenges of deploying machine learning models in real-world applications by prioritizing ethical considerations, risk assessment, and iterative development. Inspired by spacecraft engineering, MLTRL emphasizes continuous monitoring, feedback, and comprehensive testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the complexities of diverse data sources and integrates real-world data, user annotations, and synthetic data to ensure robustness and resilience. It recognizes the iterative nature of ML workflows, encouraging continuous refinement and mitigating methodological flaws.\n",
      "\n",
      "The framework promotes quantifying uncertainties and addressing the gap between research and production, facilitating the successful deployment of models in diverse domains such as computer vision, robotics, and medical imaging. MLTRL has been applied to address challenges with multiple data sources, mitigating potential bias through data governance practices.\n",
      "\n",
      "**Additional Insights:**\n",
      "\n",
      "MLTRL addresses the challenge of deploying ML models in real-world scenarios, where engineering costs and system requirements must be considered. The framework emphasizes the need for effective communication and collaboration between ML teams and engineering teams to ensure successful deployment.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "skip the earlier levels, many challenges arise when combining ML elements that were independently validated\n",
      "and developed. The MLTRL prototype-caliber code checkpoint ensures that the existing code components\n",
      "are validated and helps avoid poorly deﬁned borders and abstractions between components. ML pipelines\n",
      "often grow out of glue code, and our regimented code checkpoints motivate well-architected software that\n",
      "minimizes these danger spots.\n",
      "•Level 5 – The problematic “valley of death”, mentioned earlier in the level 5 deﬁnitions, is less prevalent in use-\n",
      "cases like this that start at a higher MLTRL level with a speciﬁc product deliverable. In this case, the product\n",
      "deliverable was a real-time object recognition and classiﬁcation of trash for a mobile recycling application.\n",
      "Still, this stage is critical for the requirements and V&V transition. This stage mitigates failure risks due to the\n",
      "disparate data sources integrated at various steps in this CV system and accounted for the end-user compute\n",
      "constraints for mobile computing. Speciﬁcally, the TRL cards from earlier stages surfaced potential issues\n",
      "with imbalanced datasets and the need for speciﬁc synthetic images. These considerations are essential for the\n",
      "data readiness and testing V&V in the productization requirements. Data quality and availability issues often\n",
      "present huge blockers because teams discover them too late in the game. Data-readiness is one class of many\n",
      "example issues teams face without MLTRL, as depicted in Fig. 2.\n",
      "•Level 6 – We were re-using a well-understood model and deployment pipeline in this use-case, meaning our\n",
      "primary challenge was around data reliability. For the problem of recognizing and classifying trash, building a\n",
      "reliable data source using only real data is almost impossible due to diversity, class imbalance, and annotation\n",
      "challenges. Therefore we chose to develop a synthetic data generator to create training data. At this MLTRL\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the challenges of deploying machine learning models in real-world applications by prioritizing ethical considerations, risk assessment, and iterative development. Inspired by spacecraft engineering, MLTRL emphasizes continuous monitoring, feedback, and comprehensive testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the complexities of diverse data sources and integrates real-world data, user annotations, and synthetic data to ensure robustness and resilience. It recognizes the iterative nature of ML workflows and encourages continuous refinement, mitigating methodological flaws.\n",
      "\n",
      "The framework promotes quantifying uncertainties and addressing the gap between research and production, facilitating the successful deployment of models in diverse domains. MLTRL has been applied to address challenges with multiple data sources, mitigating potential bias through data governance practices.\n",
      "\n",
      "**Furthermore, MLTRL addresses the challenge of deploying ML models in real-world scenarios, where engineering costs and system requirements must be considered.** The framework emphasizes the need for effective communication and collaboration between ML teams and engineering teams to ensure successful deployment.\n",
      "\n",
      "**MLTRL's iterative development process also includes the use of prototype-caliber code checkpoints to ensure code integrity and mitigate potential risks associated with poorly defined boundaries and abstractions between code components.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "level, we needed to ensure that the synthetic data generator created sufﬁciently diverse data and exposed the\n",
      "controls needed to alter the data distribution in production. Therefore, we carefully exposed APIs using the\n",
      "Unity Perception package, which allowed us to control lighting, camera parameters, target and non-target\n",
      "object placements and counts, and background textures. Additionally, we ensured that the object labeling\n",
      "matched the real-world annotator instructions and that output data formats matched real-world counterparts.\n",
      "Lastly, we established a set of statistical tests to compare synthetic and real-world data distributions. The\n",
      "MLTRL checks ensured that we understood, and in this case, adequately designed our data sources to meet\n",
      "in-production requirements.\n",
      "•Level 7 – From the previous level’s R&D TRL cards and observations, we knew relatively early in produc-\n",
      "tization that we would need to assume bias for the real data sources due to class imbalance and imperfect\n",
      "annotations. Therefore we designed tests to monitor these in the deployed application. MLTRL imposes these\n",
      "critical deployment tests well ahead of deployment, where we can easily overlook ML-speciﬁc failure modes.\n",
      "•Level 8 – As we suggested earlier, problems that stem from real-world data are near impossible to anticipate\n",
      "and design for, implying the need for level 8 ﬂight-readiness preparations. Given that we were generating\n",
      "synthetic images (with structured domain randomization) to complement the real data, we created tests for\n",
      "different data distribution shifts at multiple points in the classiﬁcation pipeline. We also implemented thorough\n",
      "shadow tests ahead of deployment to evaluate how susceptible the ML model(s) to performance regressions\n",
      "caused by data. Additionally, we also implemented these as CI/CD tests over various deployment scenarios (or\n",
      "mobile device computing speciﬁcations). Without these fully covered, documented, and automated, it would\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the challenges of deploying machine learning models in real-world applications by prioritizing ethical considerations, risk assessment, and iterative development. Inspired by spacecraft engineering, MLTRL emphasizes continuous monitoring, feedback, and comprehensive testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the complexities of diverse data sources by integrating real-world data, user annotations, and synthetic data. It recognizes the iterative nature of ML workflows and encourages continuous refinement, mitigating methodological flaws. The framework promotes quantifying uncertainties and addressing the gap between research and production, facilitating the successful deployment of models in diverse domains.\n",
      "\n",
      "MLTRL also addresses the challenge of deploying ML models in real-world scenarios, where engineering costs and system requirements must be considered. The framework emphasizes the need for effective communication and collaboration between ML teams and engineering teams to ensure successful deployment.\n",
      "\n",
      "Furthermore, MLTRL includes specific measures to ensure data quality and address potential biases in real-world scenarios. It highlights the importance of monitoring data distributions and testing for various data shifts and performance regressions throughout the deployment process.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "be impossible to pass level 8 review and deploy the technology.\n",
      "•Level 9 – Post-deployment, the monitoring tests prescribed at Levels 8 and 9 and the three main code quality\n",
      "checkpoints in the MLTRL process help surface hidden performance degradation problems, common with\n",
      "complex pipelines of data ﬂows and various models. The switchbacks depicted in Fig. 2 are typical in CV\n",
      "use-cases. For instance, miscalibrations in models pre-trained on synthetic data and ﬁne-tuned on newer real\n",
      "data can be common yet difﬁcult to catch. However, the level 7 to 4 switchback is designed precisely for these\n",
      "challenges and product improvements.\n",
      "Accelerating scientiﬁc discovery with massive particle physics simulators\n",
      "Computational models and simulation are key to scientiﬁc advances at all scales, from particle physics, to material\n",
      "design and drug discovery, to weather and climate science, and to cosmology[ 35]. Many simulators model the forward\n",
      "evolution of a system (coinciding with the arrow of time), such as the interaction of elementary particles, diffusion of\n",
      "gasses, folding of proteins, or evolution of the universe in the largest scale. The task of inference refers to ﬁnding initial\n",
      "conditions or global parameters of such systems that can lead to some observed data representing the ﬁnal outcome\n",
      "of a simulation. In probabilistic programming[ 36], this inference task is performed by deﬁning prior distributions\n",
      "over any latent quantities of interest, and obtaining posterior distributions over these latent quantities conditioned\n",
      "on observed outcomes (for example, experimental data) using Bayes rule. This process, in effect, corresponds to\n",
      "inverting the simulator such that we go from the outcomes towards the inputs that caused the outcomes. In the\n",
      "“Etalumis” project[ 37] (“simulate” spelled backwards), we are using probabilistic programming methods to invert\n",
      "12\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the challenges of deploying machine learning models in real-world applications by prioritizing ethical considerations, risk assessment, and iterative development. Inspired by spacecraft engineering, MLTRL emphasizes continuous monitoring, feedback, and comprehensive testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses the complexities of diverse data sources by integrating real-world data, user annotations, and synthetic data. It recognizes the iterative nature of ML workflows and encourages continuous refinement, mitigating methodological flaws. The framework promotes quantifying uncertainties and addressing the gap between research and production, facilitating the successful deployment of models in diverse domains.\n",
      "\n",
      "MLTRL also addresses the challenge of deploying ML models in real-world scenarios, where engineering costs and system requirements must be considered. The framework emphasizes the need for effective communication and collaboration between ML teams and engineering teams to ensure successful deployment. Additionally, MLTRL includes specific measures to ensure data quality and address potential biases in real-world scenarios, including monitoring data distributions and testing for various data shifts and performance regressions throughout the deployment process.\n",
      "\n",
      "The framework also highlights the importance of post-deployment monitoring and analysis for identifying performance degradation issues that may arise in complex ML pipelines. This iterative approach is crucial for continuous improvement and product enhancements, addressing challenges such as model miscalibrations and data shifts.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "existing, large-scale simulators via Bayesian inference. The project is as an interdisciplinary collaboration of specialists\n",
      "in probabilistic machine learning, particle physics, and high-performance computing, all essential elements to achieve\n",
      "the project outcomes. Even more, it is a multi-year project spanning multiple countries, companies, university labs, and\n",
      "government research organizations, bringing signiﬁcant challenges in project management, technology coordination\n",
      "and validation. Aided by MLTRL, there were several key challenges to overcome in this project that are common in\n",
      "scientiﬁc-ML projects:\n",
      "•Integrating with legacy systems is common in scientiﬁc and industrial use-cases, where ML methods are\n",
      "applied with existing sensor networks, infrastructure, and codebases. In this case, particle physics domain\n",
      "experts at CERN are using the SHERPA simulator[ 38], a 1 million line codebase developed over the last\n",
      "two decades. Rewriting the simulator for ML use-cases is infeasible due to the codebase size and buried\n",
      "domain knowledge, and new ML experts would need signiﬁcant onboarding to gain working knowledge of\n",
      "the codebase. It is also common to work with legacy data infrastructure, which can be poorly organized for\n",
      "machine learning (let alone preprocessed and clean) and unlikely to have followed best practices such as\n",
      "dataset versioning.\n",
      "•Coupling hardware and software architectures is non-trivial when deploying ML at scale, as performance\n",
      "constraints are often considered in deployment tests well after model and algorithm development, not to\n",
      "mention the expertise is often split across disjoint teams. This can be exacerbated in scientiﬁc-ML when\n",
      "scaling to supercomputing infrastructure, and working with massive datasets that can be in the terabytes and\n",
      "petabytes.\n",
      "•Interpretability is often a desired feature yet difﬁcult to deliver and validate in practice. Particularly in\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the challenges of deploying machine learning models in real-world applications by prioritizing ethical considerations, risk assessment, and iterative development. Inspired by spacecraft engineering, MLTRL emphasizes continuous monitoring, feedback, and comprehensive testing throughout the ML development process.\n",
      "\n",
      "MLTRL tackles the complexities of diverse data sources by integrating real-world data, user annotations, and synthetic data. It recognizes the iterative nature of ML workflows and encourages continuous refinement, mitigating methodological flaws. The framework promotes quantifying uncertainties and addressing the gap between research and production, facilitating successful deployment in diverse domains.\n",
      "\n",
      "MLTRL also addresses the challenges of deploying ML models in real-world scenarios, where engineering costs and system requirements must be considered. The framework emphasizes the need for effective communication and collaboration between ML teams and engineering teams to ensure successful deployment. Additionally, MLTRL includes specific measures to ensure data quality and address potential biases in real-world scenarios.\n",
      "\n",
      "The framework has been applied in large-scale scientific projects involving Bayesian inference and complex simulations. It tackles challenges associated with integrating ML methods with legacy systems, coupling hardware and software architectures, and ensuring model interpretability. MLTRL's iterative approach is crucial for continuous improvement and product enhancements, addressing common challenges in scientific and industrial ML projects.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "scientiﬁc ML applications such as this, mechanisms and tooling for domain experts to interpret predictions\n",
      "and models are key for usability (integrating in workﬂows and building trust).\n",
      "To this end, we will go through the MLTRL levels one by one, demonstrating how they ensure the above scientiﬁc ML\n",
      "challenges are diligently addressed.\n",
      "•Level 0 – The theoretical developments leading to Etalumis are immense and well discussed in Baydin et\n",
      "al [37]. In particular the ML theory and methods are in a relatively nascent area of ML and mathematics,\n",
      "probabilistic programming. New territory can present more challenges compared to well-traveled research\n",
      "paths, for instance in computer vision with neural networks. It is thus helpful to have a guiding framework\n",
      "when making a new path in ML research, such as MLTRL where early reviews help theoretical ML projects\n",
      "get legs.\n",
      "•Level 1-2 – Running low-level experiments in simple testbeds is generally straightforward when working\n",
      "with probabilistic programming and simulation; in a sense, this easy iteration over experiments is what\n",
      "PPL are designed for. It was additionally helpful in this project to have rich data grounded in physical\n",
      "constraints, allowing us to better isolate model behaviors (rather than data assumptions and noise). The\n",
      "MLTRL requirements documentation is particularly useful for the standard PPL experimentation workﬂow:\n",
      "model, infer, criticize, repeat (or Box’s loop) [ 39]. The evaluation step (i.e. criticizing the model) can be\n",
      "more nuanced than checking summary statistics as in deep learning and similar ML workﬂows. It is thus a\n",
      "useful practice to write down the criticism methods, metrics, and expected results as veriﬁcations for speciﬁc\n",
      "research requirements, rather than iterating over Box’s loop without a priori targets. Further, because this\n",
      "research project had a speciﬁc target application early in the process (the SHERPA simulator), the project\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the challenges of deploying machine learning models in real-world applications by prioritizing ethical considerations, risk assessment, and iterative development. Inspired by spacecraft engineering, MLTRL emphasizes continuous monitoring, feedback, and comprehensive testing throughout the ML development process.\n",
      "\n",
      "MLTRL tackles the complexities of diverse data sources by integrating real-world data, user annotations, and synthetic data. It recognizes the iterative nature of ML workflows and encourages continuous refinement, mitigating methodological flaws. The framework promotes quantifying uncertainties and addressing the gap between research and production, facilitating successful deployment in diverse domains.\n",
      "\n",
      "MLTRL also addresses the challenges of deploying ML models in real-world scenarios, where engineering costs and system requirements must be considered. The framework emphasizes the need for effective communication and collaboration between ML teams and engineering teams to ensure successful deployment. Additionally, MLTRL includes specific measures to ensure data quality and address potential biases in real-world scenarios.\n",
      "\n",
      "The framework has been applied in large-scale scientific projects involving Bayesian inference and complex simulations. It tackles challenges associated with integrating ML methods with legacy systems, coupling hardware and software architectures, and ensuring model interpretability. MLTRL's iterative approach is crucial for continuous improvement and product enhancements, addressing common challenges in scientific and industrial ML projects.\n",
      "\n",
      "**Furthermore, MLTRL helps address key scientific ML challenges by:**\n",
      "\n",
      "* Providing mechanisms and tooling for domain experts to interpret predictions and models, enhancing usability.\n",
      "* Guiding researchers through the development process with specific levels addressing various complexities.\n",
      "* Ensuring early reviews and feedback to refine theoretical ML projects.\n",
      "\n",
      "The framework emphasizes the importance of well-defined evaluation steps and clear documentation for transparent and reproducible results.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "timeline beneﬁted from recognizing simulator-integration constraints upfront as requirements, not to mention\n",
      "data availability concerns, which are often overlooked in early R&D levels. It was additionally useful to have\n",
      "CERN scientists as domain experts in the reviews at these R&D levels.\n",
      "•Level 3 – Systems development can be challenging with probabilistic programming, again because it is\n",
      "relatively nascent and much of the out-of-the-box tools and infrastructure are not there as in most ML and\n",
      "deep learning. Here in particular there’s a novel (unproven) approach for systems integration: a probabilistic\n",
      "programming execution protocol was developed to reroute random number draws in the stochastic simulator\n",
      "codebase (SHERPA) to the probabilistic programming system, thus enabling the system to control stochastic\n",
      "choices in SHERPA and run inference on its execution traces, all while keeping the legacy codebase intact! A\n",
      "more invasive method that modiﬁes SHERPA would not have been acceptable. If it were not for MLTRL forcing\n",
      "systems considerations this early in the Etalumis project lifecycle, this could have been an insurmountable\n",
      "hurdle later when multiple codebases and infrastructures come into play. By the same token, systems planning\n",
      "here helped enable the signiﬁcant HPC scaling later: the team deﬁned the need for HPC support well ahead\n",
      "of actually running HPC, in order to build the prototype code in a way that would readily map to HPC (in\n",
      "addition to local or cloud CPU and GPU). The data engineering challenges in this system’s development\n",
      "13\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the challenges of deploying machine learning models in real-world applications by prioritizing ethical considerations, risk assessment, and iterative development. Inspired by spacecraft engineering, MLTRL emphasizes continuous monitoring, feedback, and comprehensive testing throughout the ML development process.\n",
      "\n",
      "MLTRL tackles the complexities of diverse data sources by integrating real-world data, user annotations, and synthetic data. It recognizes the iterative nature of ML workflows and encourages continuous refinement, mitigating methodological flaws. The framework promotes quantifying uncertainties and addressing the gap between research and production, facilitating successful deployment in diverse domains.\n",
      "\n",
      "MLTRL also addresses the challenges of deploying ML models in real-world scenarios, where engineering costs and system requirements must be considered. The framework emphasizes the need for effective communication and collaboration between ML teams and engineering teams to ensure successful deployment. Additionally, MLTRL includes specific measures to ensure data quality and address potential biases in real-world scenarios.\n",
      "\n",
      "**Furthermore, MLTRL helps address key scientific ML challenges by:**\n",
      "\n",
      "* Providing mechanisms and tooling for domain experts to interpret predictions and models, enhancing usability.\n",
      "* Guiding researchers through the development process with specific levels addressing various complexities.\n",
      "* Ensuring early reviews and feedback to refine theoretical ML projects.\n",
      "\n",
      "**MLTRL's emphasis on systems considerations proved invaluable in complex projects.** For example, in the Etalumis project, MLTRL facilitated the integration of probabilistic programming with existing codebases, mitigating risks and enabling successful deployment. This highlights MLTRL's adaptability and its potential to overcome challenges inherent in real-world ML deployments.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "nonetheless persist – that is, data pipelines and APIs that can integrate various sources and infrastructures, and\n",
      "normalize data from various databases – although MLTRL helps consider these at the an earlier stage that can\n",
      "help inform architecture design.\n",
      "•Level 4 – The natural “embedded switchback” from Level 4 to 2 (see the Methods section) provided an efﬁcient\n",
      "path toward developing an improved, amortized inference method–i.e., using a computationally expensive\n",
      "deep learning based inference algorithm to train only once, in order to then do fast, repeated inference in the\n",
      "SHERPA model. Leveraging cyclic R&D methods, the Etalumis project could iteratively improve inference\n",
      "methods without stalling the broader system development, ultimately producing the largest scale posterior\n",
      "inference in a Turing-complete probabilistic programming system. Achieving this scale through iterative R&D\n",
      "along the main project lifecycle was additionally enabled by working with with NERSC engineers and their\n",
      "Cori supercomputer to progressively scale smaller R&D tests to the goal supercomputing deployment scenario.\n",
      "Typical ML workﬂows that follow simple linear progressions[ 6,40] would not enable ramping up in this\n",
      "fashion, and can actual prevent scaling R&D to production due to lack of systems engineering processes (like\n",
      "MLTRL) connecting research to deployment.\n",
      "•Level 5 – Multi-org international collaborations can be riddled with communication and teamwork issues,\n",
      "in particular at this pivotal stage where teams transition from R&D to application and product development.\n",
      "First, MLTRL as a lingua franca was key to the team effort bringing Etalumis proof-of-concept into the\n",
      "larger effort of applying it to massive high-energy physics simulators. It was also critical at this stage to\n",
      "clearly communicate end-user requirements across the various teams and organizations, which must be deﬁned\n",
      "in MLTRL requirements docs with V&V measures – the essential science-user requirements were mainly\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the challenges of deploying machine learning models in real-world applications by prioritizing ethical considerations, risk assessment, and iterative development. Inspired by spacecraft engineering, MLTRL emphasizes continuous monitoring, feedback, and comprehensive testing throughout the ML development process.\n",
      "\n",
      "MLTRL tackles the complexities of diverse data sources by integrating real-world data, user annotations, and synthetic data. It recognizes the iterative nature of ML workflows and encourages continuous refinement, mitigating methodological flaws. The framework promotes quantifying uncertainties and addressing the gap between research and production, facilitating successful deployment in diverse domains.\n",
      "\n",
      "MLTRL also addresses the challenges of deploying ML models in real-world scenarios, where engineering costs and system requirements must be considered. The framework emphasizes the need for effective communication and collaboration between ML teams and engineering teams to ensure successful deployment. Additionally, MLTRL includes specific measures to ensure data quality and address potential biases in real-world scenarios.\n",
      "\n",
      "**Furthermore, MLTRL helps address key scientific ML challenges by:**\n",
      "\n",
      "* Providing mechanisms and tooling for domain experts to interpret predictions and models, enhancing usability.\n",
      "* Guiding researchers through the development process with specific levels addressing various complexities.\n",
      "* Ensuring early reviews and feedback to refine theoretical ML projects.\n",
      "\n",
      "**MLTRL's emphasis on systems considerations proved invaluable in complex projects.** For example, in the Etalumis project, MLTRL facilitated the integration of probabilistic programming with existing codebases, mitigating risks and enabling successful deployment. This highlights MLTRL's adaptability and its potential to overcome challenges inherent in real-world ML deployments.\n",
      "\n",
      "**MLTRL also helps establish robust data infrastructure:** MLTRL considers data pipelines and APIs that can integrate various sources and infrastructures, and normalize data from various databases, aiding in informed architecture design.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "in MLTRL requirements docs with V&V measures – the essential science-user requirements were mainly\n",
      "for model and prediction interpretability, uncertainty estimation, and code usability. If there are concerns\n",
      "over these features, MLTRL switchbacks can help to quickly cycle back and improve modeling choices in a\n",
      "transparent, efﬁcient way – generally in ML projects, these fundamental issues with usability are caught too\n",
      "late, even after deployment. In the probabilistic generative model setting we’ve deﬁned in Etalumis, Bayesian\n",
      "inference gives results that are interpretable because they include exact locations and processes in the model\n",
      "that are associated with each prediction. Working with ML methods that are inherently interpretable, we are\n",
      "well-positioned to deliver interpretable interfaces for the end-users later in the project lifecycle.\n",
      "•Level 6-9 – The standard MLTRL protocol apply in these application-to-deployment stages, with several\n",
      "Etalumis-speciﬁc highlights. First, given the signiﬁcant research contributions in both probabilistic pro-\n",
      "gramming and scientiﬁc-ML, it’s important to share the code publicly. The development and deployment\n",
      "of the open-source code repository PPXvibranched into a separate MLTRL path from the Etalumis path\n",
      "for deployment at CERN. It’s useful to have systems engineering enable clean separation of requirements,\n",
      "deployments, etc. when there are different development and product lifecycles originating from a common\n",
      "parent project. For example, in this case it was useful to employ MLTRL switchbacks in the open-sourcing\n",
      "process, isolated from the CERN application paths, in order to add support for additional programming\n",
      "languages so PPX can apply to more scientiﬁc simulators – both directions beneﬁted signiﬁcantly the from\n",
      "the data pipelines considerations brought up levels earlier, where open-sourcing required different data APIs\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the challenges of deploying machine learning models in real-world applications by prioritizing ethical considerations, risk assessment, and iterative development. Inspired by spacecraft engineering, MLTRL emphasizes continuous monitoring, feedback, and comprehensive testing throughout the ML development process.\n",
      "\n",
      "MLTRL tackles diverse data sources by integrating real-world data, user annotations, and synthetic data, recognizing the iterative nature of ML workflows and encouraging continuous refinement. The framework promotes quantifying uncertainties and addressing the gap between research and production, facilitating successful deployment across domains.\n",
      "\n",
      "MLTRL also addresses the challenges of deploying ML models in real-world scenarios, considering engineering costs and system requirements. It emphasizes the need for effective communication and collaboration between ML teams and engineering teams to ensure successful deployment. Additionally, MLTRL includes measures to ensure data quality and address potential biases in real-world scenarios.\n",
      "\n",
      "**MLTRL addresses key scientific ML challenges by:**\n",
      "\n",
      "* Providing mechanisms and tooling for domain experts to interpret predictions and models.\n",
      "* Guiding researchers through the development process with specific levels addressing various complexities.\n",
      "* Ensuring early reviews and feedback to refine theoretical ML projects.\n",
      "\n",
      "**Furthermore, MLTRL promotes robust data infrastructure:** MLTRL considers data pipelines and APIs that can integrate various sources and infrastructures, aiding in informed architecture design.\n",
      "\n",
      "**MLTRL's emphasis on systems considerations proved invaluable in complex projects.** For example, in the Etalumis project, MLTRL facilitated the integration of probabilistic programming with existing codebases, mitigating risks and enabling successful deployment.\n",
      "\n",
      "**MLTRL also helps establish clear requirements and facilitate open-source code release:** Detailed requirements for model interpretability, uncertainty estimation, and code usability were identified in MLTRL requirements documents. The framework encourages open-source code releases to enhance accessibility and collaboration.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "and data transformations to enable broad usability. Second, related to the open-source code deliverable and\n",
      "the scientiﬁc ML user requirements we noted above, the late stages of MLTRL reviews include higher level\n",
      "stakeholders and speciﬁc end-users, yet again enforcing these scientiﬁc usability requirements are met. An\n",
      "example result of this in Etalumis is the ability to output human-readable execution traces of the SHERPA\n",
      "runs and inference, enabling never before possible step-by-step interpretability of the black-box simulator.\n",
      "The scientiﬁc ML perspective additionally brings to forefront an end-to-end data perspective that is pertinent in\n",
      "essentially all ML use-cases: these systems are only useful to the extent they provide comprehensive data analyses that\n",
      "integrate the data consumed and generated in these workﬂows, from raw domain data to machine-learned models. These\n",
      "data analyses drive reproducibility, explainability, and experiment data understanding, which are critical requirements\n",
      "in scientiﬁc endeavors and ML broadly.\n",
      "Causal inference & ML in medicine\n",
      "Understanding cause and effect relationships is crucial for accurate and actionable decision-making in many settings,\n",
      "from healthcare and epidemiology, to economics and government policy development. Unfortunately, standard\n",
      "machine learning algorithms can only ﬁnd patterns and correlation in data, and as correlation is not causation, their\n",
      "predictions cannot be conﬁdently used for understanding cause and effect. Indeed, relying on correlations extracted\n",
      "from observational data to guide decision-making can lead to embarrassing, costly, and even dangerous mistakes,\n",
      "such as concluding that asthma reduces pneumonia mortality risk [ 41], and that smoking reduces risk of developing\n",
      "vigithub.com/pyprob/ppx\n",
      "14\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the challenges of deploying machine learning models in real-world applications by prioritizing ethical considerations, risk assessment, and iterative development. Inspired by spacecraft engineering, MLTRL emphasizes continuous monitoring, feedback, and comprehensive testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses diverse data sources by integrating real-world data, user annotations, and synthetic data, recognizing the iterative nature of ML workflows and encouraging continuous refinement. It promotes quantifying uncertainties and addressing the gap between research and production, facilitating successful deployment across domains.\n",
      "\n",
      "MLTRL also addresses the challenges of deploying ML models in real-world scenarios, considering engineering costs and system requirements. It emphasizes the need for effective communication and collaboration between ML teams and engineering teams to ensure successful deployment. Additionally, MLTRL includes measures to ensure data quality and address potential biases in real-world scenarios.\n",
      "\n",
      "**Furthermore, MLTRL promotes robust data infrastructure and provides guidance for domain experts to interpret models and understand predictions.** It also emphasizes the importance of end-to-end data analysis for reproducibility, explainability, and experiment understanding.\n",
      "\n",
      "**MLTRL's emphasis on systems considerations proved invaluable in complex projects, where it facilitated the integration of probabilistic programming with existing codebases.** The framework also encourages open-source code releases to enhance accessibility and collaboration.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "severe COVID-19 [ 42]. Fortunately, there has been much recent development in a ﬁeld known as causal inference that\n",
      "can quantitatively make sense of cause and effect from purely observational data[ 43]. The ability of causal inference\n",
      "algorithms to quantify causal impact rests on a number of important checks and assumptions–beyond those employed\n",
      "in standard machine learning or purely statistical methodology–that must be carefully deliberated over during their\n",
      "development and training. These speciﬁc checks and assumptions are as follows:\n",
      "•Specifying cause-and-effect relationships between relevant variables– One of the most important assump-\n",
      "tions underlying causal inference is the structure of the causal relations between quantities of interest. The\n",
      "gold standard for determining causal relations is to perform a randomised controlled trial, but in most cases\n",
      "these cannot be employed due to ethical concerns, technological infeasibility, or prohibitive cost. In these\n",
      "situations, domain experts have to be consulted to determine the causal relationships. It is important in these\n",
      "situations to carefully address the manner in which such domain knowledge was extracted from experts, the\n",
      "number and diversity of experts involved, the amount of consensus between experts, and so on. The need for\n",
      "careful documentation of this knowledge and its periodic review is made clear in the MLTRL framework, as\n",
      "we shall see below.\n",
      "•Identiﬁability– Another vital component of building causal models is whether the causal question of interest\n",
      "isidentiﬁable from the causal structure speciﬁed for the model together with observational (and sometimes\n",
      "experimental) data.\n",
      "•Adjusting for and monitoring confounding bias– An important aspect of causal model performance, not\n",
      "present in standard machine learning algorithms, is confounding bias adjustment. The standard approach is to\n",
      "employ propensity score matching to remove such bias. However, the quality of bias adjustment achieved in\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the challenges of deploying machine learning models in real-world applications by prioritizing ethical considerations, risk assessment, and iterative development. Inspired by spacecraft engineering, MLTRL emphasizes continuous monitoring, feedback, and comprehensive testing throughout the ML development process.\n",
      "\n",
      "MLTRL addresses diverse data sources by integrating real-world data, user annotations, and synthetic data, recognizing the iterative nature of ML workflows and encouraging continuous refinement. It promotes quantifying uncertainties and addressing the gap between research and production, facilitating successful deployment across domains.\n",
      "\n",
      "MLTRL also addresses the challenges of deploying ML models in real-world scenarios, considering engineering costs and system requirements. It emphasizes the need for effective communication and collaboration between ML teams and engineering teams to ensure successful deployment. Additionally, MLTRL includes measures to ensure data quality and address potential biases in real-world scenarios.\n",
      "\n",
      "**Furthermore, MLTRL promotes robust data infrastructure and provides guidance for domain experts to interpret models and understand predictions.** It also emphasizes the importance of end-to-end data analysis for reproducibility, explainability, and experiment understanding.\n",
      "\n",
      "**MLTRL's emphasis on systems considerations proved invaluable in complex projects, where it facilitated the integration of probabilistic programming with existing codebases.** The framework also encourages open-source code releases to enhance accessibility and collaboration.\n",
      "\n",
      "**Recent advancements in causal inference offer promising avenues for quantifying causal relationships from observational data.** These methods require careful consideration of causal relationships, model identifiability, and bias adjustment, aspects carefully addressed by MLTRL.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "any speciﬁc instance with such propensity-based matching methods needs to be checked and documented,\n",
      "with alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[44].\n",
      "•Sensitivity analysis– As causal estimates are based on generally untestable assumptions, such as observing all\n",
      "relevant confounders, it is vital to determine how sensitive the resulting predictions are to potential violations\n",
      "of these assumptions.\n",
      "•Consistency– It is crucial to understand if the learned causal estimate provably converges to the true causal\n",
      "effect in the limit of inﬁnite sample size. However, causal models cannot be validated by standard held-out\n",
      "tests, but rather require randomization or special data collection strategies to evaluate their predictions [ 45,46].\n",
      "The MLTRL framework makes transparent the need to carefully document and defend these assumptions, thus ensuring\n",
      "the safe and robust creation, deployment, and maintenance of causal models. We elucidate this with recent work by\n",
      "Richens et al.[ 47], developing a causal approach to computer-assisted diagnosis which outperforms previous purely\n",
      "machine learning based methods. To this end, we will go through the MLTRL levels one by one, demonstrating how\n",
      "they ensure the above speciﬁc checks and assumptions are naturally accounted for. This should provide a blueprint for\n",
      "how to employ the MLTRL levels in other causal inference applications.\n",
      "•Level 0 – When initially faced with a causal inference task, the ﬁrst step is always to understand the causal\n",
      "relationships between relevant variables. For instance, in Richens et al. [ 47], the ﬁrst step toward building\n",
      "the diagnostic model was specifying the causal relationships between the diverse set risk factors, diseases,\n",
      "and symptoms included in the model. To learn these relations, doctors and healthcare professionals were\n",
      "consulted to employ their expansive medical domain knowledge which was robustly evaluated by additional\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, inspired by spacecraft engineering principles. MLTRL addresses diverse data sources, promotes quantifying uncertainties, and fosters a bridge between research and production.\n",
      "\n",
      "The framework also tackles deployment challenges in real-world scenarios, considering engineering costs and system requirements. It promotes effective communication and collaboration between ML and engineering teams, while ensuring data quality and addressing potential biases. MLTRL further underscores the importance of robust data infrastructure and encourages domain experts to interpret models and understand predictions.\n",
      "\n",
      "Recent advancements in causal inference are particularly relevant to MLTRL, offering methods to quantify causal relationships from observational data. MLTRL explicitly addresses the need for careful consideration of causal relationships, model identifiability, and bias adjustment in causal inference applications.\n",
      "\n",
      "The document provides specific examples, such as a causal approach to computer-assisted diagnosis, demonstrating how MLTRL levels ensure careful documentation of assumptions, promoting the safe and robust creation, deployment, and maintenance of causal models.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "independent groups of healthcare professionals. The MLTRL framework ensured this issue is dealt with and\n",
      "documented correctly, as such knowledge is required to progress from Level 0; failure to do this has plagued\n",
      "similar healthcare AI projects [48].\n",
      "The next step of any causal analysis is to understand whether the causal question of interest is uniquely\n",
      "identiﬁable from the causal structure speciﬁed for the model together with observational and experimental data.\n",
      "In this medical diagnosis example, identiﬁcation was crucial to establish, as the causal question of interest,\n",
      "“would the observed symptoms not be present had a speciﬁc disease been cured?”, was highly non-trivial.\n",
      "Again, MLTRL ensures this vital aspect of model building is carefully considered, as a mathematical proof of\n",
      "identiﬁability would be required to graduate from Level 0.\n",
      "With both the causal structure and identiﬁability result in hand, one can progress to Level 1.\n",
      "•Level 1 – At this level, the goal is to take the estimand for the identiﬁed causal question of interest and\n",
      "devise a way to estimate it from data. To do this one will need efﬁcient ways to adjust for confounﬁng bias.\n",
      "The standard approach is to employ propensity score-based methods to remove such bias when the target\n",
      "decision is binary, and use multi-stage ML models adhering to the assumed causal structure[ 49] for continuous\n",
      "target decisions (and high-dimensional data in general). However, the quality of bias adjustment achieved in\n",
      "any speciﬁc instance with propensity-based matching methods needs to be checked and documented, with\n",
      "15\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[ 44]. As\n",
      "above, MLTRL ensures transparency and adherence to this important aspect of causal model development, as\n",
      "without it a project cannot graduate from Level 1. Even more, MLTRL ensures tests for confounding bias\n",
      "are developed early-on and maintained throughout later stages to deployment. Still, in many cases, it is not\n",
      "possible to completely remove confounding in the observed data. TRL Cards offer a transparent way to declare\n",
      "speciﬁc limitations of a causal ML method.\n",
      "•Level 2 – PoC-level tests for causal models must go beyond that of typical ML models. As discussed above,\n",
      "to ensure the estimated causal effects are robust to the assumptions required for their derivation, sensitivity\n",
      "to these assumptions must be analysed. Such sensitivity analysis is often limited to R&D experiments or\n",
      "a post-hoc feature of ML products. MLTRL on the other hand requires this throughout the lifecycle as\n",
      "components of ML test suites and gated reviews. In the case of causal ML, best practice is to employ sensitivity\n",
      "analysis for this robustness check[ 50]. MLTRL ensures this check is highlighted and adhered to, and no model\n",
      "will end up graduating Level 2–let alone being deployed–unless it is passed.\n",
      "•Level 3 – Coding best practices, as in general ML applications.\n",
      "•Level 4-5 – There are additional tests to consider when taking causal models from research to production,\n",
      "in particular at Level 4–proof of concept demonstration in a real scenario. Consistency , for example, is an\n",
      "important property of causal methods that informs us whether the method provably converges to the true\n",
      "causal graph in the limit of inﬁnite sample size. Quantifying consistency in the test suite is critical when\n",
      "datasets change from controlled laboratory settings to open-world, and when the application scales. And\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL deals with diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "PoC validation steps are more efﬁcient with MLTRL because the process facilitates early speciﬁcation of the\n",
      "evaluation metric for a causal model in Level 2. Causal models cannot be validated by standard held-out tests,\n",
      "but rather require randomization or special data collection strategies to evaluate their predictions[ 45,46]. Any\n",
      "difﬁculty in evaluating the model’s predictions will be caught early and remedied.\n",
      "•Level 6-9 – With the the causal ML components of this technology developed reliably in the previous levels,\n",
      "the rest of the levels developing this technology focused on general medical-ML deployment challenges. For\n",
      "the most part, data governance, privacy, and management that was detailed earlier in the neuropathology\n",
      "MLTRL use-case, as well as on-premises deployment.\n",
      "AI for open-source space sciences\n",
      "The CAMS (Cameras for Allsky Meteor Surveillance) project [ 51], established in 2010 by NASA, uses hundreds of\n",
      "off-the-shelf CCTV cameras to capture the meteor activity in the night sky. Initially, resident scientists would retrieve\n",
      "hard-disks containing video data captured each night and perform manual triangulation of tracks or streaks of light\n",
      "in the night sky, and compute a meteor’s trajectory, orbit, and lightcurve. Each solution was manually classiﬁed as a\n",
      "meteor or not (i.e., planes, birds, clouds, etc). In 2017, a project run by the Frontier Development Labvii[52], the AI\n",
      "accelerator for NASA and ESA, aimed to automate the data processing pipeline and replicate the scientists thought\n",
      "process to build an ML model that identiﬁes meteors in the CAMS project [ 53,54]. The data automation led to\n",
      "orders of magnitude improvements in operational efﬁciency of the system, and allowed new contributors and amateur\n",
      "astronomers to start contributing to meteor sightings. Additionally, a novel web tool allowed anybody anywhere to\n",
      "view the meteors detected in the previous night. The CAMS camera system has had six-fold global expansion of the\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL deals with diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL also facilitates efficient validation of causal models by enabling early specification of the evaluation metric in Level 2, recognizing the unique challenges in evaluating causal models.\n",
      "\n",
      "**Additional Context:**\n",
      "\n",
      "The MLTRL framework has been applied in diverse applications, such as the CAMS project, where it was used to automate the identification of meteors in astronomical images. This resulted in significant improvements in operational efficiency and increased accessibility of the system for both professionals and amateurs.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "data capture network, discovered ten new meteor showers, contributed towards instrumental evidence of previously\n",
      "predicted comets, and helped calculate parent bodies of various meteor showers. CAMS utilized the MLTRL framework\n",
      "to progress as described:\n",
      "•Level 1 – Understanding the domain and data is a prerequisite for any ML development. Extensive data\n",
      "exploration elucidated visual differences between objects in the night sky such as meteors, satellites, clouds,\n",
      "tail lights of planes, light from the eyes of cats peering into cameras, trees, and other tall objects visible in\n",
      "the moonlight. This step helped (1) understand visual properties of meteors that later deﬁned the ML model\n",
      "architecture, and (2) mitigate impact of data imbalance by proactively developing domain-oriented strategies.\n",
      "The results are well-documented on a datasheet associated with the TRL card, and discussed at the stage\n",
      "review. This MLTRL documentation forced us to consider data sharing and other privacy concerns at this early\n",
      "conceptualization stage, which is certainly relevant considering CAMS is for open-source and gathering data\n",
      "from myriad sources.\n",
      "•Level 2-3 – The agile and non-monotonic (or non-linear) development prescribed by MLTRL allowed the\n",
      "team to ﬁrst develop an approximate end-to-end pipeline that offered a path to ML model deployment and\n",
      "quick turnaround time to incorporate feedback from the regular gated reviews. Then, with relatively quicker\n",
      "viiThe NASA Frontier Development Lab and partners open-source the code and data via the SpaceML platform: spaceml.org\n",
      "16\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL deals with diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL also facilitates efficient validation of causal models by enabling early specification of the evaluation metric in Level 2, recognizing the unique challenges in evaluating causal models.\n",
      "\n",
      "**Application Example:**\n",
      "\n",
      "The MLTRL framework was applied in the CAMS project, where it enabled the automation of identifying meteors in astronomical images. This resulted in significant improvements in operational efficiency and increased accessibility of the system for both professionals and amateurs. By leveraging domain expertise and addressing data imbalance, the CAMS team successfully developed and deployed a robust automated detection system.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "experimentation, the team could improve on the quality of not just the ML model, but also scale up the systems\n",
      "development simultaneously in a non-monotonic development cycle.\n",
      "•Level 4 – With the initial pipeline in place, scalable training of baselines and initial models on real challenging\n",
      "datasets ensued. Throughout the levels, the MLTRL gated reviews were essential for making efﬁcient progress\n",
      "while ensuring robustness and functionality that meets stakeholder needs. At this stage we highlight speciﬁc\n",
      "advantages of the MLTRL review processes that had instrumental effect on the project success: With the\n",
      "required panel of mixed ML researchers and engineers, domain scientists, and product managers, the stage 4\n",
      "reviews stressed the signiﬁcance of numerical improvements and comparison to existing baselines, and helped\n",
      "identify and overcome issues with data imbalance. The team likely would have overlooked these approaches\n",
      "without the review from peers in diverse roles and teams. In general, the evolving panel of reviewers at\n",
      "different stages of the project was essential for covering a variety of veriﬁcation and validation measures –\n",
      "from helping mitigate data challenges, to open-source code quality.\n",
      "•Level 5 – To complete this R&D-to-productization level, a novel web tool called the NASA CAMS Meteor\n",
      "Shower Portalviiiwas created that allowed users to view meteor shower activity from the previous night and\n",
      "verify meteor predictions generated by the ML model. This app development was valuable for A/B testing,\n",
      "validating detected meteors and classiﬁed new meteor showers with human-AI interaction, and demonstrating\n",
      "real-world utility to stakeholders in review. ML processes without MLTRL miss out on these valuable\n",
      "development by overlooking the need for such a demo tool.\n",
      "•Level 6 – Application development was naturally driven by end-user feedback from the web app in level 5 –\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL deals with diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL also facilitates efficient validation of causal models by enabling early specification of the evaluation metric in Level 2, recognizing the unique challenges in evaluating causal models.\n",
      "\n",
      "**Application Example:**\n",
      "\n",
      "The MLTRL framework was applied in the CAMS project, where it enabled the automation of identifying meteors in astronomical images. This resulted in significant improvements in operational efficiency and increased accessibility of the system for both professionals and amateurs. By leveraging domain expertise and addressing data imbalance, the CAMS team successfully developed and deployed a robust automated detection system.\n",
      "\n",
      "**Additional Insights:**\n",
      "\n",
      "The successful application of MLTRL in the CAMS project highlights the importance of iterative development, domain expertise, and comprehensive validation throughout the ML development process. The framework emphasizes the value of diverse perspectives and collaboration in achieving robust and impactful ML solutions.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "without MLTRL it’s unlikely the team would be able to work with early productization feedback. With almost\n",
      "real time feedback coming in daily, newer methods for improving robustness of meteor identiﬁcation led to\n",
      "researching and developing a unique augmentation technique, resulting in the state of the art performance of\n",
      "the ML model. Further application development led to incorporating features that were in demand by users of\n",
      "the NASA CAMS Meteor Shower Portal: include celestial reference points through constellations, add ability\n",
      "to zoom in/out and (un)cluster showers, and provide tooling for scientiﬁc communication. The coordination of\n",
      "these features into product-caliber codebase resulted in the release of the NASA CAMS Meteor Shower Portal\n",
      "2.0 that was built by a team of citizen scientists – again we found the speciﬁc checkpoints in the MLTRL\n",
      "review were crucial for achieving these goals.\n",
      "•Level 7 – Integration was particularly challenging in two ways. First, integrating the ML and data engineering\n",
      "deliverables with the existing infrastructure and tools of the larger CAMS system, which had started devel-\n",
      "opment years earlier with other teams in partner organizations, required quantiﬁable progress for verifying\n",
      "the tech-readiness of ML models and modules. The use of technology readiness levels provided a clear and\n",
      "consistent metric for the maturity of the ML and data technologies, making for clear communication and\n",
      "efﬁcient project integration. Without MLTRL it is difﬁcult to have a conversation, let alone make progress, to-\n",
      "wards integrating AI/ML and data subsystems and components. Second, integrating open-source contributions\n",
      "into the main ML subsystem was a signiﬁcant challenge alleviated with diligent veriﬁcation and validation\n",
      "measures from MLTRL, as well as quantifying robustness with ML testing suites (using scoring measures like\n",
      "that of the ML Testing Rubric[20], and devising a checklist based on metamorphic testing[18]).\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL deals with diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL also facilitates efficient validation of causal models by enabling early specification of the evaluation metric in Level 2, recognizing the unique challenges in evaluating causal models.\n",
      "\n",
      "**Application Example:**\n",
      "\n",
      "The MLTRL framework was applied in the CAMS project, where it enabled the automation of identifying meteors in astronomical images. This resulted in significant improvements in operational efficiency and increased accessibility of the system for both professionals and amateurs. By leveraging domain expertise and addressing data imbalance, the CAMS team successfully developed and deployed a robust automated detection system.\n",
      "\n",
      "**Additional Insights:**\n",
      "\n",
      "The successful application of MLTRL in the CAMS project highlights the importance of iterative development, domain expertise, and comprehensive validation throughout the ML development process. The framework emphasizes the value of diverse perspectives and collaboration in achieving robust and impactful ML solutions.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "MLTRL provides a systematic and practical framework for developing and deploying machine learning models in real-world applications. Its emphasis on iterative development, domain expertise, and comprehensive validation has proven invaluable in projects like the CAMS system, demonstrating its effectiveness in achieving robust and impactful ML solutions.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "that of the ML Testing Rubric[20], and devising a checklist based on metamorphic testing[18]).\n",
      "•Level 8 – CAMS, like many datasets in practice, consisted of a smaller labeled subset and a much larger\n",
      "unlabeled set. In an attempt to additionally increase robustness of the ML subsystem ahead of “ﬂight readiness”,\n",
      "we looked to active learning [ 55,56] techniques to leverage the unlabeled data. Models using an initial version\n",
      "of this approach, where results of the active learning provided “weak” labels, resulted in consumption of the\n",
      "entire decade long unlabelled data collected by CAMS and slightly higher scores on deployment tests. Active\n",
      "learning showed to be a promising feature and was switched back to level 7 for further development towards\n",
      "the next deployment version, so as not to delay the rest of the project.\n",
      "•Level 9 – The ML components in CAMS require continual monitoring for model and data drifts, such as\n",
      "changes in weather, smoke, and cloud patterns that affect the view of the night sky. The data drifts may also be\n",
      "speciﬁc to locations, such as ﬁreﬂies and bugs in CAMS Australia and New Zealand stations which appear as\n",
      "false positives. The ML pipeline is largely automated with CI/CD, runs regular regression tests, and production\n",
      "of benchmarks. Manual intervention can be triggered when needed, such as sending low conﬁdence meteors for\n",
      "veriﬁcation to scientists in the CAMS project. The team also regularly releases the code, models, and web tools\n",
      "on the open-source space sciences and exploration ML toolbox, SpaceMLix. Through the SpaceML community\n",
      "and partner organizations, CAMS continually improves with feature requests, debugging, and improving data\n",
      "practices, while tracking progress with standard software release cycles and MLTRL documentation.\n",
      "viiimeteorshowers.seti.org\n",
      "ixspaceml.org\n",
      "17\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL deals with diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL also facilitates efficient validation of causal models by enabling early specification of the evaluation metric in Level 2, recognizing the unique challenges in evaluating causal models. The framework has been applied in projects like the CAMS system, where it enabled the automation of identifying meteors in astronomical images.\n",
      "\n",
      "**Additional Insights:**\n",
      "\n",
      "The successful application of MLTRL in the CAMS project highlights the importance of iterative development, domain expertise, and comprehensive validation throughout the ML development process. The framework emphasizes the value of diverse perspectives and collaboration in achieving robust and impactful ML solutions.\n",
      "\n",
      "**Application Example:**\n",
      "\n",
      "- The CAMS project utilized MLTRL to automate the identification of meteors in astronomical images, resulting in significant improvements in operational efficiency.\n",
      "- Active learning techniques were leveraged to leverage unlabeled data and enhance model robustness.\n",
      "- Continuous monitoring and automated testing were employed to address data drifts and ensure model performance over time.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "MLTRL provides a systematic and practical framework for developing and deploying machine learning models in real-world applications. Its emphasis on iterative development, domain expertise, and comprehensive validation has proven invaluable in projects like the CAMS system, demonstrating its effectiveness in achieving robust and impactful ML solutions.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "BEYOND SOFTWARE ENGINEERING\n",
      "Software engineering (SWE) practices vary signiﬁcantly across domains and industries. Some domains, such as medical\n",
      "applications, aerospace, or autonomous vehicles rely on a highly rigorous development process which is required\n",
      "by regulations. Other domains, for example advertising and e-commerce are not regulated and can employ a lenient\n",
      "approach to development. ML development should at minimum inherit the acceptable software engineering practices of\n",
      "the domain. There are, however, several key areas where ML development stands out from SWE, adding its own unique\n",
      "challenges which even most rigorous SWE practices are not able to overcome.\n",
      "For instance, the behavior of ML systems is learned from data, not speciﬁed directly in code. The data requirements\n",
      "around ML (i.e., data discovery, management, and monitoring) adds signiﬁcant complexity not seen in other types\n",
      "of SWE. There are many beneﬁts to using a data-oriented architecture (DOA) [48] with the data-ﬁrst workﬂows and\n",
      "management practices prescribed in MLTRL. DOA aims to make the data ﬂowing between elements of business logic\n",
      "more explicit and accessible with a streaming-based architecture rather than the micro-service architectures that are\n",
      "standard in software systems. One speciﬁc beneﬁt of DOA is making data available and traceable by design, which\n",
      "helps signiﬁcantly in the ML logging challenges and data governance needs we discussed in Levels 7-9. Moreover,\n",
      "MLTRL highlights data-related requirements along every step to ensure that the development process considers data\n",
      "readiness and availability.\n",
      "Not to mention an array of ML-speciﬁc failure modes; for example, models that become miscalibrated due to subtle\n",
      "data distributional shifts in the deployment setting, resulting in models that are more conﬁdent in predictions than they\n",
      "should be. MLTRL helps deﬁne ML-speciﬁc testing considerations (levels 5 and 7) to help surface these failure-modes\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL deals with diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL also facilitates efficient validation of causal models by enabling early specification of the evaluation metric in Level 2, recognizing the unique challenges in evaluating causal models. The framework has been applied in projects like the CAMS system, where it enabled the automation of identifying meteors in astronomical images.\n",
      "\n",
      "**Additionally, MLTRL highlights the importance of data-oriented architectures (DOAs) in ML development.** DOAs make data access and management more explicit and accessible, addressing unique challenges in ML logging and governance. The framework emphasizes the significance of data readiness and availability throughout the development process.\n",
      "\n",
      "Beyond these core aspects, MLTRL acknowledges the distinct challenges associated with ML systems, including data-driven behavior and unique failure modes. It proposes specific testing considerations to surface potential issues and ensure reliable deployment.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "early. ML opens up new threat vectors across the whole deployment workﬂow that otherwise aren’t risks in software\n",
      "systems: for example, a poisoning attack to contaminate the training phase of ML systems, or membership inference\n",
      "to see if a given data record was part of the model’s training. MLTRL consider these threat vectors and suggests\n",
      "relevant risk-identiﬁcation during prototyping and productization phases. More generally, ML codebases have all the\n",
      "problems for regular code, plus ML-speciﬁc issues at the system level, mainly as a consequence of added complexity\n",
      "and dynamism. The resulting entanglement, for instance, implies that the SWE practice of making isolated changes is\n",
      "often not feasible – Scully et al.[ 57] refer to this as the “changing anything changes everything” principle. Given this\n",
      "consideration, typical SWE change-management is insufﬁcient. Furthermore, ML systems almost necessarily increase\n",
      "the technical debt; package-level refactoring is generally sufﬁcient for removing technical debt in software systems, but\n",
      "this is not the case in ML systems.\n",
      "These factors and others suggest that inherited software engineering and management practices of a given domain are\n",
      "insufﬁcient for the successful development of robust and reliable ML systems. But it is not trading off one for the other:\n",
      "MLTRL can be used in synergy with the existing, industry-standard software engineering practices such as agile [ 58]\n",
      "and waterfall [ 59] to handle unique challenges of ML development. Because ML applications are a category of software,\n",
      "all best practices of building and operating software should be extended when possible to the ML application. Practices\n",
      "like version control, comprehensive testing, continuous integration and continuous deployment are all applicable to ML\n",
      "development. MLTRL provides a framework that helps extend SWE building and operating practices that are acceptable\n",
      "in a given domain to tackle the unique challenges of ML development.\n",
      "RELATED WORKS\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL also facilitates efficient validation of causal models by enabling early specification of the evaluation metric in Level 2, recognizing the unique challenges in evaluating causal models. The framework highlights the importance of data-oriented architectures (DOAs) in ML development, emphasizing the significance of data readiness and availability throughout the process.\n",
      "\n",
      "Beyond these core aspects, MLTRL acknowledges the distinct challenges associated with ML systems, including data-driven behavior and unique failure modes. It proposes specific testing considerations to surface potential issues and ensure reliable deployment. The framework emphasizes the need for adapting traditional software engineering practices to the unique characteristics of ML development, leveraging methodologies like agile and waterfall approaches alongside established practices like version control, comprehensive testing, continuous integration, and continuous deployment.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "in a given domain to tackle the unique challenges of ML development.\n",
      "RELATED WORKS\n",
      "A recent case study from Microsoft Research [ 40] similarly identiﬁes a few themes describing how ML is not equal to\n",
      "software engineering, and recommends a linear ML workﬂow with steps for data preparation through modeling and\n",
      "deploying. They deﬁne an effective workﬂow for isolated development of an ML model, but this approach does not\n",
      "ensure the technology is actually improving in quality and robustness. Their process should be repeated at progressive\n",
      "stages of development in the broader ML and data technology lifecycle. If applied in the MLTRL framework, the\n",
      "speciﬁc ingredients of the ML model workﬂow – that is, people, software, tests, objectives, etc. – evolve over time and\n",
      "subsequent stages as the technologies mature.\n",
      "There exist many recommended workﬂows for speciﬁc ML methods and areas of pipelines. For instance, a more\n",
      "iterative process for Bayesian ML [ 60] and even more speciﬁcally for probabilistic programming [ 39], a data mining\n",
      "process deﬁned in 2000 that remains widely used [ 61], others for describing data iterations [ 62], and human-computer\n",
      "interaction cycles [ 63]. In these recommended workﬂows and others, there’s an important distinction between their\n",
      "cycles and “switchback” mechanisms in MLTRL. Their cycles suggest to generically iterate over a data-modeling-\n",
      "evaluation-deployment process. Switchbacks, on the other hand, are speciﬁc, purpose-driven workﬂows for dialing\n",
      "part(s) of a project to an earlier stage – this doesn’t simply mean go back and train the model on more data, but rather\n",
      "switching back regresses the technology’s maturity level (e.g. from level 5 to level 3) such that it must again fulﬁll the\n",
      "level-by-level requirements, evaluations and reviews. See the Methods section for more details on MLTRL switchbacks.\n",
      "18\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL also facilitates efficient validation of causal models by enabling early specification of the evaluation metric in Level 2, recognizing the unique challenges in evaluating causal models. The framework highlights the importance of data-oriented architectures (DOAs) in ML development, emphasizing the significance of data readiness and availability throughout the process.\n",
      "\n",
      "Beyond these core aspects, MLTRL acknowledges the distinct challenges associated with ML systems, including data-driven behavior and unique failure modes. It proposes specific testing considerations to surface potential issues and ensure reliable deployment. The framework emphasizes the need for adapting traditional software engineering practices to the unique characteristics of ML development, leveraging methodologies like agile and waterfall approaches alongside established practices like version control, comprehensive testing, continuous integration, and continuous deployment.\n",
      "\n",
      "**The provided context suggests that MLTRL can be applied in a given domain by iteratively refining its workﬂow over time.** This iterative process involves revisiting earlier stages of development if necessary, rather than simply adding more training data. This approach ensures that the ML model workﬂow continuously improves in quality and robustness.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "In general, iteration is an important part of data, ML, and software processes. MLTRL is unique from the other\n",
      "recommended processes in many ways, and perhaps most importantly because it considers data ﬂows and ML models\n",
      "in the context of larger systems. These isolated processes (that are speciﬁc to e.g. modeling in prototype development\n",
      "or data wrangling in application development) are synergistic with MLTRL because they can be used within each level\n",
      "of the larger lifecycle or framework. For example, the Bayesian modeling processes [ 39,60] we mentioned above\n",
      "are really useful to guide developers of probabilistic ML approaches. But there are important distinctions between\n",
      "executing these modeling steps and cycles in a well-deﬁned prototyping environment with curated data and minimal\n",
      "responsibilities, versus a production environment riddled with sparse and noisy data, that interacts with the physical\n",
      "world in non-obvious ways, and can carry expensive (even hidden) consequences. MLTRL provides the necessary,\n",
      "holistic context and structure to use these and other development processes reliably and responsibly.\n",
      "Also related to our work, Google teams have proposed ML testing recommendations [ 20] and validating the data fed\n",
      "into ML systems [ 64]. For NLP applications, typical ML testing practices struggle to translate to real-world settings,\n",
      "often overestimating performance capabilities. An effective way to address this is devising a checklist of linguistic\n",
      "capabilities and test types, as in Ribeiro et al.[ 17]–interestingly their test suite was inspired by metamorphic testing,\n",
      "which we suggested earlier in Level 7 for testing systems AI integrations. A survey by Paleyes et al. [ 48] go over\n",
      "numerous case studies to discuss challenges in ML deployment. They similarly pay special attention to the need for\n",
      "ethical considerations, end-user trust, and extra security in ML deployments. On the latter point, Kumar et al. [ 65]\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL also facilitates efficient validation of causal models by enabling early specification of the evaluation metric in Level 2, recognizing the unique challenges in evaluating causal models. The framework highlights the importance of data-oriented architectures (DOAs) in ML development, emphasizing the significance of data readiness and availability throughout the process.\n",
      "\n",
      "Beyond these core aspects, MLTRL acknowledges the distinct challenges associated with ML systems, including data-driven behavior and unique failure modes. It proposes specific testing considerations to surface potential issues and ensure reliable deployment. The framework emphasizes the need for adapting traditional software engineering practices to the unique characteristics of ML development, leveraging methodologies like agile and waterfall approaches alongside established practices like version control, comprehensive testing, continuous integration, and continuous deployment.\n",
      "\n",
      "**MLTRL promotes an iterative approach to ML development, allowing refinement of the workflow over time. This iterative process involves revisiting earlier stages of development if necessary, ensuring continuous improvement in quality and robustness.** The framework's holistic approach considers data flows and ML models within larger systems, addressing the challenges of deploying ML models in real-world environments with diverse data and complex interactions.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "provide a table thoroughly breaking down new threat vectors across the whole ML deployment workﬂow (some of\n",
      "which we mentioned above). These works, notably the ML security measures and the quantiﬁcation of an ML test suite\n",
      "in a principled way – i.e., that does not use misguided heuristics such as code coverage – are valuable to include in any\n",
      "ML workﬂow including MLTRL, and are synergistic with the framework we’ve described in this paper. These analyses\n",
      "provide useful insights, but they do not provide a holistic, regimented process for the full ML lifecycle from R&D\n",
      "through deployment. An end-to-end approach is suggested by Raji et al.[ 66], but only for the speciﬁc task of auditing\n",
      "algorithms; components of AI auditing are mentioned in Level 7, and covered throughout in the review processes.\n",
      "Sculley et al.[ 57] go into more ML debt topics such as undeclared consumers and data dependencies, and go on to\n",
      "recommend an ML Testing Rubric as a production checklist [ 20]. For example, testing models by a canary process\n",
      "before serving them into production. This, along with similar shadow testing we mentioned earlier, are common in\n",
      "autonomous ML systems, notably robotics and autonomous vehicles. They explicitly call out tests in four main areas\n",
      "(ML infrastructure, model development, features and data, and monitoring of running ML systems), some of which we\n",
      "discussed earlier. For example, tests that the training and serving features compute the same values; a model may train\n",
      "on logged processes or user input, but is then served on a live feed with different inputs. In addition to the Google ML\n",
      "Testing Rubric, we advocate metamorphic testing : a SWE methodology for testing a speciﬁc set of relations between\n",
      "the outputs of multiple inputs. True to the checklists in the Google ML Testing Rubric and in MLTRL, metamorphic\n",
      "testing for ML can have a codiﬁed list of metamorphic relations[18].\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL also facilitates efficient validation of causal models by enabling early specification of the evaluation metric in Level 2, recognizing the unique challenges in evaluating causal models. The framework highlights the importance of data-oriented architectures (DOAs) in ML development, emphasizing the significance of data readiness and availability throughout the process.\n",
      "\n",
      "Beyond these core aspects, MLTRL acknowledges the distinct challenges associated with ML systems, including data-driven behavior and unique failure modes. It proposes specific testing considerations to surface potential issues and ensure reliable deployment. The framework emphasizes the need for adapting traditional software engineering practices to the unique characteristics of ML development, leveraging methodologies like agile and waterfall approaches alongside established practices like version control, comprehensive testing, continuous integration, and continuous deployment.\n",
      "\n",
      "**MLTRL promotes an iterative approach to ML development, allowing refinement of the workflow over time. This iterative process involves revisiting earlier stages of development if necessary, ensuring continuous improvement in quality and robustness.** The framework's holistic approach considers data flows and ML models within larger systems, addressing the challenges of deploying ML models in real-world environments with diverse data and complex interactions.\n",
      "\n",
      "**Additional Considerations:**\n",
      "\n",
      "The MLTRL framework is complemented by complementary approaches for assessing ML security measures and quantifying test suites in a principled manner. These analyses offer valuable insights, but require a holistic, end-to-end approach covering the entire ML lifecycle, from research and development through deployment. Existing checklists and rubrics can aid in production deployment, including testing infrastructure, model development, feature engineering, and monitoring.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "testing for ML can have a codiﬁed list of metamorphic relations[18].\n",
      "In domains such as healthcare there have been the introduction of similar checklists for data readiness – for example,\n",
      "to ensure regulatory-grade real-world-evidence (RWE) data quality [ 67] – yet these are nascent and not yet widely\n",
      "accepted. Applying AI in healthcare has led to developing guidance for regulatory protocol, which is still a work in\n",
      "progress. Larson et al.[ 68] provide a comprehensive analysis for medical imaging and AI, arriving at several regulatory\n",
      "framework recommendations that mirror what we outline as important measures in MLTRL: e.g., detailed task elements\n",
      "such as pitfalls and limitations (surfaced on TRL Cards), clear deﬁnition of an algorithm relative to the downstream\n",
      "task, deﬁning the algorithm “capability” (Level 5), real-world monitoring, and more.\n",
      "D’amour et al.[ 19] dive into the problem we noted earlier about model miscalibration. They point to the trend in machine\n",
      "learning to develop models relatively isolated from the downstream use and larger system, resulting in underspeciﬁcation\n",
      "that handicaps practical ML pipelines. This is largely problematic in deep learning pipelines, but we’ve also noted this\n",
      "risk in the case of causal inference applications. Suggested remedies include stress tests –empirical evaluations that\n",
      "probe the model’s inductive biases on practically relevant dimensions–and in general the methods we deﬁne in Level 7.\n",
      "LIMITATIONS, RESPONSIBILITIES, and ETHICS\n",
      "MLTRL has been developed, deployed, iterated, and validated in myriad environments, as demonstrated by the previous\n",
      "examples and many others. Nonetheless we strongly suggest that MLTRL not be viewed as a cure-all for machine\n",
      "learning systems engineering. Rather, MLTRL provides mechanisms to better enable ML practitioners, teams, and\n",
      "stakeholders to be diligent and responsible with these technologies and data. That is, one cannot implement MLTRL in\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL also facilitates efficient validation of causal models by enabling early specification of the evaluation metric in Level 2, recognizing the unique challenges in evaluating causal models. The framework highlights the importance of data-oriented architectures (DOAs) in ML development, emphasizing the significance of data readiness and availability throughout the process.\n",
      "\n",
      "Beyond these core aspects, MLTRL acknowledges the distinct challenges associated with ML systems, including data-driven behavior and unique failure modes. It proposes specific testing considerations to surface potential issues and ensure reliable deployment. The framework emphasizes the need for adapting traditional software engineering practices to the unique characteristics of ML development, leveraging methodologies like agile and waterfall approaches alongside established practices like version control, comprehensive testing, continuous integration, and continuous deployment.\n",
      "\n",
      "**MLTRL promotes an iterative approach to ML development, allowing refinement of the workflow over time. This iterative process involves revisiting earlier stages of development if necessary, ensuring continuous improvement in quality and robustness.** The framework's holistic approach considers data flows and ML models within larger systems, addressing the challenges of deploying ML models in real-world environments with diverse data and complex interactions.\n",
      "\n",
      "**LIMITATIONS, RESPONSIBILITIES, and ETHICS**\n",
      "\n",
      "MLTRL provides mechanisms to better enable ML practitioners, teams, and stakeholders to be diligent and responsible with these technologies and data. It emphasizes continuous improvement, iterative development, and careful consideration of ethical considerations, aligning with industry best practices and regulatory recommendations.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "an organization and turn a blind eye to the many data, ML, and integration challenges we’ve discussed here. MLTRL is\n",
      "analogous to a pilot’s checklist, not autopilot.\n",
      "19\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL also facilitates efficient validation of causal models by enabling early specification of the evaluation metric in Level 2, recognizing the unique challenges in evaluating causal models. It highlights the importance of data-oriented architectures (DOAs) in ML development, emphasizing the significance of data readiness and availability throughout the process.\n",
      "\n",
      "Beyond these core aspects, MLTRL acknowledges the distinct challenges associated with ML systems, including data-driven behavior and unique failure modes. It proposes specific testing considerations to surface potential issues and ensure reliable deployment. The framework emphasizes the need for adapting traditional software engineering practices to the unique characteristics of ML development, leveraging methodologies like agile and waterfall approaches alongside established practices like version control, comprehensive testing, continuous integration, and continuous deployment.\n",
      "\n",
      "**MLTRL promotes an iterative approach to ML development, allowing refinement of the workflow over time. This iterative process involves revisiting earlier stages of development if necessary, ensuring continuous improvement in quality and robustness.** The framework's holistic approach considers data flows and ML models within larger systems, addressing the challenges of deploying ML models in real-world environments with diverse data and complex interactions.\n",
      "\n",
      "**MLTRL is analogous to a pilot's checklist, not autopilot. Organizations must actively address the data, ML, and integration challenges associated with ML deployment, rather than blindly relying on automated solutions.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "MLTRL is intended to be complimentary to existing software development methodologies, not replace or alter them.\n",
      "Speciﬁcally, whether the team uses agile or waterfall methods, MLTRL can be adopted to help deﬁne and structure\n",
      "phases of the project, as well as the success criteria of each stage. In context of the software development process, the\n",
      "purpose of MLTRL is to help the team minimize the technical dept and risk associated with the delivery of an ML\n",
      "application by helping the development team ask necessary questions.\n",
      "We discussed many data challenges and approaches in the context of MLTRL, and should highlight again the importance\n",
      "of data considerations in any ML initiative. The data availability and quality can severely limit the ability to develop and\n",
      "deploy ML, whether MLTRL is used or not. It is again the responsibility of the ML practitioners, teams, and stakeholders\n",
      "to gather, use, and distribute data in safe, legal, ethical ways. MLTRL helps do so with rigor and transparency, but\n",
      "again is not a solution for data bias. We recommend these recent works on data bias in ML: [ 69,70,71,72,73].\n",
      "Further, AI/ML ethics is a continuously evolving, multidisciplinary space – see [ 5]. MLTRL aims to prioritize ethics\n",
      "considerations at each level of the framework, and would do well to also evolve over time with the broader AI/ML\n",
      "ethics developments.\n",
      "CONCLUSION\n",
      "We’ve described Machine Learning Technology Readiness Levels (MLTRL) , an industry-hardened systems engineering\n",
      "framework for robust, reliable, and responsible machine learning. MLTRL is derived from the processes and testing\n",
      "standards of spacecraft development, yet lean and efﬁcient for ML, data, and software workﬂows. Examples from\n",
      "several organizations across industries demonstrate the efﬁcacy of MLTRL for AI and ML technologies, from research\n",
      "and development through productization and deployment, in important domains such as healthcare and physics, with\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenge of ensuring domain expertise and mitigating biases in causal inference models. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL is intended to be complimentary to existing software development methodologies, not replace or alter them. It can be adopted to help define and structure phases of the project, as well as the success criteria of each stage. The framework emphasizes the importance of data quality and ethics, recommending best practices for safe, legal, and ethical data gathering, use, and distribution.\n",
      "\n",
      "MLTRL is derived from the processes and testing standards of spacecraft development, yet lean and efficient for ML, data, and software workflows. Examples from several organizations across industries demonstrate the efficacy of MLTRL for AI and ML technologies, from research and development through productization and deployment.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "emphasis on data readiness amongst other critical challenges. Our aim is MLTRL works in synergy with recent\n",
      "approaches in the community focused on diligent data-readiness, privacy and security, and ethics. Even more, MLTRL\n",
      "establishes a much-needed lingua franca for the AI ecosystem, and broadly for AI in the worlds of science, engineering,\n",
      "and business. Our hope is that our systems framework is adopted broadly in AI and ML organizations, and that\n",
      "“technology readiness levels” becomes common nomenclature across AI stakeholders – from researchers and engineers\n",
      "to sales-people and executive decision-makers.\n",
      "Methods\n",
      "Gated reviews\n",
      "At the end of each stage is a dedicated review period: (1) Present the technical developments along with the requirements\n",
      "and their corresponding veriﬁcation measures and validation steps, (2) make key decisions on path(s) forward (or\n",
      "backward) and timing, and (3) debrief the processx. As in the gated reviews deﬁned by TRL used by NASA, DARPA, et\n",
      "al., MLTRL stipulates speciﬁc criteria for review at each level, as well as calling out speciﬁc key decision points (noted\n",
      "in the level descriptions above). The designated reviewers will “graduate” the technology to the next level, or provide a\n",
      "list of speciﬁc tasks that are still needed (ideally with quantitative remarks). After graduation at each level, the working\n",
      "group does a brief post-mortem; we ﬁnd that a quick day or two pays dividends in cutting away technical debt and\n",
      "improving team processes. Regular gated reviews are essential for making efﬁcient progress while ensuring robustness\n",
      "and functionality that meets stakeholder needs. There are several important mechanisms in MLTRL reviews that are\n",
      "speciﬁcally useful with AI and ML technologies: First, the review panels evolve over a project lifecycle, as noted\n",
      "below. Second, MLTRL prescribes that each review runs through an AI ethics checklist deﬁned by the organization; it is\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL is derived from the processes and testing standards of spacecraft development, yet lean and efficient for ML, data, and software workflows. It can be adopted to help define and structure phases of the project, as well as the success criteria of each stage. The framework emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data gathering, use, and distribution.\n",
      "\n",
      "MLTRL establishes a lingua franca for the AI ecosystem, fostering collaboration and communication across teams. Its systematic review process, inspired by established practices in organizations like NASA and DARPA, ensures continuous improvement, robustness, and functionality that meets stakeholder needs.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "important to repeat this at each review, as the review panel and stakeholders evolve considerably over a project lifecycle.\n",
      "As previously described in the levels deﬁnitions, including ethics reviews as an integral part of early system development\n",
      "is essential for informing model speciﬁcations and avoiding unintended biases or harm[74] after deployment.\n",
      "TRL “Cards”\n",
      "In Figure 3 we succinctly showcase a key deliverable: TRL Cards . The model cards proposed by Google [ 75] are a useful\n",
      "development for external user-readiness with ML. On the other hand, our TRL Cards aim to be more information-dense,\n",
      "like datasheets for medical devices and engineering tools – see the open-source TRL Card repo for examples and\n",
      "templates (to be released at github.com/alan-turing-institute). These serve as “report cards” that grow and improve upon\n",
      "graduating levels, and provide a means of inter-team and cross-functional communication. The content of a TRL Card\n",
      "is roughly in two categories: project info, and implicit knowledge. The former clearly states info such as project owners\n",
      "xMLTRL should include regular debriefs and meta-evaluations such that process improvements can be made in a data-driven,\n",
      "efﬁcient way (rather than an annual meta-review). MLTRL is a high-level framework that each organization should operationalize in\n",
      "a way that suits their speciﬁc capabilities and resources.\n",
      "20\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data gathering, use, and distribution. It establishes a lingua franca for the AI ecosystem, fostering collaboration and communication across teams. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "The framework is designed to be adaptable and customizable, allowing organizations to tailor it to their specific capabilities and resources.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "and reviewers, development status, and semantic versioning–not just for code, also for models and data. In the latter\n",
      "category are speciﬁc insights that are typically siloed in the ML development team but should be communicated to\n",
      "other stakeholders: modeling assumptions, dataset biases, corner cases, etc. With the spread of AI and ML in critical\n",
      "application areas, we are seeing domain expert consortiums deﬁning AI reporting guidelines – e.g., Rivera et al.[ 76]\n",
      "calling for clinical trials reports for interventions involving AI – which will greatly beneﬁt from the use of our TRL\n",
      "reporting cards. We stress that these TRL Cards are key for the progression of projects, rather than documentation\n",
      "afterthoughts. The TRL Cards thus promote transparency and trust, within teams and across organizations. TRL Card\n",
      "templates will be open-sourced upon publication of this work, including methods for coordinating use with other\n",
      "reporting tools such as “Datasheets for Datasets” [24].\n",
      "Risk mitigation\n",
      "Identifying and addressing risks in a software project is not a new practice. However, akin to the MLTRL roots in\n",
      "spacecraft engineering, risk is a “ﬁrst-class citizen” here. In the deﬁnition of technical and product requirements, each\n",
      "entry has a calculation of the form risk =p(failure )×value , where the value of a component is an integer 1−10.\n",
      "Being diligent about quantifying risks across the technical requirements is a useful mechanism for ﬂagging ML-related\n",
      "vulnerabilities that can sometimes be hidden by layers of other software. MLTRL also speciﬁes that risk quantiﬁcation\n",
      "and testing strategies are required for sim-to-real development. That is, there is nearly always a non-trivial gap in\n",
      "transferring a model or algorithm from a simulation testbed to the real world. Requiring explicit sim-to-real testing\n",
      "steps in the workﬂow helps mitigate unforeseen (and often hazardous) failures. Additionally, comprehensive ML test\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing specific examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data gathering, use, and distribution. It establishes a lingua franca for the AI ecosystem, fostering collaboration and communication across teams. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "The framework is designed to be adaptable and customizable, allowing organizations to tailor it to their specific capabilities and resources. MLTRL also highlights the importance of risk mitigation, quantifying risks across technical requirements and requiring explicit sim-to-real testing steps to address the gap between simulations and real-world deployment.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "coverage that we mention throughout this paper is a critical strategy for mitigating risks anduncertainties: ML-based\n",
      "system behavior is not easily speciﬁed in advance, but rather depends on dynamic qualities of the data and on various\n",
      "model conﬁguration choices[20].\n",
      "Non-monotonic, non-linear paths\n",
      "We observe many projects beneﬁt from cyclic paths, dialing components of a technology back to a lower level. Our\n",
      "framework not only encourages cycles, we make them explicit with “switchback mechanisms” to regress the maturity\n",
      "of speciﬁc components in an AI system:\n",
      "1.Discovery switchbacks occur as a natural mechanism – new technical gaps are discovered through systems\n",
      "integration, sparking later rounds of component development[ 77]. These are most common in the R&D levels,\n",
      "for example moving a component of a proof-of-concept technology (at Level 4) back to proof-of-principle\n",
      "development (Level 2).\n",
      "2.Review switchbacks result from gated reviews, where speciﬁc components or larger subsystems may be dialed\n",
      "back to earlier levels. This switchback is one of the “key decision points” in the MLTRL project lifecycle\n",
      "(as noted in the Levels deﬁnitions), and is often a decision driven by business-needs and timing rather than\n",
      "technical concerns (for instance when mission priorities and funds shift). This mechanism is common from\n",
      "Level 6/7 to 4, which stresses the importance of this R&D to product transition phase (see Figure 2 (left)).\n",
      "3.Embedded switchbacks are predeﬁned in the MLTRL process. For example, a predeﬁned path from 4 to 2, and\n",
      "from 9 to 4. In complex systems, particularly with AI technologies, these built-in loops help mitigate technical\n",
      "debt and overcome other inefﬁciencies such as noncomprehensive V&V steps.\n",
      "Without these built-in mechanisms for cyclic development paths, it can be difﬁcult and inefﬁcient to build systems of\n",
      "modules and components at varying degrees of maturity. Contrary to traditional thought that switchback events should\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles. MLTRL addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams.\n",
      "\n",
      "The framework explicitly addresses causal inference, recognizing its importance in real-world applications. It promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis. MLTRL ensures proper documentation of these aspects, mitigating risks encountered in other AI projects.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data gathering, use, and distribution. It establishes a lingua franca for the AI ecosystem, fostering collaboration and communication across teams. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "The framework explicitly incorporates coverage as a critical strategy for mitigating risks and uncertainties associated with machine learning systems. It suggests specific mechanisms for \"switchback\" development paths, allowing for iterative refinement of models and components based on data-driven insights and project requirements.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "be suppressed and minimized, in fact they represent a natural and necessary part of the complex technology development\n",
      "process – efforts to eliminate them may stiﬂe important innovations without necessarily improving efﬁciency. This is\n",
      "a fault of the standard monotonic approaches in AI/ML projects, stage-gate processes, and even the traditional TRL\n",
      "framework.\n",
      "It is also important to note that most projects do not start at Level 0; very few ML companies engage in this low-level\n",
      "theoretical research. For example, a team looking to use an off-the-shelf object recognition model could start that\n",
      "technology at Level 3, and proceed with thorough V&V for their speciﬁc datasets and use-cases. However, no technology\n",
      "can skip levels after the MLTRL process has been initiated. The industry default (that is, without implementing MLTRL)\n",
      "is to ignorantly take pretrained models, run ﬁne tuning on their speciﬁc data, and jump to deployment, effectively\n",
      "skipping Levels 5 to 7. Additionally, we ﬁnd it is advantageous to incorporate components from other high-TRL ranking\n",
      "projects while starting new projects; MLTRL makes the veriﬁcation and validation (V&V) steps straightforward for\n",
      "integrating previously developed ML components.\n",
      "21\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, borrowing inspiration from spacecraft engineering principles.\n",
      "\n",
      "MLTRL tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. The framework promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "MLTRL also addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It establishes a lingua franca for the AI ecosystem, fostering collaboration and communication across teams. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "The framework acknowledges the inherent complexities of technology development and encourages a balanced approach to risk mitigation. It suggests mechanisms for iterative refinement of models and components based on data-driven insights and project requirements. MLTRL also promotes the reuse of validated components from other high-TRL ranking projects, simplifying the verification and validation process for new projects.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Evolving people, objectives, and measures\n",
      "As suggested earlier, much of the practical value of MLTRL comes at the transition between levels. More precisely,\n",
      "MLTRL manages these oft neglected transitions explicitly as evolving teams, objectives, and deliverables. For instance,\n",
      "the team (or working group) at Level 3 is mostly AI Research Engineers, but at Level 6 is mixed Applied AI/SW\n",
      "Engineers mixed with product managers and designers. Similarly, the review panels evolve from level to level, to match\n",
      "the changing technology development objectives. What the reviewers reference similarly evolves: notice in the level\n",
      "deﬁnitions that technical requirements and V&V guide early stages, but at and after Level 6 the product requirements\n",
      "and V&V takeover – naturally, the risk quantiﬁcation and mitigation strategies evolve in parallel. Regarding the\n",
      "deliverables, notably TRL Cards and risk matrices[ 22] (to rank and prioritize various science, technical, and project\n",
      "risks), the information develops and evolves over time as the technology matures.\n",
      "Quantiﬁable progress\n",
      "By deﬁning technology maturity in a quantitative way, MLTRL enables teams to accurately and consistently deﬁne\n",
      "their ML progress metrics. Notably industry-standard “objectives and key results” (OKRs) and “key performance\n",
      "indicators” (KPIs) [ 78] can be deﬁned as achieving certain readiness levels in a given period of time; this is a preferable\n",
      "metric in essentially all ML systems which consist of much more than a single performance score to measure progress.\n",
      "Even more, meta-review of MLTRL progress over multiple projects can provide useful insights at the organization\n",
      "level. For example, analysis of the time-per-level and the most frequent development paths/cycles can bring to light\n",
      "operational bottlenecks. Compared to conventional software engineering metrics based on sprint stories and tickets, or\n",
      "time-tracking tools, MLTRL provides a more accurate analysis of ML workﬂows.\n",
      "Communication and explanation\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles.\n",
      "\n",
      "MLTRL tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. The framework promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "MLTRL also addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It establishes a lingua franca for the AI ecosystem, fostering collaboration and communication across teams. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "The framework acknowledges the inherent complexities of technology development and encourages a balanced approach to risk mitigation. It suggests mechanisms for iterative refinement of models and components based on data-driven insights and project requirements. MLTRL also promotes the reuse of validated components from other high-TRL ranking projects, simplifying the verification and validation process for new projects.\n",
      "\n",
      "**Additionally, MLTRL explicitly addresses the evolution of teams, objectives, and deliverables across different stages of development.** It defines ML progress metrics using industry-standard \"objectives and key results\" (OKRs) and \"key performance indicators\" (KPIs), providing a more accurate analysis of ML workflows compared to conventional software engineering metrics.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Communication and explanation\n",
      "A distinct advantage of MLTRL in practice is the nomenclature: an agreed upon grading scheme for the maturity of\n",
      "an AI technology, and a framework for how/when that technology ﬁts within a product or system, enables everyone\n",
      "to communicate effectively and transparently. MLTRL also acts as a gate for interpretability and explainability–at\n",
      "the granularity of individual models and algorithms, and more crucially from a holistic, systems standpoint. Notably\n",
      "the DARPA XAIxiprogram advocates for this advance in developing AI technologies; they suggest interpretability\n",
      "and explainability are necessary at various locations in an AI system to be sufﬁcient for deployment as an AI product,\n",
      "otherwise leading to issues with ethics and bias.\n",
      "Robustness via uncertainty-aware ML\n",
      "How to design a reliable system from unreliable components has been a guiding question in the ﬁelds of computing and\n",
      "intelligence [79]. In the case of AI/ML systems, we aim to build reliable systems with myriad unreliable components:\n",
      "noisy and faulty sensors, human and AI error, and so on. There is thus signiﬁcant value to quantifying the myriad\n",
      "uncertainties, propagating them throughout a system, and arriving at a notion or measure of reliability. For this reason,\n",
      "although MLTRL applies generally to AI/ML methods and systems, we advocate for methods in the class of probabilistic\n",
      "ML, which naturally represent and manipulate uncertainty about models and predictions[ 28]. These are Bayesian\n",
      "methods that use probabilities to represent aleatoric uncertainty , measuring the noise inherent in the observations, and\n",
      "epistemic uncertainty , accounting for uncertainty in the model itself (i.e., capturing our ignorance about which model\n",
      "generated the data). In the simplest case, an uncertainty aware ML pipeline should quantify uncertainty at the points of\n",
      "sensor inputs or perception, prediction or model output, and decision or end-user action – McAllister et al.[ 29] suggest\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles.\n",
      "\n",
      "MLTRL addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. The framework promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "MLTRL also addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It establishes a lingua franca for the AI ecosystem, fostering collaboration and communication across teams. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "Moreover, MLTRL provides a clear and consistent nomenclature that enables effective communication and transparency across teams. It promotes the use of probabilistic ML methods to quantify and manage uncertainties, aligning with industry recommendations for reliable AI deployment.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "this with Bayesian deep learning models for safer autonomous vehicle pipelines. We can achieve this sufﬁciently well\n",
      "in practice for simple systems. However, we do not yet have a principled, theoretically grounded, and generalizable way\n",
      "of propagating errors and uncertainties downstream and throughout more complex AI systems – i.e., how to integrate\n",
      "different software, hardware, data, and human components while considering how errors and uncertainties propagate\n",
      "through the system. This is an important direction of our future work.\n",
      "References\n",
      "[1]Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning\n",
      "that matters. In AAAI , 2018.\n",
      "xiDARPA Explainable Artiﬁcial Intelligence (XAI)\n",
      "22\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. It emphasizes ethical considerations, risk assessment, and iterative development, drawing inspiration from spacecraft engineering principles.\n",
      "\n",
      "MLTRL addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. The framework promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "MLTRL also addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "Moreover, MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It establishes a lingua franca for the AI ecosystem, fostering collaboration and communication across teams. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "**Furthermore, MLTRL highlights the need for a principled approach to error propagation in complex AI systems, emphasizing the need to integrate different software, hardware, data, and human components while considering how errors and uncertainties cascade throughout the system.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "[2]Arnaud de la Tour, Massimo Portincaso, Kyle Blank, and Nicolas Goeldel. The dawn of the deep tech ecosystem. Technical\n",
      "report, The Boston Consulting Group, 2019.\n",
      "[3] NASA. The NASA systems engineering handbook. 2003.\n",
      "[4] United States Department of Defense. Defense acquisition guidebook. Technical report, U.S. Dept. of Defense, 2004.\n",
      "[5] D. Leslie. Understanding artiﬁcial intelligence ethics and safety. ArXiv , abs/1906.05684, 2019.\n",
      "[6]Google. Machine learning workﬂow. https://cloud.google.com/mlengine/docs/tensorflow/\n",
      "ml-solutions-overview . Accessed: 2020-12-13.\n",
      "[7]Alexander Lavin and Gregory Renard. Technology readiness levels for AI & ML. ICML Workshop on Challenges Deploying\n",
      "ML Systems , 2020.\n",
      "[8] T. Dasu and T. Johnson. Exploratory data mining and data cleaning. 2003.\n",
      "[9]M. Janssen, P. Brous, Elsa Estevez, L. Barbosa, and T. Janowski. Data governance: Organizing data for trustworthy artiﬁcial\n",
      "intelligence. Gov. Inf. Q. , 37:101493, 2020.\n",
      "[10] B. Shahriari, Kevin Swersky, Ziyu Wang, R. Adams, and N. D. Freitas. Taking the human out of the loop: A review of bayesian\n",
      "optimization. Proceedings of the IEEE , 104:148–175, 2016.\n",
      "[11] Goutham Ramakrishnan, A. Nori, Hannah Murfet, and Pashmina Cameron. Towards compliant data management systems for\n",
      "healthcare ml. ArXiv , abs/2011.07555, 2020.\n",
      "[12] Umang Bhatt, Alice Xiang, S. Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura,\n",
      "and P. Eckersley. Explainable machine learning in deployment. Proceedings of the 2020 Conference on Fairness, Accountability,\n",
      "and Transparency , 2020.\n",
      "[13] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and V . Smith. Federated learning: Challenges, methods, and future directions.\n",
      "IEEE Signal Processing Magazine , 37:50–60, 2020.\n",
      "[14] T. Ryffel, Andrew Trask, M. Dahl, Bobby Wagner, J. Mancuso, D. Rueckert, and J. Passerat-Palmbach. A generic framework\n",
      "for privacy preserving deep learning. ArXiv , abs/1811.04017, 2018.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering principles, MLTRL emphasizes ethical considerations, risk assessment, and iterative development.\n",
      "\n",
      "The framework tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. MLTRL promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It establishes a lingua franca for the AI ecosystem, fostering collaboration and communication across teams. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "**MLTRL highlights the need for a principled approach to error propagation in complex AI systems, emphasizing the need to integrate different software, hardware, data, and human components while considering how errors and uncertainties cascade throughout the system.** This aligns with broader discussions on system safety and explainability in ML models, referencing relevant publications on data governance, privacy-preserving techniques, and the need for explainable AI in deployment.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "for privacy preserving deep learning. ArXiv , abs/1811.04017, 2018.\n",
      "[15] A. Madry, Aleksandar Makelov, Ludwig Schmidt, D. Tsipras, and Adrian Vladu. Towards deep learning models resistant to\n",
      "adversarial attacks. ArXiv , abs/1706.06083, 2018.\n",
      "[16] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. ArXiv , abs/1710.11342, 2018.\n",
      "[17] Marco Túlio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models\n",
      "with checklist. In ACL, 2020.\n",
      "[18] Xiaoyuan Xie, Joshua W. K. Ho, C. Murphy, G. Kaiser, B. Xu, and T. Chen. Testing and validating machine learning classiﬁers\n",
      "by metamorphic testing. The Journal of systems and software , 84 4:544–558, 2011.\n",
      "[19] Alexander D’Amour, K. Heller, D. Moldovan, Ben Adlam, B. Alipanahi, Alex Beutel, C. Chen, Jonathan Deaton, Jacob\n",
      "Eisenstein, M. Hoffman, Farhad Hormozdiari, N. Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, M. Lucic,\n",
      "Y . Ma, Cory Y . McLean, Diana Mincu, Akinori Mitani, A. Montanari, Zachary Nado, V . Natarajan, C. Nielson, Thomas F.\n",
      "Osborne, R. Raman, K. Ramasamy, Rory Sayres, J. Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, V . Veitch,\n",
      "Max Vladymyrov, Xuezhi Wang, K. Webster, S. Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley. Underspeciﬁcation\n",
      "presents challenges for credibility in modern machine learning. ArXiv , abs/2011.03395, 2020.\n",
      "[20] Eric Breck, Shanqing Cai, E. Nielsen, M. Salib, and D. Sculley. The ml test score: A rubric for ml production readiness and\n",
      "technical debt reduction. 2017 IEEE International Conference on Big Data (Big Data) , pages 1123–1132, 2017.\n",
      "[21] A. Botchkarev. A new typology design of performance metrics to measure errors in machine learning regression algorithms.\n",
      "Interdisciplinary Journal of Information, Knowledge, and Management , 14:045–076, 2019.\n",
      "[22] N. Duijm. Recommendations on the use and design of risk matrices. Safety Science , 76:21–31, 2015.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering principles, MLTRL emphasizes ethical considerations, risk assessment, and iterative development.\n",
      "\n",
      "The framework addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. MLTRL promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It establishes a lingua franca for the AI ecosystem, fostering collaboration and communication across teams. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "**MLTRL highlights the need for a principled approach to error propagation in complex AI systems, emphasizing the need to integrate different software, hardware, data, and human components while considering how errors and uncertainties cascade throughout the system.** This aligns with broader discussions on system safety and explainability in ML models, referencing relevant publications on data governance, privacy-preserving techniques, and the need for explainable AI in deployment.\n",
      "\n",
      "**The provided context does not appear to be relevant to the existing summary, so it is not used to further refine the summary.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "[23] Louise Naud and Alexander Lavin. Manifolds for unsupervised visual anomaly detection. ArXiv , abs/2006.11364, 2020.\n",
      "[24] Timnit Gebru, J. Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, H. Wallach, Hal Daumé, and K. Crawford.\n",
      "Datasheets for datasets. ArXiv , abs/1803.09010, 2018.\n",
      "[25] B. Hutchinson, A. Smart, A. Hanna, Emily L. Denton, Christina Greer, Oddur Kjartansson, P. Barnes, and Margaret Mitchell.\n",
      "Towards accountability for machine learning datasets: Practices from software engineering and infrastructure. Proceedings of\n",
      "the 2021 ACM Conference on Fairness, Accountability, and Transparency , 2021.\n",
      "23\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering principles, MLTRL emphasizes ethical considerations, risk assessment, and iterative development.\n",
      "\n",
      "The framework addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. MLTRL promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It fosters collaboration and communication across teams by establishing a lingua franca for the AI ecosystem. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "**The provided context regarding research papers on anomaly detection, dataset accountability, and data governance is not directly relevant to the existing summary and is not used to further refine it.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "[26] P. Schulam and S. Saria. Reliable decision support using counterfactual models. In NIPS 2017 , 2017.\n",
      "[27] Towards trustable machine learning. Nature Biomedical Engineering , 2:709–710, 2018.\n",
      "[28] Zoubin Ghahramani. Probabilistic machine learning and artiﬁcial intelligence. Nature , 521:452–459, 2015.\n",
      "[29] Rowan McAllister, Yarin Gal, Alex Kendall, Mark van der Wilk, A. Shah, R. Cipolla, and Adrian Weller. Concrete problems\n",
      "for autonomous vehicle safety: Advantages of bayesian deep learning. In IJCAI , 2017.\n",
      "[30] Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I. Avilés-Rivero,\n",
      "Christian Etmann, Cathal McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, Effrossyni Gkrania-Klotsas,\n",
      "James H. F. Rudd, Evis Sala, and Carola-Bibiane Schönlieb. Common pitfalls and recommendations for using machine learning\n",
      "to detect and prognosticate for covid-19 using chest radiographs and ct scans. Nature Machine Intelligence , 3:199–217, 2021.\n",
      "[31] J. Tobin, Rachel H Fong, Alex Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep\n",
      "neural networks from simulation to the real world. 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems\n",
      "(IROS) , pages 23–30, 2017.\n",
      "[32] Arthur Juliani, Vincent-Pierre Berges, Esh Vckay, Yuan Gao, Hunter Henry, M. Mattar, and D. Lange. Unity: A general\n",
      "platform for intelligent agents. ArXiv , abs/1809.02627, 2018.\n",
      "[33] Stefan Hinterstoißer, Olivier Pauly, Tim Hauke Heibel, Martina Marek, and Martin Bokeloh. An annotation saved is an\n",
      "annotation earned: Using fully synthetic training for object instance detection. ArXiv , abs/1902.09967, 2019.\n",
      "[34] Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan Hogins, You-Cyuan Jhang, Mohsen Kamalzadeh,\n",
      "Bowen Li, Steven Leal, Pete Parisi, Cesar Romero, Wesley Smith, Alex Thaman, Samuel Warren, and Nupur Yadav. Unity\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering principles, MLTRL emphasizes ethical considerations, risk assessment, and iterative development.\n",
      "\n",
      "The framework tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. MLTRL promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It fosters collaboration and communication across teams by establishing a lingua franca for the AI ecosystem. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "**The provided context regarding research papers on anomaly detection, dataset accountability, and data governance is not directly relevant to the existing summary and is not used to further refine it.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "perception: Generate synthetic data for computer vision. CoRR , abs/2107.04259, 2021.\n",
      "[35] K. Cranmer, J. Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of the National Academy\n",
      "of Sciences , 117:30055 – 30062, 2020.\n",
      "[36] Jan-Willem van de Meent, Brooks Paige, H. Yang, and Frank Wood. An introduction to probabilistic programming. ArXiv ,\n",
      "abs/1809.10756, 2018.\n",
      "[37] Atilim Günes Baydin, Lei Shao, W. Bhimji, L. Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi,\n",
      "Bradley Gram-Hansen, Gilles Louppe, Mingfei Ma, X. Zhao, P. Torr, V . Lee, K. Cranmer, Prabhat, and F. Wood. Etalumis:\n",
      "bringing probabilistic programming to scientiﬁc simulators at scale. Proceedings of the International Conference for High\n",
      "Performance Computing, Networking, Storage and Analysis , 2019.\n",
      "[38] T. Gleisberg, S. Höche, F. Krauss, M. Schönherr, S. Schumann, F. Siegert, and J. Winter. Event generation with sherpa 1.1.\n",
      "Journal of High Energy Physics , 2009:007–007, 2009.\n",
      "[39] David M. Blei. Build, compute, critique, repeat: Data analysis with latent variable models. 2014.\n",
      "[40] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald C. Gall, Ece Kamar, Nachiappan Nagappan, Besmira\n",
      "Nushi, and Thomas Zimmermann. Software engineering for machine learning: A case study. 2019 IEEE/ACM 41st International\n",
      "Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) , 2019.\n",
      "[41] R. Ambrosino, B. Buchanan, G. Cooper, and Marvin J. Fine. The use of misclassiﬁcation costs to learn rule-based decision\n",
      "support models for cost-effective hospital admission strategies. Proceedings. Symposium on Computer Applications in Medical\n",
      "Care , pages 304–8, 1995.\n",
      "[42] Gareth J Grifﬁth, Tim T Morris, Matthew J Tudball, Annie Herbert, Giulia Mancano, Lindsey Pike, Gemma C Sharp, Jonathan\n",
      "Sterne, Tom M Palmer, George Davey Smith, et al. Collider bias undermines our understanding of covid-19 disease risk and\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering principles, MLTRL emphasizes ethical considerations, risk assessment, and iterative development.\n",
      "\n",
      "The framework tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. MLTRL promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It fosters collaboration and communication across teams by establishing a lingua franca for the AI ecosystem. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "**The provided context regarding anomaly detection, dataset accountability, and data governance is not directly relevant to the existing summary and is not used to further refine it.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "severity. Nature communications , 11(1):1–12, 2020.\n",
      "[43] J. Pearl. Theoretical impediments to machine learning with seven sparks from the causal revolution. Proceedings of the\n",
      "Eleventh ACM International Conference on Web Search and Data Mining , 2018.\n",
      "[44] T. Nguyen, G. Collins, J. Spence, J. Daurès, P. Devereaux, P. Landais, and Y . Le Manach. Double-adjustment in propensity\n",
      "score matching analysis: choosing a threshold for considering residual imbalance. BMC Medical Research Methodology , 17,\n",
      "2017.\n",
      "[45] D. Eckles and E. Bakshy. Bias and high-dimensional adjustment in observational studies of peer effects. ArXiv , abs/1706.04692,\n",
      "2017.\n",
      "[46] Yanbo Xu, Divyat Mahajan, Liz Manrao, A. Sharma, and E. Kiciman. Split-treatment analysis to rank heterogeneous causal\n",
      "effects for prospective interventions. ArXiv , abs/2011.05877, 2020.\n",
      "24\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering principles, MLTRL emphasizes ethical considerations, risk assessment, and iterative development.\n",
      "\n",
      "The framework tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. MLTRL promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It fosters collaboration and communication across teams by establishing a lingua franca for the AI ecosystem. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "**The provided context regarding anomaly detection, dataset accountability, and data governance is not directly relevant to the existing summary and is not used to further refine it.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "[47] Jonathan G Richens, C. M. Lee, and Saurabh Johri. Improving the accuracy of medical diagnosis with causal machine learning.\n",
      "Nature Communications , 11, 2020.\n",
      "[48] Andrei Paleyes, Raoul-Gabriel Urma, and N. Lawrence. Challenges in deploying machine learning: a survey of case studies.\n",
      "ArXiv , abs/2011.09926, 2020.\n",
      "[49] V . Chernozhukov, D. Chetverikov, M. Demirer, E. Duﬂo, Christian L. Hansen, Whitney K. Newey, and J. Robins. Dou-\n",
      "ble/debiased machine learning for treatment and structural parameters. Econometrics: Econometric & Statistical Methods -\n",
      "Special Topics eJournal , 2018.\n",
      "[50] Victor Veitch and Anisha Zaveri. Sense and sensitivity analysis: Simple post-hoc analysis of bias due to unobserved confounding.\n",
      "NeurIPS 2020, arXiv preprint arXiv:2003.01747 , 2020.\n",
      "[51] P. Jenniskens, P.S. Gural, L. Dynneson, B.J. Grigsby, K.E. Newman, M. Borden, M. Koop, and D. Holman. Cams: Cameras for\n",
      "allsky meteor surveillance to establish minor meteor showers. Icarus , 216(1):40 – 61, 2011.\n",
      "[52] Siddha Ganju, Anirudh Koul, Alexander Lavin, J. Veitch-Michaelis, Meher Kasam, and J. Parr. Learnings from frontier\n",
      "development lab and spaceml - ai accelerators for nasa and esa. ArXiv , abs/2011.04776, 2020.\n",
      "[53] S. Zoghbi, M. Cicco, A. P. Stapper, A. J. Ordonez, J. Collison, P. S. Gural, S. Ganju, J.-L. Galache, and P. Jenniskens. Searching\n",
      "for long-period comets with deep learning tools. In Deep Learning for Physical Science Workshop, NeurIPS , 2017.\n",
      "[54] Peter Jenniskens, Jack Baggaley, Ian Crumpton, Peter Aldous, Petr Pokorny, Diego Janches, Peter S. Gural, Dave Samuels, Jim\n",
      "Albers, Andreas Howell, Carl Johannink, Martin Breukers, Mohammad Odeh, Nicholas Moskovitz, Jack Collison, and Siddha\n",
      "Ganju. A survey of southern hemisphere meteor showers. Planetary and Space Science , 154:21 – 29, 2018.\n",
      "[55] D. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active learning with statistical models. In NIPS , 1994.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering principles, MLTRL emphasizes ethical considerations, risk assessment, and iterative development.\n",
      "\n",
      "The framework tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. MLTRL promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It fosters collaboration and communication across teams by establishing a lingua franca for the AI ecosystem. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "**The provided context regarding anomaly detection, dataset accountability, and data governance is not directly relevant to the existing summary and is not used to further refine it.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "[56] Y . Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. ArXiv , abs/1703.02910, 2017.\n",
      "[57] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young,\n",
      "Jean-François Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. In NIPS , 2015.\n",
      "[58] P. Abrahamsson, Outi Salo, Jussi Ronkainen, and Juhani Warsta. Agile software development methods: Review and analysis.\n",
      "ArXiv , abs/1709.08439, 2017.\n",
      "[59] Marco Kuhrmann, Philipp Diebold, Jürgen Münch, Paolo Tell, Vahid Garousi, Michael Felderer, Kitija Trektere, Fergal\n",
      "McCaffery, Oliver Linssen, Eckhart Hanser, and Christian R. Prause. Hybrid software and system development in practice:\n",
      "waterfall, scrum, and beyond. Proceedings of the 2017 International Conference on Software and System Process , 2017.\n",
      "[60] Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry,\n",
      "Paul-Christian Burkner, and Martin Modrak. Bayesian workﬂow. ArXiv , abs/2011.01808, 2020.\n",
      "[61] P. Chapman, J. Clinton, R. Kerber, T. Khabaza, T. Reinartz, C. Shearer, and R. Wirth. Crisp-dm 1.0: Step-by-step data mining\n",
      "guide. 2000.\n",
      "[62] Fred Hohman, Kanit Wongsuphasawat, Mary Beth Kery, and Kayur Patel. Understanding and visualizing data iteration in\n",
      "machine learning. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems , 2020.\n",
      "[63] Saleema Amershi, M. Cakmak, W. B. Knox, and T. Kulesza. Power to the people: The role of humans in interactive machine\n",
      "learning. AI Mag. , 35:105–120, 2014.\n",
      "[64] Eric Breck, Marty Zinkevich, Neoklis Polyzotis, Steven Euijong Whang, and Sudip Roy. Data validation for machine learning.\n",
      "2019.\n",
      "[65] R. Kumar, David R. O’Brien, Kendra Albert, Salomé Viljöen, and Jeffrey Snover. Failure modes in machine learning systems.\n",
      "ArXiv , abs/1911.11034, 2019.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework addresses the practical challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering principles, MLTRL emphasizes ethical considerations, risk assessment, and iterative development.\n",
      "\n",
      "The framework tackles diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. MLTRL promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It fosters collaboration and communication across teams by establishing a lingua franca for the AI ecosystem. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "**The provided context regarding anomaly detection, dataset accountability, and data governance is not directly relevant to the current summary and is not used to further refine it.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "ArXiv , abs/1911.11034, 2019.\n",
      "[66] Inioluwa Deborah Raji, Andrew Smart, Rebecca White, M. Mitchell, Timnit Gebru, B. Hutchinson, Jamila Smith-Loud, Daniel\n",
      "Theron, and P. Barnes. Closing the ai accountability gap: deﬁning an end-to-end framework for internal algorithmic auditing.\n",
      "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency , 2020.\n",
      "[67] R. Miksad and A. Abernethy. Harnessing the power of real-world evidence (rwe): A checklist to ensure regulatory-grade data\n",
      "quality. Clinical Pharmacology and Therapeutics , 103:202 – 205, 2018.\n",
      "[68] D. B. Larson, Hugh Harvey, D. Rubin, Neville Irani, J. R. Tse, and C. Langlotz. Regulatory frameworks for development and\n",
      "evaluation of artiﬁcial intelligence–based diagnostic imaging algorithms: Summary and recommendations. Journal of the\n",
      "American College of Radiology , 2020.\n",
      "25\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering principles, MLTRL emphasizes ethical considerations, risk assessment, and iterative development.\n",
      "\n",
      "The framework addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. MLTRL promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It fosters collaboration and communication across teams by establishing a lingua franca for the AI ecosystem. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "**The provided context regarding anomaly detection, dataset accountability, and data governance is not directly relevant to the current summary and is not used to further refine it.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "[69] Ninareh Mehrabi, Fred Morstatter, N. Saxena, Kristina Lerman, and A. Galstyan. A survey on bias and fairness in machine\n",
      "learning. ACM Computing Surveys (CSUR) , 54:1 – 35, 2019.\n",
      "[70] Eirini Ntoutsi, P. Fafalios, U. Gadiraju, Vasileios Iosiﬁdis, W. Nejdl, Maria-Esther Vidal, S. Ruggieri, F. Turini, S. Papadopoulos,\n",
      "Emmanouil Krasanakis, I. Kompatsiaris, K. Kinder-Kurlanda, Claudia Wagner, F. Karimi, Miriam Fernández, Harith Alani,\n",
      "B. Berendt, Tina Kruegel, C. Heinze, Klaus Broelemann, Gjergji Kasneci, T. Tiropanis, and Steffen Staab. Bias in data-driven\n",
      "ai systems - an introductory survey. ArXiv , abs/2001.09762, 2020.\n",
      "[71] E. Jo and Timnit Gebru. Lessons from archives: strategies for collecting sociocultural data in machine learning. Proceedings of\n",
      "the 2020 Conference on Fairness, Accountability, and Transparency , 2020.\n",
      "[72] J. Wiens, W. Price, and M. Sjoding. Diagnosing bias in data-driven algorithms for healthcare. Nature Medicine , 26:25–26,\n",
      "2020.\n",
      "[73] R. Challen, J. Denny, M. Pitt, L. Gompels, T. Edwards, and K. Tsaneva-Atanasova. Artiﬁcial intelligence, bias and clinical\n",
      "safety. BMJ Quality & Safety , 28:231 – 237, 2019.\n",
      "[74] Z. Obermeyer, B. Powers, C. V ogeli, and S. Mullainathan. Dissecting racial bias in an algorithm used to manage the health of\n",
      "populations. Science , 366:447 – 453, 2019.\n",
      "[75] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, In-\n",
      "ioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. Proceedings of the Conference on Fairness,\n",
      "Accountability, and Transparency , 2019.\n",
      "[76] Samantha Cruz Rivera, Xiaoxuan Liu, A. Chan, A. K. Denniston, and M. Calvert. Guidelines for clinical trial protocols for\n",
      "interventions involving artiﬁcial intelligence: the spirit-ai extension. Nature Medicine , 26:1351 – 1363, 2020.\n",
      "[77] Z. Szajnfarber. Managing innovation in architecturally hierarchical systems: Three switchback mechanisms that impact practice.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering principles, MLTRL emphasizes ethical considerations, risk assessment, and iterative development.\n",
      "\n",
      "The framework addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. MLTRL promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It fosters collaboration and communication across teams by establishing a lingua franca for the AI ecosystem. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "**The provided context regarding anomaly detection, dataset accountability, and data governance is not directly relevant to the current summary and is not used to further refine it.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "IEEE Transactions on Engineering Management , 61:633–645, 2014.\n",
      "[78] H. Zhou and Y . He. Comparative study of okr and kpi. DEStech Transactions on Economics, Business and Management , 2018.\n",
      "[79] J. Neumann. Probabilistic logic and the synthesis of reliable organisms from unreliable components. 1956.\n",
      "Acknowledgements\n",
      "The authors would like to thank Gur Kimchi, Carl Henrik Ek and Neil Lawrence for valuable discussions about this\n",
      "project.\n",
      "Author contributions statement\n",
      "A.L. conceived of the original ideas and framework, with signiﬁcant contributions towards improving the framework\n",
      "from all co-authors. A.L. initiated the use of MLTRL in practice, including the neuropathology test case discussed here.\n",
      "C.G-L. contributed insight regarding causal AI, including the section on counterfactual diagnosis. C.G-L. also made\n",
      "signiﬁcant contributions broadly in the paper, notably in the Methods descriptions and paper revisions. Si.G. contributed\n",
      "the spacecraft test case, along with early insights in the framework deﬁnitions. A.V . contributed to the deﬁnition of\n",
      "later stages involving deployment (as did A.G.), and comparison with traditional software workﬂows. Both E.X. and\n",
      "Y .G. provided insights regarding AI in academia, and Y .G. additionally contributed to the uncertainty quantiﬁcation\n",
      "methods. Su.G. and D.L. contributed the computer vision test case. A.G.B. contributed the particle physics test case,\n",
      "and signiﬁcant reviews of the writeup. A.S. contributed insights related to causal ML and AI ethics. D.N. provided\n",
      "valuable feedback on the overall framework, and contributed signiﬁcantly with the details on “switchback mechanisms”.\n",
      "S.Z. contributed to multiple paper revisions, with emphasis on clarity and applicability to broad ML users and teams.\n",
      "J.P. contributed to multiple paper revisions, and to deploying the systems ML methods broadly in practice for Earth and\n",
      "space sciences. –same goes for C.M., with additional feedback overall on the methods. All co-authors discussed the\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: ## Refined Summary:\n",
      "\n",
      "The Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering principles, MLTRL emphasizes ethical considerations, risk assessment, and iterative development.\n",
      "\n",
      "The framework addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. MLTRL promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\n",
      "\n",
      "Furthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\n",
      "\n",
      "MLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It fosters collaboration and communication across teams by establishing a lingua franca for the AI ecosystem. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\n",
      "\n",
      "**The provided context regarding anomaly detection, dataset accountability, and data governance is not directly relevant to the current summary and is not used to further refine it.**\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "content and contributed to editing the manuscript.\n",
      "Competing interests\n",
      "The authors declare no competing interests.\n",
      "Additional information\n",
      "Correspondence and requests for materials should be addressed to A.L.\n",
      "26\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Refined Summary:\\n\\nThe Machine Learning Technology Readiness Levels (MLTRL) framework tackles the practical challenges of deploying machine learning models in real-world applications. Inspired by spacecraft engineering principles, MLTRL emphasizes ethical considerations, risk assessment, and iterative development.\\n\\nThe framework addresses diverse data sources, quantifies uncertainties, and fosters collaboration between ML and engineering teams. It explicitly addresses causal inference, recognizing its importance in real-world applications. MLTRL promotes careful consideration of causal relationships, model identifiability, and bias adjustment, providing practical examples like causal computer-assisted diagnosis.\\n\\nFurthermore, MLTRL addresses the challenges of ensuring domain expertise, mitigating biases, and ensuring data readiness. It promotes iterative development, quantifying causal relationships from observational data, and achieving robust model deployment and maintenance. The framework includes specific measures to address potential bias and ensure model robustness, including sensitivity analysis and consistency checks.\\n\\nMLTRL emphasizes the importance of data quality, ethics, and data readiness, recommending best practices for safe, legal, and ethical data handling. It fosters collaboration and communication across teams by establishing a lingua franca for the AI ecosystem. Regular debriefs and meta-evaluations are also recommended for continuous improvement and efficiency.\\n\\n**The provided context regarding anomaly detection, dataset accountability, and data governance is not relevant to the current summary and is not used to further refine it.**'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain3 = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type = \"refine\",\n",
    "    verbose=True\n",
    ")\n",
    "output_summary = chain3.run(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('speech.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Malcolm X was one of the most dynamic, dramatic and influential figures of the civil rights era. He was an apostle of black nationalism, self respect, and uncompromising resistance to white oppression. Malcolm X was a polarizing figure who both energized and divided African Americans, while frightening and alienating many whites. He was an unrelenting truth-teller who declared that the mainstream civil rights movement was naïve in hoping to secure freedom through integration and nonviolence. The blazing heat of Malcolm X\\'s rhetoric sometimes overshadowed the complexity of his message, especially for those who found him threatening in the first place. Malcolm X was assassinated at age 39, but his political and cultural influence grew far greater in the years after his death than when he was alive.\\n\\nMalcolm X is now popularly seen as one of the two great martyrs of the 20th century black freedom struggle, the other being his ostensible rival, the Rev. Martin Luther King Jr. But in the spring of 1964, when Malcolm X gave his \"Ballot or the Bullet\" speech, he was regarded by a majority of white Americans as a menacing character. Malcolm X never directly called for violent revolution, but he warned that African Americans would use \"any means necessary\" – especially armed self defense – once they realized just how pervasive and hopelessly entrenched white racism had become.1\\n\\nHe was born Malcolm Little in 1925 in Omaha, Nebraska. His father, Earl, was a Baptist preacher and follower of the black nationalist Marcus Garvey. Earl Little\\'s political activism provoked threats from the Ku Klux Klan. After the family moved to Lansing, Michigan, white terrorists burned the Littles\\' home. A defiant Earl Little shot at the arsonists as they got away. In 1931, Malcolm\\'s father was found dead. His family suspected he\\'d been murdered by white vigilantes. Malcolm\\'s mother, Louise, battled mental illness and struggled to care for her eight children during the Great Depression. She was committed to a state mental institution when Malcolm was 12. He and the other young children were scattered among foster families. After completing the eighth grade, Malcolm Little dropped out when a teacher told him that his dream of becoming a lawyer was unrealistic for a \"nigger.\"2\\n\\nAs a teenager, Malcolm Little made his way to New York, where he took the street name Detroit Red and became a pimp and petty criminal. In 1946, Malcolm Little was sent to prison for burglary. He read voraciously while serving time and converted to the Black Muslim faith. He joined the Nation of Islam (NOI) and changed his name to Malcolm X, eliminating that part of his identity he called a white-imposed slave name.\\n\\nMalcom X was released in 1952 after six years in prison. With his charisma and eloquence, Malcolm rose rapidly in the Nation of Islam. He became the chief spokesman and field recruiter for NOI leader Elijah Muhammad. As historian Peniel Joseph describes it, NOI\\'s unorthodox interpretation of Islam was mixed with a doctrine of black personal responsibility and economic self-sufficiency, along with \"theological fundamentalism, anti-white mythology, and total racial separation as the means to black redemption.\"3 Wearing impeccable suits, maintaining an air of fierce dignity and adhering to a strict code of moral propriety, Malcolm X was a living demonstration of how the NOI could save a wayward people from racial submission and personal self-destruction. The Nation dismissed the conventional civil rights movement – with its protest marches and demands for equal rights legislation -- as impotent and misguided. As Malcolm X declared in this speech, the only effective solution to racial inequality was black economic and social separatism.\\n\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDf Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('random.pdf')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='TECHNOLOGY READINESS LEVELS\\nFOR MACHINE LEARNING SYSTEMS\\nAlexander Lavin∗\\nPasteur LabsCiarán M. Gilligan-Lee\\nSpotifyAlessya Visnjic\\nWhyLabsSiddha Ganju\\nNvidiaDava Newman\\nMIT\\nAtılım Güne¸ s Baydin\\nUniversity of OxfordSujoy Ganguly\\nUnity AIDanny Lange\\nUnity AIAmit Sharma\\nMicrosoft Research\\nStephan Zheng\\nSalesforce ResearchEric P. Xing\\nPetuumAdam Gibson\\nKonduitJames Parr\\nNASA Frontier Development Lab\\nChris Mattmann\\nNASA Jet Propulsion LabYarin Gal\\nAlan Turing Institute\\nABSTRACT\\nThe development and deployment of machine learning (ML) systems can be executed easily with\\nmodern tools, but the process is typically rushed and means-to-an-end. The lack of diligence can\\nlead to technical debt, scope creep and misaligned objectives, model misuse and failures, and\\nexpensive consequences. Engineering systems, on the other hand, follow well-deﬁned processes\\nand testing standards to streamline development for high-quality, reliable results. The extreme is\\nspacecraft systems, where mission critical measures and robustness are ingrained in the development\\nprocess. Drawing on experience in both spacecraft engineering and ML (from research through\\nproduct across domain areas), we have developed a proven systems engineering approach for machine\\nlearning development and deployment. Our Machine Learning Technology Readiness Levels (MLTRL)\\nframework deﬁnes a principled process to ensure robust, reliable, and responsible systems while\\nbeing streamlined for ML workﬂows, including key distinctions from traditional software engineering.\\nEven more, MLTRL deﬁnes a lingua franca for people across teams and organizations to work\\ncollaboratively on artiﬁcial intelligence and machine learning technologies. Here we describe the\\nframework and elucidate it with several real world use-cases of developing ML methods from basic\\nresearch through productization and deployment, in areas such as medical diagnostics, consumer\\ncomputer vision, satellite imagery, and particle physics.\\nKeywords: Machine Learning; Systems Engineering; Data Management; Medical AI; Space Sciences\\nIntroduction\\nThe accelerating use of artiﬁcial intelligence (AI) and machine learning (ML) technologies in systems of software,\\nhardware, data, and people introduces vulnerabilities and risks due to dynamic and unreliable behaviors; fundamentally,\\nML systems learn from data, introducing known and unknown challenges in how these systems behave and interact with\\ntheir environment. Currently the approach to building AI technologies is siloed: models and algorithms are developed\\nin testbeds isolated from real-world environments, and without the context of larger systems or broader products they’ll\\nbe integrated within for deployment. A main concern is models are typically trained and tested on only a handful of\\ncurated datasets, without measures and safeguards for future scenarios, and oblivious of the downstream tasks and\\nusers. Even more, models and algorithms are often integrated into a software stack without regard for the inherent\\nstochasticity –for instance, the massive effect random seeds have on deep reinforcement learning model performance\\n[1] – and failure modes of the ML components, which can be dangerously hidden in layers of software and abstraction.\\nOther domains of engineering, such as civil and aerospace, follow well-deﬁned processes and testing standards to\\nstreamline development for high-quality, reliable results. Technology Readiness Level (TRL) is a systems engineering\\nprotocol for deep tech[ 2] and scientiﬁc endeavors at scale, ideal for integrating many interdependent components\\n∗lavin@simulation.science\\nPreprint. Under review.arXiv:2101.03989v2  [cs.LG]  29 Nov 2021'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='andcross-functional teams of people. It is no surprise that TRL is standard process and parlance in NASA[ 3] and\\nDARPA[4].\\nFor a spaceﬂight project there are several deﬁned phases, from pre-concept to prototyping to deployed operations to\\nend-of-life, each with a series of exacting development cycles and reviews. This is in stark contrast to common machine\\nlearning and software workﬂows, which promote quick iteration, rapid deployment, and simple linear progressions. Yet\\nthe NASA technology readiness process for spacecraft systems is overkill; we need robust ML technologies integrated\\nwith larger systems of software, hardware, data, and humans, but not necessarily for missions to Mars. We aim to bring\\nsystems engineering to AI and ML by deﬁning and putting into action a lean Machine Learning Technology Readiness\\nLevels (MLTRL) framework. We draw on decades of AI and ML development, from research through production,\\nacross domains and diverse data scenarios: for example, computer vision in medical diagnostics and consumer apps,\\nautomation in self-driving vehicles and factory robotics, tools for scientiﬁc discovery and causal inference, streaming\\ntime-series in predictive maintenance and ﬁnance.\\nIn this paper we deﬁne our framework for developing and deploying robust, reliable, and responsible ML and data\\nsystems, with several real test cases of advancing models and algorithms from R&D through productization and\\ndeployment, including essential data considerations. Additionally, MLTRL prioritizes the role of AI ethics and\\nfairness, and our systems AI approach can help curb the large societal issues that can result from poorly deployed and\\nmaintained AI and ML technologies, such as the automation of systemic human bias, denial of individual autonomy,\\nand unjustiﬁable outcomes (see the Alan Turing Institute Report on Ethical AI [5]). The adoption and proliferation of\\nMLTRL provides a common nomenclature and metric across teams and industries. The standardization of MLTRL\\nacross the AI industry should help teams and organizations develop principled, safe, and trusted technologies.\\nFigure 1: MLTRL spans research through prototyping, productization, and deployment. Most ML workﬂows prescribe\\nan isolated, linear process of data processing, training, testing, and serving a model [ 6]. Those workﬂows fail to deﬁne\\nhow ML development must iterate over that basic process to become more mature and robust, and how to integrate with\\na much larger system of software, hardware, data, and people. Not to mention MLTRL continues beyond deployment:\\nmonitoring and feedback cycles are important for continuous reliability and improvement over the product lifetime.\\nResults\\nMLTRL deﬁnes technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment.\\nA TRL represents the maturity of a model or algorithmii, data pipelines, software module, or composition thereof; a\\ntypical ML system consists of many interconnected subsystems and components, and the TRL of the system is the\\nlowest level of its constituent parts [ 7]. The anatomy of a level is marked by gated reviews, evolving working groups,\\nrequirements documentation with risk calculations, progressive code and testing standards, and deliverables such as\\nTRL Cards (Figure 3) and ethics checklists.iiiThese components—which are crucial for implementing the levels in a\\niiNote we use “model” and “algorithm” somewhat interchangeably when referring to the technology under development. The\\nsame MLTRL process and methods apply for a machine translation model and for an A/B testing algorithm, for example.\\niiiTemplates and examples for MLTRL deliverables will be open-sourced upon publication at github.com/alan-turing-institute.\\n2'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='systematic fashion—as well as MLTRL metrics and methods are concretely described in examples and in the Methods\\nsection. Lastly, to emphasize the importance of data tasks in ML, from data curation [ 8] to data governance [ 9], we\\nstate several important data considerations at each MLTRL level.\\nMACHINE LEARNING TECHNOLOGY READINESS LEVELS\\nThe levels are brieﬂy deﬁned as follows and in Figure 1, and elucidated with real-world examples later.\\nLevel 0 - First Principles This is a stage for greenﬁeld AI research, initiated with a novel idea, guiding question, or\\npoking at a problem from new angles. The work mainly consists of literature review, building mathematical foundations,\\nwhite-boarding concepts and algorithms, and building an understanding of the data – for work in theoretical AI and ML,\\nhowever, there will not yet be data to work with (for example, a novel algorithm for Bayesian optimization[ 10], which\\ncould eventually be used for many domains and datasets). The outcome of Level 0 is a set of concrete ideas with sound\\nmathematical formulation, to pursue through low-level experimentation in the next stage. When relevant, this level\\nexpects conclusions about data readiness, including strategies for getting the data to be suitable for the speciﬁc ML task.\\nTo graduate, the basic principles, hypotheses, data readiness, and research plans need to be stated, referencing relevant\\nliterature. With graduation, a TRL Card should be started to succinctly document the methods and insights thus far –\\nthis key MLTRL deliverable is detailed in the Methods section and Figure 3.\\nLevel 0 data – Not a hard requirement at this stage because this is largely theoretical machine learning. That being said,\\ndata availability needs to be considered for deﬁning any research project to move past theory.\\nLevel 0 review – The reviewer here is solely the lead of the research lab or team, for instance a PhD supervisor. We\\nassess hypotheses and explorations for mathematical validity and potential novelty or utility, not necessarily code nor\\nend-to-end experiment results.\\nLevel 1 - Goal-Oriented Research To progress from basic principles to practical use, we design and run low-level\\nexperiments to analyze speciﬁc model or algorithm properties (rather than end-to-end runs for a performance benchmark\\nscore). This involves collection and processing of sample data to train and evaluate the model. This sample data need\\nnot be the full data; it may be a smaller sample that is currently available or more convenient to collect. In some\\ncases it may sufﬁce to use synthetic data as the representative sample – in the medical domain, for example, acquiring\\ndatasets can take many months due to security and privacy constraints, so generating sample data can mitigate this\\nblocker from early ML development. Further, working with the sample data provides a blueprint for the data collection\\nand processing pipeline (including answering whether it is even possible to collect all necessary data), that can be\\nscaled up for the for the next steps. The experiments, good results or not, and mathematical foundations need to pass a\\nreview process with fellow researchers before graduating to Level 2. The application is still speculative, but through\\ncomparison studies and analyses we start to understand if/how/where the technology offers potential improvements and\\nutility. Code is research-caliber : The aim here is to be quick and dirty, moving fast through iterations of experiments.\\nHacky code is okay, and full test coverage is actually discouraged, as long as the overall codebase is organized and\\nmaintainable. It is important to start semantic versioning practices early in the project lifecycle, which should cover\\ncode, models, anddatasets. This is crucial for retrospectives and reproducibility, issues with which can be costly and\\nsevere at later stages. This versioning information and additional progress should be reported on the TRL Card (see for\\nexample Figure 3).\\nLevel 1 data – At minimum we work with sample data that is representative of downstream real datasets, which can be\\na subset of real data, synthetic data, or both. Beyond driving low-level ML experiments, the sample data forces us to\\nconsider data acquisition and processing strategies at an early stage before it becomes a blocker later.\\nLevel 1 review – The panel for this gated review is entirely members of the research team, reviewing for scientiﬁc rigor\\nin early experimentation, and pointing to important concepts and prior work from their respective areas of expertise.\\nThere may be several iterations of feedback and additional experiments.\\nLevel 2 - Proof of Principle (PoP) Development Active R&D is initiated, mainly by developing and running in\\ntestbeds : simulated environments and/or simulated data that closely matches the conditions and data of real scenarios –\\nnote these are driven by model-speciﬁc technical goals, not necessarily application or product goals (yet). An important\\ndeliverable at this stage is the formal research requirements document (with well-speciﬁed veriﬁcation and validation\\n(V&V) steps)iv. Here is one of several key decision points in the broader process: The R&D team considers several\\npaths forward and sets the course: (A) prototype development towards Level 3, (B) continued R&D for longer-term\\nivArequirement is a singular documented physical or functional need that a particular design, product, or process aims to satisfy.\\nRequirements aim to specify all stakeholders’ needs while not specifying a speciﬁc solution. Deﬁnitions are incomplete without\\n3'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='research initiatives and/or publications, or some combination of A and B. We ﬁnd the culmination of this stage is often\\na bifurcation: some work moves to applied AI, while some circles back for more research. This common MLTRL cycle\\nis an instance of the non-monotonic discovery switchback mechanism (detailed in the Methods section).\\nLevel 2 data – Datasets at this stage may include publicly available benchmark datasets, semi-simulated data based\\non the data sample in Level 1, or fully simulated data based on certain assumptions about the potential deployment\\nenvironments. The data should allow researchers to characterize model properties, and highlight corner cases or\\nboundary conditions, in order to justify the utility of continuing R&D on the model.\\nLevel 2 review – To graduate from the PoP stage, the technology needs to satisfy research claims made in previous\\nstages (brought to be bare by the aforementioned PoP data in both quantitative and qualitative ways) with the analyses\\nwell-documented and reproducible.\\nLevel 3 - System Development Here we have checkpoints that push code development towards interoperability,\\nreliability, maintainability, extensibility, and scalability. Code becomes prototype-caliber : A signiﬁcant step up from\\nresearch code in robustness and cleanliness. This needs to be well-designed, well-architected for dataﬂow and interfaces,\\ngenerally covered by unit and integration tests, meet team style standards, and sufﬁciently-documented. Note the\\nprogrammers’ mentality remains that this code will someday be refactored/scrapped for productization; prototype code\\nis relatively primitive with regard to efﬁciency and reliability of the eventual system. With the transition to Level 4 and\\nproof-of-concept mode, the working group should evolve to include product engineering to help deﬁne service-level\\nagreements and objectives (SLAs and SLOs) of the eventual production system.\\nLevel 3 data – For the most part consistent with Level 2; in general, the previous level review can elucidate potential\\ngaps in data coverage and robustness to be addressed in the subsequent level. However, for test suites developed at this\\nstage, it is useful to deﬁne dedicated subsets of the experiment data as default testing sources, as well as setup mock\\ndata for speciﬁc functionalities and scenarios to be tested.\\nLevel 3 review – Teammates from applied AI and engineering are brought into the review to focus on sound software\\npractices, interfaces and documentation for future development, and version control for models and datasets. There are\\nlikely domain- or organization-speciﬁc data management considerations going forward that this review should point out\\n– e.g. standards for data tracking and compliance in healthcare [11].\\nLevel 4 - Proof of Concept (PoC) Development This stage is the seed of application-driven development; for many\\norganizations this is the ﬁrst touch-point with product managers and stakeholders beyond the R&D group. Thus TRL\\nCards and requirements documentation are instrumental in communicating the project status and onboarding new\\npeople. The aim is to demonstrate the technology in a real scenario: quick proof-of-concept examples are developed to\\nexplore candidate application areas and communicate the quantitative and qualitative results. It is essential to use real\\nand representative data for these potential applications. Thus data engineering for the PoC largely involves scaling up\\nthe data collection and processing from Level 1, which may include collecting new data or processing all available data\\nusing scaled experiment pipelines from Level 3. In some scenarios there will new datasets brought in for the PoC, for\\nexample, from an external research partner as a means of validation. Hand-in-hand with the evolution from sample to\\nreal data, the experiment metrics should evolve from ML research to the applied setting: proof-of-concept evaluations\\nshould quantify model and algorithm performance (e.g., precision and recall and various data splits), computational\\ncosts (e.g., CPU vs GPU runtimes), and also metrics that are more relevant to the eventual end-user (e.g., number\\nof false positives in the top-N predictions of a recommender system). We ﬁnd this PoC exploration reveals speciﬁc\\ndifferences between clean and controlled research data versus noisy and stochastic real-world data. The issues can\\nbe readily identiﬁed because of the well-deﬁned distinctions between those development stages in MLTRL, and then\\ntargeted for further development.\\nAI ethics processes vary across organizations, but all should engage in ethics conversations at this stage, including ethics\\nof data collection, and potential of any harm or discriminatory impacts due to the model (as the AI capabilities and\\ndatasets are known). MLTRL requires ethics considerations to be reported on TRL Cards at all stages, which generally\\nlink to an extended ethics checklist. The key decision point here is to push onward with application development or not.\\nIt is common to pause projects that pass Level 4 review, waiting for a better time to dedicate resources, and/or pull the\\ntechnology into a different project.\\nLevel 4 data – Unlike the previous stages, having real-world and representative data is critical for the PoC; even with\\nmethods for verifying that data distributions in synthetic data reliably mirror those of real data [], sufﬁcient conﬁdence\\nin the technology must be achieved with real-world data of the use-case. Further, one must consider how to obtain\\ncorresponding measures for veriﬁcation and validation (V&V). Veriﬁcation: Are we building the product right? Validation: Are we\\nbuilding the right product?\\n4'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='high-quality and consistent data required for the future model inference: generation of the data pipeline PoC that will\\nresemble the future inference pipeline that will take data from intended sources, transform it into features, and send it to\\nthe model for inference.\\nLevel 4 review – Demonstrate the utility towards one or more practical applications (each with multiple datasets), taking\\ncare to communicate assumptions and limitations, and again reviewing data-readiness: evaluating the real-world data\\nfor quality, validity, and availability. The review also evaluates security and privacy considerations – deﬁning these in\\nthe requirements document with risk quantiﬁcation is a useful mechanism for mitigating potential issues (discussed\\nfurther in the Methods section).\\nLevel 5 - Machine Learning “Capability” At this stage the technology is more than an isolated model or algorithm,\\nit is a speciﬁc capability . For instance, producing depth images from stereo vision sensors on a mobile robot is a\\nreal-world capability beyond the isolated ML technique of self-supervised learning for RGB stereo disparity estimation.\\nIn many organizations this represents a technology transition or handoff from R&D to productization. MLTRL\\nmakes this transition explicit, evolving the requisite work, guiding documentation, objectives and metrics, and team;\\nindeed, without MLTRL it is common for this stage to be erroneously leaped completely, as shown in Figure 2. An\\ninterdisciplinary working group is deﬁned, as we start developing the technology in the context of a larger real-world\\nprocess – i.e., transitioning the model or algorithm from an isolated solution to a module of a larger application. Just as\\nthe ML technology should no longer be owned entirely by ML experts, steps have been taken to share the technology\\nwith others in the organization via demos, example scripts, and/or an API; the knowledge and expertise cannot remain\\nwithin the R&D team, let alone an individual ML developer. Graduation from Level 5 should be difﬁcult, as it signiﬁes\\nthe dedication of resources to push this ML technology through productization. This transition is a common challenge\\nin deep-tech, sometimes referred to as “the valley of death” because project managers and decision-makers struggle\\nto allocate resources and align technology roadmaps to effectively move to Level 6, 7 and onward. MLTRL directly\\naddresses this challenge by stepping through the technology transition or handoff explicitly.\\nLevel 5 data – For the most part consistent with Level 4. However, considerations need to be taken for scaling of data\\npipelines: there will soon be more engineers accessing the existing data and adding more, and the data will be getting\\nmuch more use, including automated testing in later levels. With this scaling can come challenges with data governance.\\nThe data pipelines likely do not mirror the structure of the teams or broader organization. This can result in data silos,\\nduplications, unclear responsibilities, and missing control of data over its entire lifecycle. These challenges and several\\napproaches to data governance (planning and control, organizational, and risk-based) are detailed in Janssen et al. [9].\\nLevel 5 review – The veriﬁcation and validation (V&V) measures and steps deﬁned in earlier R&D stages (namely\\nLevel 2) must all be completed by now, and the product-driven requirements (and corresponding V&V) are drafted at\\nthis stage. We thoroughly review them here, and make sure there is stakeholder alignment (at the ﬁrst possible step of\\nproductization, well ahead of deployment).\\nLevel 6 - Application Development The main work here is signiﬁcant software engineering to bring the code up to\\nproduct-caliber : This code will be deployed to users and thus needs to follow precise speciﬁcations, have comprehensive\\ntest coverage, well-deﬁned APIs, etc. The resulting ML modules should be robustiﬁed towards one or more target\\nuse-cases. If those target use-cases call for model explanations, the methods need to be built and validated alongside\\nthe ML model, and tested for their efﬁcacy in faithfully interpreting the model’s decisions – crucially, this needs to be\\nin the context of downstream tasks and the end-users, as there is often a gap between ML explainability that serves\\nML engineers rather than external stakeholders[ 12]. Similarly, we need to develop the ML modules with known data\\nchallenges in mind, speciﬁcally to check the robustness of the model (and broader pipeline) to changes in the data\\ndistribution between development and deployment.\\nThe deployment setting(s) should be addressed thoroughly in the product requirements document, as ML serving (or\\ndeploying) is an overloaded term that needs careful consideration. First, there are two main types: internal, as APIs\\nfor experiments and other usage mainly by data science and ML teams, and external, meaning an ML model that\\nis embedded or consumed within a real application with real users. The serving constraints vary signiﬁcantly when\\nconsidering cloud deployment vs on-premise or hybrid, batch or streaming, open-source solution or containerized\\nexecutable, etc. Even more, the data at deployment may be limited due to compliance, or we may only have access to\\nencrypted data sources, some of which may only be accessible locally – these scenarios may call for advanced ML\\napproaches such as federated learning[ 13] and other privacy-oriented ML[ 14]. And depending on the application, an\\nML model may not be deployable without restrictions; this typically means being embedded in a rules engine workﬂow\\nwhere the ML model acts like an advisor that discovers edge cases in rules. These deployment factors are hardly\\nconsidered in model and algorithm development despite signiﬁcant inﬂuence on modeling and algorithmic choices;\\nthat said, hardware choices typically are considered early on, such as GPU versus edge devices. It is crucial to make\\nthese systems decisions at Level 6–not too early that serving scenarios and requirements are uncertain, and not too late\\n5'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='that corresponding changes to model or application development risk deployment delays or failures. This marks a key\\ndecision for the project lifecycle, as this expensive ML deployment risk is common without MLTRL (see Figure 2).\\nLevel 6 data – Additional data should be collected and operationalized at this stage towards robustifying the ML\\nmodels, algorithms, and surrounding components. These include adversarial examples to check local robustness [ 15],\\nsemantically-equivalent perturbations to check consistency of the model with respect to domain assumptions [16, 17],\\nand collecting data from different sources and checking how well the trained model generalizes to them. These\\nconsiderations are even more vital in the challenging deployment domains mentioned above with limited data access.\\nLevel 6 review – Focus is on the code quality, the set of newly deﬁned product requirements, system SLA and SLO\\nrequirements, data pipelines spec, and an AI ethics revisit now that we are closer to a real-world use-case. In particular,\\nregulatory compliance is mandated for this gated review; the data privacy and security laws are changing rapidly, and\\nmissteps with compliance can make or break the project.\\nLevel 7 - Integrations For integrating the technology into existing production systems, we recommend the working\\ngroup has a balance of infrastructure engineers andapplied AI engineers – this stage of development is vulnerable\\nto latent model assumptions and failure modes, and as such cannot be safely developed solely by software engineers.\\nImportant tools for them to build together include:\\n•Tests that run use-case speciﬁc critical scenarios and data-slices – a proper risk-quantiﬁcation table will\\nhighlight these.\\n•A “golden dataset” should be deﬁned to baseline the performance of each model and succession of models –see\\nthe computer vision app example in Figure 4–for use in the continuous integration and deployment (CI/CD)\\ntests.\\n•Metamorphic testing : a software engineering methodology for testing a speciﬁc set of relations between the\\noutputs of multiple inputs. When integrating ML modules into larger systems, a codiﬁed list of metamorphic\\nrelations[18] can provide valuable veriﬁcation and validation measures and steps.\\n•Data intervention tests that seek data bugs at various points in the pipelines, downstream to measure the\\npotential effects of data processing and ML on consumers or users of that data, as well as upstream at data\\ningestion or creation. Rather than using model performance as a proxy for data quality, it is crucial to use\\nintervention tests that instead catch data errors with mechanisms speciﬁc to data validation.\\nThese tests in particular help mitigate underspeciﬁcation in ML pipelines, a key obstacle to reliably training models that\\nbehave as expected in deployment[ 19]. On the note of reliability, it is important that quality assurance engineers (QA)\\nplay a key role here and through Level 9, overseeing data processes to ensure privacy and security, and covering audits\\nfor downstream accountability of AI methods.\\nLevel 7 data – In addition to the data for test suites discussed above, this level calls for QA to prioritize data governance :\\nhow data is obtained, managed, used, and secured by the organization. This was earlier suggested in level 5 (in order to\\npreempt related technical debt), and essential here at the main junction for integration, which may create additional\\ngovernance challenges in light of downstream effects and consumers.\\nLevel 7 review – The review should focus on the data pipelines and test suites; a scorecard like the ML Testing\\nRubric[ 20] is useful. The group should also emphasize ethical considerations at this stage, as they may be more\\nadequately addressed now (where there are many test suites put into place) rather than close to shipping later.\\nLevel 8 - Flight-ready The technology is demonstrated to work in its ﬁnal form and under expected conditions.\\nThere should be additional tests implemented at this stage covering deployment aspects, notably A/B tests, blue/green\\ndeployment tests, shadow testing, and canary testing, which enable proactive and gradual testing for changing ML\\nmethods and data. Ahead of deployment, the CI/CD system should be ready to regularly stress test the overall system\\nand ML components. In practice, problems stemming from real-world data are impossible to anticipate and design for –\\nan upstream data provider could change formats unexpectedly or a physical event could cause the customer behavior to\\nchange. Running models in shadow mode for a period of time would help stress test the infrastructure and evaluate how\\nsusceptible the ML model(s) will be to performance regressions caused by data. We observe that ML systems with\\ndata-oriented architectures are more readily tested in this manner, and better surface data quality issues, data drifts, and\\nconcept drifts – this is discussed later in the Beyond Software Engineering section. To close this stage, the key decision\\nis go or no-go for deployment, and when.\\nLevel 8 data – If not already in place, there absolutely needs to be mechanisms for automatically logging data\\ndistributions alongside model performance once deployed.\\n6'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='Level 8 review – A diligent walkthrough of every technical and product requirement, showing the corresponding\\nvalidations, and the review panel is representative of the full slate of stakeholders.\\nLevel 9 - Deployment In deploying AI and ML technologies, there is signiﬁcant need to monitor the current version,\\nand explicit considerations towards improving the next version. For instance, performance degradation can be hidden\\nand critical, and feature improvements often bring unintended consequences and constraints. Thus at this level, the\\nfocus is on maintenance engineering–i.e., methods and pipelines for ML monitoring and updating. Monitoring for data\\nquality, concept drift, and data drift is crucial; no AI system without thorough tests for these can reliably be deployed.\\nBy the same token there must be automated evaluation and reporting – if actuals[ 21] are available, continuous evaluation\\nshould be enabled, but in many cases actuals come with a delay, so it is essential to record model outputs to allow for\\nefﬁcient evaluation after the fact. To these ends, the ML pipeline should be instrumented to log system metadata, model\\nmetadata, and data itself.\\nMonitoring for data quality issues and data drifts is crucial to catch deviations in model behavior, particularly those that\\nare non-obvious in the model or product end-performance. Data logging is unique in the context of ML systems: data\\nlogs should capture statistical properties of input features and model predictions, and capture their anomalies. With\\nmonitoring for data, concept, and model drifts, the logs are to be sent to the relevant systems, applied, and research\\nengineers. The latter is often non-trivial, as the model server is not ideal for model “observability” because it does not\\nnecessarily have the right data points to link the complex layers needed to analyze and debug models. To this end,\\nMLTRL requires the drift tests to be implemented at stages well ahead of deployment, earlier than is standard practice.\\nAgain we advocate for data-ﬁrst architectures rather than the software industry-standard design by services (discussed\\nlater), which aids in surfacing and logging the relevant data types and slices when monitoring AI systems. For retraining\\nand improving models, monitoring must be enabled to catch training-serving skew and let the team know when to\\nretrain. Towards model improvements, adding or modifying features can often have unintended consequences, such as\\nintroducing latencies or even bias. To mitigate these risks, MLTRL has an embedded switchback here: any component\\nor module changes to the deployed version must cycle back to Level 7 (integrations stage) or earlier. Additionally,\\nfor quality ML products, we stress a deﬁned communication path for user feedback without roadblocks to R&D; we\\nencourage real-world feedback all the way to research, providing valuable problem constraints and perspectives.\\nLevel 9 data – Proper mechanisms for logging and inspecting data (alongside models) is critical for deploying reliable\\nAI and ML – systems that learn on data have unique monitoring requirements (detailed above). In addition to the\\ninfrastructure and test suites covering data and environment shifts, it’s important for product managers and other owners\\nto be on top of data policy shifts in domains such as ﬁnance and healthcare.\\nLevel 9 review – The review at this stage is unique, as it also helps in lifecycle management: at a regular cadence\\nthat depends on the deployed system and domain of use, owners and other stakeholders are to revisit this review and\\nrecommend switchbacks if needed (discussed in the Methods section). This additional oversight at deployment is\\nshown to help deﬁne regimented release cycles of updated versions, and provide another “eye” check for stale model\\nperformance or other system abnormalities.\\nNotice MLTRL is deﬁned as stages or levels, yet much of the value in practice is realized in the transitions: MLTRL\\nenables teams to move from one level to the next reliably and efﬁciently, and provides a guide for how teams and\\nobjectives evolve with the progressing technology.\\nDiscussion\\nMLTRL is designed to apply to many real-world use-cases involving data and ML, from simple regression models\\nused for predictive modeling energy demand or anomaly detection in datacenters, to real-time modeling in rideshare\\napplications and motion planning in warehouse robotics. For simple use-cases MLTRL may be overkill, and a subset\\nmay sufﬁce – for instance, model cards as demonstrated by Google for basic image classiﬁcation. Yet this is a ﬁne line,\\nas the same cards-only approach in the popular “Huggingface” codebases are too simplistic for the language models\\nthey represent, deployed in domains that carry signiﬁcant consequences. MLTRL becomes more valuable with more\\ncomplex, larger systems and environments, especially in risk averse domains. We thoroughly discuss this through\\nseveral real uses of MLTRL below.\\n7'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='Figure 2: Most ML and AI projects live in these sections of MLTRL, not concerned with fundamental R&D – that is,\\ncompletely using existing methods and implementations, and even pretrained models. In the left diagram, the arrows\\nshow a common development pattern with MLTRL in industry: projects go back to the ML toolbox to develop new\\nfeatures (dashed line), and frequent, incremental improvements are often a practice of jumping back a couple levels to\\nLevel 7 (which is the main systems integrations stage). At Levels 7 and 8 we stress the need for tests that run use-case\\nspeciﬁc critical scenarios and data-slices, which are highlighted by a proper risk-quantiﬁcation matrix [ 22]. Cycling\\nback to previous lower levels is not just a late-stage mechanism in MLTRL, but rather “switchbacks” occur throughout\\nthe process (as discussed in the Methods section and throughout the text). In the right diagram we show the more\\ncommon approach in industry ( without using our framework), which skips essential technology transition stages – ML\\nEngineers push straight through to deployment, ignoring important productization and systems integration factors. This\\nwill be discussed in more detail in the Methods section.\\nEXAMPLES\\nHuman-machine visual inspection\\nWhile most ML projects begin with a speciﬁc task and/or dataset, there are many that originate in ML theory without\\nany target application – i.e., projects starting MLTRL at level 0 or 1. These projects nicely demonstrate the utility of\\nMLTRL built-in switchbacks, bifurcating paths, and iteration with domain experts. An example we discuss here is a\\nnovel approach to representing data in generative vision models from Naud & Lavin[ 23], which was then developed into\\nstate-of-the-art unsupervised anomaly detection, and targeted for two human-machine visual inspection applications:\\nFirst, industrial anomaly detection, notably in precision manufacturing, to identify potential errors for human-expert\\nmanual inspection. Second, using the model to improve the accuracy and efﬁciency of neuropathology, the microscopic\\nexamination of neurosurgical specimens for cancerous tissue. In these human-machine teaming use-cases there are\\nspeciﬁc challenges impeding practical, reliable use:\\n•Hidden feedback loops can be common and problematic in real-world systems inﬂuencing their own training\\ndata: over time the behavior of users may evolve to select data inputs they prefer for the speciﬁc AI system,\\nrepresenting some skew from the training data. In this neuropathology case, selecting whole-slide images that\\nare uniquely difﬁcult for manual inspection, or even biased by that individual user. Similarly we see underlying\\nhealthcare processes can act as hidden confounders, resulting in unreliable decision support tools[26].\\n•Model availability can be limited in many deployment settings: for example, on-premises deployments\\n(common in privacy preserving domains like healthcare and banking), edge deployments (common in industrial\\nuse-cases such as manufacturing and agriculture), or from the infrastructure’s inability to scale to the volume\\nof requests. This can severely limit the team’s ability to monitor, debug, and improve deployed models.\\n•Uncertainty estimation is valuable in many AI scenarios, yet not straightforward to implement in practice.\\nThis is further complicated with multiple data sources and users, each injecting generally unknown amounts of\\nnoise and uncertainties. In medical applications it is of critical importance, to provide measures of conﬁdence\\nand sensitivity, and for AI researchers through end-users. In anomaly detection, various uncertainty measures\\ncan help calibrate the false-positive versus false-negative rates, which can be very domain speciﬁc.\\n8'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='Figure 3: The maturity of each ML technology is tracked via TRL Cards , which we describe in the Methods section.\\nHere is an example reﬂecting a neuropathology machine vision use-case[ 23], detailed in the Discussion Section. Note\\nthis is a subset of a full TRL Card, which in reality lives as a full document in an internal wiki. Notice the card\\nclearly communicates the data sources, versions, and assumptions. This helps mitigate invalid assumptions about\\nperformance and generalizability when moving from R&D to production, and promotes the use of real-world data\\nearlier in the project lifecycle. We recommend documenting datasets thoroughly with semantic versioning and tools\\nsuch as datasheets for datasets [24], and following data accountability best-practices as they evolve (see [25]).\\n•Costs of edge cases can be signiﬁcant, sometimes risking expensive machine downtime or medical failures.\\nThis is exacerbated in anomaly detection anomalies are by deﬁnition rare so they can be difﬁcult to train for,\\nespecially for the anomalies that are completely unseen until they arise in the wild.\\n•End-user trust can be difﬁcult to achieve, often preventing the adoption of ML applications, particularly in\\nthe healthcare domain and other highly regulated industries.\\nThese and additional ML challenges such as data privacy and interpretability can inhibit ML adoption in clinical practice\\nand industrial settings, but can be mitigated with MLTRL processes. We’ll describe how in the context of the Naud\\n& Lavin[ 23] example, which began at level 0 with theoretical ML work on manifold geometries, and at level 5 was\\ndirected towards specialized human-machine teaming applications utilizing the same ML method under-the-hood.\\n•Levels 0-1 – From open-ended exploration of data-representation properties in various Riemmanian manifold\\ncurvatures, we derived from ﬁrst principles and empirically identiﬁed a property with hyperbolic manifolds:\\nwhen used as a latent space for embedding data without labels, the geometry organizes the data by it’s implicit\\nhierarchical structure. Unsupervised computer vision was identiﬁed in reviews as a promising direction for\\nproof-of-principle work.\\n•Level 2 – One approach for validating the earlier theoretical developments was to generate synthetic data to\\nisolate very speciﬁc features in data we would expect represented in the latent manifold. The results showed\\npromise for anomaly detection – using the latent representation of data to automatically identify images that\\nare out-of-the-ordinary (anomalous), and also using the manifold to inspect how they are semantically different.\\nFurther, starting with an implicitly probabilistic modeling approach implied uncertainty estimation could be\\na valuable feature downstream. This made the level 2 key decision point clear: proceed with applied ML\\ndevelopment.\\n•Levels 3-5 – Proof-of-concept development and reviews demonstrated promise for several commercial appli-\\ncations relevant to the business, and also highlighted the need for several key features (deﬁned as R&D and\\nproduct requirements): interpretability (towards end-user trust), uncertainty quantiﬁcation (to show conﬁdence\\nscores), and human-in-the-loop (for domain expertise). Without the MLTRL PoC steps and review processes,\\nthese features can often be delayed until beta testing or overlooked completely – for example, the failures of\\napplying IBM Watson in medical applications [ 27]. For this technology, the applications to develop towards\\nare anomaly detection in histopathology and manufacturing, speciﬁcally inspecting whole-slide images of\\nneural tissue, and detecting defects in metallic surfaces, respectively.\\n9'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='From the systems perspective, we suggest quantifying the uncertainties of components and propagating them\\nthrough the system, which can improve safety and trust. Probabilistic ML methods, rooted in Bayesian\\nprobability theory, provide a principled approach to representing and manipulating uncertainty about models\\nand predictions[ 28]. For this reason we advocate strongly for probabilistic models and algorithms in AI\\nsystems. In this machine vision example, the MLTRL technical requirements speciﬁcally called for a\\nprobabilistic generative model to readily quantify various types of uncertainties and propagate them forward to\\nthe visualization component of the pipeline, and the product requirements called for the downstream conﬁdence\\nand sensitivity measures to be exposed to the end-user. Component uncertainties must be assembled in a\\nprincipled way to yield a meaningful measure of overall system uncertainty, based on which safe decisions can\\nbe made[29]. See the Methods section for more on uncertainty in AI systems.\\nThe early checks for data management and governance proved valuable here, as the application areas dealt\\nwith highly sensitive data that would signiﬁcantly inﬂuence the design of data pipelines and test suites. In\\nboth the neuropathology and manufacturing applications, the data management checks also raised concerns\\nabout hidden feedback loops, where users may unintentionally skew the data inputs when using the anomaly\\ndetection models in practice, for instance biasing the data towards speciﬁc subsets they subjectively need help\\nwith. Incorporating domain experts this early in the project lifecycle helped inform veriﬁcation and validation\\nsteps to help be robust to the hidden feedback loops. Not to mention their input guided us towards user-centric\\nmetrics for performance, which can often skew from ML metrics in important ways – for instance, the typical\\nacceptance ratio for false positives versus false negatives doesn’t apply to select edge cases, for which our\\nhierarchical anomaly classiﬁcation scheme was useful [23].\\nFrom prior reviews and TRL card documentation, we also identiﬁed the value of synthetic data generation\\ninto application development: anomalies are by deﬁnition rare so they are hard to come by in real datasets,\\nespecially with evolving environments in deployment settings, so the ability to generate synthetic datasets for\\nanomaly detection can accelerate the level 6-9 pipeline, and help ensure more reliable models in the wild.\\n•Level 6 (medical) – The medical inspection application experienced a bifurcation with product work proceed-\\ning while additional R&D was desired to explore improved data processing methods, while engaging with\\nclinicians and medical researchers for feedback. Proceeding through the levels in a non-linear, non-monotonic\\nway is common in MLTRL and encouraged by various switchback mechanisms (detailed in the Methods\\nsection). These practices – intentional switchbacks, frequent engagement with domain experts and users – can\\nhelp mitigate methodological ﬂaws and underlying biases that are common when applying ML to clinical\\napplications. For instance, recent work by Roberts et al. [ 30] investigated 2,122 studies applying ML to\\nCOVID-19 use-cases, ﬁnding that none of the models are sufﬁcient for clinical use due to methodological ﬂaws\\nand/or underlying biases. They go on to give many recommendations – some we’ve discussed in the context of\\nMLTRL, and more – which should be reviewed for higher quality medical-ML models and documentation.\\n•Level 6-9 (manufacturing) – Overall these stages proceeded regularly and efﬁciently for the defect detection\\nproduct. MLTRL’s embedded switchback from level 9 to 4 proved particularly useful in this lifecycle, both\\nfor incorporating feedback from the ﬁeld and for updating with research progress. On the former, the data\\ndistribution shifts from one deployment setting to another signiﬁcantly affected false-positive versus false-\\nnegative calibrations, so this was added as a feature to the CI/CD pipelines. On the latter, the built-in touch\\npoints for real-world feedback and data into the continued ML research provided valuable constraints to\\nhelp guide research, and product managers could readily understand what capabilities could be available for\\nproduct integration and when (readily communicated with TRL Cards) – for instance, later adding support for\\nvideo-based inspection for defects, and tooling for end-users to reason about uncertainty estimates (which\\nhelps establish trust).\\n•Level 7-9 (medical) – For productization the “neuropathology copilot” was handed off to a partner pharmaceu-\\ntical company to integrate into their existing software systems. The MLTRL documentation and communication\\nstreamlined the technology transfer, which can often by a time-consuming manual process. If not pursuing\\nthis path, the product would’ve likely faced many of the medical-ML deployment challenges with model\\navailability and data access; MLTRL cannot overcome the technical challenges of deploying on-premises, but\\nthe manifestation of those challenges as performance regressions, data shifts, privacy and ethics concerns, etc.\\ncan be mitigated by the system-level checks and strategies MLTRL puts forth.\\nComputer vision with real and synthetic data\\nAdvancements in physics engines and graphics processing have advanced AI environment and data-generation capabili-\\nties, putting increased emphasis on transitioning models across the simulation-to-reality gap [ 31,32,33]. To develop a\\ncomputer vision application for automated recycling, we leveraged the Unity Perception [ 34] package, a toolkit for\\ngenerating large-scale datasets for perception-based ML training and validation. We produced synthetic images to\\n10'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='\\x15A?U?HEJC\\x03?H=OOEBE?=PEKJ\\x03LELAHEJA\\x03\\n\\xa0=¡\\xa0>¡\\n,V\\x03PRGHO\\x03FRQğGHQFH\\x03\\x1f\\x03WKUHVKROG\",V\\x03GHWHFWHG\\x03REMHFW\\x03LQ\\x03WDUJHW\\x03VHW\"3URYLGH\\x03FRUUHVSRQGLQJ\\x03UHF\\\\FOLQJ\\x03LQVWUXFWLRQV,QLWLDWH\\x03KXPDQ\\x10\\x03LQ\\x10WKH\\x10ORRS\\x03SURWRFRO12<(6<(6Figure 4: Computer vision pipeline for an automated recycling application (a), which contains multiple ML models,\\nuser input, and image data from various sources. Complicated logic such as this can mask ML model performance lags\\nand failures, and also emphasized the need for R&D-to-product hand off described in MLTRL. Additional emphasis is\\nplaced on ML tests that consider the mix of real-world data with user annotations (b, right) and synthetic data generated\\nby Unity AI’s Perception tool and structured domain randomization (b, left).\\ncomplement real-world data sources (Figure 4). This application exempliﬁes three important challenges in ML product\\ndevelopment that MLTRL helps overcome:\\n•Multiple and disparate data sources are common in deployed ML pipelines yet often ignored in R&D.\\nFor instance, upstream data providers can change formats unexpectedly, or a physical event could cause the\\ncustomer behavior to change. It is nearly impossible to anticipate and design for all potential problems with\\nreal-world data and deployment. This computer vision system implemented pipelines and extended test suites\\nto cover open-source benchmark data, real user data, and synthetic data.\\n•Hidden performance degradation can be challenging to detect and debug in ML systems because gradual\\nchanges in performance may not be immediately visible. Common reasons for this challenge are that the\\nML component may be one step in a series. Additionally, local/isolated changes to an ML component’s\\nperformance may not directly affect the observed downstream performance. We can see both issues in the\\nillustrated logic diagram for the automated recycling app (Figure 4). A slight degradation in the initial CV\\nmodel may not heavily inﬂuence the following user input. However, when an uncommon input image appears\\nin the future, the app fails altogether.\\n•Model usage requirements can make or break an ML product. For example, the Netﬂix “$1M Prize” solution\\nwas never fully deployed because of signiﬁcant engineering costs in real-world scenariosv. For example,\\nengineering teams must communicate memory usage, compute power requirements, hardware availability,\\nnetwork privacy, and latency to the ML teams. ML teams often only understand the statistics or ML theory\\nbehind a model but not the system requirements or how it scales.\\nWe next elucidate these challenges and how MLTRL helps overcome them in the context of this project’s lifecycle. This\\nproject started at level 4, using largely existing ML methods with a target use-case.\\n•Level 4 – For this project, we validated most of the components in other projects. Speciﬁcally, the computer\\nvision (CV) model for object recognition and classiﬁcation was an off-the-shelf model. The synthetic data\\ngeneration method used Unity Perception, a well-established open-source project. Though this allowed us to\\nvnetﬂixtechblog.com/netﬂix-recommendations-beyond-the-5-stars-part-1\\n11'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='skip the earlier levels, many challenges arise when combining ML elements that were independently validated\\nand developed. The MLTRL prototype-caliber code checkpoint ensures that the existing code components\\nare validated and helps avoid poorly deﬁned borders and abstractions between components. ML pipelines\\noften grow out of glue code, and our regimented code checkpoints motivate well-architected software that\\nminimizes these danger spots.\\n•Level 5 – The problematic “valley of death”, mentioned earlier in the level 5 deﬁnitions, is less prevalent in use-\\ncases like this that start at a higher MLTRL level with a speciﬁc product deliverable. In this case, the product\\ndeliverable was a real-time object recognition and classiﬁcation of trash for a mobile recycling application.\\nStill, this stage is critical for the requirements and V&V transition. This stage mitigates failure risks due to the\\ndisparate data sources integrated at various steps in this CV system and accounted for the end-user compute\\nconstraints for mobile computing. Speciﬁcally, the TRL cards from earlier stages surfaced potential issues\\nwith imbalanced datasets and the need for speciﬁc synthetic images. These considerations are essential for the\\ndata readiness and testing V&V in the productization requirements. Data quality and availability issues often\\npresent huge blockers because teams discover them too late in the game. Data-readiness is one class of many\\nexample issues teams face without MLTRL, as depicted in Fig. 2.\\n•Level 6 – We were re-using a well-understood model and deployment pipeline in this use-case, meaning our\\nprimary challenge was around data reliability. For the problem of recognizing and classifying trash, building a\\nreliable data source using only real data is almost impossible due to diversity, class imbalance, and annotation\\nchallenges. Therefore we chose to develop a synthetic data generator to create training data. At this MLTRL\\nlevel, we needed to ensure that the synthetic data generator created sufﬁciently diverse data and exposed the\\ncontrols needed to alter the data distribution in production. Therefore, we carefully exposed APIs using the\\nUnity Perception package, which allowed us to control lighting, camera parameters, target and non-target\\nobject placements and counts, and background textures. Additionally, we ensured that the object labeling\\nmatched the real-world annotator instructions and that output data formats matched real-world counterparts.\\nLastly, we established a set of statistical tests to compare synthetic and real-world data distributions. The\\nMLTRL checks ensured that we understood, and in this case, adequately designed our data sources to meet\\nin-production requirements.\\n•Level 7 – From the previous level’s R&D TRL cards and observations, we knew relatively early in produc-\\ntization that we would need to assume bias for the real data sources due to class imbalance and imperfect\\nannotations. Therefore we designed tests to monitor these in the deployed application. MLTRL imposes these\\ncritical deployment tests well ahead of deployment, where we can easily overlook ML-speciﬁc failure modes.\\n•Level 8 – As we suggested earlier, problems that stem from real-world data are near impossible to anticipate\\nand design for, implying the need for level 8 ﬂight-readiness preparations. Given that we were generating\\nsynthetic images (with structured domain randomization) to complement the real data, we created tests for\\ndifferent data distribution shifts at multiple points in the classiﬁcation pipeline. We also implemented thorough\\nshadow tests ahead of deployment to evaluate how susceptible the ML model(s) to performance regressions\\ncaused by data. Additionally, we also implemented these as CI/CD tests over various deployment scenarios (or\\nmobile device computing speciﬁcations). Without these fully covered, documented, and automated, it would\\nbe impossible to pass level 8 review and deploy the technology.\\n•Level 9 – Post-deployment, the monitoring tests prescribed at Levels 8 and 9 and the three main code quality\\ncheckpoints in the MLTRL process help surface hidden performance degradation problems, common with\\ncomplex pipelines of data ﬂows and various models. The switchbacks depicted in Fig. 2 are typical in CV\\nuse-cases. For instance, miscalibrations in models pre-trained on synthetic data and ﬁne-tuned on newer real\\ndata can be common yet difﬁcult to catch. However, the level 7 to 4 switchback is designed precisely for these\\nchallenges and product improvements.\\nAccelerating scientiﬁc discovery with massive particle physics simulators\\nComputational models and simulation are key to scientiﬁc advances at all scales, from particle physics, to material\\ndesign and drug discovery, to weather and climate science, and to cosmology[ 35]. Many simulators model the forward\\nevolution of a system (coinciding with the arrow of time), such as the interaction of elementary particles, diffusion of\\ngasses, folding of proteins, or evolution of the universe in the largest scale. The task of inference refers to ﬁnding initial\\nconditions or global parameters of such systems that can lead to some observed data representing the ﬁnal outcome\\nof a simulation. In probabilistic programming[ 36], this inference task is performed by deﬁning prior distributions\\nover any latent quantities of interest, and obtaining posterior distributions over these latent quantities conditioned\\non observed outcomes (for example, experimental data) using Bayes rule. This process, in effect, corresponds to\\ninverting the simulator such that we go from the outcomes towards the inputs that caused the outcomes. In the\\n“Etalumis” project[ 37] (“simulate” spelled backwards), we are using probabilistic programming methods to invert\\n12'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='existing, large-scale simulators via Bayesian inference. The project is as an interdisciplinary collaboration of specialists\\nin probabilistic machine learning, particle physics, and high-performance computing, all essential elements to achieve\\nthe project outcomes. Even more, it is a multi-year project spanning multiple countries, companies, university labs, and\\ngovernment research organizations, bringing signiﬁcant challenges in project management, technology coordination\\nand validation. Aided by MLTRL, there were several key challenges to overcome in this project that are common in\\nscientiﬁc-ML projects:\\n•Integrating with legacy systems is common in scientiﬁc and industrial use-cases, where ML methods are\\napplied with existing sensor networks, infrastructure, and codebases. In this case, particle physics domain\\nexperts at CERN are using the SHERPA simulator[ 38], a 1 million line codebase developed over the last\\ntwo decades. Rewriting the simulator for ML use-cases is infeasible due to the codebase size and buried\\ndomain knowledge, and new ML experts would need signiﬁcant onboarding to gain working knowledge of\\nthe codebase. It is also common to work with legacy data infrastructure, which can be poorly organized for\\nmachine learning (let alone preprocessed and clean) and unlikely to have followed best practices such as\\ndataset versioning.\\n•Coupling hardware and software architectures is non-trivial when deploying ML at scale, as performance\\nconstraints are often considered in deployment tests well after model and algorithm development, not to\\nmention the expertise is often split across disjoint teams. This can be exacerbated in scientiﬁc-ML when\\nscaling to supercomputing infrastructure, and working with massive datasets that can be in the terabytes and\\npetabytes.\\n•Interpretability is often a desired feature yet difﬁcult to deliver and validate in practice. Particularly in\\nscientiﬁc ML applications such as this, mechanisms and tooling for domain experts to interpret predictions\\nand models are key for usability (integrating in workﬂows and building trust).\\nTo this end, we will go through the MLTRL levels one by one, demonstrating how they ensure the above scientiﬁc ML\\nchallenges are diligently addressed.\\n•Level 0 – The theoretical developments leading to Etalumis are immense and well discussed in Baydin et\\nal [37]. In particular the ML theory and methods are in a relatively nascent area of ML and mathematics,\\nprobabilistic programming. New territory can present more challenges compared to well-traveled research\\npaths, for instance in computer vision with neural networks. It is thus helpful to have a guiding framework\\nwhen making a new path in ML research, such as MLTRL where early reviews help theoretical ML projects\\nget legs.\\n•Level 1-2 – Running low-level experiments in simple testbeds is generally straightforward when working\\nwith probabilistic programming and simulation; in a sense, this easy iteration over experiments is what\\nPPL are designed for. It was additionally helpful in this project to have rich data grounded in physical\\nconstraints, allowing us to better isolate model behaviors (rather than data assumptions and noise). The\\nMLTRL requirements documentation is particularly useful for the standard PPL experimentation workﬂow:\\nmodel, infer, criticize, repeat (or Box’s loop) [ 39]. The evaluation step (i.e. criticizing the model) can be\\nmore nuanced than checking summary statistics as in deep learning and similar ML workﬂows. It is thus a\\nuseful practice to write down the criticism methods, metrics, and expected results as veriﬁcations for speciﬁc\\nresearch requirements, rather than iterating over Box’s loop without a priori targets. Further, because this\\nresearch project had a speciﬁc target application early in the process (the SHERPA simulator), the project\\ntimeline beneﬁted from recognizing simulator-integration constraints upfront as requirements, not to mention\\ndata availability concerns, which are often overlooked in early R&D levels. It was additionally useful to have\\nCERN scientists as domain experts in the reviews at these R&D levels.\\n•Level 3 – Systems development can be challenging with probabilistic programming, again because it is\\nrelatively nascent and much of the out-of-the-box tools and infrastructure are not there as in most ML and\\ndeep learning. Here in particular there’s a novel (unproven) approach for systems integration: a probabilistic\\nprogramming execution protocol was developed to reroute random number draws in the stochastic simulator\\ncodebase (SHERPA) to the probabilistic programming system, thus enabling the system to control stochastic\\nchoices in SHERPA and run inference on its execution traces, all while keeping the legacy codebase intact! A\\nmore invasive method that modiﬁes SHERPA would not have been acceptable. If it were not for MLTRL forcing\\nsystems considerations this early in the Etalumis project lifecycle, this could have been an insurmountable\\nhurdle later when multiple codebases and infrastructures come into play. By the same token, systems planning\\nhere helped enable the signiﬁcant HPC scaling later: the team deﬁned the need for HPC support well ahead\\nof actually running HPC, in order to build the prototype code in a way that would readily map to HPC (in\\naddition to local or cloud CPU and GPU). The data engineering challenges in this system’s development\\n13'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='nonetheless persist – that is, data pipelines and APIs that can integrate various sources and infrastructures, and\\nnormalize data from various databases – although MLTRL helps consider these at the an earlier stage that can\\nhelp inform architecture design.\\n•Level 4 – The natural “embedded switchback” from Level 4 to 2 (see the Methods section) provided an efﬁcient\\npath toward developing an improved, amortized inference method–i.e., using a computationally expensive\\ndeep learning based inference algorithm to train only once, in order to then do fast, repeated inference in the\\nSHERPA model. Leveraging cyclic R&D methods, the Etalumis project could iteratively improve inference\\nmethods without stalling the broader system development, ultimately producing the largest scale posterior\\ninference in a Turing-complete probabilistic programming system. Achieving this scale through iterative R&D\\nalong the main project lifecycle was additionally enabled by working with with NERSC engineers and their\\nCori supercomputer to progressively scale smaller R&D tests to the goal supercomputing deployment scenario.\\nTypical ML workﬂows that follow simple linear progressions[ 6,40] would not enable ramping up in this\\nfashion, and can actual prevent scaling R&D to production due to lack of systems engineering processes (like\\nMLTRL) connecting research to deployment.\\n•Level 5 – Multi-org international collaborations can be riddled with communication and teamwork issues,\\nin particular at this pivotal stage where teams transition from R&D to application and product development.\\nFirst, MLTRL as a lingua franca was key to the team effort bringing Etalumis proof-of-concept into the\\nlarger effort of applying it to massive high-energy physics simulators. It was also critical at this stage to\\nclearly communicate end-user requirements across the various teams and organizations, which must be deﬁned\\nin MLTRL requirements docs with V&V measures – the essential science-user requirements were mainly\\nfor model and prediction interpretability, uncertainty estimation, and code usability. If there are concerns\\nover these features, MLTRL switchbacks can help to quickly cycle back and improve modeling choices in a\\ntransparent, efﬁcient way – generally in ML projects, these fundamental issues with usability are caught too\\nlate, even after deployment. In the probabilistic generative model setting we’ve deﬁned in Etalumis, Bayesian\\ninference gives results that are interpretable because they include exact locations and processes in the model\\nthat are associated with each prediction. Working with ML methods that are inherently interpretable, we are\\nwell-positioned to deliver interpretable interfaces for the end-users later in the project lifecycle.\\n•Level 6-9 – The standard MLTRL protocol apply in these application-to-deployment stages, with several\\nEtalumis-speciﬁc highlights. First, given the signiﬁcant research contributions in both probabilistic pro-\\ngramming and scientiﬁc-ML, it’s important to share the code publicly. The development and deployment\\nof the open-source code repository PPXvibranched into a separate MLTRL path from the Etalumis path\\nfor deployment at CERN. It’s useful to have systems engineering enable clean separation of requirements,\\ndeployments, etc. when there are different development and product lifecycles originating from a common\\nparent project. For example, in this case it was useful to employ MLTRL switchbacks in the open-sourcing\\nprocess, isolated from the CERN application paths, in order to add support for additional programming\\nlanguages so PPX can apply to more scientiﬁc simulators – both directions beneﬁted signiﬁcantly the from\\nthe data pipelines considerations brought up levels earlier, where open-sourcing required different data APIs\\nand data transformations to enable broad usability. Second, related to the open-source code deliverable and\\nthe scientiﬁc ML user requirements we noted above, the late stages of MLTRL reviews include higher level\\nstakeholders and speciﬁc end-users, yet again enforcing these scientiﬁc usability requirements are met. An\\nexample result of this in Etalumis is the ability to output human-readable execution traces of the SHERPA\\nruns and inference, enabling never before possible step-by-step interpretability of the black-box simulator.\\nThe scientiﬁc ML perspective additionally brings to forefront an end-to-end data perspective that is pertinent in\\nessentially all ML use-cases: these systems are only useful to the extent they provide comprehensive data analyses that\\nintegrate the data consumed and generated in these workﬂows, from raw domain data to machine-learned models. These\\ndata analyses drive reproducibility, explainability, and experiment data understanding, which are critical requirements\\nin scientiﬁc endeavors and ML broadly.\\nCausal inference & ML in medicine\\nUnderstanding cause and effect relationships is crucial for accurate and actionable decision-making in many settings,\\nfrom healthcare and epidemiology, to economics and government policy development. Unfortunately, standard\\nmachine learning algorithms can only ﬁnd patterns and correlation in data, and as correlation is not causation, their\\npredictions cannot be conﬁdently used for understanding cause and effect. Indeed, relying on correlations extracted\\nfrom observational data to guide decision-making can lead to embarrassing, costly, and even dangerous mistakes,\\nsuch as concluding that asthma reduces pneumonia mortality risk [ 41], and that smoking reduces risk of developing\\nvigithub.com/pyprob/ppx\\n14'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='severe COVID-19 [ 42]. Fortunately, there has been much recent development in a ﬁeld known as causal inference that\\ncan quantitatively make sense of cause and effect from purely observational data[ 43]. The ability of causal inference\\nalgorithms to quantify causal impact rests on a number of important checks and assumptions–beyond those employed\\nin standard machine learning or purely statistical methodology–that must be carefully deliberated over during their\\ndevelopment and training. These speciﬁc checks and assumptions are as follows:\\n•Specifying cause-and-effect relationships between relevant variables– One of the most important assump-\\ntions underlying causal inference is the structure of the causal relations between quantities of interest. The\\ngold standard for determining causal relations is to perform a randomised controlled trial, but in most cases\\nthese cannot be employed due to ethical concerns, technological infeasibility, or prohibitive cost. In these\\nsituations, domain experts have to be consulted to determine the causal relationships. It is important in these\\nsituations to carefully address the manner in which such domain knowledge was extracted from experts, the\\nnumber and diversity of experts involved, the amount of consensus between experts, and so on. The need for\\ncareful documentation of this knowledge and its periodic review is made clear in the MLTRL framework, as\\nwe shall see below.\\n•Identiﬁability– Another vital component of building causal models is whether the causal question of interest\\nisidentiﬁable from the causal structure speciﬁed for the model together with observational (and sometimes\\nexperimental) data.\\n•Adjusting for and monitoring confounding bias– An important aspect of causal model performance, not\\npresent in standard machine learning algorithms, is confounding bias adjustment. The standard approach is to\\nemploy propensity score matching to remove such bias. However, the quality of bias adjustment achieved in\\nany speciﬁc instance with such propensity-based matching methods needs to be checked and documented,\\nwith alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[44].\\n•Sensitivity analysis– As causal estimates are based on generally untestable assumptions, such as observing all\\nrelevant confounders, it is vital to determine how sensitive the resulting predictions are to potential violations\\nof these assumptions.\\n•Consistency– It is crucial to understand if the learned causal estimate provably converges to the true causal\\neffect in the limit of inﬁnite sample size. However, causal models cannot be validated by standard held-out\\ntests, but rather require randomization or special data collection strategies to evaluate their predictions [ 45,46].\\nThe MLTRL framework makes transparent the need to carefully document and defend these assumptions, thus ensuring\\nthe safe and robust creation, deployment, and maintenance of causal models. We elucidate this with recent work by\\nRichens et al.[ 47], developing a causal approach to computer-assisted diagnosis which outperforms previous purely\\nmachine learning based methods. To this end, we will go through the MLTRL levels one by one, demonstrating how\\nthey ensure the above speciﬁc checks and assumptions are naturally accounted for. This should provide a blueprint for\\nhow to employ the MLTRL levels in other causal inference applications.\\n•Level 0 – When initially faced with a causal inference task, the ﬁrst step is always to understand the causal\\nrelationships between relevant variables. For instance, in Richens et al. [ 47], the ﬁrst step toward building\\nthe diagnostic model was specifying the causal relationships between the diverse set risk factors, diseases,\\nand symptoms included in the model. To learn these relations, doctors and healthcare professionals were\\nconsulted to employ their expansive medical domain knowledge which was robustly evaluated by additional\\nindependent groups of healthcare professionals. The MLTRL framework ensured this issue is dealt with and\\ndocumented correctly, as such knowledge is required to progress from Level 0; failure to do this has plagued\\nsimilar healthcare AI projects [48].\\nThe next step of any causal analysis is to understand whether the causal question of interest is uniquely\\nidentiﬁable from the causal structure speciﬁed for the model together with observational and experimental data.\\nIn this medical diagnosis example, identiﬁcation was crucial to establish, as the causal question of interest,\\n“would the observed symptoms not be present had a speciﬁc disease been cured?”, was highly non-trivial.\\nAgain, MLTRL ensures this vital aspect of model building is carefully considered, as a mathematical proof of\\nidentiﬁability would be required to graduate from Level 0.\\nWith both the causal structure and identiﬁability result in hand, one can progress to Level 1.\\n•Level 1 – At this level, the goal is to take the estimand for the identiﬁed causal question of interest and\\ndevise a way to estimate it from data. To do this one will need efﬁcient ways to adjust for confounﬁng bias.\\nThe standard approach is to employ propensity score-based methods to remove such bias when the target\\ndecision is binary, and use multi-stage ML models adhering to the assumed causal structure[ 49] for continuous\\ntarget decisions (and high-dimensional data in general). However, the quality of bias adjustment achieved in\\nany speciﬁc instance with propensity-based matching methods needs to be checked and documented, with\\n15'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[ 44]. As\\nabove, MLTRL ensures transparency and adherence to this important aspect of causal model development, as\\nwithout it a project cannot graduate from Level 1. Even more, MLTRL ensures tests for confounding bias\\nare developed early-on and maintained throughout later stages to deployment. Still, in many cases, it is not\\npossible to completely remove confounding in the observed data. TRL Cards offer a transparent way to declare\\nspeciﬁc limitations of a causal ML method.\\n•Level 2 – PoC-level tests for causal models must go beyond that of typical ML models. As discussed above,\\nto ensure the estimated causal effects are robust to the assumptions required for their derivation, sensitivity\\nto these assumptions must be analysed. Such sensitivity analysis is often limited to R&D experiments or\\na post-hoc feature of ML products. MLTRL on the other hand requires this throughout the lifecycle as\\ncomponents of ML test suites and gated reviews. In the case of causal ML, best practice is to employ sensitivity\\nanalysis for this robustness check[ 50]. MLTRL ensures this check is highlighted and adhered to, and no model\\nwill end up graduating Level 2–let alone being deployed–unless it is passed.\\n•Level 3 – Coding best practices, as in general ML applications.\\n•Level 4-5 – There are additional tests to consider when taking causal models from research to production,\\nin particular at Level 4–proof of concept demonstration in a real scenario. Consistency , for example, is an\\nimportant property of causal methods that informs us whether the method provably converges to the true\\ncausal graph in the limit of inﬁnite sample size. Quantifying consistency in the test suite is critical when\\ndatasets change from controlled laboratory settings to open-world, and when the application scales. And\\nPoC validation steps are more efﬁcient with MLTRL because the process facilitates early speciﬁcation of the\\nevaluation metric for a causal model in Level 2. Causal models cannot be validated by standard held-out tests,\\nbut rather require randomization or special data collection strategies to evaluate their predictions[ 45,46]. Any\\ndifﬁculty in evaluating the model’s predictions will be caught early and remedied.\\n•Level 6-9 – With the the causal ML components of this technology developed reliably in the previous levels,\\nthe rest of the levels developing this technology focused on general medical-ML deployment challenges. For\\nthe most part, data governance, privacy, and management that was detailed earlier in the neuropathology\\nMLTRL use-case, as well as on-premises deployment.\\nAI for open-source space sciences\\nThe CAMS (Cameras for Allsky Meteor Surveillance) project [ 51], established in 2010 by NASA, uses hundreds of\\noff-the-shelf CCTV cameras to capture the meteor activity in the night sky. Initially, resident scientists would retrieve\\nhard-disks containing video data captured each night and perform manual triangulation of tracks or streaks of light\\nin the night sky, and compute a meteor’s trajectory, orbit, and lightcurve. Each solution was manually classiﬁed as a\\nmeteor or not (i.e., planes, birds, clouds, etc). In 2017, a project run by the Frontier Development Labvii[52], the AI\\naccelerator for NASA and ESA, aimed to automate the data processing pipeline and replicate the scientists thought\\nprocess to build an ML model that identiﬁes meteors in the CAMS project [ 53,54]. The data automation led to\\norders of magnitude improvements in operational efﬁciency of the system, and allowed new contributors and amateur\\nastronomers to start contributing to meteor sightings. Additionally, a novel web tool allowed anybody anywhere to\\nview the meteors detected in the previous night. The CAMS camera system has had six-fold global expansion of the\\ndata capture network, discovered ten new meteor showers, contributed towards instrumental evidence of previously\\npredicted comets, and helped calculate parent bodies of various meteor showers. CAMS utilized the MLTRL framework\\nto progress as described:\\n•Level 1 – Understanding the domain and data is a prerequisite for any ML development. Extensive data\\nexploration elucidated visual differences between objects in the night sky such as meteors, satellites, clouds,\\ntail lights of planes, light from the eyes of cats peering into cameras, trees, and other tall objects visible in\\nthe moonlight. This step helped (1) understand visual properties of meteors that later deﬁned the ML model\\narchitecture, and (2) mitigate impact of data imbalance by proactively developing domain-oriented strategies.\\nThe results are well-documented on a datasheet associated with the TRL card, and discussed at the stage\\nreview. This MLTRL documentation forced us to consider data sharing and other privacy concerns at this early\\nconceptualization stage, which is certainly relevant considering CAMS is for open-source and gathering data\\nfrom myriad sources.\\n•Level 2-3 – The agile and non-monotonic (or non-linear) development prescribed by MLTRL allowed the\\nteam to ﬁrst develop an approximate end-to-end pipeline that offered a path to ML model deployment and\\nquick turnaround time to incorporate feedback from the regular gated reviews. Then, with relatively quicker\\nviiThe NASA Frontier Development Lab and partners open-source the code and data via the SpaceML platform: spaceml.org\\n16'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='experimentation, the team could improve on the quality of not just the ML model, but also scale up the systems\\ndevelopment simultaneously in a non-monotonic development cycle.\\n•Level 4 – With the initial pipeline in place, scalable training of baselines and initial models on real challenging\\ndatasets ensued. Throughout the levels, the MLTRL gated reviews were essential for making efﬁcient progress\\nwhile ensuring robustness and functionality that meets stakeholder needs. At this stage we highlight speciﬁc\\nadvantages of the MLTRL review processes that had instrumental effect on the project success: With the\\nrequired panel of mixed ML researchers and engineers, domain scientists, and product managers, the stage 4\\nreviews stressed the signiﬁcance of numerical improvements and comparison to existing baselines, and helped\\nidentify and overcome issues with data imbalance. The team likely would have overlooked these approaches\\nwithout the review from peers in diverse roles and teams. In general, the evolving panel of reviewers at\\ndifferent stages of the project was essential for covering a variety of veriﬁcation and validation measures –\\nfrom helping mitigate data challenges, to open-source code quality.\\n•Level 5 – To complete this R&D-to-productization level, a novel web tool called the NASA CAMS Meteor\\nShower Portalviiiwas created that allowed users to view meteor shower activity from the previous night and\\nverify meteor predictions generated by the ML model. This app development was valuable for A/B testing,\\nvalidating detected meteors and classiﬁed new meteor showers with human-AI interaction, and demonstrating\\nreal-world utility to stakeholders in review. ML processes without MLTRL miss out on these valuable\\ndevelopment by overlooking the need for such a demo tool.\\n•Level 6 – Application development was naturally driven by end-user feedback from the web app in level 5 –\\nwithout MLTRL it’s unlikely the team would be able to work with early productization feedback. With almost\\nreal time feedback coming in daily, newer methods for improving robustness of meteor identiﬁcation led to\\nresearching and developing a unique augmentation technique, resulting in the state of the art performance of\\nthe ML model. Further application development led to incorporating features that were in demand by users of\\nthe NASA CAMS Meteor Shower Portal: include celestial reference points through constellations, add ability\\nto zoom in/out and (un)cluster showers, and provide tooling for scientiﬁc communication. The coordination of\\nthese features into product-caliber codebase resulted in the release of the NASA CAMS Meteor Shower Portal\\n2.0 that was built by a team of citizen scientists – again we found the speciﬁc checkpoints in the MLTRL\\nreview were crucial for achieving these goals.\\n•Level 7 – Integration was particularly challenging in two ways. First, integrating the ML and data engineering\\ndeliverables with the existing infrastructure and tools of the larger CAMS system, which had started devel-\\nopment years earlier with other teams in partner organizations, required quantiﬁable progress for verifying\\nthe tech-readiness of ML models and modules. The use of technology readiness levels provided a clear and\\nconsistent metric for the maturity of the ML and data technologies, making for clear communication and\\nefﬁcient project integration. Without MLTRL it is difﬁcult to have a conversation, let alone make progress, to-\\nwards integrating AI/ML and data subsystems and components. Second, integrating open-source contributions\\ninto the main ML subsystem was a signiﬁcant challenge alleviated with diligent veriﬁcation and validation\\nmeasures from MLTRL, as well as quantifying robustness with ML testing suites (using scoring measures like\\nthat of the ML Testing Rubric[20], and devising a checklist based on metamorphic testing[18]).\\n•Level 8 – CAMS, like many datasets in practice, consisted of a smaller labeled subset and a much larger\\nunlabeled set. In an attempt to additionally increase robustness of the ML subsystem ahead of “ﬂight readiness”,\\nwe looked to active learning [ 55,56] techniques to leverage the unlabeled data. Models using an initial version\\nof this approach, where results of the active learning provided “weak” labels, resulted in consumption of the\\nentire decade long unlabelled data collected by CAMS and slightly higher scores on deployment tests. Active\\nlearning showed to be a promising feature and was switched back to level 7 for further development towards\\nthe next deployment version, so as not to delay the rest of the project.\\n•Level 9 – The ML components in CAMS require continual monitoring for model and data drifts, such as\\nchanges in weather, smoke, and cloud patterns that affect the view of the night sky. The data drifts may also be\\nspeciﬁc to locations, such as ﬁreﬂies and bugs in CAMS Australia and New Zealand stations which appear as\\nfalse positives. The ML pipeline is largely automated with CI/CD, runs regular regression tests, and production\\nof benchmarks. Manual intervention can be triggered when needed, such as sending low conﬁdence meteors for\\nveriﬁcation to scientists in the CAMS project. The team also regularly releases the code, models, and web tools\\non the open-source space sciences and exploration ML toolbox, SpaceMLix. Through the SpaceML community\\nand partner organizations, CAMS continually improves with feature requests, debugging, and improving data\\npractices, while tracking progress with standard software release cycles and MLTRL documentation.\\nviiimeteorshowers.seti.org\\nixspaceml.org\\n17'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='BEYOND SOFTWARE ENGINEERING\\nSoftware engineering (SWE) practices vary signiﬁcantly across domains and industries. Some domains, such as medical\\napplications, aerospace, or autonomous vehicles rely on a highly rigorous development process which is required\\nby regulations. Other domains, for example advertising and e-commerce are not regulated and can employ a lenient\\napproach to development. ML development should at minimum inherit the acceptable software engineering practices of\\nthe domain. There are, however, several key areas where ML development stands out from SWE, adding its own unique\\nchallenges which even most rigorous SWE practices are not able to overcome.\\nFor instance, the behavior of ML systems is learned from data, not speciﬁed directly in code. The data requirements\\naround ML (i.e., data discovery, management, and monitoring) adds signiﬁcant complexity not seen in other types\\nof SWE. There are many beneﬁts to using a data-oriented architecture (DOA) [48] with the data-ﬁrst workﬂows and\\nmanagement practices prescribed in MLTRL. DOA aims to make the data ﬂowing between elements of business logic\\nmore explicit and accessible with a streaming-based architecture rather than the micro-service architectures that are\\nstandard in software systems. One speciﬁc beneﬁt of DOA is making data available and traceable by design, which\\nhelps signiﬁcantly in the ML logging challenges and data governance needs we discussed in Levels 7-9. Moreover,\\nMLTRL highlights data-related requirements along every step to ensure that the development process considers data\\nreadiness and availability.\\nNot to mention an array of ML-speciﬁc failure modes; for example, models that become miscalibrated due to subtle\\ndata distributional shifts in the deployment setting, resulting in models that are more conﬁdent in predictions than they\\nshould be. MLTRL helps deﬁne ML-speciﬁc testing considerations (levels 5 and 7) to help surface these failure-modes\\nearly. ML opens up new threat vectors across the whole deployment workﬂow that otherwise aren’t risks in software\\nsystems: for example, a poisoning attack to contaminate the training phase of ML systems, or membership inference\\nto see if a given data record was part of the model’s training. MLTRL consider these threat vectors and suggests\\nrelevant risk-identiﬁcation during prototyping and productization phases. More generally, ML codebases have all the\\nproblems for regular code, plus ML-speciﬁc issues at the system level, mainly as a consequence of added complexity\\nand dynamism. The resulting entanglement, for instance, implies that the SWE practice of making isolated changes is\\noften not feasible – Scully et al.[ 57] refer to this as the “changing anything changes everything” principle. Given this\\nconsideration, typical SWE change-management is insufﬁcient. Furthermore, ML systems almost necessarily increase\\nthe technical debt; package-level refactoring is generally sufﬁcient for removing technical debt in software systems, but\\nthis is not the case in ML systems.\\nThese factors and others suggest that inherited software engineering and management practices of a given domain are\\ninsufﬁcient for the successful development of robust and reliable ML systems. But it is not trading off one for the other:\\nMLTRL can be used in synergy with the existing, industry-standard software engineering practices such as agile [ 58]\\nand waterfall [ 59] to handle unique challenges of ML development. Because ML applications are a category of software,\\nall best practices of building and operating software should be extended when possible to the ML application. Practices\\nlike version control, comprehensive testing, continuous integration and continuous deployment are all applicable to ML\\ndevelopment. MLTRL provides a framework that helps extend SWE building and operating practices that are acceptable\\nin a given domain to tackle the unique challenges of ML development.\\nRELATED WORKS\\nA recent case study from Microsoft Research [ 40] similarly identiﬁes a few themes describing how ML is not equal to\\nsoftware engineering, and recommends a linear ML workﬂow with steps for data preparation through modeling and\\ndeploying. They deﬁne an effective workﬂow for isolated development of an ML model, but this approach does not\\nensure the technology is actually improving in quality and robustness. Their process should be repeated at progressive\\nstages of development in the broader ML and data technology lifecycle. If applied in the MLTRL framework, the\\nspeciﬁc ingredients of the ML model workﬂow – that is, people, software, tests, objectives, etc. – evolve over time and\\nsubsequent stages as the technologies mature.\\nThere exist many recommended workﬂows for speciﬁc ML methods and areas of pipelines. For instance, a more\\niterative process for Bayesian ML [ 60] and even more speciﬁcally for probabilistic programming [ 39], a data mining\\nprocess deﬁned in 2000 that remains widely used [ 61], others for describing data iterations [ 62], and human-computer\\ninteraction cycles [ 63]. In these recommended workﬂows and others, there’s an important distinction between their\\ncycles and “switchback” mechanisms in MLTRL. Their cycles suggest to generically iterate over a data-modeling-\\nevaluation-deployment process. Switchbacks, on the other hand, are speciﬁc, purpose-driven workﬂows for dialing\\npart(s) of a project to an earlier stage – this doesn’t simply mean go back and train the model on more data, but rather\\nswitching back regresses the technology’s maturity level (e.g. from level 5 to level 3) such that it must again fulﬁll the\\nlevel-by-level requirements, evaluations and reviews. See the Methods section for more details on MLTRL switchbacks.\\n18'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='In general, iteration is an important part of data, ML, and software processes. MLTRL is unique from the other\\nrecommended processes in many ways, and perhaps most importantly because it considers data ﬂows and ML models\\nin the context of larger systems. These isolated processes (that are speciﬁc to e.g. modeling in prototype development\\nor data wrangling in application development) are synergistic with MLTRL because they can be used within each level\\nof the larger lifecycle or framework. For example, the Bayesian modeling processes [ 39,60] we mentioned above\\nare really useful to guide developers of probabilistic ML approaches. But there are important distinctions between\\nexecuting these modeling steps and cycles in a well-deﬁned prototyping environment with curated data and minimal\\nresponsibilities, versus a production environment riddled with sparse and noisy data, that interacts with the physical\\nworld in non-obvious ways, and can carry expensive (even hidden) consequences. MLTRL provides the necessary,\\nholistic context and structure to use these and other development processes reliably and responsibly.\\nAlso related to our work, Google teams have proposed ML testing recommendations [ 20] and validating the data fed\\ninto ML systems [ 64]. For NLP applications, typical ML testing practices struggle to translate to real-world settings,\\noften overestimating performance capabilities. An effective way to address this is devising a checklist of linguistic\\ncapabilities and test types, as in Ribeiro et al.[ 17]–interestingly their test suite was inspired by metamorphic testing,\\nwhich we suggested earlier in Level 7 for testing systems AI integrations. A survey by Paleyes et al. [ 48] go over\\nnumerous case studies to discuss challenges in ML deployment. They similarly pay special attention to the need for\\nethical considerations, end-user trust, and extra security in ML deployments. On the latter point, Kumar et al. [ 65]\\nprovide a table thoroughly breaking down new threat vectors across the whole ML deployment workﬂow (some of\\nwhich we mentioned above). These works, notably the ML security measures and the quantiﬁcation of an ML test suite\\nin a principled way – i.e., that does not use misguided heuristics such as code coverage – are valuable to include in any\\nML workﬂow including MLTRL, and are synergistic with the framework we’ve described in this paper. These analyses\\nprovide useful insights, but they do not provide a holistic, regimented process for the full ML lifecycle from R&D\\nthrough deployment. An end-to-end approach is suggested by Raji et al.[ 66], but only for the speciﬁc task of auditing\\nalgorithms; components of AI auditing are mentioned in Level 7, and covered throughout in the review processes.\\nSculley et al.[ 57] go into more ML debt topics such as undeclared consumers and data dependencies, and go on to\\nrecommend an ML Testing Rubric as a production checklist [ 20]. For example, testing models by a canary process\\nbefore serving them into production. This, along with similar shadow testing we mentioned earlier, are common in\\nautonomous ML systems, notably robotics and autonomous vehicles. They explicitly call out tests in four main areas\\n(ML infrastructure, model development, features and data, and monitoring of running ML systems), some of which we\\ndiscussed earlier. For example, tests that the training and serving features compute the same values; a model may train\\non logged processes or user input, but is then served on a live feed with different inputs. In addition to the Google ML\\nTesting Rubric, we advocate metamorphic testing : a SWE methodology for testing a speciﬁc set of relations between\\nthe outputs of multiple inputs. True to the checklists in the Google ML Testing Rubric and in MLTRL, metamorphic\\ntesting for ML can have a codiﬁed list of metamorphic relations[18].\\nIn domains such as healthcare there have been the introduction of similar checklists for data readiness – for example,\\nto ensure regulatory-grade real-world-evidence (RWE) data quality [ 67] – yet these are nascent and not yet widely\\naccepted. Applying AI in healthcare has led to developing guidance for regulatory protocol, which is still a work in\\nprogress. Larson et al.[ 68] provide a comprehensive analysis for medical imaging and AI, arriving at several regulatory\\nframework recommendations that mirror what we outline as important measures in MLTRL: e.g., detailed task elements\\nsuch as pitfalls and limitations (surfaced on TRL Cards), clear deﬁnition of an algorithm relative to the downstream\\ntask, deﬁning the algorithm “capability” (Level 5), real-world monitoring, and more.\\nD’amour et al.[ 19] dive into the problem we noted earlier about model miscalibration. They point to the trend in machine\\nlearning to develop models relatively isolated from the downstream use and larger system, resulting in underspeciﬁcation\\nthat handicaps practical ML pipelines. This is largely problematic in deep learning pipelines, but we’ve also noted this\\nrisk in the case of causal inference applications. Suggested remedies include stress tests –empirical evaluations that\\nprobe the model’s inductive biases on practically relevant dimensions–and in general the methods we deﬁne in Level 7.\\nLIMITATIONS, RESPONSIBILITIES, and ETHICS\\nMLTRL has been developed, deployed, iterated, and validated in myriad environments, as demonstrated by the previous\\nexamples and many others. Nonetheless we strongly suggest that MLTRL not be viewed as a cure-all for machine\\nlearning systems engineering. Rather, MLTRL provides mechanisms to better enable ML practitioners, teams, and\\nstakeholders to be diligent and responsible with these technologies and data. That is, one cannot implement MLTRL in\\nan organization and turn a blind eye to the many data, ML, and integration challenges we’ve discussed here. MLTRL is\\nanalogous to a pilot’s checklist, not autopilot.\\n19'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='MLTRL is intended to be complimentary to existing software development methodologies, not replace or alter them.\\nSpeciﬁcally, whether the team uses agile or waterfall methods, MLTRL can be adopted to help deﬁne and structure\\nphases of the project, as well as the success criteria of each stage. In context of the software development process, the\\npurpose of MLTRL is to help the team minimize the technical dept and risk associated with the delivery of an ML\\napplication by helping the development team ask necessary questions.\\nWe discussed many data challenges and approaches in the context of MLTRL, and should highlight again the importance\\nof data considerations in any ML initiative. The data availability and quality can severely limit the ability to develop and\\ndeploy ML, whether MLTRL is used or not. It is again the responsibility of the ML practitioners, teams, and stakeholders\\nto gather, use, and distribute data in safe, legal, ethical ways. MLTRL helps do so with rigor and transparency, but\\nagain is not a solution for data bias. We recommend these recent works on data bias in ML: [ 69,70,71,72,73].\\nFurther, AI/ML ethics is a continuously evolving, multidisciplinary space – see [ 5]. MLTRL aims to prioritize ethics\\nconsiderations at each level of the framework, and would do well to also evolve over time with the broader AI/ML\\nethics developments.\\nCONCLUSION\\nWe’ve described Machine Learning Technology Readiness Levels (MLTRL) , an industry-hardened systems engineering\\nframework for robust, reliable, and responsible machine learning. MLTRL is derived from the processes and testing\\nstandards of spacecraft development, yet lean and efﬁcient for ML, data, and software workﬂows. Examples from\\nseveral organizations across industries demonstrate the efﬁcacy of MLTRL for AI and ML technologies, from research\\nand development through productization and deployment, in important domains such as healthcare and physics, with\\nemphasis on data readiness amongst other critical challenges. Our aim is MLTRL works in synergy with recent\\napproaches in the community focused on diligent data-readiness, privacy and security, and ethics. Even more, MLTRL\\nestablishes a much-needed lingua franca for the AI ecosystem, and broadly for AI in the worlds of science, engineering,\\nand business. Our hope is that our systems framework is adopted broadly in AI and ML organizations, and that\\n“technology readiness levels” becomes common nomenclature across AI stakeholders – from researchers and engineers\\nto sales-people and executive decision-makers.\\nMethods\\nGated reviews\\nAt the end of each stage is a dedicated review period: (1) Present the technical developments along with the requirements\\nand their corresponding veriﬁcation measures and validation steps, (2) make key decisions on path(s) forward (or\\nbackward) and timing, and (3) debrief the processx. As in the gated reviews deﬁned by TRL used by NASA, DARPA, et\\nal., MLTRL stipulates speciﬁc criteria for review at each level, as well as calling out speciﬁc key decision points (noted\\nin the level descriptions above). The designated reviewers will “graduate” the technology to the next level, or provide a\\nlist of speciﬁc tasks that are still needed (ideally with quantitative remarks). After graduation at each level, the working\\ngroup does a brief post-mortem; we ﬁnd that a quick day or two pays dividends in cutting away technical debt and\\nimproving team processes. Regular gated reviews are essential for making efﬁcient progress while ensuring robustness\\nand functionality that meets stakeholder needs. There are several important mechanisms in MLTRL reviews that are\\nspeciﬁcally useful with AI and ML technologies: First, the review panels evolve over a project lifecycle, as noted\\nbelow. Second, MLTRL prescribes that each review runs through an AI ethics checklist deﬁned by the organization; it is\\nimportant to repeat this at each review, as the review panel and stakeholders evolve considerably over a project lifecycle.\\nAs previously described in the levels deﬁnitions, including ethics reviews as an integral part of early system development\\nis essential for informing model speciﬁcations and avoiding unintended biases or harm[74] after deployment.\\nTRL “Cards”\\nIn Figure 3 we succinctly showcase a key deliverable: TRL Cards . The model cards proposed by Google [ 75] are a useful\\ndevelopment for external user-readiness with ML. On the other hand, our TRL Cards aim to be more information-dense,\\nlike datasheets for medical devices and engineering tools – see the open-source TRL Card repo for examples and\\ntemplates (to be released at github.com/alan-turing-institute). These serve as “report cards” that grow and improve upon\\ngraduating levels, and provide a means of inter-team and cross-functional communication. The content of a TRL Card\\nis roughly in two categories: project info, and implicit knowledge. The former clearly states info such as project owners\\nxMLTRL should include regular debriefs and meta-evaluations such that process improvements can be made in a data-driven,\\nefﬁcient way (rather than an annual meta-review). MLTRL is a high-level framework that each organization should operationalize in\\na way that suits their speciﬁc capabilities and resources.\\n20'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='and reviewers, development status, and semantic versioning–not just for code, also for models and data. In the latter\\ncategory are speciﬁc insights that are typically siloed in the ML development team but should be communicated to\\nother stakeholders: modeling assumptions, dataset biases, corner cases, etc. With the spread of AI and ML in critical\\napplication areas, we are seeing domain expert consortiums deﬁning AI reporting guidelines – e.g., Rivera et al.[ 76]\\ncalling for clinical trials reports for interventions involving AI – which will greatly beneﬁt from the use of our TRL\\nreporting cards. We stress that these TRL Cards are key for the progression of projects, rather than documentation\\nafterthoughts. The TRL Cards thus promote transparency and trust, within teams and across organizations. TRL Card\\ntemplates will be open-sourced upon publication of this work, including methods for coordinating use with other\\nreporting tools such as “Datasheets for Datasets” [24].\\nRisk mitigation\\nIdentifying and addressing risks in a software project is not a new practice. However, akin to the MLTRL roots in\\nspacecraft engineering, risk is a “ﬁrst-class citizen” here. In the deﬁnition of technical and product requirements, each\\nentry has a calculation of the form risk =p(failure )×value , where the value of a component is an integer 1−10.\\nBeing diligent about quantifying risks across the technical requirements is a useful mechanism for ﬂagging ML-related\\nvulnerabilities that can sometimes be hidden by layers of other software. MLTRL also speciﬁes that risk quantiﬁcation\\nand testing strategies are required for sim-to-real development. That is, there is nearly always a non-trivial gap in\\ntransferring a model or algorithm from a simulation testbed to the real world. Requiring explicit sim-to-real testing\\nsteps in the workﬂow helps mitigate unforeseen (and often hazardous) failures. Additionally, comprehensive ML test\\ncoverage that we mention throughout this paper is a critical strategy for mitigating risks anduncertainties: ML-based\\nsystem behavior is not easily speciﬁed in advance, but rather depends on dynamic qualities of the data and on various\\nmodel conﬁguration choices[20].\\nNon-monotonic, non-linear paths\\nWe observe many projects beneﬁt from cyclic paths, dialing components of a technology back to a lower level. Our\\nframework not only encourages cycles, we make them explicit with “switchback mechanisms” to regress the maturity\\nof speciﬁc components in an AI system:\\n1.Discovery switchbacks occur as a natural mechanism – new technical gaps are discovered through systems\\nintegration, sparking later rounds of component development[ 77]. These are most common in the R&D levels,\\nfor example moving a component of a proof-of-concept technology (at Level 4) back to proof-of-principle\\ndevelopment (Level 2).\\n2.Review switchbacks result from gated reviews, where speciﬁc components or larger subsystems may be dialed\\nback to earlier levels. This switchback is one of the “key decision points” in the MLTRL project lifecycle\\n(as noted in the Levels deﬁnitions), and is often a decision driven by business-needs and timing rather than\\ntechnical concerns (for instance when mission priorities and funds shift). This mechanism is common from\\nLevel 6/7 to 4, which stresses the importance of this R&D to product transition phase (see Figure 2 (left)).\\n3.Embedded switchbacks are predeﬁned in the MLTRL process. For example, a predeﬁned path from 4 to 2, and\\nfrom 9 to 4. In complex systems, particularly with AI technologies, these built-in loops help mitigate technical\\ndebt and overcome other inefﬁciencies such as noncomprehensive V&V steps.\\nWithout these built-in mechanisms for cyclic development paths, it can be difﬁcult and inefﬁcient to build systems of\\nmodules and components at varying degrees of maturity. Contrary to traditional thought that switchback events should\\nbe suppressed and minimized, in fact they represent a natural and necessary part of the complex technology development\\nprocess – efforts to eliminate them may stiﬂe important innovations without necessarily improving efﬁciency. This is\\na fault of the standard monotonic approaches in AI/ML projects, stage-gate processes, and even the traditional TRL\\nframework.\\nIt is also important to note that most projects do not start at Level 0; very few ML companies engage in this low-level\\ntheoretical research. For example, a team looking to use an off-the-shelf object recognition model could start that\\ntechnology at Level 3, and proceed with thorough V&V for their speciﬁc datasets and use-cases. However, no technology\\ncan skip levels after the MLTRL process has been initiated. The industry default (that is, without implementing MLTRL)\\nis to ignorantly take pretrained models, run ﬁne tuning on their speciﬁc data, and jump to deployment, effectively\\nskipping Levels 5 to 7. Additionally, we ﬁnd it is advantageous to incorporate components from other high-TRL ranking\\nprojects while starting new projects; MLTRL makes the veriﬁcation and validation (V&V) steps straightforward for\\nintegrating previously developed ML components.\\n21'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='Evolving people, objectives, and measures\\nAs suggested earlier, much of the practical value of MLTRL comes at the transition between levels. More precisely,\\nMLTRL manages these oft neglected transitions explicitly as evolving teams, objectives, and deliverables. For instance,\\nthe team (or working group) at Level 3 is mostly AI Research Engineers, but at Level 6 is mixed Applied AI/SW\\nEngineers mixed with product managers and designers. Similarly, the review panels evolve from level to level, to match\\nthe changing technology development objectives. What the reviewers reference similarly evolves: notice in the level\\ndeﬁnitions that technical requirements and V&V guide early stages, but at and after Level 6 the product requirements\\nand V&V takeover – naturally, the risk quantiﬁcation and mitigation strategies evolve in parallel. Regarding the\\ndeliverables, notably TRL Cards and risk matrices[ 22] (to rank and prioritize various science, technical, and project\\nrisks), the information develops and evolves over time as the technology matures.\\nQuantiﬁable progress\\nBy deﬁning technology maturity in a quantitative way, MLTRL enables teams to accurately and consistently deﬁne\\ntheir ML progress metrics. Notably industry-standard “objectives and key results” (OKRs) and “key performance\\nindicators” (KPIs) [ 78] can be deﬁned as achieving certain readiness levels in a given period of time; this is a preferable\\nmetric in essentially all ML systems which consist of much more than a single performance score to measure progress.\\nEven more, meta-review of MLTRL progress over multiple projects can provide useful insights at the organization\\nlevel. For example, analysis of the time-per-level and the most frequent development paths/cycles can bring to light\\noperational bottlenecks. Compared to conventional software engineering metrics based on sprint stories and tickets, or\\ntime-tracking tools, MLTRL provides a more accurate analysis of ML workﬂows.\\nCommunication and explanation\\nA distinct advantage of MLTRL in practice is the nomenclature: an agreed upon grading scheme for the maturity of\\nan AI technology, and a framework for how/when that technology ﬁts within a product or system, enables everyone\\nto communicate effectively and transparently. MLTRL also acts as a gate for interpretability and explainability–at\\nthe granularity of individual models and algorithms, and more crucially from a holistic, systems standpoint. Notably\\nthe DARPA XAIxiprogram advocates for this advance in developing AI technologies; they suggest interpretability\\nand explainability are necessary at various locations in an AI system to be sufﬁcient for deployment as an AI product,\\notherwise leading to issues with ethics and bias.\\nRobustness via uncertainty-aware ML\\nHow to design a reliable system from unreliable components has been a guiding question in the ﬁelds of computing and\\nintelligence [79]. In the case of AI/ML systems, we aim to build reliable systems with myriad unreliable components:\\nnoisy and faulty sensors, human and AI error, and so on. There is thus signiﬁcant value to quantifying the myriad\\nuncertainties, propagating them throughout a system, and arriving at a notion or measure of reliability. For this reason,\\nalthough MLTRL applies generally to AI/ML methods and systems, we advocate for methods in the class of probabilistic\\nML, which naturally represent and manipulate uncertainty about models and predictions[ 28]. These are Bayesian\\nmethods that use probabilities to represent aleatoric uncertainty , measuring the noise inherent in the observations, and\\nepistemic uncertainty , accounting for uncertainty in the model itself (i.e., capturing our ignorance about which model\\ngenerated the data). In the simplest case, an uncertainty aware ML pipeline should quantify uncertainty at the points of\\nsensor inputs or perception, prediction or model output, and decision or end-user action – McAllister et al.[ 29] suggest\\nthis with Bayesian deep learning models for safer autonomous vehicle pipelines. We can achieve this sufﬁciently well\\nin practice for simple systems. However, we do not yet have a principled, theoretically grounded, and generalizable way\\nof propagating errors and uncertainties downstream and throughout more complex AI systems – i.e., how to integrate\\ndifferent software, hardware, data, and human components while considering how errors and uncertainties propagate\\nthrough the system. This is an important direction of our future work.\\nReferences\\n[1]Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning\\nthat matters. In AAAI , 2018.\\nxiDARPA Explainable Artiﬁcial Intelligence (XAI)\\n22'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='[2]Arnaud de la Tour, Massimo Portincaso, Kyle Blank, and Nicolas Goeldel. The dawn of the deep tech ecosystem. Technical\\nreport, The Boston Consulting Group, 2019.\\n[3] NASA. The NASA systems engineering handbook. 2003.\\n[4] United States Department of Defense. Defense acquisition guidebook. Technical report, U.S. Dept. of Defense, 2004.\\n[5] D. Leslie. Understanding artiﬁcial intelligence ethics and safety. ArXiv , abs/1906.05684, 2019.\\n[6]Google. Machine learning workﬂow. https://cloud.google.com/mlengine/docs/tensorflow/\\nml-solutions-overview . Accessed: 2020-12-13.\\n[7]Alexander Lavin and Gregory Renard. Technology readiness levels for AI & ML. ICML Workshop on Challenges Deploying\\nML Systems , 2020.\\n[8] T. Dasu and T. Johnson. Exploratory data mining and data cleaning. 2003.\\n[9]M. Janssen, P. Brous, Elsa Estevez, L. Barbosa, and T. Janowski. Data governance: Organizing data for trustworthy artiﬁcial\\nintelligence. Gov. Inf. Q. , 37:101493, 2020.\\n[10] B. Shahriari, Kevin Swersky, Ziyu Wang, R. Adams, and N. D. Freitas. Taking the human out of the loop: A review of bayesian\\noptimization. Proceedings of the IEEE , 104:148–175, 2016.\\n[11] Goutham Ramakrishnan, A. Nori, Hannah Murfet, and Pashmina Cameron. Towards compliant data management systems for\\nhealthcare ml. ArXiv , abs/2011.07555, 2020.\\n[12] Umang Bhatt, Alice Xiang, S. Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura,\\nand P. Eckersley. Explainable machine learning in deployment. Proceedings of the 2020 Conference on Fairness, Accountability,\\nand Transparency , 2020.\\n[13] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and V . Smith. Federated learning: Challenges, methods, and future directions.\\nIEEE Signal Processing Magazine , 37:50–60, 2020.\\n[14] T. Ryffel, Andrew Trask, M. Dahl, Bobby Wagner, J. Mancuso, D. Rueckert, and J. Passerat-Palmbach. A generic framework\\nfor privacy preserving deep learning. ArXiv , abs/1811.04017, 2018.\\n[15] A. Madry, Aleksandar Makelov, Ludwig Schmidt, D. Tsipras, and Adrian Vladu. Towards deep learning models resistant to\\nadversarial attacks. ArXiv , abs/1706.06083, 2018.\\n[16] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. ArXiv , abs/1710.11342, 2018.\\n[17] Marco Túlio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models\\nwith checklist. In ACL, 2020.\\n[18] Xiaoyuan Xie, Joshua W. K. Ho, C. Murphy, G. Kaiser, B. Xu, and T. Chen. Testing and validating machine learning classiﬁers\\nby metamorphic testing. The Journal of systems and software , 84 4:544–558, 2011.\\n[19] Alexander D’Amour, K. Heller, D. Moldovan, Ben Adlam, B. Alipanahi, Alex Beutel, C. Chen, Jonathan Deaton, Jacob\\nEisenstein, M. Hoffman, Farhad Hormozdiari, N. Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, M. Lucic,\\nY . Ma, Cory Y . McLean, Diana Mincu, Akinori Mitani, A. Montanari, Zachary Nado, V . Natarajan, C. Nielson, Thomas F.\\nOsborne, R. Raman, K. Ramasamy, Rory Sayres, J. Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, V . Veitch,\\nMax Vladymyrov, Xuezhi Wang, K. Webster, S. Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley. Underspeciﬁcation\\npresents challenges for credibility in modern machine learning. ArXiv , abs/2011.03395, 2020.\\n[20] Eric Breck, Shanqing Cai, E. Nielsen, M. Salib, and D. Sculley. The ml test score: A rubric for ml production readiness and\\ntechnical debt reduction. 2017 IEEE International Conference on Big Data (Big Data) , pages 1123–1132, 2017.\\n[21] A. Botchkarev. A new typology design of performance metrics to measure errors in machine learning regression algorithms.\\nInterdisciplinary Journal of Information, Knowledge, and Management , 14:045–076, 2019.\\n[22] N. Duijm. Recommendations on the use and design of risk matrices. Safety Science , 76:21–31, 2015.\\n[23] Louise Naud and Alexander Lavin. Manifolds for unsupervised visual anomaly detection. ArXiv , abs/2006.11364, 2020.\\n[24] Timnit Gebru, J. Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, H. Wallach, Hal Daumé, and K. Crawford.\\nDatasheets for datasets. ArXiv , abs/1803.09010, 2018.\\n[25] B. Hutchinson, A. Smart, A. Hanna, Emily L. Denton, Christina Greer, Oddur Kjartansson, P. Barnes, and Margaret Mitchell.\\nTowards accountability for machine learning datasets: Practices from software engineering and infrastructure. Proceedings of\\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency , 2021.\\n23'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='[26] P. Schulam and S. Saria. Reliable decision support using counterfactual models. In NIPS 2017 , 2017.\\n[27] Towards trustable machine learning. Nature Biomedical Engineering , 2:709–710, 2018.\\n[28] Zoubin Ghahramani. Probabilistic machine learning and artiﬁcial intelligence. Nature , 521:452–459, 2015.\\n[29] Rowan McAllister, Yarin Gal, Alex Kendall, Mark van der Wilk, A. Shah, R. Cipolla, and Adrian Weller. Concrete problems\\nfor autonomous vehicle safety: Advantages of bayesian deep learning. In IJCAI , 2017.\\n[30] Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I. Avilés-Rivero,\\nChristian Etmann, Cathal McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, Effrossyni Gkrania-Klotsas,\\nJames H. F. Rudd, Evis Sala, and Carola-Bibiane Schönlieb. Common pitfalls and recommendations for using machine learning\\nto detect and prognosticate for covid-19 using chest radiographs and ct scans. Nature Machine Intelligence , 3:199–217, 2021.\\n[31] J. Tobin, Rachel H Fong, Alex Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep\\nneural networks from simulation to the real world. 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems\\n(IROS) , pages 23–30, 2017.\\n[32] Arthur Juliani, Vincent-Pierre Berges, Esh Vckay, Yuan Gao, Hunter Henry, M. Mattar, and D. Lange. Unity: A general\\nplatform for intelligent agents. ArXiv , abs/1809.02627, 2018.\\n[33] Stefan Hinterstoißer, Olivier Pauly, Tim Hauke Heibel, Martina Marek, and Martin Bokeloh. An annotation saved is an\\nannotation earned: Using fully synthetic training for object instance detection. ArXiv , abs/1902.09967, 2019.\\n[34] Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan Hogins, You-Cyuan Jhang, Mohsen Kamalzadeh,\\nBowen Li, Steven Leal, Pete Parisi, Cesar Romero, Wesley Smith, Alex Thaman, Samuel Warren, and Nupur Yadav. Unity\\nperception: Generate synthetic data for computer vision. CoRR , abs/2107.04259, 2021.\\n[35] K. Cranmer, J. Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of the National Academy\\nof Sciences , 117:30055 – 30062, 2020.\\n[36] Jan-Willem van de Meent, Brooks Paige, H. Yang, and Frank Wood. An introduction to probabilistic programming. ArXiv ,\\nabs/1809.10756, 2018.\\n[37] Atilim Günes Baydin, Lei Shao, W. Bhimji, L. Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi,\\nBradley Gram-Hansen, Gilles Louppe, Mingfei Ma, X. Zhao, P. Torr, V . Lee, K. Cranmer, Prabhat, and F. Wood. Etalumis:\\nbringing probabilistic programming to scientiﬁc simulators at scale. Proceedings of the International Conference for High\\nPerformance Computing, Networking, Storage and Analysis , 2019.\\n[38] T. Gleisberg, S. Höche, F. Krauss, M. Schönherr, S. Schumann, F. Siegert, and J. Winter. Event generation with sherpa 1.1.\\nJournal of High Energy Physics , 2009:007–007, 2009.\\n[39] David M. Blei. Build, compute, critique, repeat: Data analysis with latent variable models. 2014.\\n[40] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald C. Gall, Ece Kamar, Nachiappan Nagappan, Besmira\\nNushi, and Thomas Zimmermann. Software engineering for machine learning: A case study. 2019 IEEE/ACM 41st International\\nConference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) , 2019.\\n[41] R. Ambrosino, B. Buchanan, G. Cooper, and Marvin J. Fine. The use of misclassiﬁcation costs to learn rule-based decision\\nsupport models for cost-effective hospital admission strategies. Proceedings. Symposium on Computer Applications in Medical\\nCare , pages 304–8, 1995.\\n[42] Gareth J Grifﬁth, Tim T Morris, Matthew J Tudball, Annie Herbert, Giulia Mancano, Lindsey Pike, Gemma C Sharp, Jonathan\\nSterne, Tom M Palmer, George Davey Smith, et al. Collider bias undermines our understanding of covid-19 disease risk and\\nseverity. Nature communications , 11(1):1–12, 2020.\\n[43] J. Pearl. Theoretical impediments to machine learning with seven sparks from the causal revolution. Proceedings of the\\nEleventh ACM International Conference on Web Search and Data Mining , 2018.\\n[44] T. Nguyen, G. Collins, J. Spence, J. Daurès, P. Devereaux, P. Landais, and Y . Le Manach. Double-adjustment in propensity\\nscore matching analysis: choosing a threshold for considering residual imbalance. BMC Medical Research Methodology , 17,\\n2017.\\n[45] D. Eckles and E. Bakshy. Bias and high-dimensional adjustment in observational studies of peer effects. ArXiv , abs/1706.04692,\\n2017.\\n[46] Yanbo Xu, Divyat Mahajan, Liz Manrao, A. Sharma, and E. Kiciman. Split-treatment analysis to rank heterogeneous causal\\neffects for prospective interventions. ArXiv , abs/2011.05877, 2020.\\n24'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='[47] Jonathan G Richens, C. M. Lee, and Saurabh Johri. Improving the accuracy of medical diagnosis with causal machine learning.\\nNature Communications , 11, 2020.\\n[48] Andrei Paleyes, Raoul-Gabriel Urma, and N. Lawrence. Challenges in deploying machine learning: a survey of case studies.\\nArXiv , abs/2011.09926, 2020.\\n[49] V . Chernozhukov, D. Chetverikov, M. Demirer, E. Duﬂo, Christian L. Hansen, Whitney K. Newey, and J. Robins. Dou-\\nble/debiased machine learning for treatment and structural parameters. Econometrics: Econometric & Statistical Methods -\\nSpecial Topics eJournal , 2018.\\n[50] Victor Veitch and Anisha Zaveri. Sense and sensitivity analysis: Simple post-hoc analysis of bias due to unobserved confounding.\\nNeurIPS 2020, arXiv preprint arXiv:2003.01747 , 2020.\\n[51] P. Jenniskens, P.S. Gural, L. Dynneson, B.J. Grigsby, K.E. Newman, M. Borden, M. Koop, and D. Holman. Cams: Cameras for\\nallsky meteor surveillance to establish minor meteor showers. Icarus , 216(1):40 – 61, 2011.\\n[52] Siddha Ganju, Anirudh Koul, Alexander Lavin, J. Veitch-Michaelis, Meher Kasam, and J. Parr. Learnings from frontier\\ndevelopment lab and spaceml - ai accelerators for nasa and esa. ArXiv , abs/2011.04776, 2020.\\n[53] S. Zoghbi, M. Cicco, A. P. Stapper, A. J. Ordonez, J. Collison, P. S. Gural, S. Ganju, J.-L. Galache, and P. Jenniskens. Searching\\nfor long-period comets with deep learning tools. In Deep Learning for Physical Science Workshop, NeurIPS , 2017.\\n[54] Peter Jenniskens, Jack Baggaley, Ian Crumpton, Peter Aldous, Petr Pokorny, Diego Janches, Peter S. Gural, Dave Samuels, Jim\\nAlbers, Andreas Howell, Carl Johannink, Martin Breukers, Mohammad Odeh, Nicholas Moskovitz, Jack Collison, and Siddha\\nGanju. A survey of southern hemisphere meteor showers. Planetary and Space Science , 154:21 – 29, 2018.\\n[55] D. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active learning with statistical models. In NIPS , 1994.\\n[56] Y . Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. ArXiv , abs/1703.02910, 2017.\\n[57] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young,\\nJean-François Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. In NIPS , 2015.\\n[58] P. Abrahamsson, Outi Salo, Jussi Ronkainen, and Juhani Warsta. Agile software development methods: Review and analysis.\\nArXiv , abs/1709.08439, 2017.\\n[59] Marco Kuhrmann, Philipp Diebold, Jürgen Münch, Paolo Tell, Vahid Garousi, Michael Felderer, Kitija Trektere, Fergal\\nMcCaffery, Oliver Linssen, Eckhart Hanser, and Christian R. Prause. Hybrid software and system development in practice:\\nwaterfall, scrum, and beyond. Proceedings of the 2017 International Conference on Software and System Process , 2017.\\n[60] Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry,\\nPaul-Christian Burkner, and Martin Modrak. Bayesian workﬂow. ArXiv , abs/2011.01808, 2020.\\n[61] P. Chapman, J. Clinton, R. Kerber, T. Khabaza, T. Reinartz, C. Shearer, and R. Wirth. Crisp-dm 1.0: Step-by-step data mining\\nguide. 2000.\\n[62] Fred Hohman, Kanit Wongsuphasawat, Mary Beth Kery, and Kayur Patel. Understanding and visualizing data iteration in\\nmachine learning. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems , 2020.\\n[63] Saleema Amershi, M. Cakmak, W. B. Knox, and T. Kulesza. Power to the people: The role of humans in interactive machine\\nlearning. AI Mag. , 35:105–120, 2014.\\n[64] Eric Breck, Marty Zinkevich, Neoklis Polyzotis, Steven Euijong Whang, and Sudip Roy. Data validation for machine learning.\\n2019.\\n[65] R. Kumar, David R. O’Brien, Kendra Albert, Salomé Viljöen, and Jeffrey Snover. Failure modes in machine learning systems.\\nArXiv , abs/1911.11034, 2019.\\n[66] Inioluwa Deborah Raji, Andrew Smart, Rebecca White, M. Mitchell, Timnit Gebru, B. Hutchinson, Jamila Smith-Loud, Daniel\\nTheron, and P. Barnes. Closing the ai accountability gap: deﬁning an end-to-end framework for internal algorithmic auditing.\\nProceedings of the 2020 Conference on Fairness, Accountability, and Transparency , 2020.\\n[67] R. Miksad and A. Abernethy. Harnessing the power of real-world evidence (rwe): A checklist to ensure regulatory-grade data\\nquality. Clinical Pharmacology and Therapeutics , 103:202 – 205, 2018.\\n[68] D. B. Larson, Hugh Harvey, D. Rubin, Neville Irani, J. R. Tse, and C. Langlotz. Regulatory frameworks for development and\\nevaluation of artiﬁcial intelligence–based diagnostic imaging algorithms: Summary and recommendations. Journal of the\\nAmerican College of Radiology , 2020.\\n25'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='[69] Ninareh Mehrabi, Fred Morstatter, N. Saxena, Kristina Lerman, and A. Galstyan. A survey on bias and fairness in machine\\nlearning. ACM Computing Surveys (CSUR) , 54:1 – 35, 2019.\\n[70] Eirini Ntoutsi, P. Fafalios, U. Gadiraju, Vasileios Iosiﬁdis, W. Nejdl, Maria-Esther Vidal, S. Ruggieri, F. Turini, S. Papadopoulos,\\nEmmanouil Krasanakis, I. Kompatsiaris, K. Kinder-Kurlanda, Claudia Wagner, F. Karimi, Miriam Fernández, Harith Alani,\\nB. Berendt, Tina Kruegel, C. Heinze, Klaus Broelemann, Gjergji Kasneci, T. Tiropanis, and Steffen Staab. Bias in data-driven\\nai systems - an introductory survey. ArXiv , abs/2001.09762, 2020.\\n[71] E. Jo and Timnit Gebru. Lessons from archives: strategies for collecting sociocultural data in machine learning. Proceedings of\\nthe 2020 Conference on Fairness, Accountability, and Transparency , 2020.\\n[72] J. Wiens, W. Price, and M. Sjoding. Diagnosing bias in data-driven algorithms for healthcare. Nature Medicine , 26:25–26,\\n2020.\\n[73] R. Challen, J. Denny, M. Pitt, L. Gompels, T. Edwards, and K. Tsaneva-Atanasova. Artiﬁcial intelligence, bias and clinical\\nsafety. BMJ Quality & Safety , 28:231 – 237, 2019.\\n[74] Z. Obermeyer, B. Powers, C. V ogeli, and S. Mullainathan. Dissecting racial bias in an algorithm used to manage the health of\\npopulations. Science , 366:447 – 453, 2019.\\n[75] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, In-\\nioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. Proceedings of the Conference on Fairness,\\nAccountability, and Transparency , 2019.\\n[76] Samantha Cruz Rivera, Xiaoxuan Liu, A. Chan, A. K. Denniston, and M. Calvert. Guidelines for clinical trial protocols for\\ninterventions involving artiﬁcial intelligence: the spirit-ai extension. Nature Medicine , 26:1351 – 1363, 2020.\\n[77] Z. Szajnfarber. Managing innovation in architecturally hierarchical systems: Three switchback mechanisms that impact practice.\\nIEEE Transactions on Engineering Management , 61:633–645, 2014.\\n[78] H. Zhou and Y . He. Comparative study of okr and kpi. DEStech Transactions on Economics, Business and Management , 2018.\\n[79] J. Neumann. Probabilistic logic and the synthesis of reliable organisms from unreliable components. 1956.\\nAcknowledgements\\nThe authors would like to thank Gur Kimchi, Carl Henrik Ek and Neil Lawrence for valuable discussions about this\\nproject.\\nAuthor contributions statement\\nA.L. conceived of the original ideas and framework, with signiﬁcant contributions towards improving the framework\\nfrom all co-authors. A.L. initiated the use of MLTRL in practice, including the neuropathology test case discussed here.\\nC.G-L. contributed insight regarding causal AI, including the section on counterfactual diagnosis. C.G-L. also made\\nsigniﬁcant contributions broadly in the paper, notably in the Methods descriptions and paper revisions. Si.G. contributed\\nthe spacecraft test case, along with early insights in the framework deﬁnitions. A.V . contributed to the deﬁnition of\\nlater stages involving deployment (as did A.G.), and comparison with traditional software workﬂows. Both E.X. and\\nY .G. provided insights regarding AI in academia, and Y .G. additionally contributed to the uncertainty quantiﬁcation\\nmethods. Su.G. and D.L. contributed the computer vision test case. A.G.B. contributed the particle physics test case,\\nand signiﬁcant reviews of the writeup. A.S. contributed insights related to causal ML and AI ethics. D.N. provided\\nvaluable feedback on the overall framework, and contributed signiﬁcantly with the details on “switchback mechanisms”.\\nS.Z. contributed to multiple paper revisions, with emphasis on clarity and applicability to broad ML users and teams.\\nJ.P. contributed to multiple paper revisions, and to deploying the systems ML methods broadly in practice for Earth and\\nspace sciences. –same goes for C.M., with additional feedback overall on the methods. All co-authors discussed the\\ncontent and contributed to editing the manuscript.\\nCompeting interests\\nThe authors declare no competing interests.\\nAdditional information\\nCorrespondence and requests for materials should be addressed to A.L.\\n26')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebBased Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community. document_loaders import WebBaseLoader\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(web_path=\"https://en.wikipedia.org/wiki/Chicago\",\n",
    "                       bs_kwargs = dict(parse_only=bs4.SoupStrainer(\n",
    "                        class_ = (\"mw-body\")\n",
    "                       ))\n",
    "                       )\n",
    "page = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Chicago'}, page_content='\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nChicago\\n\\n\\n\\n226 languages\\n\\n\\n\\n\\nAcèhAfrikaansAlemannischአማርኛAnarâškielâÆngliscالعربيةAragonésܐܪܡܝܐԱրեւմտահայերէնArmãneashtiArpetanAsturianuAtikamekwAvañe\\'ẽAzərbaycancaتۆرکجهBasa BaliBamanankanবাংলা閩南語 / Bân-lâm-gúБашҡортсаБеларускаяБеларуская (тарашкевіца)भोजपुरीBikol CentralBislamaБългарскиBoarischBosanskiBrezhonegБуряадCatalàЧӑвашлаCebuanoČeštinaChavacano de ZamboangaChi-ChewaChiShonaCorsuCymraegDagbanliDanskDavvisámegiellaDeitschDeutschDiné bizaadDolnoserbskiडोटेलीEestiΕλληνικάEmiliàn e rumagnòlЭрзяньEspañolEsperantoEstremeñuEuskaraفارسیFiji HindiFøroysktFrançaisFryskFulfuldeFurlanGaeilgeGaelgGàidhligGalego贛語Gĩkũyũ𐌲𐌿𐍄𐌹𐍃𐌺客家語 / Hak-kâ-ngîХальмг한국어HausaHawaiʻiՀայերենहिन्दीHornjoserbsceHrvatskiIdoIgboIlokanoBahasa IndonesiaInterlinguaInterlingueᐃᓄᒃᑎᑐᑦ / inuktitutИронIsiXhosaIsiZuluÍslenskaItalianoעבריתJawaಕನ್ನಡKapampanganКъарачай-малкъарქართულიҚазақшаKernowekIkirundiKiswahiliKreyòl ayisyenKriyòl gwiyannenKurdîКыргызчаКырык марыLadinLadinoLatinaLatviešuLëtzebuergeschLietuviųLigureLimburgsLingálaLingua Franca NovaLivvinkarjalaLombardMagyarМакедонскиMalagasyമലയാളംMaltiMāoriमराठीმარგალურიمصرىمازِرونیBahasa Melayu閩東語 / Mìng-dĕ̤ng-ngṳ̄MirandésМокшеньМонголမြန်မာဘာသာNa Vosa VakavitiNederlandsNedersaksiesनेपालीनेपाल भाषा日本語NapulitanoНохчийнNordfriiskNorsk bokmålNorsk nynorskNovialOccitanОлык марийOʻzbekcha / ўзбекчаਪੰਜਾਬੀPälzischپنجابیPapiamentuپښتوPicardPiemontèisTok PisinPlattdüütschPolskiPortuguêsQaraqalpaqshaQırımtatarcaReo tahitiRomânăRomani čhibRumantschRuna SimiРусиньскыйРусскийСаха тылаSängöSarduScotsSeelterskSesothoShqipSicilianuSimple EnglishسنڌيSlovenčinaSlovenščinaŚlůnskiSoomaaligaکوردیSranantongoСрпски / srpskiSrpskohrvatski / српскохрватскиSuomiSvenskaTagalogதமிழ்TaqbaylitTarandíneТатарча / tatarçaతెలుగుTetunไทยТоҷикӣTsetsêhestâheseTürkçeTürkmençeTwiУдмуртBasa UgiУкраїнськаاردوئۇيغۇرچە / UyghurcheVahcuenghVènetoVepsän kel’Tiếng ViệtVolapükVõroWalon文言Winaray吴语ייִדישYorùbá粵語ZazakiZeêuwsŽemaitėška中文Tolışi\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadView sourceView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadView sourceView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata item\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\nWikimedia CommonsWikinewsWikiquoteWikivoyage\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCoordinates: 41°52′55″N 87°37′40″W\\ufeff / \\ufeff41.88194°N 87.62778°W\\ufeff / 41.88194; -87.62778\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\nLargest city in the state of Illinois, United States\\nThis article is about the city in Illinois. For the band, see Chicago (band). For other uses, see Chicago (disambiguation).\\n\\n\\nCity in Illinois, United StatesChicagoCityThe LoopChicago River\"L\" trainWrigley FieldNavy PierArt Institute of ChicagoBuckingham Fountain\\n\\nFlagSealLogoEtymology: Miami-Illinois: shikaakwa (\\'wild onion\\' or \\'wild garlic\\')Nicknames:\\xa0Windy City and othersMottoes:\\xa0Latin: Urbs in Horto (City in a Garden); I WillInteractive map of ChicagoChicagoShow map of IllinoisChicagoShow map of the United StatesCoordinates: 41°52′55″N 87°37′40″W\\ufeff / \\ufeff41.88194°N 87.62778°W\\ufeff / 41.88194; -87.62778[1]CountryUnited StatesStateIllinoisCountiesCook (majority) and DuPage (minority)Settledc.\\u20091780; 244\\xa0years ago\\xa0(1780)Incorporated (town)August\\xa012, 1833; 191 years ago\\xa0(1833-08-12)Incorporated (city)March\\xa04, 1837; 187 years ago\\xa0(1837-03-04)Founded byJean Baptiste Point du\\xa0SableGovernment\\xa0•\\xa0TypeMayor–council\\xa0•\\xa0BodyChicago City Council\\xa0•\\xa0MayorBrandon Johnson (D)\\xa0•\\xa0City ClerkAnna Valencia (D)\\xa0•\\xa0City TreasurerMelissa Conyears-Ervin (D)Area[2]\\xa0•\\xa0City234.53\\xa0sq\\xa0mi (607.44\\xa0km2)\\xa0•\\xa0Land227.73\\xa0sq\\xa0mi (589.82\\xa0km2)\\xa0•\\xa0Water6.80\\xa0sq\\xa0mi (17.62\\xa0km2)Elevation[1] (mean)597.18\\xa0ft (182.02\\xa0m)Highest\\xa0elevation– near Blue Island672\\xa0ft (205\\xa0m)Lowest\\xa0elevation– at Lake Michigan578\\xa0ft (176\\xa0m)Population\\xa0(2020)[3]\\xa0•\\xa0City2,746,388\\xa0•\\xa0Estimate\\xa0(2022)[3]2,665,039\\xa0•\\xa0Rank5th in North America3rd in the United States1st in Illinois\\xa0•\\xa0Density12,059.84/sq\\xa0mi (4,656.33/km2)\\xa0•\\xa0Urban[4]8,671,746 (US: 3rd)\\xa0•\\xa0Urban\\xa0density3,709.2/sq\\xa0mi (1,432.1/km2)\\xa0•\\xa0Metro[5]9,618,502 (US: 3rd)DemonymChicagoanGDP[6]\\xa0•\\xa0Chicago (MSA)$832.9 billion (2022)Time zoneUTC−06:00 (CST)\\xa0•\\xa0Summer (DST)UTC−05:00 (CDT)ZIP Code prefixes606xx, 607xx, 608xxArea codes312, 773, 872FIPS code17-14000GNIS feature ID0428803Websitechicago.gov\\nChicago[a] is the most populous city in the U.S. state of Illinois and in the Midwestern United States. With a population of 2,746,388, as of the 2020 census,[9] it is the third-most populous city in the United States after New York City and Los Angeles. As the seat of Cook County, the second-most populous county in the U.S., Chicago is the center of the Chicago metropolitan area, often colloquially called \"Chicagoland\" and home to 9.6 million residents.\\nLocated on the shore of Lake Michigan, Chicago was incorporated as a city in 1837 near a portage between the Great Lakes and the Mississippi River watershed. It grew rapidly in the mid-19th century.[10][11] In 1871, the Great Chicago Fire destroyed several square miles and left more than 100,000 homeless,[12] but Chicago\\'s population continued to grow.[11] Chicago made noted contributions to urban planning and architecture, such as the Chicago School, the development of the City Beautiful movement, and the steel-framed skyscraper.[13][14]\\nChicago is an international hub for finance, culture, commerce, industry, education, technology, telecommunications, and transportation. It has the largest and most diverse derivatives market in the world, generating 20% of all volume in commodities and financial futures alone.[15] O\\'Hare International Airport is routinely ranked among the world\\'s top six busiest airports by passenger traffic,[16] and the region is also the nation\\'s railroad hub.[17] The Chicago area has one of the highest gross domestic products (GDP) of any urban region in the world, generating $689\\xa0billion in 2018.[18] Chicago\\'s economy is diverse, with no single industry employing more than 14% of the workforce.[15]\\nChicago is a major destination for tourism, including visitors to its cultural institutions, and Lake Michigan beaches. Chicago\\'s culture has contributed much to the visual arts, literature, film, theater, comedy (especially improvisational comedy), food, dance, and music (particularly jazz, blues, soul, hip-hop, gospel,[19] and electronic dance music, including house music). Chicago is home to the Chicago Symphony Orchestra and the Lyric Opera of Chicago, while the Art Institute of Chicago provides an influential visual arts museum and art school. The Chicago area also hosts the University of Chicago, Northwestern University, and the University of Illinois Chicago, among other institutions of learning. Professional sports in Chicago include all major professional leagues, including two Major League Baseball teams.\\n\\n\\nEtymology and nicknames\\nMain article: Nicknames of Chicago\\nSee also: Windy City (nickname) and List of Chicago placename etymologies\\nThe name Chicago is derived from a French rendering of the indigenous Miami-Illinois word shikaakwa for a wild relative of the onion; it is known to botanists as Allium tricoccum and known more commonly as \"ramps\". The first known reference to the site of the current city of Chicago as \"Checagou\" was by Robert de\\xa0LaSalle around 1679 in a memoir.[20] Henri Joutel, in his journal of 1688, noted that the eponymous wild \"garlic\" grew profusely in the area.[21] According to his diary of late September 1687:\\n\\n... when we arrived at the said place called \"Chicagou\" which, according to what we were able to learn of it, has taken this name because of the quantity of garlic which grows in the forests in this region.[21]\\nThe city has had several nicknames throughout its history, such as the Windy City, Chi-Town, Second City, and City of the Big Shoulders.[22]\\n\\nHistory\\nMain article: History of Chicago\\nFor a chronological guide, see Timeline of Chicago history.\\nBeginnings\\nTraditional Potawatomi regalia on display at the Field Museum of Natural HistoryIn the mid-18th century, the area was inhabited by the Potawatomi, an indigenous tribe who had succeeded the Miami, Sauk and Meskwaki peoples in this region.[23]\\nAn artist\\'s rendering of the Great Chicago Fire of 1871\\nHome Insurance Building (1885)\\nCourt of Honor at the World\\'s Columbian Exposition in 1893\\nThe first known permanent settler in Chicago was trader Jean Baptiste Point du Sable. Du Sable was of African descent, perhaps born in the French colony of Saint-Domingue (Haiti), and established the settlement in the 1780s. He is commonly known as the \"Founder of Chicago.\"[24][25][26]\\nIn 1795, following the victory of the new United States in the Northwest Indian War, an area that was to be part of Chicago was turned over to the U.S. for a military post by native tribes in accordance with the Treaty of Greenville. In 1803, the U.S. Army constructed Fort Dearborn, which was destroyed during the War of 1812 in the Battle of Fort Dearborn by the Potawatomi before being later rebuilt.[27]\\nAfter the War of 1812, the Ottawa, Ojibwe, and Potawatomi tribes ceded additional land to the United States in the 1816 Treaty of St. Louis. The Potawatomi were forcibly removed from their land after the 1833 Treaty of Chicago and sent west of the Mississippi River as part of the federal policy of Indian removal.[28][29][30]\\n\\n19th century\\nThe location and course of the Illinois and Michigan Canal (completed 1848)State and Madison streets, once known as the busiest intersection in the world (1897)\\nOn August 12, 1833, the Town of Chicago was organized with a population of about 200.[30] Within seven years it grew to more than 6,000 people. On June 15, 1835, the first public land sales began with Edmund Dick Taylor as Receiver of Public Monies. The City of Chicago was incorporated on Saturday, March 4, 1837,[31] and for several decades was the world\\'s fastest-growing city.[32]\\nAs the site of the Chicago Portage,[33] the city became an important transportation hub between the eastern and western United States. Chicago\\'s first railway, Galena and Chicago Union Railroad, and the Illinois and Michigan Canal opened in 1848. The canal allowed steamboats and sailing ships on the Great Lakes to connect to the Mississippi River.[34][35][36][37]\\nA flourishing economy brought residents from rural communities and immigrants from abroad. Manufacturing and retail and finance sectors became dominant, influencing the American economy.[38] The Chicago Board of Trade (established 1848) listed the first-ever standardized \"exchange-traded\" forward contracts, which were called futures contracts.[39]\\nIn the 1850s, Chicago gained national political prominence as the home of Senator Stephen Douglas, the champion of the Kansas–Nebraska Act and the \"popular sovereignty\" approach to the issue of the spread of slavery.[40] These issues also helped propel another Illinoisan, Abraham Lincoln, to the national stage. Lincoln was nominated in Chicago for U.S. president at the 1860 Republican National Convention, which was held in a purpose-built auditorium called the Wigwam. He defeated Douglas in the general election, and this set the stage for the American Civil War.\\nTo accommodate rapid population growth and demand for better sanitation, the city improved its infrastructure. In February 1856, Chicago\\'s Common Council approved Chesbrough\\'s plan to build the United States\\' first comprehensive sewerage system.[41] The project raised much of central Chicago to a new grade with the use of jackscrews for raising buildings.[42] While elevating Chicago, and at first improving the city\\'s health, the untreated sewage and industrial waste now flowed into the Chicago River, and subsequently into Lake Michigan, polluting the city\\'s primary freshwater source.\\nThe city responded by tunneling two miles (3.2\\xa0km) out into Lake Michigan to newly built water cribs. In 1900, the problem of sewage contamination was largely resolved when the city completed a major engineering feat. It reversed the flow of the Chicago River so that the water flowed away from Lake Michigan rather than into it. This project began with the construction and improvement of the Illinois and Michigan Canal, and was completed with the Chicago Sanitary and Ship Canal that connects to the Illinois River, which flows into the Mississippi River.[43][44][45]\\nIn 1871, the Great Chicago Fire destroyed an area about 4 miles (6.4\\xa0km) long and 1-mile (1.6\\xa0km) wide, a large section of the city at the time.[46][47][48] Much of the city, including railroads and stockyards, survived intact,[49] and from the ruins of the previous wooden structures arose more modern constructions of steel and stone. These set a precedent for worldwide construction.[50][51] During its rebuilding period, Chicago constructed the world\\'s first skyscraper in 1885, using steel-skeleton construction.[52][53]\\nThe city grew significantly in size and population by incorporating many neighboring townships between 1851 and 1920, with the largest annexation happening in 1889, with five townships joining the city, including the Hyde Park Township, which now comprises most of the South Side of Chicago and the far southeast of Chicago, and the Jefferson Township, which now makes up most of Chicago\\'s Northwest Side.[54] The desire to join the city was driven by municipal services that the city could provide its residents.\\nChicago\\'s flourishing economy attracted huge numbers of new immigrants from Europe and migrants from the Eastern United States. Of the total population in 1900, more than 77% were either foreign-born or born in the United States of foreign parentage. Germans, Irish, Poles, Swedes, and Czechs made up nearly two-thirds of the foreign-born population (by 1900, whites were 98.1% of the city\\'s population).[55][56]\\nLabor conflicts followed the industrial boom and the rapid expansion of the labor pool, including the Haymarket affair on May 4, 1886, and in 1894 the Pullman Strike. Anarchist and socialist groups played prominent roles in creating very large and highly organized labor actions. Concern for social problems among Chicago\\'s immigrant poor led Jane Addams and Ellen Gates Starr to found Hull House in 1889.[57] Programs that were developed there became a model for the new field of social work.[58]\\nDuring the 1870s and 1880s, Chicago attained national stature as the leader in the movement to improve public health. City laws and later, state laws that upgraded standards for the medical profession and fought urban epidemics of cholera, smallpox, and yellow fever were both passed and enforced. These laws became templates for public health reform in other cities and states.[59]\\nThe city established many large, well-landscaped municipal parks, which also included public sanitation facilities. The chief advocate for improving public health in Chicago was John H. Rauch, M.D. Rauch established a plan for Chicago\\'s park system in 1866. He created Lincoln Park by closing a cemetery filled with shallow graves, and in 1867, in response to an outbreak of cholera he helped establish a new Chicago Board of Health. Ten years later, he became the secretary and then the president of the first Illinois State Board of Health, which carried out most of its activities in Chicago.[60]\\nIn the 1800s, Chicago became the nation\\'s railroad hub, and by 1910 over 20 railroads operated passenger service out of six different downtown terminals.[61][62] In 1883, Chicago\\'s railway managers needed a general time convention, so they developed the standardized system of North American time zones.[63] This system for telling time spread throughout the continent.\\nIn 1893, Chicago hosted the World\\'s Columbian Exposition on former marshland at the present location of Jackson Park. The Exposition drew 27.5\\xa0million visitors, and is considered the most influential world\\'s fair in history.[64][65] The University of Chicago, formerly at another location, moved to the same South Side location in 1892. The term \"midway\" for a fair or carnival referred originally to the Midway Plaisance, a strip of park land that still runs through the University of Chicago campus and connects the Washington and Jackson Parks.[66][67]\\n\\n20th and 21st centuries\\n1900 to 1939\\nAerial motion film photography of Chicago in 1914 as filmed by A. Roy Knabenshue\\nDuring World War I and the 1920s there was a major expansion in industry. The availability of jobs attracted African Americans from the Southern United States. Between 1910 and 1930, the African American population of Chicago increased dramatically, from 44,103 to 233,903.[68] This Great Migration had an immense cultural impact, called the Chicago Black Renaissance, part of the New Negro Movement, in art, literature, and music.[69] Continuing racial tensions and violence, such as the Chicago race riot of 1919, also occurred.[70]\\nThe ratification of the 18th amendment to the Constitution in 1919 made the production and sale (including exportation) of alcoholic beverages illegal in the United States. This ushered in the beginning of what is known as the gangster era, a time that roughly spans from 1919 until 1933 when Prohibition was repealed. The 1920s saw gangsters, including Al Capone, Dion O\\'Banion, Bugs Moran and Tony Accardo battle law enforcement and each other on the streets of Chicago during the Prohibition era.[71] Chicago was the location of the infamous St. Valentine\\'s Day Massacre in 1929, when Al Capone sent men to gun down members of a rival gang, North Side, led by Bugs Moran.[72]\\n\\nChicago tenants picket against rent increases (March 1920)\\nFrom 1920 to 1921, the city was affected by a series of tenant rent strikes in it. Which lead to the formation of the Chicago Tenants Protective association, passage of the Kessenger tenant laws, and of a heat ordinance that legally required flats to be kept above 68\\xa0°F during winter months by landlords.[73][74][75][76][77][78]\\n\\nChicago was the first American city to have a homosexual-rights organization. The organization, formed in 1924, was called the Society for Human Rights. It produced the first American publication for homosexuals, Friendship and Freedom. Police and political pressure caused the organization to disband.[79]Men outside a soup kitchen during the Great Depression (1931)\\nThe Great Depression brought unprecedented suffering to Chicago, in no small part due to the city\\'s heavy reliance on heavy industry. Notably, industrial areas on the south side and neighborhoods lining both branches of the Chicago River were devastated; by 1933 over 50% of industrial jobs in the city had been lost, and unemployment rates amongst blacks and Mexicans in the city were over 40%. The Republican political machine in Chicago was utterly destroyed by the economic crisis, and every mayor since 1931 has been a Democrat.[80]\\nFrom 1928 to 1933, the city witnessed a tax revolt, and the city was unable to meet payroll or provide relief efforts. The fiscal crisis was resolved by 1933, and at the same time, federal relief funding began to flow into Chicago.[80] Chicago was also a hotbed of labor activism, with Unemployed Councils contributing heavily in the early depression to create solidarity for the poor and demand relief; these organizations were created by socialist and communist groups. By 1935 the Workers Alliance of America begun organizing the poor, workers, the unemployed. In the spring of 1937 Republic Steel Works witnessed the Memorial Day massacre of 1937 in the neighborhood of East Side.\\nIn 1933, Chicago Mayor Anton Cermak was fatally wounded in Miami, Florida, during a failed assassination attempt on President-elect Franklin D. Roosevelt. In 1933 and 1934, the city celebrated its centennial by hosting the Century of Progress International Exposition World\\'s Fair.[81] The theme of the fair was technological innovation over the century since Chicago\\'s founding.[82]\\n\\n1940 to 1979\\nThe Chicago Picasso (1967) inspired a new era in urban public art.\\nDuring World War II, the city of Chicago alone produced more steel than the United Kingdom every year from 1939 – 1945, and more than Nazi Germany from 1943 – 1945.[83]\\n\\nProtesters in Grant Park outside the 1968 Democratic National Convention\\nThe Great Migration, which had been on pause due to the Depression, resumed at an even faster pace in the second wave, as hundreds of thousands of blacks from the South arrived in the city to work in the steel mills, railroads, and shipping yards.[84]\\nOn December 2, 1942, physicist Enrico Fermi conducted the world\\'s first controlled nuclear reaction at the University of Chicago as part of the top-secret Manhattan Project. This led to the creation of the atomic bomb by the United States, which it used in World War II in 1945.[85]\\nMayor Richard J. Daley, a Democrat, was elected in 1955, in the era of machine politics. In 1956, the city conducted its last major expansion when it annexed the land under O\\'Hare airport, including a small portion of DuPage County.[86]\\nBy the 1960s, white residents in several neighborhoods left the city for the suburban areas – in many American cities, a process known as white flight – as Blacks continued to move beyond the Black Belt.[87] While home loan discriminatory redlining against blacks continued, the real estate industry practiced what became known as blockbusting, completely changing the racial composition of whole neighborhoods.[88] Structural changes in industry, such as globalization and job outsourcing, caused heavy job losses for lower-skilled workers. At its peak during the 1960s, some 250,000 workers were employed in the steel industry in Chicago, but the steel crisis of the 1970s and 1980s reduced this number to just 28,000 in 2015. In 1966, Martin Luther King Jr. and Albert Raby led the Chicago Freedom Movement, which culminated in agreements between Mayor Richard J. Daley and the movement leaders.[89]\\nTwo years later, the city hosted the tumultuous 1968 Democratic National Convention, which featured physical confrontations both inside and outside the convention hall, with anti-war protesters, journalists and bystanders being beaten by police.[90] Major construction projects, including the Sears Tower (now known as the Willis Tower, which in 1974 became the world\\'s tallest building), University of Illinois at Chicago, McCormick Place, and O\\'Hare International Airport, were undertaken during Richard J. Daley\\'s tenure.[91] In 1979, Jane Byrne, the city\\'s first female mayor, was elected. She was notable for temporarily moving into the crime-ridden Cabrini-Green housing project and for leading Chicago\\'s school system out of a financial crisis.[92]\\n\\n1980 to present\\nIn 1983, Harold Washington became the first black mayor of Chicago. Washington\\'s first term in office directed attention to poor and previously neglected minority neighborhoods. He was re‑elected in 1987 but died of a heart attack soon after.[93] Washington was succeeded by 6th ward alderperson Eugene Sawyer, who was elected by the Chicago City Council and served until a special election.\\nRichard M. Daley, son of Richard J. Daley, was elected in 1989. His accomplishments included improvements to parks and creating incentives for sustainable development, as well as closing Meigs Field in the middle of the night and destroying the runways. After successfully running for re-election five times, and becoming Chicago\\'s longest-serving mayor, Richard M. Daley declined to run for a seventh term.[94][95]\\nIn 1992, a construction accident near the Kinzie Street Bridge produced a breach connecting the Chicago River to a tunnel below, which was part of an abandoned freight tunnel system extending throughout the downtown Loop district. The tunnels filled with 250\\xa0million US gallons (1,000,000\\xa0m3) of water, affecting buildings throughout the district and forcing a shutdown of electrical power.[96] The area was shut down for three days and some buildings did not reopen for weeks; losses were estimated at $1.95\\xa0billion.[96]\\nOn February 23, 2011, Rahm Emanuel, a former White House Chief of Staff and member of the House of Representatives, won the mayoral election.[97] Emanuel was sworn in as mayor on May 16, 2011, and won re-election in 2015.[98] Lori Lightfoot, the city\\'s first African American woman mayor and its first openly LGBTQ mayor, was elected to succeed Emanuel as mayor in 2019.[99] All three city-wide elective offices were held by women (and women of color) for the first time in Chicago history: in addition to Lightfoot, the city clerk was Anna Valencia and the city treasurer was Melissa Conyears-Ervin.[100]\\nOn May 15, 2023, Brandon Johnson assumed office as the 57th mayor of Chicago.\\n\\nGeography\\nMain article: Geography of Chicago\\n Chicago skyline at sunset in October 2020, from near Fullerton Avenue looking south\\nTopography\\nAerial view of the Chicago Loop in 2012\\nDowntown and the North Side with beaches lining the waterfront\\nA satellite image of Chicago\\nChicago is located in northeastern Illinois on the southwestern shores of freshwater Lake Michigan. It is the principal city in the Chicago Metropolitan Area, situated in both the Midwestern United States and the Great Lakes region. The city rests on a continental divide at the site of the Chicago Portage, connecting the Mississippi River and the Great Lakes watersheds. In addition to it lying beside Lake Michigan, two rivers—the Chicago River in downtown and the Calumet River in the industrial far South Side—flow either entirely or partially through the city.[101][102]\\nChicago\\'s history and economy are closely tied to its proximity to Lake Michigan. While the Chicago River historically handled much of the region\\'s waterborne cargo, today\\'s huge lake freighters use the city\\'s Lake Calumet Harbor on the South Side. The lake also provides another positive effect: moderating Chicago\\'s climate, making waterfront neighborhoods slightly warmer in winter and cooler in summer.[103]\\nWhen Chicago was founded in 1837, most of the early building was around the mouth of the Chicago River, as can be seen on a map of the city\\'s original 58 blocks.[104] The overall grade of the city\\'s central, built-up areas is relatively consistent with the natural flatness of its overall natural geography, generally exhibiting only slight differentiation otherwise. The average land elevation is 579\\xa0ft (176.5\\xa0m) above sea level. While measurements vary somewhat,[105] the lowest points are along the lake shore at 578\\xa0ft (176.2\\xa0m), while the highest point, at 672\\xa0ft (205\\xa0m), is the morainal ridge of Blue Island in the city\\'s far south side.[106]\\nLake Shore Drive runs adjacent to a large portion of Chicago\\'s waterfront. Some of the parks along the waterfront include Lincoln Park, Grant Park, Burnham Park, and Jackson Park. There are 24 public beaches across 26 miles (42\\xa0km) of the waterfront.[107] Landfill extends into portions of the lake providing space for Navy Pier, Northerly Island, the Museum Campus, and large portions of the McCormick Place Convention Center. Most of the city\\'s high-rise commercial and residential buildings are close to the waterfront.\\nAn informal name for the entire Chicago metropolitan area is \"Chicagoland\", which generally means the city and all its suburbs, though different organizations have slightly different definitions.[108][109][110]\\n\\nCommunities\\nSee also: Community areas in Chicago and List of neighborhoods in Chicago\\nCommunity areas of Chicago\\nMajor sections of the city include the central business district, called the Loop, and the North, South, and West Sides.[111] The three sides of the city are represented on the Flag of Chicago by three horizontal white stripes.[112] The North Side is the most-densely-populated residential section of the city, and many high-rises are located on this side of the city along the lakefront.[113] The South Side is the largest section of the city, encompassing roughly 60% of the city\\'s land area. The South Side contains most of the facilities of the Port of Chicago.[114]\\nIn the late-1920s, sociologists at the University of Chicago subdivided the city into 77 distinct community areas, which can further be subdivided into over 200 informally defined neighborhoods.[115][116]\\n\\nStreetscape\\nMain article: Roads and expressways in Chicago\\nChicago\\'s streets were laid out in a street grid that grew from the city\\'s original townsite plot, which was bounded by Lake Michigan on the east, North Avenue on the north, Wood Street on the west, and 22nd Street on the south.[117] Streets following the Public Land Survey System section lines later became arterial streets in outlying sections. As new additions to the city were platted, city ordinance required them to be laid out with eight streets to the mile in one direction and sixteen in the other direction, about one street per 200 meters in one direction and one street per 100 meters in the other direction. The grid\\'s regularity provided an efficient means of developing new real estate property. A scattering of diagonal streets, many of them originally Native American trails, also cross the city (Elston, Milwaukee, Ogden, Lincoln, etc.). Many additional diagonal streets were recommended in the Plan of Chicago, but only the extension of Ogden Avenue was ever constructed.[118]\\nIn 2021, Chicago was ranked the fourth-most walkable large city in the United States.[119] Many of the city\\'s residential streets have a wide patch of grass or trees between the street and the sidewalk itself. This helps to keep pedestrians on the sidewalk further away from the street traffic. Chicago\\'s Western Avenue is the longest continuous urban street in the world.[120] Other notable streets include Michigan Avenue, State Street, 95th Street, Cicero Avenue, Clark Street, and Belmont Avenue. The City Beautiful movement inspired Chicago\\'s boulevards and parkways.[121]\\n\\nArchitecture\\nMain article: Architecture of ChicagoFurther information: List of tallest buildings in Chicago and List of Chicago Landmarks\\nThe Chicago Building (1904–05) is a prime example of the Chicago School, displaying both variations of the Chicago window.\\nThe destruction caused by the Great Chicago Fire led to the largest building boom in the history of the nation. In 1885, the first steel-framed high-rise building, the Home Insurance Building, rose in the city as Chicago ushered in the skyscraper era,[53] which would then be followed by many other cities around the world.[122] Today, Chicago\\'s skyline is among the world\\'s tallest and densest.[123]\\nSome of the United States\\' tallest towers are located in Chicago; Willis Tower (formerly Sears Tower) is the second tallest building in the Western Hemisphere after One World Trade Center, and Trump International Hotel and Tower is the third tallest in the country.[124] The Loop\\'s historic buildings include the Chicago Board of Trade Building, the Fine Arts Building, 35\\xa0East Wacker, and the Chicago Building, 860-880 Lake Shore Drive Apartments by Mies van der Rohe. Many other architects have left their impression on the Chicago skyline such as Daniel Burnham, Louis Sullivan, Charles B. Atwood, John Root, and Helmut Jahn.[125][126]\\nThe Merchandise Mart, once first on the list of largest buildings in the world, currently listed as 44th-largest 2013 as September 9, 2013, had its own zip\\xa0code until 2008, and stands near the junction of the North and South branches of the Chicago River.[127] Presently, the four tallest buildings in the city are Willis Tower (formerly the Sears Tower, also a building with its own zip code), Trump International Hotel and Tower, the Aon Center (previously the Standard Oil Building), and the John Hancock Center. Industrial districts, such as some areas on the South Side, the areas along the Chicago Sanitary and Ship Canal, and the Northwest Indiana area are clustered.[128]\\nChicago gave its name to the Chicago School and was home to the Prairie School, two movements in architecture.[129] Multiple kinds and scales of houses, townhouses, condominiums, and apartment buildings can be found throughout Chicago. Large swaths of the city\\'s residential areas away from the lake are characterized by brick bungalows built from the early 20th century through the end of World War II. Chicago is also a prominent center of the Polish Cathedral style of church architecture. The Chicago suburb of Oak Park was home to famous architect Frank Lloyd Wright, who had designed The Robie House located near the University of Chicago.[130][131]\\nA popular tourist activity is to take an architecture boat tour along the Chicago River.[132]\\n\\nMonuments and public art\\nMain article: List of public art in Chicago\\nReplica of Daniel Chester French\\'s Statue of The Republic at the site of the World\\'s Columbian Exposition\\nChicago is famous for its outdoor public art with donors establishing funding for such art as far back as Benjamin Ferguson\\'s 1905 trust.[133] A number of Chicago\\'s public art works are by modern figurative artists. Among these are Chagall\\'s Four Seasons; the Chicago Picasso; Miro\\'s Chicago; Calder\\'s Flamingo; Oldenburg\\'s Batcolumn; Moore\\'s Large Interior Form, 1953-54, Man Enters the Cosmos and Nuclear Energy; Dubuffet\\'s Monument with Standing Beast, Abakanowicz\\'s Agora; and, Anish Kapoor\\'s Cloud Gate which has become an icon of the city. Some events which shaped the city\\'s history have also been memorialized by art works, including the Great Northern Migration (Saar) and the centennial of statehood for Illinois. Finally, two fountains near the Loop also function as monumental works of art: Plensa\\'s Crown Fountain as well as Burnham and Bennett\\'s Buckingham Fountain.[134][135]\\n\\nClimate\\nMain article: Climate of Chicago\\nThe Chicago River during the January 2014 cold wave\\nThe city lies within the typical hot-summer humid continental climate (Köppen: Dfa), and experiences four distinct seasons.[136][137][138] Summers are hot and humid, with frequent heat waves. The July daily average temperature is 75.4\\xa0°F (24.1\\xa0°C), with afternoon temperatures peaking at 84.5\\xa0°F (29.2\\xa0°C). In a normal summer, temperatures reach at least 90\\xa0°F (32\\xa0°C) on 17\\xa0days, with lakefront locations staying cooler when winds blow off the lake. Winters are relatively cold and snowy. Blizzards do occur, such as in winter 2011.[139] There are many sunny but cold days. The normal winter high from December through March is about 36\\xa0°F (2\\xa0°C). January and February are the coldest months. A polar vortex in January 2019 nearly broke the city\\'s cold record of −27\\xa0°F (−33\\xa0°C), which was set on January 20, 1985.[140][141][142] Measurable snowfall can continue through the first or second week of April.[143]\\nSpring and autumn are mild, short seasons, typically with low humidity. Dew point temperatures in the summer range from an average of 55.8\\xa0°F (13.2\\xa0°C) in June to 61.7\\xa0°F (16.5\\xa0°C) in July.[144] They can reach nearly 80\\xa0°F (27\\xa0°C), such as during the July 2019 heat wave. The city lies within USDA plant hardiness zone 6a, transitioning to 5b in the suburbs.[145]\\nAccording to the National Weather Service, Chicago\\'s highest official temperature reading of 105\\xa0°F (41\\xa0°C) was recorded on July 24, 1934.[146] Midway Airport reached 109\\xa0°F (43\\xa0°C) one day prior and recorded a heat index of 125\\xa0°F (52\\xa0°C) during the 1995 heatwave.[147] The lowest official temperature of −27\\xa0°F (−33\\xa0°C) was recorded on January 20, 1985, at O\\'Hare Airport.[144][147] Most of the city\\'s rainfall is brought by thunderstorms, averaging 38 a year. The region is prone to severe thunderstorms during the spring and summer which can produce large hail, damaging winds, and occasionally tornadoes.[148]\\nLike other major cities, Chicago experiences an urban heat island, making the city and its suburbs milder than surrounding rural areas, especially at night and in winter. The proximity to Lake Michigan tends to keep the Chicago lakefront somewhat cooler in summer and less brutally cold in winter than inland parts of the city and suburbs away from the lake.[149] Northeast winds from wintertime cyclones departing south of the region sometimes bring the city lake-effect snow.[150]\\n\\n\\n\\n\\nClimate data for Chicago (Midway International Airport), 1991–2020 normals,[b] extremes 1928–present\\n\\n\\nMonth\\n\\nJan\\n\\nFeb\\n\\nMar\\n\\nApr\\n\\nMay\\n\\nJun\\n\\nJul\\n\\nAug\\n\\nSep\\n\\nOct\\n\\nNov\\n\\nDec\\n\\nYear\\n\\n\\nRecord high °F (°C)\\n\\n67(19)\\n\\n75(24)\\n\\n86(30)\\n\\n92(33)\\n\\n102(39)\\n\\n107(42)\\n\\n109(43)\\n\\n104(40)\\n\\n102(39)\\n\\n94(34)\\n\\n81(27)\\n\\n72(22)\\n\\n109(43)\\n\\n\\nMean maximum °F (°C)\\n\\n53.4(11.9)\\n\\n57.9(14.4)\\n\\n72.0(22.2)\\n\\n81.5(27.5)\\n\\n89.2(31.8)\\n\\n93.9(34.4)\\n\\n96.0(35.6)\\n\\n94.2(34.6)\\n\\n90.8(32.7)\\n\\n82.8(28.2)\\n\\n68.0(20.0)\\n\\n57.5(14.2)\\n\\n97.1(36.2)\\n\\n\\nMean daily maximum °F (°C)\\n\\n32.8(0.4)\\n\\n36.8(2.7)\\n\\n47.9(8.8)\\n\\n60.0(15.6)\\n\\n71.5(21.9)\\n\\n81.2(27.3)\\n\\n85.2(29.6)\\n\\n83.1(28.4)\\n\\n76.5(24.7)\\n\\n63.7(17.6)\\n\\n49.6(9.8)\\n\\n37.7(3.2)\\n\\n60.5(15.8)\\n\\n\\nDaily mean °F (°C)\\n\\n26.2(−3.2)\\n\\n29.9(−1.2)\\n\\n39.9(4.4)\\n\\n50.9(10.5)\\n\\n61.9(16.6)\\n\\n71.9(22.2)\\n\\n76.7(24.8)\\n\\n75.0(23.9)\\n\\n67.8(19.9)\\n\\n55.3(12.9)\\n\\n42.4(5.8)\\n\\n31.5(−0.3)\\n\\n52.4(11.3)\\n\\n\\nMean daily minimum °F (°C)\\n\\n19.5(−6.9)\\n\\n22.9(−5.1)\\n\\n32.0(0.0)\\n\\n41.7(5.4)\\n\\n52.4(11.3)\\n\\n62.7(17.1)\\n\\n68.1(20.1)\\n\\n66.9(19.4)\\n\\n59.2(15.1)\\n\\n46.8(8.2)\\n\\n35.2(1.8)\\n\\n25.3(−3.7)\\n\\n44.4(6.9)\\n\\n\\nMean minimum °F (°C)\\n\\n−3(−19)\\n\\n3.4(−15.9)\\n\\n14.1(−9.9)\\n\\n28.2(−2.1)\\n\\n39.1(3.9)\\n\\n49.3(9.6)\\n\\n58.6(14.8)\\n\\n57.6(14.2)\\n\\n45.0(7.2)\\n\\n31.8(−0.1)\\n\\n19.7(−6.8)\\n\\n5.3(−14.8)\\n\\n−6.5(−21.4)\\n\\n\\nRecord low °F (°C)\\n\\n−25(−32)\\n\\n−20(−29)\\n\\n−7(−22)\\n\\n10(−12)\\n\\n28(−2)\\n\\n35(2)\\n\\n46(8)\\n\\n43(6)\\n\\n29(−2)\\n\\n20(−7)\\n\\n−3(−19)\\n\\n−20(−29)\\n\\n−25(−32)\\n\\n\\nAverage precipitation inches (mm)\\n\\n2.30(58)\\n\\n2.12(54)\\n\\n2.66(68)\\n\\n4.15(105)\\n\\n4.75(121)\\n\\n4.53(115)\\n\\n4.02(102)\\n\\n4.10(104)\\n\\n3.33(85)\\n\\n3.86(98)\\n\\n2.73(69)\\n\\n2.33(59)\\n\\n40.88(1,038)\\n\\n\\nAverage snowfall inches (cm)\\n\\n12.5(32)\\n\\n10.1(26)\\n\\n5.7(14)\\n\\n1.0(2.5)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n0.1(0.25)\\n\\n1.5(3.8)\\n\\n7.9(20)\\n\\n38.8(99)\\n\\n\\nAverage precipitation days (≥ 0.01 in)\\n\\n11.5\\n\\n9.4\\n\\n11.1\\n\\n12.0\\n\\n12.4\\n\\n11.1\\n\\n10.0\\n\\n9.3\\n\\n8.4\\n\\n10.8\\n\\n10.2\\n\\n10.8\\n\\n127.0\\n\\n\\nAverage snowy days (≥ 0.1 in)\\n\\n8.9\\n\\n6.4\\n\\n3.9\\n\\n0.9\\n\\n0.0\\n\\n0.0\\n\\n0.0\\n\\n0.0\\n\\n0.0\\n\\n0.2\\n\\n1.6\\n\\n6.3\\n\\n28.2\\n\\n\\nAverage ultraviolet index\\n\\n1\\n\\n2\\n\\n4\\n\\n6\\n\\n7\\n\\n9\\n\\n9\\n\\n8\\n\\n6\\n\\n4\\n\\n2\\n\\n1\\n\\n5\\n\\n\\nSource 1: NOAA[151][144][147], WRCC[152]\\n\\n\\nSource 2: Weather Atlas (UV)[153]\\n\\n\\n\\n\\n\\nClimate data for Chicago (O\\'Hare Int\\'l Airport), 1991–2020 normals,[b] extremes 1871–present[c]\\n\\n\\nMonth\\n\\nJan\\n\\nFeb\\n\\nMar\\n\\nApr\\n\\nMay\\n\\nJun\\n\\nJul\\n\\nAug\\n\\nSep\\n\\nOct\\n\\nNov\\n\\nDec\\n\\nYear\\n\\n\\nRecord high °F (°C)\\n\\n67(19)\\n\\n75(24)\\n\\n88(31)\\n\\n91(33)\\n\\n98(37)\\n\\n104(40)\\n\\n105(41)\\n\\n102(39)\\n\\n101(38)\\n\\n94(34)\\n\\n81(27)\\n\\n71(22)\\n\\n105(41)\\n\\n\\nMean maximum °F (°C)\\n\\n52.3(11.3)\\n\\n56.8(13.8)\\n\\n71.0(21.7)\\n\\n80.9(27.2)\\n\\n88.0(31.1)\\n\\n93.1(33.9)\\n\\n94.9(34.9)\\n\\n93.2(34.0)\\n\\n89.7(32.1)\\n\\n81.7(27.6)\\n\\n67.0(19.4)\\n\\n56.4(13.6)\\n\\n96.0(35.6)\\n\\n\\nMean daily maximum °F (°C)\\n\\n31.6(−0.2)\\n\\n35.7(2.1)\\n\\n47.0(8.3)\\n\\n59.0(15.0)\\n\\n70.5(21.4)\\n\\n80.4(26.9)\\n\\n84.5(29.2)\\n\\n82.5(28.1)\\n\\n75.5(24.2)\\n\\n62.7(17.1)\\n\\n48.4(9.1)\\n\\n36.6(2.6)\\n\\n59.5(15.3)\\n\\n\\nDaily mean °F (°C)\\n\\n25.2(−3.8)\\n\\n28.8(−1.8)\\n\\n39.0(3.9)\\n\\n49.7(9.8)\\n\\n60.6(15.9)\\n\\n70.6(21.4)\\n\\n75.4(24.1)\\n\\n73.8(23.2)\\n\\n66.3(19.1)\\n\\n54.0(12.2)\\n\\n41.3(5.2)\\n\\n30.5(−0.8)\\n\\n51.3(10.7)\\n\\n\\nMean daily minimum °F (°C)\\n\\n18.8(−7.3)\\n\\n21.8(−5.7)\\n\\n31.0(−0.6)\\n\\n40.3(4.6)\\n\\n50.6(10.3)\\n\\n60.8(16.0)\\n\\n66.4(19.1)\\n\\n65.1(18.4)\\n\\n57.1(13.9)\\n\\n45.4(7.4)\\n\\n34.1(1.2)\\n\\n24.4(−4.2)\\n\\n43.0(6.1)\\n\\n\\nMean minimum °F (°C)\\n\\n−4.5(−20.3)\\n\\n0.5(−17.5)\\n\\n11.8(−11.2)\\n\\n25.6(−3.6)\\n\\n36.7(2.6)\\n\\n46.0(7.8)\\n\\n54.5(12.5)\\n\\n54.3(12.4)\\n\\n41.8(5.4)\\n\\n29.7(−1.3)\\n\\n17.3(−8.2)\\n\\n3.2(−16.0)\\n\\n−8.5(−22.5)\\n\\n\\nRecord low °F (°C)\\n\\n−27(−33)\\n\\n−21(−29)\\n\\n−12(−24)\\n\\n7(−14)\\n\\n27(−3)\\n\\n35(2)\\n\\n45(7)\\n\\n42(6)\\n\\n29(−2)\\n\\n14(−10)\\n\\n−2(−19)\\n\\n−25(−32)\\n\\n−27(−33)\\n\\n\\nAverage precipitation inches (mm)\\n\\n1.99(51)\\n\\n1.97(50)\\n\\n2.45(62)\\n\\n3.75(95)\\n\\n4.49(114)\\n\\n4.10(104)\\n\\n3.71(94)\\n\\n4.25(108)\\n\\n3.19(81)\\n\\n3.43(87)\\n\\n2.42(61)\\n\\n2.11(54)\\n\\n37.86(962)\\n\\n\\nAverage snowfall inches (cm)\\n\\n11.3(29)\\n\\n10.7(27)\\n\\n5.5(14)\\n\\n1.3(3.3)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n0.2(0.51)\\n\\n1.8(4.6)\\n\\n7.6(19)\\n\\n38.4(98)\\n\\n\\nAverage extreme snow depth inches (cm)\\n\\n6.3(16)\\n\\n6.3(16)\\n\\n4.0(10)\\n\\n0.6(1.5)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n0.0(0.0)\\n\\n1.5(3.8)\\n\\n3.9(9.9)\\n\\n9.8(25)\\n\\n\\nAverage precipitation days (≥ 0.01 in)\\n\\n11.0\\n\\n9.4\\n\\n10.8\\n\\n12.3\\n\\n12.5\\n\\n11.1\\n\\n9.7\\n\\n9.4\\n\\n8.5\\n\\n10.5\\n\\n10.0\\n\\n10.6\\n\\n125.8\\n\\n\\nAverage snowy days (≥ 0.1 in)\\n\\n8.5\\n\\n6.4\\n\\n4.0\\n\\n1.0\\n\\n0.0\\n\\n0.0\\n\\n0.0\\n\\n0.0\\n\\n0.0\\n\\n0.2\\n\\n1.6\\n\\n6.1\\n\\n27.8\\n\\n\\nAverage relative humidity (%)\\n\\n72.2\\n\\n71.6\\n\\n69.7\\n\\n64.9\\n\\n64.1\\n\\n65.6\\n\\n68.5\\n\\n70.7\\n\\n71.1\\n\\n68.6\\n\\n72.5\\n\\n75.5\\n\\n69.6\\n\\n\\nAverage dew point °F (°C)\\n\\n13.6(−10.2)\\n\\n17.6(−8.0)\\n\\n27.1(−2.7)\\n\\n35.8(2.1)\\n\\n45.7(7.6)\\n\\n55.8(13.2)\\n\\n61.7(16.5)\\n\\n61.0(16.1)\\n\\n53.8(12.1)\\n\\n41.7(5.4)\\n\\n31.6(−0.2)\\n\\n20.1(−6.6)\\n\\n38.8(3.8)\\n\\n\\nMean monthly sunshine hours\\n\\n135.8\\n\\n136.2\\n\\n187.0\\n\\n215.3\\n\\n281.9\\n\\n311.4\\n\\n318.4\\n\\n283.0\\n\\n226.6\\n\\n193.2\\n\\n113.3\\n\\n106.3\\n\\n2,508.4\\n\\n\\nPercent possible sunshine\\n\\n46\\n\\n46\\n\\n51\\n\\n54\\n\\n62\\n\\n68\\n\\n69\\n\\n66\\n\\n60\\n\\n56\\n\\n38\\n\\n37\\n\\n56\\n\\n\\nSource: NOAA (relative humidity, dew point and sun 1961–1990)[144][156][157]\\n\\n\\n\\n\\nSunshine data for Chicago\\n\\n\\nMonth\\n\\nJan\\n\\nFeb\\n\\nMar\\n\\nApr\\n\\nMay\\n\\nJun\\n\\nJul\\n\\nAug\\n\\nSep\\n\\nOct\\n\\nNov\\n\\nDec\\n\\nYear\\n\\n\\nMean daily daylight hours\\n\\n10.0\\n\\n11.0\\n\\n12.0\\n\\n13.0\\n\\n15.0\\n\\n15.0\\n\\n15.0\\n\\n14.0\\n\\n12.0\\n\\n11.0\\n\\n10.0\\n\\n9.0\\n\\n12.2\\n\\n\\nSource: Weather Atlas[158]\\n\\nTime zone\\nAs in the rest of the state of Illinois, Chicago forms part of the Central Time Zone. The border with the Eastern Time Zone is located a short distance to the east, used in Michigan and certain parts of Indiana.\\n\\nDemographics\\nMain article: Demographics of Chicago\\n\\n\\nHistorical population\\nCensusPop.Note%±\\n18404,470—185029,963570.3%1860112,172274.4%1870298,977166.5%1880503,18568.3%18901,099,850118.6%19001,698,57554.4%19102,185,28328.7%19202,701,70523.6%19303,376,43825.0%19403,396,8080.6%19503,620,9626.6%19603,550,404−1.9%19703,366,957−5.2%19803,005,072−10.7%19902,783,726−7.4%20002,896,0164.0%20102,695,598−6.9%20202,746,3881.9%2023 (est.)2,664,452[159]−3.0%United States Census Bureau[160]2010–2020[9]\\n\\nDuring its first hundred years, Chicago was one of the fastest-growing cities in the world. When founded in 1833, fewer than 200 people had settled on what was then the American frontier. By the time of its first census, seven years later, the population had reached over 4,000. In the forty years from 1850 to 1890, the city\\'s population grew from slightly under 30,000 to over 1\\xa0million. At the end of the 19th century, Chicago was the fifth-largest city in the world,[161] and the largest of the cities that did not exist at the dawn of the century. Within sixty years of the Great Chicago Fire of 1871, the population went from about 300,000 to over 3\\xa0million,[162] and reached its highest ever recorded population of 3.6\\xa0million for the 1950 census.\\nFrom the last two decades of the 19th century, Chicago was the destination of waves of immigrants from Ireland, Southern, Central and Eastern Europe, including Italians, Jews, Russians, Poles, Greeks, Lithuanians, Bulgarians, Albanians, Romanians, Turks, Croatians, Serbs, Bosnians, Montenegrins and Czechs.[163][164] To these ethnic groups, the basis of the city\\'s industrial working class, were added an additional influx of African Americans from the American South—with Chicago\\'s black population doubling between 1910 and 1920 and doubling again between 1920 and 1930.[163] Chicago has a significant Bosnian population, many of whom arrived in the 1990s and 2000s.[165]\\nIn the 1920s and 1930s, the great majority of African Americans moving to Chicago settled in a so‑called \"Black Belt\" on the city\\'s South Side.[163] A large number of blacks also settled on the West Side. By 1930, two-thirds of Chicago\\'s black population lived in sections of the city which were 90% black in racial composition.[163] Around that time, a lesser known fact about African Americans on the North Side is that the block of 4600 Winthrop Avenue in Uptown was the only block African Americans could live or open establishments.[166][167] Chicago\\'s South Side emerged as United States second-largest urban black concentration, following New York\\'s Harlem. In 1990, Chicago\\'s South Side and the adjoining south suburbs constituted the largest black majority region in the entire United States.[163] Since the 1980s, Chicago has had a massive exodus of  African Americans (primarily from the South and West sides) to its suburbs or outside its metropolitan area.[168] The above average crime and cost of living were leading reasons for the fast declining African American population in Chicago.[169][170][171]\\nMost of Chicago\\'s foreign-born population were born in Mexico, Poland or India.[172]\\nChicago\\'s population declined in the latter half of the 20th century, from over 3.6\\xa0million in 1950 down to under 2.7\\xa0million by 2010. By the time of the official census count in 1990, it was overtaken by Los Angeles as the United States\\' second largest city.[173]\\nThe city has seen a rise in population for the 2000 census and after a decrease in 2010, it rose again for the 2020 census.[174]\\nAccording to U.S. census estimates as of July\\xa02019[update], Chicago\\'s largest racial or ethnic group is non-Hispanic White at 32.8% of the population, Blacks at 30.1% and the Hispanic population at 29.0% of the population.[175][176][177][178]\\n\\n\\n\\nRacial composition\\n\\n2020[179]\\n2010[180]\\n1990[178]\\n1970[178]\\n1940[178]\\n\\n\\nWhite (non-Hispanic)\\n\\n31.4%\\n31.7%\\n37.9%\\n59.0%[d]\\n91.2%\\n\\n\\nHispanic or Latino\\n\\n29.8%\\n28.9%\\n19.6%\\n7.4%[d]\\n0.5%\\n\\n\\nBlack or African American (non-Hispanic)\\n\\n28.7%\\n32.3%\\n39.1%\\n32.7%\\n8.2%\\n\\n\\nAsian (non-Hispanic)\\n\\n6.9%\\n5.4%\\n3.7%\\n0.9%\\n0.1%\\n\\n\\nTwo or more races (non-Hispanic)\\n\\n2.6%\\n1.3%\\nn/a\\nn/a\\nn/a\\n\\nEthnic origins in Chicago\\nMap of racial distribution in Chicago, 2010 U.S. census. Each dot is 25 people: ⬤\\xa0White ⬤\\xa0Black ⬤\\xa0Asian ⬤\\xa0Hispanic ⬤\\xa0Other\\n\\nRacial and ethnic composition as of the 2020 census[181][182]\\n\\n\\nRace or Ethnicity\\n\\nRace Alone\\n\\nTotal [e]\\n\\n\\nWhite\\n\\n35.9%\\n\\n35.9\\xa0\\n\\n45.6%\\n\\n45.6\\xa0\\n\\n\\nBlack or African American\\n\\n29.2%\\n\\n29.2\\xa0\\n\\n30.8%\\n\\n30.8\\xa0\\n\\n\\nHispanic or Latino[f]\\n\\n—\\n\\n\\n\\n29.8%\\n\\n29.8\\xa0\\n\\n\\nAsian\\n\\n7.0%\\n\\n7\\xa0\\n\\n8.0%\\n\\n8\\xa0\\n\\n\\nNative American\\n\\n1.3%\\n\\n1.3\\xa0\\n\\n2.6%\\n\\n2.6\\xa0\\n\\n\\nMixed\\n\\n10.8%\\n\\n10.8\\xa0\\n\\n—\\n\\n\\n\\n\\nOther\\n\\n15.8%\\n\\n15.8\\xa0\\n\\n—\\n\\n\\n\\nChicago has the third-largest LGBT population in the United States. In 2018, the Chicago Department of Health, estimated 7.5% of the adult population, approximately 146,000 Chicagoans, were LGBTQ.[183] In 2015, roughly 4% of the population identified as LGBT.[184][185] Since the 2013 legalization of same-sex marriage in Illinois, over 10,000 same-sex couples have wed in Cook County, a majority of them in Chicago.[186][187]\\nChicago became a \"de jure\" sanctuary city in 2012 when Mayor Rahm Emanuel and the City Council passed the Welcoming City Ordinance.[188]\\nAccording to the U.S. Census Bureau\\'s American Community Survey data estimates for 2022, the median income for a household in the city was $70,386,and the per capita income was $45,449. Male full-time workers had a median income of $68,870 versus $60,987 for females.[189] About 17.2% of the population lived below the poverty line.[190] In 2018, Chicago ranked seventh globally for the highest number of ultra-high-net-worth residents with roughly 3,300 residents worth more than $30\\xa0million.[191]\\nAccording to the 2022 American Community Survey, the specific ancestral groups having 10,000 or more persons in Chicago were:[192][193][194]\\n\\n\\nMexican (586,906)\\nGerman (200,726)\\nIrish (184,983)\\nPolish (129,468)\\nItalian (100,915)\\nPuerto Rican (101,625)\\nEnglish (87,282)\\nChinese (67,951)\\nIndian (48,535)\\nFilipino (39,048)\\nFrench (25,629)\\nRussian (24,707)\\nSwedish (21,795)\\nArab (19,432)\\nWest Indian (18,636)\\nGuatemalan (18,205)\\nScottish (17,121)\\nKorean (16,224)\\nEcuadorian (15,935)\\nNigerian (15,064)\\nGreek (14,946)\\nNorwegian (13,391)\\nColombian (13,785)\\nUkrainian (12,956)\\nVietnamese (12,280)\\nCuban (11,765)\\nCzech (11,313)\\nRomanian (11,237)\\nLithuanian (11,235)\\nDutch (11,196)\\nPersons who did not report or classify an ancestry were 548,790.\\n\\nReligion\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReligion in Chicago (2014)[195][196]\\n\\n\\xa0\\xa0Protestantism (35%)\\xa0\\xa0Roman Catholicism (34%)\\xa0\\xa0Eastern Orthodoxy (1%)\\xa0\\xa0Jehovah\\'s Witness (1%)\\xa0\\xa0No religion (22%)\\xa0\\xa0Judaism (3%)\\xa0\\xa0Islam (2%)\\xa0\\xa0Buddhism (1%)\\xa0\\xa0Hinduism (1%)\\n\\n\\nAccording to a 2014 study by the Pew Research Center, Christianity is the most prevalently practiced religion in Chicago (71%),[196] with the city being the fourth-most religious metropolis in the United States after Dallas, Atlanta and Houston.[196] Roman Catholicism and Protestantism are the largest branches (34% and 35% respectively), followed by Eastern Orthodoxy and Jehovah\\'s Witnesses with 1% each.[195] Chicago also has a sizable non-Christian population. Non-Christian groups include Irreligious (22%), Judaism (3%), Islam (2%), Buddhism (1%) and Hinduism (1%).[195]\\nChicago is the headquarters of several religious denominations, including the Evangelical Covenant Church and the Evangelical Lutheran Church in America. It is the seat of several dioceses. The Fourth Presbyterian Church is one of the largest Presbyterian congregations in the United States based on memberships.[197] Since the 20th century Chicago has also been the headquarters of the Assyrian Church of the East.[198] In 2014 the Catholic Church was the largest individual Christian denomination (34%), with the Roman Catholic Archdiocese of Chicago being the largest Catholic jurisdiction. Evangelical Protestantism form the largest theological Protestant branch (16%), followed by Mainline Protestants (11%), and historically Black churches (8%). Among denominational Protestant branches, Baptists formed the largest group in Chicago (10%); followed by Nondenominational (5%); Lutherans (4%); and Pentecostals (3%).[195]\\nNon-Christian faiths accounted for 7% of the religious population in 2014. Judaism has at least 261,000 adherents which is 3% of the population, making it the second largest religion.[199][195] A 2020 study estimated the total Jewish population of the Chicago metropolitan area, both religious and irreligious, at 319,500.[200]\\nThe first two Parliament of the World\\'s Religions in 1893 and 1993 were held in Chicago.[201] Many international religious leaders have visited Chicago, including Mother Teresa, the Dalai Lama[202] and Pope John Paul\\xa0II in 1979.[203]\\n\\nEconomy\\nMain article: Economy of Chicago\\nSee also: List of companies in the Chicago metropolitan area\\nFederal Reserve Bank of Chicago\\nThe Chicago Board of Trade Building\\nChicago has the third-largest gross metropolitan product in the United States—about $670.5\\xa0billion according to September 2017 estimates.[204] The city has also been rated as having the most balanced economy in the United States, due to its high level of diversification.[205] The Chicago metropolitan area has the third-largest science and engineering work force of any metropolitan area in the nation.[206] Chicago was the base of commercial operations for industrialists John Crerar, John Whitfield Bunn, Richard Teller Crane, Marshall Field, John Farwell, Julius Rosenwald, and many other commercial visionaries who laid the foundation for Midwestern and global industry.\\nChicago is a major world financial center, with the second-largest central business district in the United States, following Midtown Manhattan.[207] The city is the seat of the Federal Reserve Bank of Chicago, the Bank\\'s Seventh District. The city has major financial and futures exchanges, including the Chicago Stock Exchange, the Chicago Board Options Exchange (CBOE), and the Chicago Mercantile Exchange (the \"Merc\"), which is owned, along with the Chicago Board of Trade (CBOT), by Chicago\\'s CME Group. In 2017, Chicago exchanges traded 4.7\\xa0billion in derivatives.[citation needed] Chase Bank has its commercial and retail banking headquarters in Chicago\\'s Chase Tower.[208] Academically, Chicago has been influential through the Chicago school of economics, which fielded 12 Nobel Prize winners.\\nThe city and its surrounding metropolitan area contain the third-largest labor pool in the United States with about 4.63\\xa0million workers.[209] Illinois is home to 66 Fortune 1000 companies, including those in Chicago.[210] The city of Chicago also hosts 12 Fortune Global 500 companies and 17 Financial Times 500 companies. The city claims three Dow\\xa030 companies: aerospace giant Boeing, which moved its headquarters from Seattle to the Chicago Loop in 2001;[211] McDonald\\'s; and Walgreens Boots Alliance.[212] For six consecutive years from 2013 through 2018, Chicago was ranked the nation\\'s top metropolitan area for corporate relocations.[213] However, three Fortune 500 companies left Chicago in 2022, leaving the city with 35, still second to New York City.[214]\\nManufacturing, printing, publishing, and food processing also play major roles in the city\\'s economy. Several medical products and services companies are headquartered in the Chicago area, including Baxter International, Boeing, Abbott Laboratories, and the Healthcare division of General Electric. Prominent food companies based in Chicago include the world headquarters of Conagra, Ferrara Candy Company, Kraft Heinz, McDonald\\'s, Mondelez International, and Quaker Oats.[citation needed] Chicago has been a hub of the retail sector since its early development, with Montgomery Ward, Sears, and Marshall Field\\'s. Today the Chicago metropolitan area is the headquarters of several retailers, including Walgreens, Sears, Ace Hardware, Claire\\'s, ULTA Beauty, and Crate & Barrel.[citation needed]\\nLate in the 19th century, Chicago was part of the bicycle craze, with the Western Wheel Company, which introduced stamping to the production process and significantly reduced costs,[215] while early in the 20th century, the city was part of the automobile revolution, hosting the Brass Era car builder Bugmobile, which was founded there in 1907.[216] Chicago was also the site of the Schwinn Bicycle Company.\\nChicago is a major world convention destination. The city\\'s main convention center is McCormick Place. With its four interconnected buildings, it is the largest convention center in the nation and third-largest in the world.[217] Chicago also ranks third in the U.S. (behind Las Vegas and Orlando) in number of conventions hosted annually.[218]\\nChicago\\'s minimum wage for non-tipped employees is one of the highest in the nation and reached $15 in 2021.[219][220]\\n\\nCulture and contemporary life\\nMain article: Culture of ChicagoFurther information: List of people from Chicago\\nAerial view of Navy Pier located in the Streeterville neighborhood, one of the most visited attractions in the Midwestern United States.\\nThe city\\'s waterfront location and nightlife attracts residents and tourists alike. Over a third of the city population is concentrated in the lakefront neighborhoods from Rogers Park in the north to South Shore in the south.[221] The city has many upscale dining establishments as well as many ethnic restaurant districts. These districts include the Mexican American neighborhoods, such as Pilsen along 18th street, and La Villita along 26th Street; the Puerto Rican enclave of Paseo Boricua in the Humboldt Park neighborhood; Greektown, along South Halsted Street, immediately west of downtown;[222] Little Italy, along Taylor Street; Chinatown in Armour Square; Polish Patches in West Town; Little Seoul in Albany Park around Lawrence Avenue; Little Vietnam near Broadway in Uptown; and the Desi area, along Devon Avenue in West Ridge.[223]\\nDowntown is the center of Chicago\\'s financial, cultural, governmental, and commercial institutions and the site of Grant Park and many of the city\\'s skyscrapers. Many of the city\\'s financial institutions, such as the CBOT and the Federal Reserve Bank of Chicago, are located within a section of downtown called \"The Loop\", which is an eight-block by five-block area of city streets that is encircled by elevated rail tracks. The term \"The Loop\" is largely used by locals to refer to the entire downtown area as well. The central area includes the Near North Side, the Near South Side, and the Near West Side, as well as the Loop. These areas contribute famous skyscrapers, abundant restaurants, shopping, museums, Soldier Field, convention facilities, parkland, and beaches.[citation needed]\\n\\nNature Boardwalk at the Lincoln Park Zoo, North Side\\nLincoln Park contains the Lincoln Park Zoo and the Lincoln Park Conservatory. The River North Gallery District features the nation\\'s largest concentration of contemporary art galleries outside of New York City.[citation needed] Lake View is home to Boystown, the city\\'s large LGBT nightlife and culture center. The Chicago Pride Parade, held the last Sunday in June, is one of the world\\'s largest with over a million people in attendance.[224]\\nNorth Halsted Street is the main thoroughfare of Boystown.[225]\\nThe South Side neighborhood of Hyde Park is the home of former U.S. President Barack Obama. It also contains the University of Chicago, ranked one of the world\\'s top ten universities,[226] and the Museum of Science and Industry. The 6-mile (9.7\\xa0km) long Burnham Park stretches along the waterfront of the South Side. Two of the city\\'s largest parks are also located on this side of the city: Jackson Park, bordering the waterfront, hosted the World\\'s Columbian Exposition in 1893, and is the site of the aforementioned museum; and slightly west sits Washington Park. The two parks themselves are connected by a wide strip of parkland called the Midway Plaisance, running adjacent to the University of Chicago. The South Side hosts one of the city\\'s largest parades, the annual African American Bud Billiken Parade and Picnic, which travels through Bronzeville to Washington Park. Ford Motor Company has an automobile assembly plant on the South Side in Hegewisch, and most of the facilities of the Port of Chicago are also on the South Side.[citation needed]\\nThe West Side holds the Garfield Park Conservatory, one of the largest collections of tropical plants in any U.S. city. Prominent Latino cultural attractions found here include Humboldt Park\\'s Institute of Puerto Rican Arts and Culture and the annual Puerto Rican People\\'s Parade, as well as the National Museum of Mexican Art and St. Adalbert\\'s Church in Pilsen. The Near West Side holds the University of Illinois at Chicago and was once home to Oprah Winfrey\\'s Harpo Studios, the site of which has been rebuilt as the global headquarters of McDonald\\'s.[citation needed]\\nThe city\\'s distinctive accent, made famous by its use in classic films like The Blues Brothers and television programs like the Saturday Night Live skit \"Bill Swerski\\'s Superfans\", is an advanced form of Inland Northern American English. This dialect can also be found in other cities bordering the Great Lakes such as Cleveland, Milwaukee, Detroit, and Rochester, New York, and most prominently features a rearrangement of certain vowel sounds, such as the short \\'a\\' sound as in \"cat,\" which can sound more like \"kyet\" to outsiders. The accent remains well associated with the city.[227]\\n\\nEntertainment and the arts \\nFurther information: Theater in Chicago, Visual arts of Chicago, and Music of Chicago\\nSee also: List of theaters in Chicago\\nChicago Theatre\\nRenowned Chicago theater companies include the Goodman Theatre in the Loop; the Steppenwolf Theatre Company and Victory Gardens Theater in Lincoln Park; and the Chicago Shakespeare Theater at Navy Pier. Broadway In Chicago offers Broadway-style entertainment at five theaters: the Nederlander Theatre, CIBC Theatre, Cadillac Palace Theatre, Auditorium Building of Roosevelt University, and Broadway Playhouse at Water Tower Place. Polish language productions for Chicago\\'s large Polish speaking population can be seen at the historic Gateway Theatre in Jefferson Park. Since 1968, the Joseph Jefferson Awards are given annually to acknowledge excellence in theater in the Chicago area. Chicago\\'s theater community spawned modern improvisational theater, and includes the prominent groups The Second City and I.O. (formerly ImprovOlympic).[citation needed]\\nThe Chicago Symphony Orchestra (CSO) performs at Symphony Center, and is recognized as one of the best orchestras in the world.[228] Also performing regularly at Symphony Center is the Chicago Sinfonietta, a more diverse and multicultural counterpart to the CSO. In the summer, many outdoor concerts are given in Grant Park and Millennium Park. Ravinia Festival, located 25 miles (40\\xa0km) north of Chicago, is the summer home of the CSO, and is a favorite destination for many Chicagoans. The Civic Opera House is home to the Lyric Opera of Chicago.[229] The Lithuanian Opera Company of Chicago was founded by Lithuanian Chicagoans in 1956,[230] and presents operas in Lithuanian.\\nThe Joffrey Ballet and Chicago Festival Ballet perform in various venues, including the Harris Theater in Millennium Park. Chicago has several other contemporary and jazz dance troupes, such as the Hubbard Street Dance Chicago and Chicago Dance Crash.[citation needed]\\n\\nJay Pritzker Pavilion\\nOther live-music genre which are part of the city\\'s cultural heritage include Chicago blues, Chicago soul, jazz, and gospel. The city is the birthplace of house music (a popular form of electronic dance music) and industrial music, and is the site of an influential hip hop scene. In the 1980s and 90s, the city was the global center for house and industrial music, two forms of music created in Chicago, as well as being popular for alternative rock, punk, and new wave. The city has been a center for rave culture, since the 1980s. A flourishing independent rock music culture brought forth Chicago indie. Annual festivals feature various acts, such as Lollapalooza and the Pitchfork Music Festival.[citation needed] Lollapalooza originated in Chicago in 1991 and at first travelled to many cities, but as of 2005 its home has been Chicago.[231] A 2007 report on the Chicago music industry by the University of Chicago Cultural Policy Center ranked Chicago third among metropolitan U.S. areas in \"size of music industry\" and fourth among all U.S. cities in \"number of concerts and performances\".[232]\\nChicago has a distinctive fine art tradition. For much of the twentieth century, it nurtured a strong style of figurative surrealism, as in the works of Ivan Albright and Ed Paschke. In 1968 and 1969, members of the Chicago Imagists, such as Roger Brown, Leon Golub, Robert Lostutter, Jim Nutt, and Barbara Rossi produced bizarre representational paintings. Henry Darger is one of the most celebrated figures of outsider art.[233]\\n\\nTourism\\nMain article: Tourism in Chicago\\nSee also: List of beaches in Chicago\\nFerries offer sightseeing tours and water-taxi transportation along the Chicago River and Lake Michigan.\\nIn 2014[update], Chicago attracted 50.17\\xa0million domestic leisure travelers, 11.09\\xa0million domestic business travelers and 1.308\\xa0million overseas visitors.[234] These visitors contributed more than US$13.7 billion to Chicago\\'s economy.[234] Upscale shopping along the Magnificent Mile and State Street, thousands of restaurants, as well as Chicago\\'s eminent architecture, continue to draw tourists. The city is the United States\\' third-largest convention destination. A 2017 study by Walk Score ranked Chicago the sixth-most walkable of fifty largest cities in the United States.[235] Most conventions are held at McCormick Place, just south of Soldier Field. Navy Pier, located just east of Streeterville, is 3,000\\xa0ft (910\\xa0m) long and houses retail stores, restaurants, museums, exhibition halls and auditoriums. Chicago was the first city in the world to ever erect a Ferris wheel. The Willis Tower (formerly named Sears Tower) is a popular destination for tourists.[236]\\n\\nMuseums\\nFurther information: List of museums and cultural institutions in Chicago\\nThe Field Museum of Natural History\\nAmong the city\\'s museums are the Adler Planetarium & Astronomy Museum, the Field Museum of Natural History, and the Shedd Aquarium. The Museum Campus joins the southern section of Grant Park, which includes the renowned Art Institute of Chicago. Buckingham Fountain anchors the downtown park along the lakefront. The University of Chicago\\'s Institute for the Study of Ancient Cultures, West Asia & North Africa has an extensive collection of ancient Egyptian and Near Eastern archaeological artifacts. Other museums and galleries in Chicago include the Chicago History Museum, the Driehaus Museum, the DuSable Museum of African American History, the Museum of Contemporary Art, the Peggy Notebaert Nature Museum, the Polish Museum of America, the Museum of Broadcast Communications, the Pritzker Military Library, the Chicago Architecture Foundation, and the Museum of Science and Industry.[citation needed]\\n\\nCuisine\\nSee also: Culture of Chicago §\\xa0Food and drink, Chicago farmers\\' markets, and List of Michelin starred restaurants in Chicago\\nChicago-style deep-dish pizza\\nChicago lays claim to a large number of regional specialties that reflect the city\\'s ethnic and working-class roots. Included among these are its nationally renowned deep-dish pizza; this style is said to have originated at Pizzeria Uno.[237] The Chicago-style thin crust is also popular in the city.[238] Certain Chicago pizza favorites include Lou Malnati\\'s and Giordano\\'s.[239]\\nThe Chicago-style hot dog, typically an all-beef hot dog, is loaded with an array of toppings that often includes pickle relish, yellow mustard, pickled sport peppers, tomato wedges, dill pickle spear and topped off with celery salt on a poppy seed bun.[240] Enthusiasts of the Chicago-style hot dog frown upon the use of ketchup as a garnish, but may prefer to add giardiniera.[241][242][243]\\n\\nA Polish market in Chicago\\nA distinctly Chicago sandwich, the Italian beef sandwich is thinly sliced beef simmered in au jus and served on an Italian roll with sweet peppers or spicy giardiniera. A popular modification is the Combo—an Italian beef sandwich with the addition of an Italian sausage. The Maxwell Street Polish is a grilled or deep-fried kielbasa—on a hot dog roll, topped with grilled onions, yellow mustard, and hot sport peppers.[244]\\nChicken Vesuvio is roasted bone-in chicken cooked in oil and garlic next to garlicky oven-roasted potato wedges and a sprinkling of green peas. The Puerto Rican-influenced jibarito is a sandwich made with flattened, fried green plantains instead of bread. The mother-in-law is a tamale topped with chili and served on a hot dog bun.[245] The tradition of serving the Greek dish saganaki while aflame has its origins in Chicago\\'s Greek community.[246] The appetizer, which consists of a square of fried cheese, is doused with Metaxa and flambéed table-side.[247] Chicago-style barbecue features hardwood smoked rib tips and hot links which were traditionally cooked in an aquarium smoker, a Chicago invention.[248] Annual festivals feature various Chicago signature dishes, such as Taste of Chicago and the Chicago Food Truck Festival.[249]\\nOne of the world\\'s most decorated restaurants and a recipient of three Michelin stars, Alinea is located in Chicago. Well-known chefs who have had restaurants in Chicago include: Charlie Trotter, Rick Tramonto, Grant Achatz, and Rick Bayless. In 2003, Robb Report named Chicago the country\\'s \"most exceptional dining destination\".[250]\\n\\nLiterature\\nFurther information: Chicago literature\\n\"Chicago\" by Carl Sandburg\\nChicago literature finds its roots in the city\\'s tradition of lucid, direct journalism, lending to a strong tradition of social realism. In the Encyclopedia of Chicago, Northwestern University Professor Bill Savage describes Chicago fiction as prose which tries to \"capture the essence of the city, its spaces and its people.\" The challenge for early writers was that Chicago was a frontier outpost that transformed into a global metropolis in the span of two generations. Narrative fiction of that time, much of it in the style of \"high-flown romance\" and \"genteel realism\", needed a new approach to describe the urban social, political, and economic conditions of Chicago.[251] Nonetheless, Chicagoans worked hard to create a literary tradition that would stand the test of time,[252] and create a \"city of feeling\" out of concrete, steel, vast lake, and open prairie.[253] Much notable Chicago fiction focuses on the city itself, with social criticism keeping exultation in check.\\nAt least three short periods in the history of Chicago have had a lasting influence on American literature.[254] These include from the time of the Great Chicago Fire to about 1900, what became known as the Chicago Literary Renaissance in the 1910s and early 1920s, and the period of the Great Depression through the 1940s.\\nWhat would become the influential Poetry magazine was founded in 1912 by Harriet Monroe, who was working as an art critic for the Chicago Tribune. The magazine discovered such poets as Gwendolyn Brooks, James Merrill, and John Ashbery.[255] T.\\xa0S.\\xa0Eliot\\'s first professionally published poem, \"The Love Song of J.\\xa0Alfred Prufrock\", was first published by Poetry. Contributors have included Ezra Pound, William Butler Yeats, William Carlos Williams, Langston Hughes, and Carl Sandburg, among others. The magazine was instrumental in launching the Imagist and Objectivist poetic movements. From the 1950s through 1970s, American poetry continued to evolve in Chicago.[256] In the 1980s, a modern form of poetry performance began in Chicago, the poetry slam.[257]\\n\\nSports\\nMain article: Sports in Chicago\\nSoldier FieldWrigley FieldUnited CenterGuaranteed Rate Field\\nThe city has two Major League Baseball (MLB) teams: the Chicago Cubs of the National League play in Wrigley Field on the North Side; and the Chicago White Sox of the American League play in Guaranteed Rate Field on the South Side. The two teams have faced each other in a World Series only once, in 1906.[258]\\nThe Cubs are the oldest Major League Baseball team to have never changed their city;[259] they have played in Chicago since 1871.[260] They had the dubious honor of having the longest championship drought in American professional sports, failing to win a World Series between 1908 and 2016. The White Sox have played on the South Side continuously since 1901. They have won three World Series titles (1906, 1917, 2005) and six American League pennants, including the first in 1901.\\nThe Chicago Bears, one of the last two remaining charter members of the National Football League (NFL), have won nine NFL Championships, including the 1985 Super Bowl\\xa0XX. The Bears play their home games at Soldier Field.\\nThe Chicago Bulls of the National Basketball Association (NBA) is one of the most recognized basketball teams in the world.[261] During the 1990s, with Michael Jordan leading them, the Bulls won six NBA championships in eight seasons.[262][263]\\nThe Chicago Blackhawks of the National Hockey League (NHL) began play in 1926, and are one of the \"Original Six\" teams of the NHL. The Blackhawks have won six Stanley Cups, including in 2010, 2013, and 2015. Both the Bulls and the Blackhawks play at the United Center.[264]\\n\\n\\nMajor league professional teams in Chicago (ranked by attendance)\\n\\n\\nClub\\n\\nLeague\\n\\nSport\\n\\nVenue\\n\\nAttendance\\n\\nFounded\\n\\nChampionships\\n\\n\\nChicago Bears\\n\\nNFL\\n\\nFootball\\n\\nSoldier Field\\n\\n61,142\\n\\n1919\\n\\n9 Championships (1 Super Bowl)\\n\\n\\nChicago Cubs\\n\\nMLB\\n\\nBaseball\\n\\nWrigley Field\\n\\n41,649\\n\\n1870\\n\\n3 World Series\\n\\n\\nChicago White Sox\\n\\nMLB\\n\\nBaseball\\n\\nGuaranteed Rate Field\\n\\n40,615\\n\\n1900\\n\\n3 World Series\\n\\n\\nChicago Blackhawks\\n\\nNHL\\n\\nIce hockey\\n\\nUnited Center\\n\\n21,653\\n\\n1926\\n\\n6 Stanley Cups\\n\\n\\nChicago Bulls\\n\\nNBA\\n\\nBasketball\\n\\n20,776\\n\\n1966\\n\\n6 NBA Championships\\n\\n\\nChicago Fire\\n\\nMLS\\n\\nSoccer\\n\\nSoldier Field\\n\\n17,383\\n\\n1997\\n\\n1 MLS Cup, 1 Supporters Shield\\n\\n\\nChicago Sky\\n\\nWNBA\\n\\nBasketball\\n\\nWintrust Arena\\n\\n10,387\\n\\n2006\\n\\n1 WNBA Championships\\n\\n\\nChicago Red Stars\\n\\nNWSL\\n\\nSoccer\\n\\nSeatGeek Stadium\\n\\n5,863\\n\\n2013\\n\\n1 WPSL Elite championship\\n\\nChicago Half Marathon on Lake Shore Drive on the South Side\\nChicago Fire FC is a member of Major League Soccer (MLS) and plays at Soldier Field. The Fire have won one league title and four U.S.\\xa0Open Cups, since their founding in 1997. In 1994, the United States hosted a successful FIFA World Cup with games played at Soldier Field.[265]\\nThe Chicago Red Stars are a team in the National Women\\'s Soccer League (NWSL). They previously played in Women\\'s Professional Soccer (WPS), of which they were a founding member, before joining the NWSL in 2013. They play at SeatGeek Stadium in Bridgeview, Illinois.\\nThe Chicago Sky is a professional basketball team playing in the Women\\'s National Basketball Association (WNBA). They play home games at the Wintrust Arena. The team was founded before the 2006 WNBA season began.[266]\\nThe Chicago Marathon has been held each year since 1977 except for 1987, when a half marathon was run in its place. The Chicago Marathon is one of six World Marathon Majors.[267]\\nFive area colleges play in Division\\xa0I conferences: two from major conferences—the DePaul Blue Demons (Big East Conference) and the Northwestern Wildcats (Big Ten Conference)—and three from other D1 conferences—the Chicago State Cougars (Northeast Conference); the Loyola Ramblers (Atlantic 10 Conference); and the UIC Flames (Missouri Valley Conference).[268]\\nChicago has also entered into esports with the creation of the Chicago Huntsmen, a professional Call of Duty team that participates within the CDL.[citation needed]\\n\\nParks and greenspace\\nMain articles: Parks in Chicago, Chicago Boulevard System, and Cook County Forest Preserves\\nBuckingham Fountain is located in Grant Park in the Loop\\nWhen Chicago was incorporated in 1837, it chose the motto Urbs in Horto, a Latin phrase which means \"City in a Garden\". Today, the Chicago Park District consists of more than 570 parks with over 8,000 acres (3,200\\xa0ha) of municipal parkland. There are 31 sand beaches, a plethora of museums, two world-class conservatories, and 50 nature areas.[269] Lincoln Park, the largest of the city\\'s parks, covers 1,200 acres (490\\xa0ha) and has over 20\\xa0million visitors each year, making it third in the number of visitors after Central Park in New York City, and the National Mall and Memorial Parks in Washington, D.C.[270]\\nThere is a historic boulevard system,[271] a network of wide, tree-lined boulevards which connect a number of Chicago parks.[272] The boulevards and the parks were authorized by the Illinois legislature in 1869.[273] A number of Chicago neighborhoods emerged along these roadways in the 19th century.[272] The building of the boulevard system continued intermittently until 1942. It includes nineteen boulevards, eight parks, and six squares, along twenty-six miles of interconnected streets.[274] The Chicago Park Boulevard System Historic District was listed on the National Register of Historic Places in 2018.[275][276]\\nWith berths for more than 6,000 boats, the Chicago Park District operates the nation\\'s largest municipal harbor system.[277] In addition to ongoing beautification and renewal projects for the existing parks, a number of new parks have been added in recent years, such as the Ping Tom Memorial Park in Chinatown, DuSable Park on the Near North Side, and most notably, Millennium Park, which is in the northwestern corner of one of Chicago\\'s oldest parks, Grant Park in the Chicago Loop.[citation needed]\\nThe wealth of greenspace afforded by Chicago\\'s parks is further augmented by the Cook County Forest Preserves, a network of open spaces containing forest, prairie, wetland, streams, and lakes that are set aside as natural areas which lie along the city\\'s outskirts,[278] including both the Chicago Botanic Garden in Glencoe and the Brookfield Zoo in Brookfield.[279] Washington Park is also one of the city\\'s biggest parks; covering nearly 400 acres (160\\xa0ha). The park is listed on the National Register of Historic Places listings in South Side Chicago.[280]\\n\\nLaw and government\\nGovernment\\nMain article: Government of Chicago\\nDaley Plaza and the Chicago Picasso, with City Hall-County Building visible in background. At right, the Daley Center contains the state law courts.\\nThe government of the City of Chicago is divided into executive and legislative branches. The mayor of Chicago is the chief executive, elected by general election for a term of four years, with no term limits. The current mayor is Brandon Johnson. The mayor appoints commissioners and other officials who oversee the various departments. As well as the mayor, Chicago\\'s clerk and treasurer are also elected citywide. The City Council is the legislative branch and is made up of 50 alderpersons, one elected from each ward in the city.[281] The council takes official action through the passage of ordinances and resolutions and approves the city budget.[282]\\nThe Chicago Police Department provides law enforcement and the Chicago Fire Department provides fire suppression and emergency medical services for the city and its residents. Civil and criminal law cases are heard in the Cook County Circuit Court of the State of Illinois court system, or in the Northern District of Illinois, in the federal system. In the state court, the public prosecutor is the Illinois state\\'s attorney; in the Federal court it is the United States attorney.\\n\\nPolitics\\nMain article: Political history of Chicago\\n\\nPresidential election results in Chicago[283]\\n\\n\\nYear\\n\\nDemocratic\\n\\nRepublican\\n\\nOthers\\n\\n\\n2020\\n\\n82.5% 944,735\\n\\n15.8% 181,234\\n\\n1.6% 18,772\\n\\n\\n2016\\n\\n82.9% 912,945\\n\\n12.3% 135,320\\n\\n4.8% 53,262\\n\\nDuring much of the last half of the 19th century, Chicago\\'s politics were dominated by a growing Democratic Party organization. During the 1880s and 1890s, Chicago had a powerful radical tradition with large and highly organized socialist, anarchist and labor organizations.[284] For much of the 20th century, Chicago has been among the largest and most reliable Democratic strongholds in the United States; with Chicago\\'s Democratic vote the state of Illinois has been \"solid blue\" in presidential elections since 1992. Even before then, it was not unheard of for Republican presidential candidates to win handily in downstate Illinois, only to lose statewide due to large Democratic margins in Chicago. The citizens of Chicago have not elected a Republican mayor since 1927, when William Thompson was voted into office. The strength of the party in the city is partly a consequence of Illinois state politics, where the Republicans have come to represent rural and farm concerns while the Democrats support urban issues such as Chicago\\'s public school funding.[citation needed]\\nChicago contains less than 25% of the state\\'s population, but it is split between eight of Illinois\\' 17 districts in the United States House of Representatives. All eight of the city\\'s representatives are Democrats; only two Republicans have represented a significant portion of the city since 1973, for one term each: Robert P. Hanrahan from 1973 to 1975, and Michael Patrick Flanagan from 1995 to 1997.[citation needed]\\nMachine politics persisted in Chicago after the decline of similar machines in other large U.S. cities.[285] During much of that time, the city administration found opposition mainly from a liberal \"independent\" faction of the Democratic Party. The independents finally gained control of city government in 1983 with the election of Harold Washington (in office 1983–1987). From 1989 until May 16, 2011, Chicago was under the leadership of its longest-serving mayor, Richard M. Daley, the son of Richard J. Daley. Because of the dominance of the Democratic Party in Chicago, the Democratic primary vote held in the spring is generally more significant than the general elections in November for U.S. House and Illinois State seats. The aldermanic, mayoral, and other city offices are filled through nonpartisan elections with runoffs as needed.[286]\\nThe city is home of former United States President Barack Obama and First Lady Michelle Obama; Barack Obama was formerly a state legislator representing Chicago and later a U.S. senator. The Obamas\\' residence is located near the University of Chicago in Kenwood on the city\\'s south side.[287]\\n\\nCrime\\nMain articles: Crime in Chicago and Timeline of organized crime in Chicago\\nFord Explorer SUV as a Chicago Police Department vehicle, 2021\\nChicago\\'s crime rate in 2020 was 3,926 per 100,000 people.[288] Chicago experienced major rises in violent crime in the 1920s, in the late 1960s, and in the 2020s.[289][290] Chicago\\'s biggest criminal justice challenges have changed little over the last 50 years, and statistically reside with homicide, armed robbery, gang violence, and aggravated battery. Chicago has a higher murder rate than the larger cities of New York and Los Angeles. However, while it has a large absolute number of crimes due to its size, Chicago is not among the top-25 most violent cities in the United States.[291][292]\\nMurder rates in Chicago vary greatly depending on the neighborhood in question.[293] The neighborhoods of Englewood on the South Side, and Austin on the West side, for example, have homicide rates that are ten times higher than other parts of the city.[294] Chicago has an estimated population of over 100,000 active gang members from nearly 60 factions.[295][296] According to reports in 2013, \"most of Chicago\\'s violent crime comes from gangs trying to maintain control of drug-selling territories,\"[297] and is specifically related to the activities of the Sinaloa Cartel, which is active in several American cities.[298] Violent crime rates vary significantly by area of the city, with more economically developed areas having low rates, but other sections have much higher rates of crime.[297] In 2013, the violent crime rate was 910 per 100,000 people;[299] the murder rate was 10.4 per 100,000 – while high crime districts saw 38.9 murders, low crime districts saw 2.5 murders per 100,000.[300]\\nChicago has a long history of public corruption that regularly draws the attention of federal law enforcement and federal prosecutors.[301] From 2012 to 2019, 33 Chicago alderpersons were convicted on corruption charges, roughly one third of those elected in the time period. A report from the Office of the Legislative Inspector General noted that over half of Chicago\\'s elected alderpersons took illegal campaign contributions in 2013.[302] Most corruption cases in Chicago are prosecuted by the U.S. Attorney\\'s office, as legal jurisdiction makes most offenses punishable as a federal crime.[303]\\n\\nEducation\\nSchools and libraries\\nWhen it was opened in 1991, the central Harold Washington Library appeared in Guinness World Records as the largest municipal public library building in the world.\\nChicago Public Schools (CPS) is the governing body of the school district that contains over 600 public elementary and high schools citywide, including several selective-admission magnet schools. There are eleven selective enrollment high schools in the Chicago Public Schools, designed to meet the needs of Chicago\\'s most academically advanced students. These schools offer a rigorous curriculum with mainly honors and Advanced Placement (AP) courses.[304] Walter Payton College Prep High School is ranked number one in the city of Chicago and the state of Illinois.[305]\\nChicago high school rankings are determined by the average test scores on state achievement tests.[306] The district, with an enrollment exceeding 400,545 students (2013–2014 20th Day Enrollment), is the third-largest in the U.S.[307] On September 10, 2012, teachers for the Chicago Teachers Union went on strike for the first time since 1987 over pay, resources, and other issues.[308] According to data compiled in 2014, Chicago\\'s \"choice system\", where students who test or apply and may attend one of a number of public high schools (there are about 130), sorts students of different achievement levels into different schools (high performing, middle performing, and low performing schools).[309]\\nChicago has a network of Lutheran schools,[310]and several private schools are run by other denominations and faiths, such as the Ida Crown Jewish Academy in West Ridge. The Roman Catholic Archdiocese of Chicago operates Catholic schools, that include Jesuit preparatory schools and others. A number of private schools are completely secular. There are also the private Chicago Academy for the Arts, a high school focused on six different categories of the arts and the public Chicago High School for the Arts, a high school focused on five categories (visual arts, theatre, musical theatre, dance, and music) of the arts.[311]\\nThe Chicago Public Library system operates three regional libraries and 77 neighborhood branches, including the central library.[312]\\n\\nColleges and universities\\nFor a more comprehensive list, see List of colleges and universities in Chicago.\\nThe University of Chicago campus as seen from the Midway Plaisance\\nSince the 1850s, Chicago has been a world center of higher education and research with several universities. These institutions consistently rank among the top \"National Universities\" in the United States, as determined by U.S. News & World Report.[313] Highly regarded universities in Chicago and the surrounding area are the University of Chicago; Northwestern University; Illinois Institute of Technology; Loyola University Chicago; DePaul University; Columbia College Chicago and the University of Illinois at Chicago. Other notable schools include: Chicago State University; the School of the Art Institute of Chicago; East–West University; National Louis University; North Park University; Northeastern Illinois University; Robert Morris University Illinois; Roosevelt University; Saint Xavier University; Rush University; and Shimer College.[314]\\nWilliam Rainey Harper, the first president of the University of Chicago, was instrumental in the creation of the junior college concept, establishing nearby Joliet Junior College as the first in the nation in 1901.[315] His legacy continues with the multiple community colleges in the Chicago proper, including the seven City Colleges of Chicago: Richard J. Daley College, Kennedy–King College, Malcolm X College, Olive–Harvey College, Truman College, Harold Washington College, and Wilbur Wright College, in addition to the privately held MacCormac College.[citation needed]\\nChicago also has a high concentration of post-baccalaureate institutions, graduate schools, seminaries, and theological schools, such as the Adler School of Professional Psychology, The Chicago School of Professional Psychology, the Erikson Institute, The Institute for Clinical Social Work, the Lutheran School of Theology at Chicago, the Catholic Theological Union, the Moody Bible Institute, and the University of Chicago Divinity School.[citation needed]\\n\\nMedia\\nFurther information: Media in Chicago and Chicago International Film Festival\\nWGN began in the early days of radio and developed into a multi-platform broadcaster, including a cable television super-station.\\nChicago was home of The Oprah Winfrey Show from 1986 until 2011 and other Harpo Production operations until 2015.\\nTelevision\\nThe Chicago metropolitan area is a major media hub and the third-largest media market in the United States, after New York City and Los Angeles.[316] Each of the big four U.S. television networks, CBS, ABC, NBC and Fox, directly owns and operates a high-definition television station in Chicago (WBBM 2, WLS 7, WMAQ 5 and WFLD 32, respectively). Former CW affiliate WGN-TV 9, which was owned from its inception by Tribune Broadcasting (now owned by the Nexstar Media Group since 2019), is carried with some programming differences, as \"WGN America\" on cable and satellite TV nationwide and in parts of the Caribbean. WGN America eventually became NewsNation in 2021.\\nChicago has also been the home of several prominent talk shows, including The Oprah Winfrey Show, Steve Harvey Show, The Rosie Show, The Jerry Springer Show, The Phil Donahue Show, The Jenny Jones Show, and more. The city also has one PBS member station (its second: WYCC 20, removed its affiliation with PBS in 2017[317]): WTTW 11, producer of shows such as Sneak Previews, The Frugal Gourmet, Lamb Chop\\'s Play-Along and The McLaughlin Group. As of 2018[update], Windy City Live is Chicago\\'s only daytime talk show, which is hosted by Val Warner and Ryan Chiaverini at ABC7 Studios with a live weekday audience. Since 1999, Judge Mathis also films his syndicated arbitration-based reality court show at the NBC Tower. Beginning in January 2019, Newsy began producing 12 of its 14 hours of live news programming per day from its new facility in Chicago.[citation needed]\\n\\nTelevision stations\\nMost of Chicago\\'s television stations are owned and operated by the big television network companies.\\nThey are:\\n\\nWBBM-TV (2), owned and operated by CBS.\\nWMAQ-TV (5), owned and operated by NBC.\\nWLS-TV (7), owned and operated by ABC.\\nWGN-TV (9), an independent station owned by Nexstar Media Group.\\nWTTW (11), a PBS member station owned by Window to the World Communications, Inc.\\nWCIU-TV (26), a CW and MeTV affiliate owned by Weigel Broadcasting.\\nWFLD (32), owned and operated by Fox.\\nWWTO-TV (35), owned and operated by TBN, licensed in Naperville.\\nWCPX-TV (38), owned and operated by Ion Television.\\nWSNS-TV (44), owned and operated by Telemundo.\\nWPWR-TV (50), owned and operated by MyNetworkTV (Fox), licensed to Gary, Indiana.\\nWYIN (56), a PBS member station owned by Northwest Indiana Public Broadcasting, Inc., licensed in Gary, Indiana.\\nWTVK (59), an independent station owned by Venture Technologies Group, licensed in Oswego, Illinois.\\nWXFT-DT (60), owned and operated by Unimas.\\nWJYS (62), an independent station owned by Millennial Telecommunications, Inc., licensed to Hammond, Indiana.\\nWGBO-DT (66), owned and operated by Univision.\\nNewspapers\\nTwo major daily newspapers are published in Chicago: the Chicago Tribune and the Chicago Sun-Times, with the Tribune having the larger circulation. There are also several regional and special-interest newspapers and magazines, such as Chicago, the Dziennik Związkowy (Polish Daily News), Draugas (the Lithuanian daily newspaper), the Chicago Reader, the SouthtownStar, the Chicago Defender, the Daily Herald, Newcity,[318][319] StreetWise and the Windy City Times. The entertainment and cultural magazine Time Out Chicago and GRAB magazine are also published in the city, as well as local music magazine Chicago Innerview. In addition, Chicago is the home of satirical national news outlet, The Onion, as well as its sister pop-culture publication, The A.V. Club.[320]\\n\\nMovies and filming\\nMain articles: List of movies set in Chicago and List of television shows set in Chicago\\nRadio\\nChicago has five 50,000 watt AM radio stations: the Audacy-owned WBBM and WSCR; the Tribune Broadcasting-owned WGN; the Cumulus Media-owned WLS; and the ESPN Radio-owned WMVP. Chicago is also home to a number of national radio shows, including Beyond the Beltway with Bruce DuMont on Sunday evenings.[citation needed]\\nChicago Public Radio produces nationally aired programs such as PRI\\'s This American Life and NPR\\'s Wait Wait...Don\\'t Tell Me!.[citation needed]\\n\\nInfrastructure\\nTransportation\\nFurther information: Transportation in Chicago\\nAerial photo of the Jane Byrne Interchange (2022) after reconstruction, initially opened in the 1960s\\nChicago is a major transportation hub in the United States. It is an important component in global distribution, as it is the third-largest inter-modal port in the world after Hong Kong and Singapore.[321]\\nThe city of Chicago has a higher than average percentage of households without a car. In 2015, 26.5 percent of Chicago households were without a car, and increased slightly to 27.5 percent in 2016. The national average was 8.7 percent in 2016. Chicago averaged 1.12 cars per household in 2016, compared to a national average of 1.8.[322]\\n\\nParking\\nDue to Chicago\\'s Wheel Tax,[323] residents of Chicago who own a vehicle are required to purchase a Chicago City Vehicle Sticker.[324] In established Residential Parking Zones, only local residents can purchase Zone-specific parking stickers for themselves and guests.[325][326]\\nChicago since 2009 has relinquished rights to its public street parking.[327] In 2008, as Chicago struggled to close a growing budget deficit, the city agreed to a 75-year, $1.16 billion deal to lease its parking meter system to an operating company created by Morgan Stanley, called Chicago Parking Meters LLC. Daley said the \"agreement is very good news for the taxpayers of Chicago because it will provide more than $1 billion in net proceeds that can be used during this very difficult economy.\"[328]\\nThe rights of the parking ticket lease end in 2081, and since 2022 have already recouped over $1.5 billion in revenue for Chicago Parking Meters LLC investors.[329]\\n\\nExpressways\\nFurther information: Roads and expressways in Chicago\\nSeven mainline and four auxiliary interstate highways (55, 57, 65 (only in Indiana), 80 (also in Indiana), 88, 90 (also in Indiana), 94 (also in Indiana), 190, 290, 294, and 355) run through Chicago and its suburbs. Segments that link to the city center are named after influential politicians, with three of them named after former U.S.\\xa0Presidents (Eisenhower, Kennedy, and Reagan) and one named after two-time Democratic candidate Adlai Stevenson.\\nThe Kennedy and Dan Ryan Expressways are the busiest state maintained routes in the entire state of Illinois.[330]\\n\\nTransit systems\\nChicago Union Station, opened in 1925, is the third-busiest passenger rail terminal in the United States.\\nThe Regional Transportation Authority (RTA) coordinates the operation of the three service boards: CTA, Metra, and Pace.\\n\\nThe Chicago Transit Authority (CTA) handles public transportation in the City of Chicago and a few adjacent suburbs outside of the Chicago city limits. The CTA operates an extensive network of buses and a rapid transit elevated and subway system known as the Chicago \"L\" or just the \"L\" (short for \"elevated\"), with lines designated by colors. These rapid transit lines also serve both Midway and O\\'Hare Airports. The CTA\\'s rail lines consist of the Red, Blue, Green, Orange, Brown, Purple, Pink, and Yellow lines. Both the Red and Blue lines offer 24‑hour service which makes Chicago one of a handful of cities around the world (and one of two in the United States, the other being New York City) to offer rail service 24 hours a day, every day of the year, within the city\\'s limits.\\nMetra, the nation\\'s second-most used passenger regional rail network, operates an 11-line commuter rail service in Chicago and throughout the Chicago suburbs. The Metra Electric Line shares its trackage with Northern Indiana Commuter Transportation District\\'s South Shore Line, which provides commuter service between South Bend and Chicago.\\nPace provides bus and paratransit service in over 200 surrounding suburbs with some extensions into the city as well. A 2005 study found that one quarter of commuters used public transit.[331]\\nGreyhound Lines provides inter-city bus service to and from the city at the Chicago Bus Station, and Chicago is also the hub for the Midwest network of Megabus (North America).\\n\\nPassenger rail\\nAmtrak train on the Empire Builder route departs Chicago from Union Station.\\nAmtrak long distance and commuter rail services originate from Union Station.[332] Chicago is one of the largest hubs of passenger rail service in the nation.[333] The services terminate in the San Francisco area, Washington, D.C., New York City, New Orleans, Portland, Seattle, Milwaukee, Quincy, St.\\xa0Louis, Carbondale, Boston, Grand Rapids, Port Huron, Pontiac, Los Angeles, and San Antonio. Future services will terminate at Rockford and Moline. An attempt was made in the early 20th century to link Chicago with New York City via the Chicago\\xa0– New York Electric Air Line Railroad. Parts of this were built, but it was never completed.\\n\\nBicycle and scooter sharing systems\\nIn July 2013, the bicycle-sharing system Divvy was launched with 750 bikes and 75 docking stations[334] It is operated by Lyft for the Chicago Department of Transportation.[335] As of July 2019, Divvy operated 5800 bicycles at 608 stations, covering almost all of the city, excluding Pullman, Rosedale, Beverly, Belmont Cragin and Edison Park.[336]\\nIn May 2019, The City of Chicago announced its Chicago\\'s Electric Shared Scooter Pilot Program, scheduled to run from June 15 to October 15.[337] The program started on June 15 with 10 different scooter companies, including scooter sharing market leaders Bird, Jump, Lime and Lyft.[338] Each company was allowed to bring 250 electric scooters, although both Bird and Lime claimed that they experienced a higher demand for their scooters.[339] The program ended on October 15, with nearly 800,000 rides taken.[340]\\n\\nFreight rail\\nChicago is the largest hub in the railroad industry.[341] All five Class I railroads meet in Chicago. As of 2002[update], severe freight train congestion caused trains to take as long to get through the Chicago region as it took to get there from the West Coast of the country (about 2 days).[342] According to U.S. Department of Transportation, the volume of imported and exported goods transported via rail to, from, or through Chicago is forecast to increase nearly 150 percent between 2010 and 2040.[343] CREATE, the Chicago Region Environmental and Transportation Efficiency Program, comprises about 70 programs, including crossovers, overpasses and underpasses, that intend to significantly improve the speed of freight movements in the Chicago area.[344]\\n\\nAirports\\nFurther information: Transportation in Chicago §\\xa0Airports\\nO\\'Hare International Airport\\nChicago is served by O\\'Hare International Airport, the world\\'s busiest airport measured by airline operations,[345] on the far Northwest Side, and Midway International Airport on the Southwest Side. In 2005, O\\'Hare was the world\\'s busiest airport by aircraft movements and the second-busiest by total passenger traffic.[346] Both O\\'Hare and Midway are owned and operated by the City of Chicago. Gary/Chicago International Airport and Chicago Rockford International Airport, located in Gary, Indiana and Rockford, Illinois, respectively, can serve as alternative Chicago area airports, however they do not offer as many commercial flights as O\\'Hare and Midway. In recent years the state of Illinois has been leaning towards building an entirely new airport in the Illinois suburbs of Chicago.[347] The City of Chicago is the world headquarters for United Airlines, the world\\'s third-largest airline.\\n\\nPort authority\\nMain article: Port of Chicago\\nThe Port of Chicago consists of several major port facilities within the city of Chicago operated by the Illinois International Port District (formerly known as the Chicago Regional Port District). The central element of the Port District, Calumet Harbor, is maintained by the U.S. Army Corps of Engineers.[348]\\n\\nIroquois Landing Lakefront Terminal: at the mouth of the Calumet River, it includes 100 acres (0.40\\xa0km2) of warehouses and facilities on Lake Michigan with over 780,000 square meters (8,400,000 square feet) of storage.\\nLake Calumet terminal: located at the union of the Grand Calumet River and Little Calumet River 6 miles (9.7\\xa0km) inland from Lake Michigan. Includes three transit sheds totaling over 29,000 square meters (310,000 square feet) adjacent to over 900 linear meters (3,000 linear feet) of ship and barge berthing.\\nGrain (14\\xa0million bushels) and bulk liquid (800,000 barrels) storage facilities along Lake Calumet.\\nThe Illinois International Port district also operates Foreign trade zone No.\\xa022, which extends 60 miles (97\\xa0km) from Chicago\\'s city limits.\\nUtilities\\nElectricity for most of northern Illinois is provided by Commonwealth Edison, also known as ComEd. Their service territory borders Iroquois County to the south, the Wisconsin border to the north, the Iowa border to the west and the Indiana border to the east. In northern Illinois, ComEd (a division of Exelon) operates the greatest number of nuclear generating plants in any U.S. state. Because of this, ComEd reports indicate that Chicago receives about 75% of its electricity from nuclear power. Recently, the city began installing wind turbines on government buildings to promote renewable energy.[349][350][351]\\nNatural gas is provided by Peoples Gas, a subsidiary of Integrys Energy Group, which is headquartered in Chicago.\\nDomestic and industrial waste was once incinerated but it is now landfilled, mainly in the Calumet area. From 1995 to 2008, the city had a blue bag program to divert recyclable refuse from landfills.[352] Because of low participation in the blue bag programs, the city began a pilot program for blue bin recycling like other cities. This proved successful and blue bins were rolled out across the city.[353]\\n\\nHealth systems\\nPrentice Women\\'s Hospital on the Northwestern Memorial Hospital Downtown Campus\\nThe Illinois Medical District is on the Near West Side. It includes Rush University Medical Center, ranked as the second best hospital in the Chicago metropolitan area by U.S. News & World Report for 2014–16, the University of Illinois Medical Center at Chicago, Jesse Brown VA Hospital, and John H. Stroger Jr. Hospital of Cook County, one of the busiest trauma centers in the nation.[354]\\nTwo of the country\\'s premier academic medical centers reside in Chicago, including Northwestern Memorial Hospital and the University of Chicago Medical Center. The Chicago campus of Northwestern University includes the Feinberg School of Medicine; Northwestern Memorial Hospital, which is ranked as the best hospital in the Chicago metropolitan area by U.S. News & World Report for 2017–18;[355] the Shirley Ryan AbilityLab (formerly named the Rehabilitation Institute of Chicago), which is ranked the best U.S. rehabilitation hospital by U.S. News & World Report;[356] the new Prentice Women\\'s Hospital; and Ann & Robert H. Lurie Children\\'s Hospital of Chicago.\\nThe University of Illinois College of Medicine at UIC is the second largest medical school in the United States (2,600 students including those at campuses in Peoria, Rockford and Urbana–Champaign).[357]\\nIn addition, the Chicago Medical School and Loyola University Chicago\\'s Stritch School of Medicine are located in the suburbs of North Chicago and Maywood, respectively. The Midwestern University Chicago College of Osteopathic Medicine is in Downers Grove.\\nThe American Medical Association, Accreditation Council for Graduate Medical Education, Accreditation Council for Continuing Medical Education, American Osteopathic Association, American Dental Association, Academy of General Dentistry, Academy of Nutrition and Dietetics, American Association of Nurse Anesthetists, American College of Surgeons, American Society for Clinical Pathology, American College of Healthcare Executives, the American Hospital Association and Blue Cross and Blue Shield Association are all based in Chicago.\\n\\nSister cities\\nMain article: List of sister cities of Chicago\\nSee also\\nChicago area water quality\\nChicago Wilderness\\nGentrification of Chicago\\nIndex of Illinois-related articles\\nList of cities with the most skyscrapers\\nNational Register of Historic Places listings in Central Chicago\\nNational Register of Historic Places listings in North Side Chicago\\nNational Register of Historic Places listings in West Side Chicago\\nUSS Chicago, four ships\\nExplanatory notes\\n\\n\\n^ /ʃɪˈkɑːɡoʊ/ ⓘ shih-KAH-goh, locally also /ʃɪˈkɔːɡoʊ/ shih-KAW-goh;[7] Miami-Illinois: Shikaakwa; Ojibwe: Zhigaagong[8]\\n\\n^ a b Mean monthly maxima and minima (i.e. the expected highest and lowest temperature readings at any point during the year or given month) calculated based on data at said location from 1991 to 2020.\\n\\n^ Official records for Chicago were kept at various locations in downtown from January 1871 to December 31, 1925, University of Chicago from January 1, 1926 to June 30, 1942, Midway Airport from July 1, 1942 to January 16, 1980, and at O\\'Hare Airport since January 17, 1980.[154][155]\\n\\n^ a b From 15% sample\\n\\n^ The total for each race includes those who reported that race alone or in combination with other races. People who reported a combination of multiple races may be counted multiple times, so the sum of all percentages will exceed 100%.\\n\\n^ Hispanic and Latino origins are separate from race in the U.S. Census. The Census does not distinguish between Latino origins alone or in combination. This row counts Hispanics and Latinos of any race.\\n\\n\\nReferences\\nCitations\\n\\n\\n^ a b \"City of Chicago\". Geographic Names Information System. United States Geological Survey, United States Department of the Interior.\\n\\n^ \"2020 U.S. Gazetteer Files\". United States Census Bureau. Archived from the original on March 15, 2022. Retrieved March 15, 2022.\\n\\n^ a b \"QuickFacts: Chicago city, Illinois\". census.gov. United States Census Bureau. Archived from the original on December 18, 2023. Retrieved February 29, 2024.\\n\\n^ \"List of 2020 Census Urban Areas\". census.gov. United States Census Bureau. Archived from the original on January 14, 2023. Retrieved January 8, 2023.\\n\\n^ \"2020 Population and Housing State Data\". United States Census Bureau. Archived from the original on August 24, 2021. Retrieved August 22, 2021.\\n\\n^ \"Total Gross Domestic Product for Chicago-Naperville-Elgin, IL-IN-WI (MSA)\". fred.stlouisfed.org.\\n\\n^ Wells, John C. (1982). Accents of English. Vol.\\xa03: Beyond the British Isles (pp.\\xa0i–xx, 467–674). Cambridge University Press. p.\\xa0476. doi:10.1017/CBO9780511611766. ISBN\\xa00-52128541-0\\xa0.\\n\\n^ Carrico, Natalya (March 18, 2019). \"\\'We\\'re still here\\'\". Chicago Reader. Archived from the original on December 13, 2020. Retrieved January 12, 2021.\\n\\n^ a b \"QuickFacts: Chicago city, Illinois\". United States Census Bureau. Archived from the original on October 7, 2021. Retrieved August 19, 2021.\\n\\n^ Keating, Ann Durkin. \"Metropolitan Growth\". In Grossman, Keating & Reiff (2004).\\n\\n^ a b \"Demography: Chicago as a Modern World City\". Encyclopedia of Chicago. Archived from the original on October 12, 2022. Retrieved March 4, 2022.\\n\\n^ Cohen, Jennie (October 7, 2011). \"Urban Infernos Throughout History\". History. Archived from the original on February 25, 2021. Retrieved June 24, 2017.\\n\\n^ \"Skyscrapers\". Encyclopedia of Chicago. Retrieved June 24, 2017.\\n\\n^ Glancey, Jonathan (October 5, 2015). \"The city that changed architecture forever\". BBC News. Archived from the original on May 11, 2020. Retrieved April 30, 2018.\\n\\n^ a b \"Economy\". World Business Chicago. Archived from the original on February 12, 2017. Retrieved May 3, 2018.\\n\\n^ \"2017 Passenger Summary – Annual Traffic Data\". ACI World. Archived from the original on May 29, 2020. Retrieved November 16, 2019.\\n\\n^ Rodriguez, Alex (January 26, 2014). \"Chicago takes on the world\". Chicago Tribune. Sec. 1 p. 15.\\n\\n^ \"CAGDP2 Gross domestic product (GDP) by county and metropolitan area\". Bureau of Economic Analysis. December 12, 2019. Archived from the original on October 23, 2018. Retrieved December 15, 2019.\\n\\n^ Marovich, Robert M. (2015). A City Called Heaven: Chicago and the Birth of Gospel Music. Urbana, IL: University of Illinois Press. p.\\xa07. ISBN\\xa0978-0-252-08069-2.\\n\\n^ Quaife, Milo M. (1933). Checagou: From Indian Wigwam to Modern City, 1673–1835. Chicago, IL: University of Chicago Press. OCLC\\xa01865758.\\n\\n^ a b Swenson, John F. (Winter 1991). \"Chicagoua/Chicago: The origin, meaning, and etymology of a place name\". Illinois Historical Journal. 84 (4): 235–248. ISSN\\xa00748-8149. OCLC\\xa025174749.\\n\\n^ Marcus, Sarah S. \"Chicago\\'s Twentieth-Century Cultural Exports\". In Grossman, Keating & Reiff (2004).\\n\\n^ Keating, Ann Durkin (2005). Chicagoland: City and Suburbs in the Railroad Age. The University of Chicago Press. p.\\xa025. ISBN\\xa00-226-42882-6. LCCN\\xa02005002198.\\n\\n^ Genzen (2007), pp.\\xa010–11, 14–15.\\n\\n^ Keating (2005), pp.\\xa030–31, 221.\\n\\n^ Swenson, John W (1999). \"Jean Baptiste Point de Sable—The Founder of Modern Chicago\". Early Chicago. Early Chicago, Inc. Archived from the original on January 16, 2005. Retrieved August 8, 2010.\\n\\n^ Genzen (2007), pp.\\xa016–17.\\n\\n^ Buisseret (1990), pp.\\xa022–23, 68, 80–81.\\n\\n^ Keating (2005), pp.\\xa030–32.\\n\\n^ a b \"Timeline: Early Chicago History\". Chicago: City of the Century. WGBH Educational Foundation And Window to the World Communications, Inc. 2003. Archived from the original on March 25, 2009. Retrieved May 26, 2009.\\n\\n^ \"Act of Incorporation for the City of Chicago, 1837\". State of Illinois. Archived from the original on March 7, 2011. Retrieved March 3, 2011.\\n\\n^ Walter Nugent. \"Demography Archived October 12, 2022, at the Wayback Machine\" in Encyclopedia of Chicago. Chicago Historical Society.\\n\\n^ Keating (2005), p.\\xa027.\\n\\n^ Buisseret (1990), pp.\\xa086–98.\\n\\n^ Condit (1973), pp.\\xa030–31.\\n\\n^ Genzen (2007), pp.\\xa024–25.\\n\\n^ Keating (2005), pp.\\xa026–29, 35–39.\\n\\n^ Conzen, Michael P. \"Global Chicago\". Encyclopedia of Chicago. Chicago Historical Society. Archived from the original on November 12, 2015. Retrieved December 6, 2015.\\n\\n^ \"Timeline-of-achievements\". CME Group. Archived from the original on January 7, 2012. Retrieved January 20, 2013.\\n\\n^ \"Stephen Douglas\". University of Chicago. Archived from the original on June 9, 2011. Retrieved May 29, 2011.\\n\\n^ \"Chicago Daily Tribune, Thursday Morning, February 14\". nike-of-samothrace.net. Archived from the original on March 25, 2014. Retrieved May 4, 2009.\\n\\n^ Addis, Cameron (August 22, 2015). \"5 Bull Moose From a Bully Pulpit\". Austin Community College. Archived from the original on February 27, 2021. Retrieved March 21, 2021.\\n\\n^ Condit (1973), pp.\\xa015–18, 243–245.\\n\\n^ Genzen (2007), pp.\\xa027–29, 38–43.\\n\\n^ Buisseret (1990), pp.\\xa0154–155, 172–173, 204–205.\\n\\n^ Buisseret (1990), pp.\\xa0148–149.\\n\\n^ Genzen (2007), pp.\\xa032–37.\\n\\n^ Lowe (2000), pp.\\xa087–97.\\n\\n^ Lowe (2000), p.\\xa099.\\n\\n^ Bruegmann, Robert (2005). \"Built Environment of the Chicago Region\". Encyclopedia of Chicago. Chicago Historical Society. Archived from the original on May 5, 2021. Retrieved December 5, 2013.\\n\\n^ Condit (1973), pp.\\xa09–11.\\n\\n^ Allen, Frederick E. (February 2003). \"Where They Went to See the Future\". American Heritage. 54 (1). Archived from the original on February 20, 2007. Retrieved December 5, 2013.\\n\\n^ a b Lowe (2000), pp.\\xa0121, 129.\\n\\n^ Cain, Louis P. (2005). \"Annexations\". The Electronic Encyclopedia of Chicago. Chicago Historical Society. Retrieved December 14, 2015.\\n\\n^ Chisholm, Hugh, ed. (1911). \"Chicago\"\\xa0. Encyclopædia Britannica. Vol.\\xa06 (11th\\xa0ed.). Cambridge University Press. pp.\\xa0118–125, see page 124, first para. Population.—Of the total population in 1900 not less than 34.6% were foreign-born; the number of persons either born abroad, or born in the United States of foreign parentage (i.e. father or both parents foreign), was 77.4% of the population, and in the total number of males of voting age the foreign-born predominated (53.4%).\\n\\n^ \"Race and Hispanic Origin for Selected Cities and Other Places: Earliest Census to 1990\". U.S. Census Bureau. Archived from the original on August 12, 2012.\\n\\n^ \"Hull House Maps Its Neighborhood\". Encyclopedia of Chicago. Chicago Historical Society. Archived from the original on May 9, 2013. Retrieved April 11, 2013.\\n\\n^ Johnson, Mary Ann. \"Hull House\". Encyclopedia of Chicago. Chicago Historical Society. Archived from the original on March 28, 2013. Retrieved April 12, 2013.\\n\\n^ Sandvick, Clinton (2009). \"Enforcing Medical Licensing in Illinois: 1877–1890\". Yale Journal of Biology and Medicine. 82 (2): 67–74. PMC\\xa02701151. PMID\\xa019562006.\\n\\n^ Beatty, William K. (1991). \"John H. Rauch\\xa0– Public Health, Parks and Politics\". Proceedings of the Institute of Medicine of Chicago. 44: 97–118.\\n\\n^ Condit (1973), pp.\\xa043–49, 58, 318–319.\\n\\n^ Holland, Kevin J. (2001). Classic American Railroad Terminals. Osceola, WI: MBI. pp.\\xa066–91. ISBN\\xa09780760308325. OCLC\\xa045908903.\\n\\n^ United States. Office of the Commissioner of Railroads (1883). Report to the Secretary of the Interior. U.S. Government Printing Office. p.\\xa019. Archived from the original on July 9, 2023. Retrieved July 8, 2020.\\n\\n^ \"Chicago\\'s Rich History\". Chicago Convention and Tourism Bureau. Archived from the original on June 10, 2011. Retrieved June 10, 2011.\\n\\n^ Lowe (2000), pp.\\xa0148–154, 158–169.\\n\\n^ \"Exhibits on the Midway Plaisance, 1893\". Encyclopedia of Chicago. Chicago Historical Society. Archived from the original on October 29, 2012. Retrieved April 12, 2013.\\n\\n^ Harper, Douglas. \"midway\". Chicago Manual Style (CMS). Online Etymology Dictionary. Archived from the original on June 16, 2013. Retrieved April 12, 2013.\\n\\n^ Martin, Elizabeth Anne (1993). \"Detroit and the Great Migration, 1916–1929\". Bentley Historical Library Bulletin. 40. University of Michigan. Archived from the original on June 15, 2008. Retrieved December 5, 2013.\\n\\n^ Darlene Clark Hine (2005). \"Chicago Black Renaissance\". Encyclopedia of Chicago. Chicago Historical Society. Archived from the original on October 17, 2022. Retrieved August 6, 2013.\\n\\n^ Essig, Steven (2005). \"Race Riots\". Encyclopedia of Chicago. Chicago Historical Society. Archived from the original on June 23, 2021. Retrieved August 6, 2013.\\n\\n^ \"Gang (crime)\\xa0– History\". Britannica Online Encyclopedia. 2009. Archived from the original on April 16, 2009. Retrieved June 1, 2009.\\n\\n^ O\\'Brien, John. \"The St. Valentine\\'s Day Massacre\". Chicago Tribune. Archived from the original on May 10, 2013. Retrieved April 12, 2013.\\n\\n^ Robbins, Mark W. (2017). \"5. Rent War! Middle-Class Tenant Organizing\". Middle Class Union: Organizing the \\'Consuming Public\\' in Post-World War I America. University of Michigan Press. doi:10.3998/mpub.9343785. ISBN\\xa0978-0-472-13033-7. JSTOR\\xa010.3998/mpub.9343785. Archived from the original on April 4, 2024. Retrieved April 4, 2024.{{cite book}}:  CS1 maint: bot: original URL status unknown (link)\\n\\n^ \"U.S. Lists Rent War Flats; Tax Dodgers Hunted: Some Landlords Admit \"Error\" in Income\". Chicago Daily Tribune. March 24, 1921. Retrieved April 5, 2024.\\n\\n^ \"Rent Hog Gets Wallop in Bills Passed in Senate: One Measure Gives Tenants 60 Days In Which to Vacate Property\". wikipedialibrary.wmflabs.org. Belleville Daily Advocate. March 30, 1921. Retrieved April 5, 2024.\\n\\n^ \"Love Flees Cold Flats, Tenants\\' Leader Argues: Heated Charges Fly in Heat Ordinance Fight\". Chicago Tribune. December 28, 1921. Retrieved April 11, 2024.\\n\\n^ \"Fine Landlord $25 In Test Case on New Heat Law\". Chicago Tribune. December 7, 1922. Retrieved April 11, 2024.\\n\\n^ \"Progress by Degrees: A History of the Chicago Heat Ordinance - The RentConfident Blog - RentConfident, Chicago IL\". April 30, 2021. Archived from the original on April 30, 2021. Retrieved April 11, 2024.{{cite web}}:  CS1 maint: bot: original URL status unknown (link)\\n\\n^ \"Timeline: Milestones in the American Gay Rights Movement\". PBS. Archived from the original on May 22, 2013. Retrieved April 12, 2013.\\n\\n^ a b \"Great Depression\". Encyclopedia of Chicago. Chicago History Museum. Archived from the original on April 11, 2018. Retrieved April 27, 2018.\\n\\n^ \"Century of Progress World\\'s Fair, 1933–1934 (University of Illinois at Chicago)\\xa0: Home\". Collections.carli.illinois.edu. Archived from the original on July 18, 2011. Retrieved July 3, 2011.\\n\\n^ Robert W. Rydell. \"Century of Progress Exposition\". Encyclopedia of Chicago. Chicago Historical Society. Archived from the original on May 14, 2011. Retrieved July 3, 2011.\\n\\n^ \"Chicago\\'s Long and Extraordinary Labor History\". ibew.org. Retrieved October 24, 2023.\\n\\n^ \"World War II\". Encyclopedia of Chicago. Chicago History Museum. Archived from the original on March 28, 2018. Retrieved April 27, 2018.\\n\\n^ \"CP-1 (Chicago Pile 1 Reactor)\". Argonne National Laboratory. U.S. Department of Energy. Archived from the original on May 8, 2019. Retrieved April 12, 2013.\\n\\n^ Szymczak, Patricia (June 18, 1989). \"O\\'Hare suburbs under fire\". Chicago Tribune. Archived from the original on July 20, 2022. Retrieved July 20, 2022.\\n\\n^ Steffes, Tracey L (2015). \"Managing School Integration and White Flight: The Debate over Chicago\\'s Future in the 1960\\'s\". Journal of Urban History. 42 (4). doi:10.1177/0096144214566970. ISSN\\xa00096-1442. S2CID\\xa0147531740. Archived from the original on July 9, 2023. Retrieved June 24, 2022.\\n\\n^ Mehlhorn, Dmitri (December 1998). \"A Requiem for Blockbusting: Law, Economics, and Race-Based Real Estate Speculation\". Fordham Law Review. 67: 1145–1161.\\n\\n^ Lentz, Richard (1990). Symbols, the News Magazines, and Martin Luther King. LSU Press. p.\\xa0230. ISBN\\xa00-8071-2524-5.\\n\\n^ Mailer, Norman. \"Brief History Of Chicago\\'s 1968 Democratic Convention\". Facts on File, CQ\\'s Guide to U.S. Elections. CNN. Archived from the original on March 18, 2022. Retrieved May 5, 2013.\\n\\n^ Cillizza, Chris (September 23, 2009). \"The Fix – Hall of Fame – The Case for Richard J. Daley\". The Washington Post. Archived from the original on February 1, 2013. Retrieved April 22, 2013.\\n\\n^ Dold, R. Bruce (February 27, 1979). \"Jane Byrne elected mayor of Chicago\". Chicago Tribune. Archived from the original on July 15, 2014. Retrieved April 17, 2020.\\n\\n^ Rivlin, Gary; Larry Bennett (November 25, 2012). \"The legend of Harold Washington\". Chicago Tribune. Archived from the original on May 10, 2013. Retrieved April 12, 2013.\\n\\n^ \"Chicago and the Legacy of the Daley Dynasty\". Time. September 9, 2010. Archived from the original on September 11, 2010. Retrieved April 12, 2013.\\n\\n^ \"National Building Museum to honor Daley for greening of Chicago\". Chicago Tribune. April 8, 2009. Archived from the original on May 10, 2013. Retrieved April 12, 2013.\\n\\n^ a b \"1992 Loop Flood Brings Chaos, Billions In Losses\". CBS2 Chicago. April 14, 2007. Archived from the original on September 27, 2007. Retrieved January 11, 2008.\\n\\n^ \"News: Rahm Emanuel wins Chicago mayoral race\". NBC News. February 23, 2011. Archived from the original on June 1, 2020. Retrieved July 3, 2011.\\n\\n^ Tareen, Sophia; Burnett, Sarah (April 7, 2015). \"Chicago Mayor Rahm Emanuel wins 2nd term in runoff victory\". Business Insider. Archived from the original on April 3, 2019. Retrieved April 3, 2019.\\n\\n^ Bosman, Julie; Smith, Mitch; Davey, Monica (April 2, 2019). \"Lori Lightfoot Is Elected Chicago Mayor, Becoming First Black Woman to Lead City\". The New York Times. Archived from the original on January 1, 2022. Retrieved April 3, 2019.\\n\\n^ Perez, Juan Jr. \"With Mayor Lori Lightfoot\\'s inauguration, 3 women of color now hold top citywide offices: \\'Chicago was ready for this\\'\". Chicago Tribune. MSN. Archived from the original on July 13, 2019. Retrieved May 21, 2019.\\n\\n^ Condit (1973), pp.\\xa05–6.\\n\\n^ Genzen (2007), pp.\\xa06–9.\\n\\n^ Angel, Jim. \"State Climatologist Office for Illinois\". Illinois State Water Survey. Prairie Research Institute. Archived from the original on July 24, 2013. Retrieved August 4, 2013.\\n\\n^ \"Thompson\\'s Plat of 1830\". Chicago Historical Society. 2004. Archived from the original on April 23, 2007. Retrieved July 3, 2011.\\n\\n^ \"The Elevation of Chicago: A Statistical Mystery\". Chicago Public Library. September 29, 2014. Archived from the original on June 9, 2022. Retrieved November 22, 2018.\\n\\n^ \"Chicago Facts\" (PDF). Northeastern Illinois University. p.\\xa046. Archived from the original (PDF) on November 10, 2013. Retrieved August 28, 2013.\\n\\n^ Fulton, Jeff. \"Public Beaches in Chicago\". USA Today. Archived from the original on March 4, 2016. Retrieved August 28, 2013.\\n\\n^ \"Chicago Tribune Classifieds map of Chicagoland\". Chicago Tribune. Archived from the original on July 13, 2022. Retrieved May 4, 2009.\\n\\n^ \"Chicagoland Region\". EnjoyIllinois.com. Illinois Department of Tourism. Archived from the original on September 28, 2011. Retrieved August 14, 2009.\\n\\n^ \"Fast Facts About The Chicagoland Chamber of Commerce\". Chicagoland Chamber of Commerce. Archived from the original on February 9, 2009. Retrieved January 6, 2014.\\n\\n^ \"South Side\". Encyclopedia.chicagohistory.org. August 1, 1971. Archived from the original on October 17, 2007. Retrieved June 10, 2013.\\n\\n^ \"Municipal Flag of Chicago\". Chicago Public Library. Archived from the original on June 15, 2013. Retrieved March 22, 2013.\\n\\n^ \"Lakeview (Chicago, Illinois)\". Chicago Tribune. Archived from the original on September 28, 2013. Retrieved September 25, 2013.\\n\\n^ \"CPS Teacher Housing: Chicago Communities\". Chicago Public Schools. Archived from the original on March 21, 2013. Retrieved March 22, 2013.\\n\\n^ \"List of Chicago Neighborhoods – Chicago\". StreetAdvisor. Archived from the original on July 30, 2011. Retrieved June 10, 2013.\\n\\n^ \"Chicago and its Neighborhoods\". articlecell. Archived from the original on April 10, 2013. Retrieved March 22, 2013.\\n\\n^ \"Gulp! How Chicago Gobbled Its Neighbors\". Archived from the original on January 15, 2016. Retrieved April 20, 2016.\\n\\n^ Condit (1973), pp.\\xa031, 52–53.\\n\\n^ \"Chicago neighborhoods on Walk Score\". walkscore.com. Retrieved July 16, 2024.\\n\\n^ Rodolphe El-Khoury; Edward Robbins (June 19, 2004). Shaping the City: Studies in History, Theory and Urban Design. Taylor & Francis. pp.\\xa060–. ISBN\\xa0978-0-415-26189-0. Archived from the original on July 9, 2023. Retrieved May 9, 2013.\\n\\n^ Lopez, Russell (2012). \"Nineteenth-Century Reform Movements\". Building American Public Health: Urban Planning, Architecture, & the Quest for Better Health in the United States. The 1893 Columbian Exhibition. Palgrave Macmillan. p.\\xa041. ISBN\\xa0978-1-137-00243-3. Archived from the original on July 9, 2023. Retrieved September 19, 2019.\\n\\n^ \"The Home Insurance Building\". Chicago Architecture Info. Archived from the original on September 17, 2014. Retrieved September 23, 2014.\\n\\n^ World\\'s Tallest Cities Archived March 8, 2007, at the Wayback Machine. UltrapolisProject.com.\\n\\n^ \"U.S.A.\\'s tallest buildings – Top 20\". Emporis. Archived from the original on June 3, 2013. Retrieved September 14, 2013.\\n\\n^ Bach (1980), p.\\xa09.\\n\\n^ Lowe (2000), pp.\\xa0118–127.\\n\\n^ Pridmore, Jay (2003). The Merchandise Mart. Pomegranate Communications. ISBN\\xa00-7649-2497-4. LCCN\\xa02003051164.\\n\\n^ Bach (1980), pp.\\xa070, 99–100, 146–147.\\n\\n^ Condit, Carl W. (1998). The Chicago School of Architecture. University of Chicago Press. ISBN\\xa00-226-11455-4.\\n\\n^ Hoffmann, Donald (1984). Frank Lloyd Wright\\'s Robie House: The Illustrated Story of an Architectural Masterpiece. New York: Dover Publications. pp.\\xa019–25. ISBN\\xa00-486-24582-9.\\n\\n^ \"Frederick C. Robie House\". Frank Lloyd Wright Trust. Archived from the original on September 10, 2014. Retrieved September 23, 2014.\\n\\n^ \"Chicago Architecture Foundation River Cruise Aboard Chicago\\'s First Lady Cruises\". Chicago Architecture Foundation – CAF. Archived from the original on June 19, 2018. Retrieved May 29, 2018.\\n\\n^ \"The Public Art Scene You\\'re Missing in Chicago\". Conde Nast Traveler. October 1, 2013. Archived from the original on October 16, 2013. Retrieved November 18, 2013.\\n\\n^ \"Crown Fountain in Millennium Park\". chicago.gov. Retrieved October 24, 2023.\\n\\n^ \"Clarence F. Buckingham Memorial Fountain | Chicago Park District\". www.chicagoparkdistrict.com. Retrieved October 24, 2023.\\n\\n^ \"Climate of Chicago – Illinois State Climatologist\". Archived from the original on March 24, 2020. Retrieved March 24, 2020.\\n\\n^ Mölders, Nicole; Kramm, Gerhard (July 5, 2014). Lectures in Meteorology. Springer. ISBN\\xa0978-3-319-02144-7. Archived from the original on July 9, 2023. Retrieved November 9, 2020.\\n\\n^ \"Chicago, Illinois Köppen Climate Classification (Weatherbase)\". Weatherbase. Archived from the original on May 19, 2020. Retrieved March 24, 2020.\\n\\n^ Photos: The blizzard of 2011 Archived April 22, 2019, at the Wayback Machine Chicago Tribune\\n\\n^ Extreme cold in Midwest will finally begin to ease grasp Archived January 31, 2019, at the Wayback Machine CNN, Holly Yan and Madeline Holcombe, January 31, 2019\\n\\n^ At 23 below, Wednesday marked Chicago\\'s 4th coldest temperature recorded Archived April 23, 2019, at the Wayback Machine Chicago Tribune, Jonathon Berlin and Kori Rumore, January 31, 2019\\n\\n^ University of Iowa student dies during polar vortex; 7 other deaths linked to wintry blast Archived January 31, 2019, at the Wayback Machine Fox News, Stephen Sorace, January 31, 2019\\n\\n^ Weather History: When Does Chicago Typically See Its Final Snow of the Season? Archived March 13, 2023, at the Wayback Machine NBC, April 11, 2022\\n\\n^ a b c d \"NowData – NOAA Online Weather Data\". NWS Romeoville, IL. Retrieved May 14, 2021.\\n\\n^ \"USDA Plant Hardiness Zone Map\". USDA/Agricultural Research Center, PRISM Climate Group Oregon State University. Archived from the original on February 27, 2014. Retrieved June 16, 2014.\\n\\n^ Chicago\\'s Official Records Archived October 28, 2012, at the Wayback Machine. National Weather Service. Retrieved November 25, 2012.\\n\\n^ a b c \"Top 20 Weather Events of the Century for Chicago and Northeast Illinois 1900–1999\". NWS Romeoville, IL. Retrieved June 16, 2014.\\n\\n^ \"A Study of Chicago\\'s Significant Tornadoes\". National Weather Service. NOAA. Archived from the original on November 5, 2013. Retrieved May 10, 2013.\\n\\n^ \"Heat Island Effect\" (PDF). Archived (PDF) from the original on November 1, 2008. Retrieved April 20, 2016.\\n\\n^ \"Ask Tom: Does Chicago Get Lake-Effect Snow?\". Chicago Tribune. Archived from the original on March 7, 2018. Retrieved January 6, 2018.\\n\\n^ \"Station: Chicago Midway AP 3SW, IL\". U.S. Climate Normals 2020: U.S. Monthly Climate Normals (1991–2020). National Climatic Data Center. Retrieved May 14, 2021.\\n\\n^ \"Chicago Midway AP 3 SW, Illinois\". Western Regional Climate Center. Retrieved June 12, 2014.\\n\\n^ \"Chicago, IL - Detailed climate information and monthly weather forecast\". Weather Atlas. Yu Media Group. Retrieved June 29, 2019.\\n\\n^ \"History of the Chicago and Rockford Weather Observation Sites\". weather.gov. Retrieved November 9, 2021.\\n\\n^ ThreadEx\\n\\n^ \"Station: Chicago Ohare Intl AP, IL\". U.S. Climate Normals 2020: U.S. Monthly Climate Normals (1991–2020). National Oceanic and Atmospheric Administration. Retrieved May 14, 2021.\\n\\n^ \"Chicago/O\\'Hare, IL Climate Normals 1961–1990\". National Oceanic and Atmospheric Administration. Retrieved July 18, 2020.\\n\\n^ \"Chicago, Illinois, USA – Monthly weather forecast and Climate data\". Weather Atlas. Archived from the original on April 15, 2019. Retrieved February 7, 2019.\\n\\n^ \"City and Town Population Totals: 2020-2023\". United States Census Bureau. Retrieved May 20, 2024.\\n\\n^ \"Census of Population and Housing\". U.S. Census Bureau. Archived from the original on July 17, 2022. Retrieved March 19, 2007.\\n\\n^ \"Top 10 Cities of the Year 1900\". Geography.about.com. Archived from the original on September 20, 2005. Retrieved May 4, 2009.\\n\\n^ \"Chicago Growth 1850–1990: Maps by Dennis McClendon\". University Illinois Chicago. Archived from the original on December 11, 2012. Retrieved August 19, 2007.\\n\\n^ a b c d e Lizabeth Cohen, Making a New Deal: Industrial Workers in Chicago, 1919–1939. Cambridge, England: Cambridge University Press, 1990; pp.\\xa033–34.\\n\\n^ \"Russians\". encyclopedia.chicagohistory.org. Archived from the original on March 18, 2022. Retrieved January 6, 2022.\\n\\n^ \"Bosnians\". encyclopedia.chicagohistory.org. Archived from the original on March 19, 2023. Retrieved July 2, 2023.\\n\\n^ Giles, Sharon (2023). \"Uptown Girl\". In Sha, Mandy; Lee, Cassandra (eds.). If we can do it, you can, too!. Amazon Digital Services LLC - Kdp. pp.\\xa044–46. ISBN\\xa09798379235413.\\n\\n^ Reddy, Gayatri (November 2, 2021). \"Winthrop Family and Black Resilience on the North Side of Chicago\". The Chicago Reporter. Retrieved December 20, 2023.\\n\\n^ \"Fact Sheet: Black Population Loss in Chicago\" (PDF). Great Cities Institute University of Illinois at Chicago. July 2019. Retrieved April 3, 2024.\\n\\n^ Loury, Alden (June 13, 2023). \"Chicago areas with steep Black population decline see more violence and job loss\". WBEZ Chicago.\\n\\n^ \"As the Black population continues to drop in Chicago and Illinois, few regret their move: \\'I have peace\\'\". Chicago Tribune. November 22, 2021.\\n\\n^ Schamisso, Ben (February 7, 2022). \"Chicago is Seeing an Exodus of Black Americans\". Scripps News. Retrieved April 3, 2024.\\n\\n^ \"Chicago\\'s Immigrants Break Old Patterns\". September 2003.\\n\\n^ Marshall Ingwerson (April 13, 1984). \"It\\'s official: Los Angeles ousts Chicago as No. 2 city\". The Christian Science Monitor. Archived from the original on August 16, 2017. Retrieved January 28, 2017.\\n\\n^ \"U.S. Census website\". United States Census Bureau. Archived from the original on July 9, 2021. Retrieved September 1, 2014.\\n\\n^ \"Comparative Demographic Estimates – 2019 American Community Survey 1-Year Estimates Chicago\". U.S. Census Bureau. Archived from the original on July 9, 2021. Retrieved September 20, 2019.\\n\\n^ Armentrout, Mitchell (September 14, 2017). \"Census: Hispanics surpass blacks as Chicago\\'s 2nd-largest racial group\". Chicago Sun-Times. Archived from the original on September 29, 2017.\\n\\n^ Ross, Jeremy (September 15, 2017). \"Hispanic Population Surges In Chicago, New Census Data Shows\". CBS News. Archived from the original on March 4, 2022.\\n\\n^ a b c d \"Illinois – Race and Hispanic Origin for Selected Cities and Other Places: Earliest Census to 1990\". U.S. Census Bureau. Archived from the original on August 12, 2012. Retrieved April 22, 2012.\\n\\n^ \"2020: DEC Redistricting Data (PL 94-171)\". US Census Bureau. Archived from the original on July 12, 2022. Retrieved February 4, 2022.\\n\\n^ \"Chicago (city), Illinois\". State & County QuickFacts. U.S. Census Bureau. Archived from the original on December 31, 2014.\\n\\n^ \"Explore Census Data\". data.census.gov. Retrieved October 27, 2023.\\n\\n^ \"Explore Census Data\". data.census.gov. Retrieved October 27, 2023.\\n\\n^ \"Healthy Chicago databook: Lesbian, Gay, Bisexual & Transgender Health\" (PDF). Chicago Department of Public Health. March 2018. Archived (PDF) from the original on December 30, 2020. Retrieved November 9, 2021.\\n\\n^ \"San Francisco Metro Area Ranks Highest in LGBT Percentage\". gallup.com. March 20, 2015. Archived from the original on October 22, 2015. Retrieved August 15, 2016.\\n\\n^ \"The Metro Areas With the Largest, and Smallest, Gay Populations\". The New York Times. March 21, 2015. Archived from the original on January 1, 2022.\\n\\n^ Leonor Vivanco (April 18, 2016). \"Same-sex marriage licenses could hit 10,000 in Cook County this summer\". Chicago Tribune. Archived from the original on June 9, 2022. Retrieved April 17, 2020.\\n\\n^ Shields, Nick (August 31, 2016). \"10,000th same-sex couple issued marriage license in Cook County\". Cook County Clerk. Archived from the original (Press release) on December 13, 2016. Retrieved January 6, 2017.\\n\\n^ \"Chapter 2–173 Welcoming City Ordinance\" (PDF). Municipal Code of Chicago. Archived (PDF) from the original on February 2, 2017. Retrieved January 25, 2017.\\n\\n^ \"Grid View: Table B19326 - Census Reporter\". censusreporter.org. Retrieved June 29, 2024.\\n\\n^ \"Census profile: Chicago, IL\". Census Reporter. Retrieved June 29, 2024.\\n\\n^ \"These are the cities with the most ultra-rich people\". Crain\\'s Chicago Business. September 6, 2018. Archived from the original on June 11, 2022. Retrieved September 10, 2018.\\n\\n^ \"Grid View: Table B04006 - Census Reporter\". censusreporter.org. Retrieved June 29, 2024.\\n\\n^ \"Grid View: Table B03001 - Census Reporter\". censusreporter.org. Retrieved June 29, 2024.\\n\\n^ \"Grid View: Table B02018 - Census Reporter\". censusreporter.org. Retrieved June 29, 2024.\\n\\n^ a b c d e \"Religious Landscape Study\". Pew Research Center. May 11, 2015. Archived from the original on March 26, 2022. Retrieved October 24, 2020.\\n\\n^ a b c Major U.S. metropolitan areas differ in their religious profiles Archived June 9, 2021, at the Wayback Machine, Pew Research Center\\n\\n^ \"Table 6 Fifteen Largest PC(USA) Congregations Based on Membership Size, 2014\" (PDF). Research Services, Presbyterian Church (U.S.A.). Archived (PDF) from the original on August 1, 2016. Retrieved January 8, 2017.\\n\\n^ Baum, Wilhelm; Winkler, Dietmar W. (2003). The Church of the East: A Concise History. London-New York: Routledge-Curzon. ISBN\\xa0978-1-134-43019-2. Archived from the original on July 9, 2023. Retrieved October 24, 2020.\\n\\n^ \"World Jewish Population\". SimpleToRemember.com. Archived from the original on July 25, 2005. Retrieved October 24, 2020.\\n\\n^ \"Metropolitan Chicago Jewish Population Study (MCJPS) Interactive Mapping Tool\". Jewish United Fund. Retrieved December 29, 2023.\\n\\n^ Avant, Gerry (September 11, 1993). \"Parliament of World\\'s Religions\". Archived from the original on June 11, 2022. Retrieved September 25, 2018.\\n\\n^ Watts, Greg (2009). Mother Teresa: Faith in the Darkness. Lion Books. pp.\\xa067–. ISBN\\xa0978-0-7459-5283-3. Archived from the original on July 9, 2023. Retrieved June 16, 2015.\\n\\n^ Davis, Robert (October 5, 1979). \"Pope John Paul II in Chicago\". Chicago Tribune. Archived from the original on July 8, 2014. Retrieved September 27, 2013.\\n\\n^ \"Gross Domestic Product by Metropolitan Area, 2016\". bea.gov. Archived from the original on January 11, 2018. Retrieved June 7, 2018.\\n\\n^ \"Moody\\'s: Chicago\\'s Economy Most Balanced in US (January 23, 2003)\" (PDF). Archived from the original (PDF) on November 29, 2003.. Accessed from World Business Chicago.\\n\\n^ \"Washington area richest, most educated in US: report\". The Washington Post. June 8, 2006. Archived from the original on December 22, 2017. Retrieved April 17, 2010.\\n\\n^ \"The ten largest US central business districts | Modern Cities\". moderncities.com. Archived from the original on July 27, 2020. Retrieved February 1, 2020.\\n\\n^ \"JPMorgan History | The History of Our Firm\". Jpmorganchase.com. Archived from the original on January 17, 2010. Retrieved November 6, 2010.\\n\\n^ \"Chicago Area Employment — February 2018\". bls.gov/regions/midwest. U.S. Bureau of Labor Statistics. Archived from the original on September 22, 2018. Retrieved May 3, 2018.\\n\\n^ \"FORTUNE 500 2007: States – Illinois\". CNNMoney.com. Archived from the original on September 8, 2007. Retrieved September 13, 2007.\\n\\n^ \"The World According to GaWC 2008\". Globalization and World Cities Research Network. GaWC Loughborough University. Archived from the original on August 26, 2011. Retrieved April 29, 2009.\\n\\n^ \"Dow 30 Companies\". CNNMoney. Archived from the original on April 24, 2013. Retrieved July 21, 2019.\\n\\n^ \"Chicago Named Nation\\'s Top Metro Area for Corporate Relocation For the Sixth Straight Year\". World Business Chicago. March 25, 2019. Archived from the original on July 21, 2019. Retrieved July 21, 2019.\\n\\n^ Dylan Sharkey (October 17, 2022). \"Chicago\\'s Fortune 500 headquarters are shrinking\". Illinois Policy. Archived from the original on November 15, 2022. Retrieved November 9, 2022. Chicago has lost three Fortune 500 headquarters in 2022.\\n\\n^ Norcliffe 2001, p.\\xa0107\\n\\n^ Clymer 1950, p.\\xa0178\\n\\n^ \"Retrieved January 26, 2010\". Exhibitorhost.com. September 26, 1987. Archived from the original on March 15, 2010. Retrieved April 17, 2010.\\n\\n^ Carpenter, Dave (April 26, 2006). \"Las Vegas rules convention world\". USA Today. Associated Press. Archived from the original on April 27, 2019. Retrieved January 6, 2014.\\n\\n^ \"Minimum Wage\". chicago.gov. Archived from the original on July 24, 2020. Retrieved July 24, 2020.\\n\\n^ Elejalde-Ruiz, Alexia. \"Chicago City Council raises minimum wage to $15 by 2021, but restaurant servers still will get lower tipped wage\". Chicago Tribune. Archived from the original on November 27, 2019. Retrieved February 1, 2020.\\n\\n^ \"Chicago Demographics\" (PDF). City of Chicago. Archived (PDF) from the original on October 14, 2013. Retrieved August 21, 2013.\\n\\n^ Zeldes, Leah A. (August 27, 2009). \"Opaa! Chicago Taste of Greece flies this weekend\". Dining Chicago. Chicago\\'s Restaurant & Entertainment Guide, Inc. Archived from the original on May 24, 2016. Retrieved September 14, 2013.\\n\\n^ \"Ethnic Dining in Chicago\". Frommers. Archived from the original on July 1, 2017. Retrieved September 14, 2013.\\n\\n^ \"How Chicago\\'s Pride Parade Grew from a Small March to a Big Event\". WTTW Chicago. June 28, 2019. Archived from the original on June 9, 2022. Retrieved October 9, 2019.\\n\\n^ Peregrin, Tony (April 25, 2012). \"Instagreeter Program Launches in Boystown\". Chicago Tribune. Archived from the original on April 9, 2022. Retrieved April 1, 2019.\\n\\n^ \"The World University Rankings\". Times Higher Education. Archived from the original on May 29, 2015. Retrieved September 2, 2013.\\n\\n^ Gordon, Matthew J. (2004). \"New York, Philadelphia, and other northern cities: phonology\". Kortmann, Bernd, Kate Burridge, Rajend Mesthrie, Edgar W. Schneider and Clive Upton (eds). A Handbook of Varieties of English Archived July 9, 2023, at the Wayback Machine. Volume 1: Phonology, Volume 2: Morphology and Syntax. Berlin / New York: Mouton de Gruyter. p. 297.\\n\\n^ Huizenga, Tom (November 21, 2008). \"Chicago Symphony Tops U.S. Orchestras\". NPR. Archived from the original on October 28, 2021. Retrieved December 31, 2008.\\n\\n^ \"Lyric Opera House history\". lyricopera.org. Retrieved October 24, 2023.\\n\\n^ \"About the Lithuanian Opera Company, Inc. in Chicago\". Lithuanian Opera Co. Archived from the original on December 21, 2005. Retrieved September 14, 2006.\\n\\n^ \"Lollapalooza | History & Facts\". Encyclopædia Britannica. Archived from the original on October 13, 2022. Retrieved October 13, 2022.\\n\\n^ Lawrence Rothfield; Don Coursey; Sarah Lee; Daniel Silver; Wendy Norri (November 21, 2007). \"Chicago Music City: A Summary Report on the Music Industry in Chicago\" (PDF). The Cultural Policy Center at the University of Chicago. Archived from the original (PDF) on January 16, 2013. Retrieved November 8, 2012.\\n\\n^ June Skinner Sawyers (2012). Chicago Portraits New Edition. Northwestern University Press. p.\\xa084. ISBN\\xa0978-0-8101-2649-7.\\n\\n^ a b \"2014 Chicago Tourism Profile\" (PDF). Choose Chicago. 2015. Archived from the original (PDF) on January 16, 2016. Retrieved June 10, 2015.\\n\\n^ \"2017 City and Neighborhood Rankings\". Walk Score. 2017. Archived from the original on January 31, 2017. Retrieved August 10, 2019.\\n\\n^ \"Skydeck Chicago at Willis Tower\". Skydeck Chicago. Archived from the original on November 21, 2022. Retrieved November 21, 2022.\\n\\n^ Bendersky, Ari (May 8, 2012). \"Chicago\\'s Deep Dish History: It All Started With Uno\\'s\". Eater.com. Archived from the original on July 22, 2012. Retrieved April 27, 2013.\\n\\n^ Fischer, MD, Stuart J. \"Chicago: Landmarks, Pizza, Politics, and Jazz\". American Academy of Orthopaedic Surgeons. Archived from the original on April 2, 2013. Retrieved April 27, 2013.\\n\\n^ Gemignani, Tony. (2014). The pizza bible\\xa0: the world\\'s favorite pizza styles, from Neapolitan, deep-dish, wood-fired, Sicilian, calzones and focaccia to New York, New Haven, Detroit, and more (First\\xa0ed.). Clarkson Potter/Ten Speed. ISBN\\xa0978-1-60774-605-8. OCLC\\xa0879642419.\\n\\n^ \"Classic Chicago Hot Dog\". Emril Lagasse. 1999. Archived from the original on April 15, 2003. Retrieved September 3, 2007.\\n\\n^ \"Recipe Detail: Chicago Style Hot Dog\". Archived from the original on August 15, 2008.\\n\\n^ Gibson, Kelly; Portia Belloc Lowndes (2008). The Slow Food guide to Chicago: Restaurants, markets, bars. Chelsea Green Publishing. p.\\xa0384. ISBN\\xa0978-1-931498-61-6. Archived from the original on July 9, 2023. Retrieved February 18, 2010. no self-respecting Chicagoan would think of using ketchup as a condiment\\xa0...\\n\\n^ Fodor\\'s (2009). Fodor\\'s Chicago 2010. Fodor\\'s. p.\\xa0352. ISBN\\xa0978-1-4000-0860-5. Retrieved February 18, 2010. Make sure to never add ketchup to your Chicago-style hot dog: a major no-no among hot dog aficionados.\\n\\n^ Zeldes, Leah A. (January 22, 2010). \"City of the big sandwiches: Four uncommon Chicago meals on a bun\". Dining Chicago. Chicago\\'s Restaurant & Entertainment Guide, Inc. Archived from the original on May 11, 2011. Retrieved June 16, 2010.\\n\\n^ Sula, Mike (December 26, 1996). \"Omnivorous: On the Trail of the Delta Tamale\". Chicago Reader. Archived from the original on May 5, 2009. Retrieved July 3, 2011.\\n\\n^ \"History\". The Parthenon. Archived from the original on June 8, 2002. Retrieved May 30, 2011.\\n\\n^ Zeldes, Leah A (September 30, 2002). \"How to Eat Like a Chicagoan\". Chicago\\'s Restaurant Guide. Archived from the original on October 1, 2002. Retrieved September 30, 2002.\\n\\n^ \"Don\\'t forget South Side barbecue in Chicago as Texas-style ascends\". Chicago Tribune. September 26, 2022. Retrieved July 27, 2023.\\n\\n^ Raymond, Marcella (June 22, 2019). \"Weekend festival celebrates food trucks in Chicago\". Chicago\\'s Very Own WGN 9. Archived from the original on June 21, 2021. Retrieved March 14, 2021.\\n\\n^ \"Robb Report Editors Name Chicago As Country\\'s Finest Dining Destination\". Robb Report. Archived from the original on January 7, 2014.\\n\\n^ \"Fiction\". chicagohistory.org. Archived from the original on January 18, 2022. Retrieved August 9, 2012.\\n\\n^ \"Literary Cultures\". chicagohistory.org. Archived from the original on October 11, 2022. Retrieved August 9, 2012.\\n\\n^ \"Literary Images of Chicago\". chicagohistory.org. Archived from the original on October 8, 2022. Retrieved August 9, 2012.\\n\\n^ \"Chicago Literary Renaissance\". chicagohistory.org. Archived from the original on September 21, 2022. Retrieved August 9, 2012.\\n\\n^ Goodyear, Dana, \"The Moneyed Muse: What can two hundred million dollars do for poetry?\" Archived June 30, 2014, at the Wayback Machine, article, The New Yorker, February 19 and 26 double issue, 2007\\n\\n^ Diggory, Terence (April 22, 2015). Encyclopedia of the New York School Poets. Infobase Learning. ISBN\\xa0978-1-4381-4066-7. Archived from the original on July 9, 2023. Retrieved April 20, 2018.\\n\\n^ Rodriguez, Luis (January 4, 2011). Hearts and Hands: Creating Community in Violent Times. Seven Stories Press. ISBN\\xa0978-1-60980-057-4. Archived from the original on July 9, 2023. Retrieved April 20, 2018.\\n\\n^ \"When will the White Sox and Cubs meet in the World Series? Sooner than you think\". ESPN. July 26, 2017. Retrieved October 24, 2023.\\n\\n^ Santo, Ron; Pepe, Phil (April 1, 2005). \"Preface by Phil Pepe\". Few and Chosen Cubs: Defining Cubs Greatness Across the Eras. Chicago, IL: Triumph Books. p.\\xa0xxi. ISBN\\xa0978-1-57243-710-4.\\n\\n^ \"MLB Teams and Baseball Encyclopedia\". Baseball-Reference.com. Archived from the original on May 16, 2018. Retrieved April 20, 2016.\\n\\n^ Cahill, Dan (December 22, 2015). \"Bulls are second-most popular U.S. team on Facebook\". Chicago Sun-Times. Archived from the original on August 16, 2017. Retrieved December 21, 2016.\\n\\n^ Martin, Clare. \"The Bulls Dynasty\". National Basketball Association. Archived from the original on December 9, 2013. Retrieved November 10, 2013.\\n\\n^ Markovits, Andrei S.; Rensmann, Lars (2010). Gaming the World: How Sports Are Reshaping Global Politics and Culture. Princeton: Princeton University Press. p.\\xa089. ISBN\\xa0978-0-691-13751-3. Archived from the original on July 9, 2023. Retrieved November 9, 2020.\\n\\n^ \"Frequently Asked Questions | United Center\". www.unitedcenter.com. Retrieved October 24, 2023.\\n\\n^ \"World Cup 2014 countdown: Diana Ross and the opening ceremony of USA\". The Independent. March 4, 2014.\\n\\n^ \"Chicago Tribune 07 Oct 2005, page Page 4-12\". Retrieved July 17, 2023 – via Newspapers.com.\\n\\n^ \"World Marathon Majors\" (PDF). The LaSalle Bank Marathon. Archived from the original (PDF) on October 20, 2006. Retrieved July 25, 2007.\\n\\n^ \"NCAA Members By Division\". NCAA. Archived from the original on April 15, 2013. Retrieved September 25, 2013.\\n\\n^ \"History\". Chicago Park District. Archived from the original on December 19, 2016. Retrieved September 23, 2014.\\n\\n^ \"City Park Facts Report\" (PDF). The Trust for Public Land. February 2014. p.\\xa030. Archived from the original (PDF) on September 20, 2016. Retrieved September 23, 2014.\\n\\n^ \"Chicago Park Boulevard System Historic District\" map Archived September 28, 2015, at the Wayback Machine, City of Chicago. Retrieved March 31, 2016.\\n\\n^ a b \"Biking the Boulevards with Geoffrey Baer\". WTTW. Archived from the original on March 22, 2016. Retrieved March 31, 2016.\\n\\n^ Bledstein, Burton J. \"Chicago\\'s Park & Boulevard System\" (PDF). University of Illinois at Chicago. Archived from the original (PDF) on June 12, 2010. Retrieved April 7, 2016.\\n\\n^ \"Chicago Park Boulevard System Historic District\" Archived March 22, 2016, at the Wayback Machine, The Cultural Landscape Foundation. Retrieved March 31, 2016.\\n\\n^ \"Weekly List of Actions Taken on Properties: 02/01/2019 Through 2/7/2019\". National Park Service. Archived from the original on February 10, 2019. Retrieved February 17, 2019.\\n\\n^ \"National Register of Historic Places Registration Form: Chicago Park Boulevard System Historic District\" (PDF). gis.hpa.state.il.us. November 9, 2018. Archived from the original (PDF) on February 18, 2019. Retrieved February 18, 2019.\\n\\n^ \"Harbors\". Chicago Park District. Archived from the original on June 6, 2016. Retrieved October 9, 2013.\\n\\n^ \"Forest Preserve District of Cook County\". University of Illinois at Chicago. Archived from the original on May 5, 2012. Retrieved August 28, 2013.\\n\\n^ \"Affiliates\". Forest Preserve District of Cook County. Archived from the original on March 8, 2012. Retrieved August 28, 2013.\\n\\n^ National Park Service (2004). \"National Register of Historic Places\". Archived from the original on May 9, 2023. Retrieved May 24, 2023.\\n\\n^ \"City Council, Your Ward & Alderperson\". chicago.gov. Retrieved December 10, 2023.\\n\\n^ \"Chicago Government\". City of Chicago. Archived from the original on November 11, 2018. Retrieved October 13, 2013.\\n\\n^ \"Dave\\'s Redistricting\". Archived from the original on February 28, 2023. Retrieved June 7, 2023.\\n\\n^ Schneirov (1998), pp.\\xa0173–174.\\n\\n^ Montejano (1999), pp.\\xa033–34.\\n\\n^ \"7 big ideas for making Illinois more (small-d) democratic – CHANGE Illinois\". March 28, 2022. Archived from the original on August 16, 2022. Retrieved June 18, 2022.\\n\\n^ Blakley, Derrick (July 27, 2016). \"With Michelle Obama In Town, Speculation About Future For Their Home\". cbslocal.com. Archived from the original on July 30, 2016. Retrieved July 30, 2016.\\n\\n^ \"2021 Year End Summary Crime Statistics\". Chicago Police Department. Archived from the original on July 15, 2022. Retrieved July 15, 2022.\\n\\n^ \"Chicago Police Annual Report 1967\" (PDF). Chicago Police Department. Archived from the original (PDF) on March 4, 2016. Retrieved July 26, 2015.\\n\\n^ \"Chicago Police Annual Report 2017\" (PDF). chicagopolice.org. Chicago Police Department. p.\\xa068. Archived from the original (PDF) on December 3, 2018. Retrieved December 24, 2018.\\n\\n^ Monkovic, Toni; Asher, Jeff (June 16, 2021). \"Why People Misperceive Crime Trends (Chicago Is Not the Murder Capital)\". The New York Times. ISSN\\xa00362-4331. Archived from the original on April 25, 2023. Retrieved July 9, 2023.\\n\\n^ Fieldstadt, Elisha (February 23, 2022). \"Highest murder rates in the U.S. - The most deadly cities\". CBS News. Archived from the original on November 7, 2021. Retrieved July 9, 2023.\\n\\n^ Moser, Whet (August 14, 2012). \"Gawker Glosses Chicago\\'s Murder Problem\". Chicago (August 2012). Chicago Tribune Media Group. Archived from the original on September 3, 2014. Retrieved August 28, 2014.\\n\\n^ Christensen, Jen (March 14, 2014). \"Tackling Chicago\\'s \\'crime gap\\'\". CNN. Archived from the original on August 27, 2014. Retrieved August 28, 2014.\\n\\n^ \"Chicago Gang Violence: By The Numbers\". ABC News. Archived from the original on December 22, 2015. Retrieved December 17, 2015.\\n\\n^ \"Chicago Most Gang-Infested City in U.S., Officials Say\". NBC Chicago. January 26, 2012. Archived from the original on January 2, 2016. Retrieved December 17, 2015.\\n\\n^ a b Lippert, John; Cattan, Nacha; Parker, Mario (September 17, 2013). \"Heroin Pushed on Chicago by Cartel Fueling Gang Murders\". Bloomberg News. Archived from the original on October 6, 2013. Retrieved October 12, 2013.\\n\\n^ \"Probing Ties Between Mexican Cartel And Chicago\\'s Violence\". Morning Edition. NPR. September 17, 2013. Archived from the original on October 14, 2013. Retrieved October 12, 2013.\\n\\n^ Hinz, Greg; Corfman, Thomas. \"Rahm Emanuel\\'s performance as Chicago mayor\". Crain\\'s Chicago Business. Archived from the original on April 19, 2016. Retrieved April 20, 2016.\\n\\n^ Hinz, Greg; Corfman, Thomas. \"Rahm Emanuel\\'s performance as Chicago mayor\". Crain\\'s Chicago Business. Archived from the original on April 19, 2016. Retrieved April 20, 2016.\\n\\n^ \"Chicago\\'s \\'hall of shame\\'\". Chicago Tribune. February 24, 2012. Archived from the original on June 21, 2020. Retrieved June 20, 2020.\\n\\n^ Austin Berg (November 16, 2015). \"More than half of Chicago aldermen took illegal campaign cash in 2013 | City Limits\". Chicagonow.com. Archived from the original on May 27, 2017. Retrieved December 17, 2015.\\n\\n^ \"Northern District of Illinois – Department of Justice\". November 13, 2014. Archived from the original on June 14, 2023. Retrieved July 9, 2023.\\n\\n^ \"Chicago Public Schools\\xa0: Selective enrollment\". Chicago Public Schools. Archived from the original on August 27, 2010. Retrieved August 30, 2010.\\n\\n^ \"These Are the Best High Schools in Illinois\". usnews.com. Archived from the original on June 28, 2018.\\n\\n^ \"Top 100 Chicago-area high schools\". Chicago Sun-Times. Archived from the original on November 1, 2010. Retrieved October 30, 2010.\\n\\n^ \"At-a-glance: Stats and Facts\". Chicago Public Schools. September 17, 2014. Archived from the original on July 30, 2020. Retrieved October 2, 2014.\\n\\n^ \"Chicago teachers on strike\". Time Out Chicago Kids. Archived from the original on September 16, 2012. Retrieved September 10, 2012.\\n\\n^ Lutton, Linda; Metzger, Brendan (July 16, 2014). \"The Big Sort\". WBEZ. Archived from the original on October 6, 2014. Retrieved October 2, 2014.\\n\\n^ Pogorzelski & Maloof 2008, p.\\xa058\\n\\n^ \"Chicago High School for the Arts\". chiarts.org. Archived from the original on September 26, 2016. Retrieved September 26, 2016.\\n\\n^ \"Chicago Public Library\". chicago.gov. Archived from the original on July 9, 2023. Retrieved May 23, 2022.\\n\\n^ \"2024 Best Colleges in Illinois\". U.S. News & World Report. Retrieved October 4, 2023.\\n\\n^ \"Chicago, Illinois Colleges and Universities\". Free-4u.com. Archived from the original on October 16, 2016. Retrieved January 8, 2017.\\n\\n^ \"History\". Joliet Junior College. 2009. Archived from the original on August 1, 2009. Retrieved July 19, 2009.\\n\\n^ \"Nielsen Media 2009–2010 Local Market Estimates\". Nielsen Media Research. Broadcast Employment Services. September 27, 2009. Archived from the original on August 28, 2008. Retrieved May 17, 2010.\\n\\n^ \"Window to The World Communications presents WYCC MHz Worldview beginning April 23, 2018 | WTTW Chicago\". Window To The World Communications. WTTW. April 23, 2018. Archived from the original on March 30, 2019. Retrieved March 29, 2019.\\n\\n^ Hollingsworth, Chauncey (May 10, 1995). \"Shakey Ground: Arts Magazines Find Chicago\\'s Landscape Still Hostile To New Ventures\". Chicago Tribune. Archived from the original on December 4, 2010. Retrieved October 31, 2010. A vast expanse of the local cultural landscape lay unexplored between the realm of free arts weeklies like NewCity and the Reader and commercial ventures like Chicago magazine\\xa0... NewCity wasn\\'t quite as sophisticated two years ago as it is now.\\n\\n^ \"Chicago Daily News II: This Time It\\'s Digital\". Chicago Tribune. December 9, 2005. Archived from the original on December 6, 2010. Retrieved October 31, 2010. The competition\\xa0... Newcity are in the digital space,\\xa0...\\n\\n^ \"The Onion celebrates controversial Chicago move with banjo playing, steak tartare\". Crain\\'s Chicago Business. August 2012. Archived from the original on July 9, 2023. Retrieved June 11, 2013.\\n\\n^ Madigan (2004), p.\\xa052.\\n\\n^ \"Car Ownership in U.S. Cities Data and Map\". Governing. December 9, 2014. Archived from the original on May 11, 2018. Retrieved May 4, 2018.\\n\\n^ \"Chicago Wheel Tax Administrative Rules\" (PDF). City of Chicago Office of the City Clerk. January 27, 2021. Retrieved February 1, 2024.\\n\\n^ \"Vehicle Stickers\". City of Chicago Office of the City Clerk. 2024. Retrieved February 1, 2024.\\n\\n^ \"Residential Zone Parking\". City of Chicago Office of the City Clerk. December 12, 2018. Retrieved February 1, 2024.\\n\\n^ \"Chicago Residential Parking Zones\". jkalov.carto.com. 2015. Retrieved February 1, 2024.\\n\\n^ \"Paying for Parking: It\\'s Snow Joke\". NBC 5 Chicago. January 7, 2010. Archived from the original on August 12, 2023. Retrieved August 12, 2023.\\n\\n^ \"FAIL: The Reader\\'s Parking Meter Investigation; Ben Joravsky and Mick Dumke\\'s report on the privatization of Chicago\\'s parking meters, how the deal went down, and its fallout\". Chicago Reader. December 10, 2009. Joravsky, Ben; Dumke, Mick (April 9, 2009). \"FAIL, Part One: Chicago\\'s Parking Meter Lease Deal; How Daley and his crew hid their process from the public, ignored their own rules, railroaded the City Council, and screwed the taxpayers on the parking meter lease deal\". Chicago Reader. Joravsky, Ben; Dumke, Mick (May 21, 2009). \"FAIL, Part Two: One BILLION Dollars! New evidence suggests Chicago leased out its parking meters for a fraction of what they\\'re worth\". Chicago Reader. Joravsky, Ben; Dumke, Mick (June 18, 2009). \"FAIL, Part Three: The Insiders; Who benefited from the parking meter fiasco\". Chicago Reader.\\n\\n^ \"Parking meter deal gets even worse for Chicago taxpayers, annual audit shows\". Chicago Sun-Times. May 26, 2022. Archived from the original on May 26, 2022. Retrieved August 12, 2023.\\n\\n^ \"Illinois Department of Transportation\". Dot.il.gov. Archived from the original on May 28, 2010. Retrieved April 17, 2010.\\n\\n^ \"New Yorkers are top transit users\" Archived May 16, 2008, at the Wayback Machine, by Les Christie,CNNmoney.com, June 29, 2007. Retrieved September 21, 2009.\\n\\n^ \"Amtrak\". Chicago Union Station. Archived from the original on June 30, 2023. Retrieved June 30, 2023.\\n\\n^ Garcia, Evan (February 23, 2017). \"Chicago Highlighted as the US Railroad Capital by Trains Magazine\". WTTW News. Archived from the original on April 17, 2023. Retrieved June 30, 2023.\\n\\n^ \"Chicago Welcomes Divvy Bike Sharing System\" (Press release). Mayor of Chicago. July 1, 2013. Retrieved December 1, 2019.\\n\\n^ Buckley, Madeline (March 12, 2019). \"Divvy to get $50 million upgrade from Lyft investment in exchange for ride revenue under contract proposal\". Chicago Tribune. Archived from the original on May 13, 2019. Retrieved December 1, 2019.\\n\\n^ Wisniwski, Mary (June 8, 2019). \"City gets ready to spread Divvy bikes to Far South Side\". Chicago Tribune. Archived from the original on October 28, 2019. Retrieved October 28, 2019.\\n\\n^ \"City Of Chicago Announces E-Scooter Pilot Program And Call For Vendors\". CBS 2. May 2, 2019. Archived from the original on March 5, 2020. Retrieved December 1, 2019.\\n\\n^ Wiesniewski, Mary (June 17, 2019). \"Electric shared scooters have arrived in Chicago: Here\\'s what you need to know\". Chicago Tribune. Archived from the original on September 5, 2019. Retrieved December 1, 2019.\\n\\n^ Freund, Sara (August 16, 2019). \"Just like Lime, Bird says biggest rider complaint is not enough scooters\". Curbed Chicago. Archived from the original on August 16, 2019. Retrieved December 1, 2019.\\n\\n^ Hofmann, Eva (December 1, 2019). \"Should Chicago keep e-scooter program going?\". The London Gazette. Archived from the original on November 8, 2019. Retrieved December 1, 2019.\\n\\n^ \"About\". March 19, 2012. Archived from the original on April 19, 2016. Retrieved April 20, 2016.\\n\\n^ Winsor, Jeromie (July 14, 2003). \"Metropolitan Planning Council\". Metroplanning.org. Archived from the original on May 30, 2012. Retrieved May 4, 2009.\\n\\n^ \"CREATE Program Benefits Fact Sheets\". CREATE. Archived from the original on August 14, 2015. Retrieved July 20, 2015.\\n\\n^ \"CREATE projects\". CREATE. CREATE.org. Archived from the original on August 13, 2015. Retrieved July 20, 2015.\\n\\n^ \"Annual Traffic Data – 2015 Preliminary\". Airports Council International. Archived from the original on April 5, 2016. Retrieved May 6, 2015.\\n\\n^ \"Preliminary Traffic Results for 2005 Show Firm Rebound (March 14, 2006)\" (PDF). Archived from the original (PDF) on June 23, 2006.\\xa0(520\\xa0KB). Airports Council International.\\n\\n^ Metsch, Steve. \"Top IDOT official says third airport will be built\". Chicago Sun-Times. Archived from the original on August 1, 2012. Retrieved June 11, 2013.\\n\\n^ \"Calumet Harbor and River\". US Army Corps of Engineers. Archived from the original on June 10, 2013. Retrieved June 12, 2013.\\n\\n^ \"IIT.edu\". IIT.edu. June 20, 2003. Archived from the original on June 5, 2008. Retrieved May 4, 2009.\\n\\n^ \"KentLaw.edu\". KentLaw.edu. Archived from the original on September 27, 2007. Retrieved May 4, 2009.\\n\\n^ Martin LaMonica Staff Writer; CNET News. \"\\'Micro\\' wind turbines are coming to town | CNET News.com\". CNET. Archived from the original on July 9, 2023. Retrieved May 4, 2009.\\n\\n^ \"Waste Disposal\". Encyclopedia.chicagohistory.org. Archived from the original on June 5, 2012. Retrieved March 31, 2012.\\n\\n^ Bentley, Chris (July 1, 2015). \"What really happens to Chicago\\'s blue cart recycling?\". WBEZ91.5 Chicago Public Media. Chicago Public Media. Archived from the original on December 11, 2015. Retrieved December 10, 2015.\\n\\n^ Havertz, Rieke. \"Counting Bullets: A Night at a Chicago Trauma Unit\". Pulitzer Center on Crisis Reporting. Archived from the original on August 23, 2013. Retrieved September 2, 2013.\\n\\n^ \"Rankings\". health.usnews.com. Archived from the original on February 13, 2021. Retrieved April 12, 2021.\\n\\n^ \"Rehabilitation Institute of Chicago\". U.S. News & World Report. Archived from the original on August 19, 2013. Retrieved September 2, 2013.\\n\\n^ \"Fact sheet\" (PDF). aamc.org. Archived (PDF) from the original on October 16, 2016. Retrieved October 11, 2016.\\n\\n\\nCited references\\n\\nBach, Ira J. (1980), Chicago\\'s Famous Buildings, The University of Chicago Press, ISBN\\xa00-226-03396-1, LCCN\\xa079023365\\nBuisseret, David (1990), Historic Illinois From The Air, The University of Chicago Press, ISBN\\xa00-226-07989-9, LCCN\\xa089020648\\nClymer, Floyd (1950), Treasury of Early American Automobiles, 1877–1925, New York: Bonanza Books, OCLC\\xa01966986\\nCondit, Carl W. (1973), Chicago 1910–29: Building, Planning, and Urban Technology, The University of Chicago Press, ISBN\\xa00-226-11456-2, LCCN\\xa072094791\\nGenzen, Jonathan (2007), The Chicago River: A History in Photographs, Westcliffe Publishers, Inc., ISBN\\xa0978-1-56579-553-2, LCCN\\xa02006022119\\nGrossman, James R.; Keating, Ann Durkin; Reiff, Janice L., eds. (2004), The Encyclopedia of Chicago, University of Chicago Press, ISBN\\xa00-226-31015-9, OCLC\\xa054454572\\nHolli, Melvin G., and Jones, Peter d\\'A., eds. Biographical Dictionary of American Mayors, 1820-1980 (Greenwood Press, 1981) short scholarly biographies each of the city\\'s mayors 1820 to 1980. online; see index at pp.\\xa0406–411 for list.\\nLowe, David Garrard (2000), Lost Chicago, New York: Watson-Guptill Publications, ISBN\\xa00-8230-2871-2, LCCN\\xa000107305\\nMadigan, Charles, ed. (2004), Global Chicago, Urbana: University of Illinois Press, ISBN\\xa00-252-02941-0, OCLC\\xa054400307\\nMontejano, David, ed. (1999), Chicano Politics and Society in the Late Twentieth Century, Austin: University of Texas Press, ISBN\\xa00-292-75215-6, OCLC\\xa038879251\\nNorcliffe, Glen (2001), The Ride to Modernity: The Bicycle in Canada, 1869–1900, Toronto: University of Toronto Press, ISBN\\xa00-8020-4398-4, OCLC\\xa046625313\\nPogorzelski, Daniel; Maloof, John (2008), Portage Park, Arcadia Publishing, ISBN\\xa0978-0-7385-5229-3, archived from the original on July 9, 2023, retrieved November 9, 2020\\nSchneirov, Richard (1998), Labor and urban politics: class conflict and the origins of modern liberalism in Chicago, 1864–97, Urbana: University of Illinois Press, ISBN\\xa00-252-06676-6, OCLC\\xa037246254\\n\\nFurther reading\\n\\nCronon, William (1992) [1991], Nature\\'s Metropolis: Chicago and the Great West, New York: W.W. Norton, ISBN\\xa00-393-30873-1, OCLC\\xa026609682\\nGranacki, Victoria (2004), Chicago\\'s Polish Downtown, Arcadia Pub, ISBN\\xa0978-0-7385-3286-8, LCCN\\xa02004103888\\nJirasek, Rita Arias; Tortolero, Carlos (2001), Mexican Chicago, Arcadia Pub, ISBN\\xa0978-0-7385-0756-9, LCCN\\xa02001088175\\nMiller, Donald L. (1996), City of the Century: The Epic of Chicago and the Making of America, New York: Simon & Schuster, ISBN\\xa00-684-80194-9, OCLC\\xa0493430274\\nPacyga, Dominic A. (2009), Chicago: A Biography, Chicago: University of Chicago Press, ISBN\\xa0978-0-226-64431-8, OCLC\\xa0298670853\\nSampson, Robert J. (2012), Great American City: Chicago and the Enduring Neighborhood Effect, Chicago: University of Chicago Press, ISBN\\xa0978-0-226-73456-9\\nSawyer, R. Keith (2002), Improvised dialogue: emergence and creativity in conversation, Westport, Conn.: Ablex Pub., ISBN\\xa01-56750-677-1, OCLC\\xa059373382\\nSlaton, Deborah, ed. (1997), Wild Onions: A Brief Guide to Landmarks and Lesser-Known Structures in Chicago\\'s Loop (2nd\\xa0ed.), Champaign, IL: Association for Preservation Technology International, OCLC\\xa042362348\\nSmith, Carl S. (2006), The Plan of Chicago: Daniel Burnham and the Remaking of the American City, Chicago visions + revisions, Chicago: University of Chicago Press, ISBN\\xa00-226-76471-0, OCLC\\xa0261199152\\nSpears, Timothy B. (2005), Chicago dreaming: Midwesterners and the city, 1871–1919, Chicago: University of Chicago Press, ISBN\\xa00-226-76874-0, OCLC\\xa056086689\\nSwanson, Stevenson (1997), Chicago Days: 150 Defining Moments in the Life of a Great City, Chicago Tribune (Firm), Chicago: Cantigny First Division Foundation, ISBN\\xa01-890093-03-3, OCLC\\xa036066057\\nZurawski, Joseph W. (2007), Polish Chicago: Our History—Our Recipes, G. Bradley Pub, Inc., ISBN\\xa0978-0-9774512-2-7\\n\\nExternal links\\nListen to this article (38 minutes)\\nThis audio file was created from a revision of this article dated 22\\xa0July\\xa02005\\xa0(2005-07-22), and does not reflect subsequent edits.(Audio help\\xa0· More spoken articles)\\nEncyclopedia of Chicago (2004), comprehensive coverage of city and suburbs, past and present\\nOfficial website (Website archives at the Wayback Machine (archive index))\\nChoose Chicago—Official tourism website\\nChicago History Archived June 9, 2022, at the Wayback Machine\\nMaps of Chicago from the American Geographical Society Library\\nHistoric American Landscapes Survey (HALS) No.\\xa0IL-10, \"Chicago Cityscape, Chicago, Cook County, IL\", 45\\xa0photos, 4\\xa0photo caption pages\\nChicago – LocalWiki Local Chicago Wiki\\n\"Chicago\"\\xa0. Encyclopædia Britannica. Vol.\\xa06 (11th\\xa0ed.). 1911. pp.\\xa0118–125.\\n\"Chicago\"\\xa0. Encyclopædia Britannica. Vol.\\xa030 (12th\\xa0ed.). 1922.\\nvteChicago\\nArchitecture\\nBeaches\\nClimate\\nColleges and universities\\nCommunity areas\\nCrime\\ngangs\\nCulture\\nDemographics\\nEconomy\\ncompanies\\nExpressways\\nFlag\\nGeography\\nGovernment\\nHarbor\\nHistory\\npolitics\\ntimeline\\nLandmarks\\nLiterature\\nMedia\\nNewspapers\\nMetropolitan area\\nMuseums\\nNeighborhoods\\nParks\\nlist\\nPeople\\nmusic\\nmusicians\\ntheater\\nPublic schools\\nlist\\nSkyscrapers\\nSports\\nTourism\\nTransportation\\nVisual arts\\n\\n Portal\\n Category\\n\\nLinks to related articles\\nvteCommunity areas in ChicagoFar North\\nRogers Park\\nWest Ridge\\nUptown\\nLincoln Square\\nEdison Park\\nNorwood Park\\nJefferson Park\\nForest Glen\\nNorth Park\\nAlbany Park\\nO\\'Hare\\nEdgewater\\nNorthwest\\nPortage Park\\nIrving Park\\nDunning\\nMontclare\\nBelmont Cragin\\nHermosa\\nNorth\\nNorth Center\\nLake View\\nLincoln Park\\nAvondale\\nLogan Square\\nCentral\\nNear North Side\\nThe Loop\\nNear South Side\\nWest\\nHumboldt Park\\nWest Town\\nAustin\\nWest Garfield Park\\nEast Garfield Park\\nNear West Side\\nNorth Lawndale\\nSouth Lawndale\\nLower West Side\\nSouth\\nArmour Square\\nDouglas\\nOakland\\nFuller Park\\nEnglewood\\nGrand Boulevard\\nKenwood\\nWashington Park\\nHyde Park\\nWoodlawn\\nSouth Shore\\nBridgeport\\nGreater Grand Crossing\\nSouthwest\\nGarfield Ridge\\nArcher Heights\\nBrighton Park\\nMcKinley Park\\nNew City\\nWest Elsdon\\nGage Park\\nClearing\\nWest Lawn\\nChicago Lawn\\nWest Englewood\\nFar Southwest\\nAshburn\\nAuburn Gresham\\nBeverly\\nWashington Heights\\nMount Greenwood\\nMorgan Park\\nFar Southeast\\nChatham\\nAvalon Park\\nSouth Chicago\\nBurnside\\nCalumet Heights\\nRoseland\\nPullman\\nSouth Deering\\nEast Side\\nWest Pullman\\nRiverdale\\nHegewisch\\n\\nvteNeighborhoods in ChicagoRecognized by the city\\nAlbany Park\\nAndersonville\\nArcher Heights\\nAshburn\\nAuburn Gresham\\nAvalon Park\\nAvondale\\nBack of the Yards\\nBelmont Cragin\\nBeverly\\nBridgeport\\nBrighton Park\\nBronzeville\\nBucktown\\nBurnside\\nCalumet Heights\\nChatham\\nChinatown\\nClearing\\nDunning\\nEast Side\\nEdgewater\\nEdison Park\\nEnglewood\\nForest Glen\\nGage Park\\nGalewood\\nGarfield Ridge\\nGold Coast\\nGreater Grand Crossing\\nGreektown\\nHegewisch\\nHermosa\\nHoman Square\\nHumboldt Park\\nHyde Park\\nIrving Park\\nJefferson Park\\nKenwood\\nLake View\\nLincoln Park\\nLincoln Square\\nLittle Italy\\nLittle Village\\nLogan Square\\nThe Loop\\nMagnificent Mile\\nMcKinley Park\\nMontclare\\nMorgan Park\\nMount Greenwood\\nNear North Side\\nNear West Side\\nNew City\\nNorth Park\\nOakland\\nO\\'Hare\\nOld Town\\nPilsen\\nPrinter\\'s Row\\nPullman\\nRiver North\\nRiverdale\\nRobert Taylor Homes\\nRogers Park\\nSauganash\\nSouth Chicago\\nSouth Deering\\nSouth Shore\\nStreeterville\\nUkrainian Village\\nUptown\\nWashington Heights\\nWashington Park\\nWest Elsdon\\nWest Ridge\\nWest Town\\nWicker Park\\nWoodlawn\\nWrigleyville\\nOther districts and areas recognized by the community\\nAltgeld Gardens\\nArmour Square\\nBig Oaks\\nBowmanville\\nBoystown\\nBudlong Woods\\nBuena Park\\nCanaryville\\nCentral Station\\nChicago Lawn\\nChrysler Village\\nEast Garfield Park\\nEdgebrook\\nFernwood\\nFuller Park\\nFulton River District\\nGladstone Park\\nGoose Island\\nGrand Boulevard\\nGroveland Park\\nHeart of Chicago\\nHollywood Park\\nIllinois Medical District\\nIndian Village\\nThe Island\\nJeffery Manor\\nJackowo\\nKensington\\nKilbourn Park\\nK-Town\\nKosciuszko Park\\nLilydale\\nLower West Side\\nMargate Park\\nMarquette Park\\nMarshall Square\\nMayfair\\nMuseum Campus\\nNew Chinatown\\nNoble Square\\nNorth Austin\\nNorth Center\\nNorth Halsted\\nNorth Lawndale\\nNorth Mayfair\\nOld Edgebrook\\nOld Irving Park\\nOld Norwood Park\\nOld Town Triangle\\nOriole Park\\nPalmer Square\\nPeterson Park\\nPill Hill\\nPortage Park\\nPrairie District\\nPolish Village\\nPrairie Shores\\nPrinceton Park\\nPulaski Park\\nRavenswood\\nRavenswood Manor\\nRiver West\\nRoseland\\nRosemoor\\nRoscoe Village\\nSaint Ben\\'s\\nScottsdale\\nSheridan Park\\nSmith Park\\nSouth Lawndale\\nSouth Edgebrook\\nTri-Taylor\\nUnion Ridge\\nThe Villa\\nWaclawowo\\nWest Beverly\\nWest Englewood\\nWest Garfield Park\\nWest Lakeview\\nWest Lawn\\nWest Loop Gate\\nWest Pullman\\nWest Rogers Park\\nWildwood\\n\\nvteMunicipalities and communities of Cook County, Illinois, United StatesCounty seat: ChicagoCities\\nBerwyn\\nBlue Island\\nBurbank\\nCalumet City\\nChicago‡\\nChicago Heights\\nCountry Club Hills\\nCountryside\\nDes Plaines\\nElgin‡\\nElmhurst‡\\nEvanston\\nHarvey\\nHickory Hills\\nHometown\\nMarkham\\nNorthlake\\nOak Forest\\nPalos Heights\\nPalos Hills\\nPark Ridge\\nProspect Heights\\nRolling Meadows\\nSummit\\nMap of Illinois highlighting Cook CountyTowns\\nCicero\\nVillages\\nAlsip\\nArlington Heights‡\\nBarrington‡\\nBarrington Hills‡\\nBartlett‡\\nBedford Park\\nBellwood\\nBensenville‡\\nBerkeley\\nBridgeview\\nBroadview\\nBrookfield\\nBuffalo Grove‡\\nBurnham\\nBurr Ridge‡\\nCalumet Park\\nChicago Ridge\\nCrestwood\\nDeer Park‡\\nDeerfield‡\\nDixmoor\\nDolton\\nEast Dundee‡\\nEast Hazel Crest\\nElk Grove Village‡\\nElmwood Park\\nEvergreen Park\\nFlossmoor\\nFord Heights\\nForest Park\\nForest View\\nFrankfort‡\\nFranklin Park\\nGlencoe\\nGlenview\\nGlenwood\\nGolf\\nHanover Park‡\\nHarwood Heights\\nHazel Crest\\nHillside\\nHinsdale‡\\nHodgkins\\nHoffman Estates\\nHomer Glen‡\\nHomewood\\nIndian Head Park\\nInverness\\nJustice\\nKenilworth\\nLa Grange\\nLa Grange Park\\nLansing\\nLemont‡\\nLincolnwood\\nLynwood\\nLyons\\nMatteson‡\\nMaywood\\nMcCook\\nMelrose Park\\nMerrionette Park\\nMidlothian\\nMorton Grove\\nMount Prospect\\nNiles\\nNorridge\\nNorth Riverside\\nNorthbrook\\nNorthfield\\nOak Brook‡\\nOak Lawn\\nOak Park\\nOlympia Fields\\nOrland Hills\\nOrland Park‡\\nPalatine\\nPalos Park\\nPark Forest‡\\nPhoenix\\nPosen\\nRichton Park\\nRiver Forest\\nRiver Grove\\nRiverdale\\nRiverside\\nRobbins\\nRoselle‡\\nRosemont\\nSauk Village‡\\nSchaumburg‡\\nSchiller Park\\nSkokie\\nSouth Barrington\\nSouth Chicago Heights\\nSouth Holland\\nSteger‡\\nStickney\\nStone Park\\nStreamwood\\nThornton\\nTinley Park‡\\nUniversity Park‡\\nWestchester\\nWestern Springs\\nWheeling‡\\nWillow Springs‡\\nWilmette\\nWinnetka\\nWoodridge‡\\nWorth\\nTownships\\nBarrington\\nBerwyn\\nBloom\\nBremen\\nCalumet\\nCicero\\nElk Grove\\nHanover\\nLemont\\nLeyden\\nLyons\\nMaine\\nNew Trier\\nNiles\\nNorthfield\\nNorwood Park\\nOak Park\\nOrland\\nPalatine\\nPalos\\nProviso\\nRich\\nRiver Forest\\nRiverside\\nSchaumburg\\nStickney\\nThornton\\nWheeling\\nWorth\\nFormer: Evanston\\n• Hyde Park\\n• Jefferson\\n• Lake\\n• Lake View\\n• North Chicago\\n• Rogers Park\\n• South Chicago\\n• West Chicago\\n\\nUnincorporatedcommunities\\nCentral Stickney\\nHines\\nIndian Hill\\nLa Grange Highlands\\nNottingham Park\\nSag Bridge\\nSutton\\nOther Communities\\nOrchard Place\\nTechny\\nFootnotes‡This populated place also has portions in an adjacent county or counties\\nIllinois portal\\nUnited States portal\\n\\nvteMunicipalities and communities of DuPage County, Illinois, United StatesCounty seat: WheatonCities\\nAurora‡\\nBatavia‡\\nChicago‡\\nDarien\\nElmhurst‡\\nGeneva‡\\nNaperville‡\\nOakbrook Terrace\\nSt. Charles‡\\nWarrenville\\nWest Chicago\\nWheaton\\nWood Dale\\nMap of Illinois highlighting DuPage CountyVillages\\nAddison\\nBartlett‡\\nBensenville‡\\nBloomingdale\\nBolingbrook‡\\nBurr Ridge‡\\nCarol Stream\\nClarendon Hills\\nDowners Grove\\nElk Grove Village‡\\nGlen Ellyn\\nGlendale Heights\\nHanover Park‡\\nHinsdale‡\\nItasca\\nLemont‡\\nLisle\\nLombard\\nOak Brook‡\\nRoselle‡\\nSchaumburg‡\\nVilla Park\\nWayne‡\\nWestmont\\nWillow Springs‡\\nWillowbrook\\nWinfield\\nWoodridge‡\\nTownships\\nAddison\\nBloomingdale\\nDowners Grove\\nLisle\\nMilton\\nNaperville\\nWayne\\nWinfield\\nYork\\nUnincorporatedcommunities\\nBelmont\\nButterfield\\nCloverdale\\nEola\\nFlowerfield\\nFullersburg\\nKeeneyville\\nLakewood\\nMedinah\\nMunger\\nNorth Glen Ellyn\\nPalisades\\nSouth Elmhurst\\nSwift\\nYork Center\\nGhost towns/Neighborhoods\\nGostyn\\nOntarioville\\nTedens\\nWeston\\nFootnotes‡This populated place also has portions in an adjacent county or counties\\nIllinois portal\\nUnited States portal\\n\\nvteChicago metropolitan areaMajor city\\nChicago\\nCities(over 30,000 in 2020)\\nAurora\\nBerwyn\\nCalumet City\\nCrown Point\\nCrystal Lake\\nDeKalb\\nDes Plaines\\nElgin\\nElmhurst\\nEvanston\\nGary\\nHammond\\nHighland Park\\nJoliet\\nKenosha\\nNaperville\\nNorth Chicago\\nPark Ridge\\nPortage\\nSt. Charles\\nValparaiso\\nWaukegan\\nWheaton\\nTowns and villages(over 30,000 in 2020)\\nAddison\\nArlington Heights\\nBartlett\\nBolingbrook\\nBuffalo Grove\\nCarol Stream\\nCarpentersville\\nCicero\\nDowners Grove\\nElk Grove Village\\nGlendale Heights\\nGlenview\\nGurnee\\nHanover Park\\nHoffman Estates\\nLombard\\nMerrillville\\nMount Prospect\\nMundelein\\nNiles\\nNorthbrook\\nOak Lawn\\nOak Park\\nOrland Park\\nOswego\\nPalatine\\nPlainfield\\nRomeoville\\nSchaumburg\\nSkokie\\nStreamwood\\nTinley Park\\nWheeling\\nWoodridge\\nCounties\\nCook\\nDeKalb\\nDuPage\\nGrundy\\nJasper\\nKane\\nKankakee\\nKendall\\nKenosha\\nLake, IL\\nLake, IN\\nMcHenry\\nNewton\\nPorter\\nWill\\nRegions\\nGreat Lakes\\nNorthern Illinois\\nNorthern Indiana\\nSub-regions\\nChicago Southland\\nEastern Ridges and Lowlands\\nFox Valley (Illinois)\\nGolden Corridor\\nIllinois Technology and Research Corridor\\nNorth Shore (Chicago)\\nNorthwest Indiana\\nIllinois, United States\\nvteLandmarks in ChicagoApartments\\n860–880 Lake Shore Drive Apartments\\nBelle Shore Apartment Hotel\\nBrewster Apartments\\nBryn Mawr Apartment Hotel\\nFisher Studio Houses\\nHotel St. Benedict Flats\\nKeck-Gottschalk-Keck Apartments\\nKenna Apartments\\nPowhatan Apartments\\nThree Arts Club\\nWaller Apartments\\nYale Apartments\\nCulture\\nAdler Planetarium\\nAuditorium Building\\nBiograph Theater\\nBush Temple of Music\\nChess Records Office and Studio\\nChicago Bee Building\\nChicago Cultural Center\\nChicago Theatre\\nCivic Opera Building\\nCongress Theater\\nEssanay Studios\\nField Museum of Natural History\\nFine Arts Building\\nFormer Chicago Historical Society Building\\nHarris and Selwyn Theaters\\nKrause Music Store\\nLorado Taft Midway Studios\\nMedinah Temple\\nMuseum of Science and Industry\\nNavy Pier Headhouse and Auditorium\\nNew Regal Theater\\nOrchestra Hall\\nPortage Theater\\nPui Tak Center\\nShedd Aquarium\\nSouth Shore Cultural Center\\nSouth Side Community Art Center\\nSunset Cafe\\nThalia Hall\\nThree Arts Club\\nTree Studio Building and Annexes\\nUnity Hall\\nUptown Theatre\\nWhistle Stop Inn\\nWoman\\'s Athletic Club\\nYondorf Block and Hall\\nEducation\\nAmerican School of Correspondence\\nAssumption School\\nDuSable High School\\nGeorge Herbert Jones Laboratory\\nImmaculata High School\\nJames Ward Public School\\nLindblom Math & Science Academy\\nMain Building and Machinery Hall, Illinois Institute of Technology\\nSchurz High School\\nSexton School\\nSt. Ignatius College Prep\\nWendell Phillips Academy High School\\nHistoric districts\\nAlta Vista Terrace\\nArlington & Roslyn Place\\nArlington-Deming\\nArmitage-Halsted\\nAstor Street\\nBeverly/Morgan Railroad\\nBissell Street\\nBlack Metropolis–Bronzeville\\nBurling Row House\\nCalumet/Giles Prairie\\nCermak Road Bridge\\nDover Street\\nEast Lake Shore Drive\\nEast Village\\nFive Houses on Avers\\nFremont Row House\\nGreenwood Row House\\nHawthorne Place\\nHyde Park–Kenwood\\nIIT Campus\\nJackson Boulevard\\nJackson Park Highlands\\nJewelers Row\\nKenwood\\nLogan Square Boulevards\\nLongwood Drive\\nLoop Retail\\nMcCormick Row House\\nMichigan Boulevard\\nMichigan-Wacker\\nMid-North\\nMilwaukee-Diversey-Kimball\\nMotor Row\\nNewport Avenue\\nNorth Kenwood\\nOakdale Avenue\\nOakland\\nOld Chicago Water Tower\\nOld Edgebrook\\nOld Town Triangle\\nPrairie Avenue\\nPrinting House Row\\nPullman\\nSchoenhofen Brewery\\nSeven Houses on Lake Shore Drive\\nSurf-Pine Grove\\nTerra Cotta Row\\nUkrainian Village\\nVilla\\nWalter Burley Griffin Place\\nWashington Park Court\\nWicker Park\\nHouses\\nAbbott (Robert)\\nAbbott (Dr. Wallace)\\nAdams\\nAmerican System\\nBach\\nBachman\\nBeeson\\nCable\\nCharnley\\nClarke\\nColvin\\nCompton\\nDe Priest\\nDewes (August)\\nDewes (Francis)\\nDuPont–Whitehouse\\nDu Sable Homesite\\nElam\\nEliel\\nFoster\\nGauler\\nGerber\\nGlessner\\nGroesbeck\\nHazelton-Mikota\\nHeller\\nHitchcock\\nHull\\nIglehart\\nJackson-Thomas\\nJones\\nKent\\nKing–Nash\\nLathrop\\nLillie\\nLion House\\nMadlener\\nMcCormick Double\\nMcGill\\nMiller\\nMillikan\\nNickerson\\nNoble–Seymour–Crippen\\nNorthwestern University Settlement House\\nPalliser\\'s Cottage Home No. 35\\nPate-Comiskey\\nPeters\\nRaber\\nRace\\nRath\\nRobie\\nRoloson\\nSandburg\\nSchlect\\nSchock (F. R.)\\nSchock (Four Houses)\\nSchock (Marie)\\nSoldiers\\' Home\\nTheurer-Wrigley\\nTurzak\\nWalser\\nWeintraub\\nWells-Barnett\\nWheeler–Kohn\\nWilliams\\nWingert\\nMemorials and monuments\\nAbraham Lincoln: The Man\\nBuckingham Fountain\\nChicago Pile-1\\nNuclear Energy\\nDouglas Tomb\\nFort Dearborn Site\\nGetty Tomb\\nHaymarket\\nHeald Square Monument\\nIllinois–Indiana State Line Boundary Marker\\nRosehill Cemetery Entrance\\nSite of the John and Mary Jones House\\nSite of the Origin of the Chicago Fire of 1871\\nSite of the Origins of the I&M Canal\\nStatue of The Republic\\nUnion Stock Yard Gate\\nVictory Monument\\nWigwam (Site of the Sauganash Hotel)\\nMunicipal\\n63rd Street Bathing Pavilion\\nCalumet Park Fieldhouse\\nChicago City Hall\\nColumbus Park\\nCourthouse Place\\nDrake Fountain\\nHumboldt Park Boathouse Pavilion\\nIndian Boundary Park Fieldhouse\\nShedd Park Fieldhouse\\nWashington Square Park\\nPlaces of worship\\nAll Saints Episcopal Church\\nCanaan Baptist Church of Christ Building\\nChurch of the Epiphany\\nEbenezer Missionary Baptist Church\\nEighth Church of Christ, Scientist\\nEpiscopal Church of the Atonement and Parish House\\nEpworth United Methodist Church\\nFirst Baptist Congregational Church\\nFirst Church of Deliverance\\nFirst Congregational Church of Austin\\nFourth Presbyterian Church\\nHoly Trinity Orthodox Cathedral and Rectory\\nKAM Isaiah Israel\\nKenwood Evangelical Church\\nMadonna Della Strada Chapel\\nMetropolitan Apostolic Community Church Building\\nMetropolitan Missionary Baptist Church\\nOld St. Patrick\\'s Church\\nPilgrim Baptist Church\\nQuinn Chapel\\nRoberts Temple Church of God in Christ Building\\nRockefeller Memorial Chapel\\nSecond Presbyterian Church\\nSt. Gelasius Church Building\\nThird Unitarian Church Building\\nTruevine Missionary Baptist Church Building\\nSkyscrapers\\n330 North Wabash\\n333 North Michigan\\n35 East Wacker\\n860–880 Lake Shore Drive\\nAllerton Hotel\\nAuditorium\\nThe Blackstone Hotel\\nBrooks\\nBryn Mawr Apartment Hotel\\nCarbide & Carbon\\nCarson, Pirie, Scott\\nChicago Board of Trade\\nChicago Building\\nCivic Opera House\\nDaley Center\\nFisher\\nGage Group\\nHeyworth\\nHolden Block\\nHyde Park–Kenwood National Bank\\nInland Steel\\nLondon Guarantee\\nManhattan\\nMarquette\\nMarshall Field and Company\\nMather Tower\\nMonadnock\\nMundelein College Skyscraper\\nNew York Life Insurance\\nOld Colony\\nOld Dearborn Bank\\nOne North LaSalle\\nPalmer House Hotel\\nPalmolive\\nPittsfield\\nPowhatan Apartments\\nReliance\\nRoanoke\\nRookery\\nTribune Tower\\nWashington Block\\nTransportation\\nChicago & Alton Railway Bridge\\nChicago & Illinois Western Railway Bridge\\nChicago & North Western Railway Power House\\nChicago & Northwestern Railway Bridge\\nChicago & Western Indiana Railroad Bridge\\nChicago Harbor Lighthouse\\nChicago, Milwaukee & St. Paul Railway Bridge No. Z-2\\nCortland Street Drawbridge\\nDearborn Station\\nGarfield Boulevard \"L\" Station and Overpass\\nIllinois Central Railroad Swing Bridge 1\\nIllinois Central Railroad Swing Bridge 2\\nLake Shore & Michigan Southern Railway Bridges\\nLaSalle Street Cable Car Powerhouse\\nMichigan Avenue Bridge and Esplanade\\nPennsylvania Railroad Bridge\\nPennsylvania Railroad \"Eight Track\" Bridge\\nSt. Charles Air Line Bridge\\nUnion Station\\n\\nvteState of IllinoisSpringfield (capital)Topics\\nIndex\\nAbortion\\nAfrican Americans\\nBuildings and structures\\nCensus areas\\nClimate change\\nCrime\\nCommunications\\nCulture\\nDelegations\\nEarthquakes\\nEconomy\\nEducation\\nEnergy\\nEnvironment\\nGeography\\nGovernment\\nHealth\\nHistory\\nHomelessness\\nLanguages\\nLaw\\nMilitary\\nMusic\\nPeople\\nPolitics\\nPortal\\nProtected areas\\nScience and technology\\nSister cities\\nSociety\\nSports\\nSymbols\\nTourism\\nTransportation\\nWindmills\\nRegions\\nAmerican Bottom\\nBloomington–Normal metropolitan area\\nCentral Illinois\\nChampaign–Urbana metropolitan area\\nChicago metropolitan area\\nCollar counties\\nCorn Belt\\nDriftless Area\\nForgottonia\\nFox Valley\\nIllinois–Indiana–Kentucky tri-state area\\nMetro East\\nMetro Lakeland\\nMississippi Alluvial Plain\\nNorth Shore\\nNorthern Illinois\\nNorthwestern Illinois\\nPeoria metropolitan area\\nQuad Cities\\nRiver Bend\\nRockford metropolitan area\\nSouthern Illinois\\nWabash Valley\\nMunicipalities\\nAlton/Granite City/Edwardsville\\nArlington Heights/Palatine\\nAurora/Naperville/Oswego/Plainfield\\nBartlett/Hanover Park/Streamwood\\nBelleville/East St. Louis/Collinsville/O\\'Fallon\\nBerwyn/Cicero\\nBloomington/Normal\\nBolingbrook/Romeoville\\nBuffalo Grove/Wheeling\\nCalumet City\\nCanton\\nCarbondale\\nCarol Stream/Glendale Heights\\nCentralia\\nChampaign/Urbana\\nCharleston/Mattoon\\nChicago\\nChicago Heights\\nCrystal Lake/Algonquin\\nDanville\\nDecatur\\nDeKalb/Sycamore\\nDes Plaines/Mount Prospect/Park Ridge\\nDixon\\nDowners Grove/Woodridge\\nEffingham\\nElgin/Carpentersville\\nElmhurst/Lombard/Addison\\nEvanston/Skokie\\nFreeport\\nGalesburg\\nGlenview/Northbrook\\nHarrisburg\\nJacksonville\\nJoliet\\nKankakee/Bradley/Bourbonnais\\nLincoln\\nMacomb\\nMarion/Herrin\\nMoline/East Moline/Rock Island\\nMount Vernon\\nMundelein\\nOak Lawn\\nOak Park\\nOrland Park/Tinley Park\\nOttawa/Streator/LaSalle/Peru\\nPeoria/Pekin/East Peoria/Morton/Washington\\nPontiac\\nQuincy\\nRochelle\\nRockford/Belvidere/Machesney Park/Loves Park\\nSt. Charles\\nSchaumburg/Hoffman Estates/Elk Grove Village\\nSpringfield\\nSterling/Rock Falls\\nTaylorville\\nWaukegan/North Chicago/Gurnee\\nWheaton\\nCounties\\nAdams\\nAlexander\\nBond\\nBoone\\nBrown\\nBureau\\nCalhoun\\nCarroll\\nCass\\nChampaign\\nChristian\\nClark\\nClay\\nClinton\\nColes\\nCook\\nCrawford\\nCumberland\\nDeKalb\\nDeWitt\\nDouglas\\nDuPage\\nEdgar\\nEdwards\\nEffingham\\nFayette\\nFord\\nFranklin\\nFulton\\nGallatin\\nGreene\\nGrundy\\nHamilton\\nHancock\\nHardin\\nHenderson\\nHenry\\nIroquois\\nJackson\\nJasper\\nJefferson\\nJersey\\nJo Daviess\\nJohnson\\nKane\\nKankakee\\nKendall\\nKnox\\nLake\\nLaSalle\\nLawrence\\nLee\\nLivingston\\nLogan\\nMacon\\nMacoupin\\nMadison\\nMarion\\nMarshall\\nMason\\nMassac\\nMcDonough\\nMcHenry\\nMcLean\\nMenard\\nMercer\\nMonroe\\nMontgomery\\nMorgan\\nMoultrie\\nOgle\\nPeoria\\nPerry\\nPiatt\\nPike\\nPope\\nPulaski\\nPutnam\\nRandolph\\nRichland\\nRock Island\\nSaline\\nSangamon\\nSchuyler\\nScott\\nShelby\\nSt. Clair\\nStark\\nStephenson\\nTazewell\\nUnion\\nVermilion\\nWabash\\nWarren\\nWashington\\nWayne\\nWhite\\nWhiteside\\nWill\\nWilliamson\\nWinnebago\\nWoodford\\n Illinois portal\\nvteMidwestern United StatesTopics\\nCulture\\nGeography\\nEconomy\\nGovernment and politics\\nHistory\\nSports\\nSubregions\\nNorthern Midwest\\nEast North Central states\\nWest North Central states\\nSothern Midwest\\nStates\\nOhio\\nIndiana\\nMichigan\\nIllinois\\nMissouri\\nIowa\\nWisconsin\\nMinnesota\\nNorth Dakota\\nSouth Dakota\\nNebraska\\nKansas\\nMajor cities\\nChicago\\nDetroit\\nMinneapolis\\nSt. Paul\\nSt. Louis\\nCleveland\\nColumbus\\nDayton\\nCincinnati\\nGrand Rapids\\nFort Wayne\\nIndianapolis\\nMilwaukee\\nGreen Bay\\nMadison\\nDes Moines\\nKansas City\\nWichita\\nOmaha\\nSioux Falls\\nRapid City\\nFargo\\nRochester\\nToledo\\nState capitals\\nColumbus\\nIndianapolis\\nLansing\\nSpringfield\\nJefferson City\\nDes Moines\\nMadison\\nSt. Paul\\nBismarck\\nPierre\\nLincoln\\nTopeka\\n\\nvtePan American Games host citiesSummer\\n1951:  Buenos Aires\\n1955:  Mexico City\\n1959:  Chicago\\n1963:  São Paulo\\n1967:  Winnipeg\\n1971:  Cali\\n1975:  Mexico City\\n1979:  San Juan\\n1983:  Caracas\\n1987:  Indianapolis\\n1991:  Havana\\n1995:  Mar del Plata\\n1999:  Winnipeg\\n2003:  Santo Domingo\\n2007:  Rio de Janeiro\\n2011:  Guadalajara\\n2015:  Toronto\\n2019:  Lima\\n2023:  Santiago\\n2027:  Barranquilla\\nWinter\\n1990:  Las Leñas\\n\\nvteGreat Lakes megalopolis as defined by the RPAIncludes all metropolitan areas that have a population of 150,000 or greater according to the most recent national census.Great Lakes region cities\\nBrantford\\n Buffalo–Niagara Falls\\nBuffalo\\nNiagara Falls\\nChicago\\ncity\\nCleveland\\ncity\\nDetroit\\ncity\\nErie\\ncity\\nGrand Rapids\\ncity\\nGuelph\\nHamilton\\nHolland\\nKalamazoo\\ncity\\nLansing\\ncity\\nLondon\\nMilwaukee\\ncity\\nMuskegon\\nNiagara Region\\nSt. Catharines\\nNiagara Falls\\nWelland\\nOshawa\\nRochester, New York\\ncity\\nSouth Bend\\ncity\\nToledo\\ncity\\nToronto\\ncity\\nWaterloo Region\\nKitchener\\nCambridge\\nWaterloo\\nWindsor\\nSurrounding cities\\nAkron\\ncity\\nAltoona\\nAnn Arbor\\nBarrie\\nBloomington\\nBloomington\\nBloomington–Normal\\nBloomington\\nNormal\\nCanton\\ncity\\nChampaign\\ncity\\nCincinnati\\ncity\\nColumbus\\ncity\\nDayton\\ncity\\nElkhart\\nFlint\\nFort Wayne\\ncity\\nFox Cities\\nAppleton\\nOshkoth\\nGreen Bay\\ncity\\nIndianapolis\\ncity\\nJanesville - Beloit\\nKankakee\\ncity\\nKingston\\nLafayette\\ncity\\nMadison\\ncity\\nMahoning Valley\\nYoungstown\\nMinneapolis–Saint Paul\\nMinneapolis\\nSaint Paul\\nGreater Montreal\\nLaval\\nLongueuil\\nMontreal\\nNational Capital Region\\nOttawa\\nGatineau\\nPittsburgh\\ncity\\nPeoria\\ncity\\nPeterborough\\nRockford\\ncity\\nRochester, Minnesota\\ncity\\nSaguenay\\nSt. Cloud\\ncity\\nSaginaw\\ncity\\nSpringfield\\ncity\\nSudbury\\nTerre Haute\\ncity\\nTrois-Rivières\\nQuad Cities metro\\nDavenport\\nGreater Quebec\\ncity\\nCities of states south of region\\nKansas City\\ncity\\nLouisville\\ncity\\nSt. Louis\\ncity\\nTopeka\\ncity\\nWheeling\\ncity\\nOther metro-regions\\nQuebec City–Windsor Corridor\\nGolden Horseshoe\\nGreater Toronto and Hamilton Area\\nDetroit–Windsor\\nGreater Pittsburgh\\nMetro East\\nQuad Cities\\nOther megaregions\\nvteCounty seats of Illinois\\nAlbion\\nAledo\\nBelleville\\nBelvidere\\nBenton\\nBloomington\\nCairo\\nCambridge\\nCarlinville\\nCarlyle\\nCarmi\\nCarrollton\\nCarthage\\nCharleston\\nChester\\nChicago\\nClinton\\nDanville\\nDecatur\\nDixon\\nEdwardsville\\nEffingham\\nElizabethtown\\nEureka\\nFairfield\\nFreeport\\nGalena\\nGalesburg\\nGeneva\\nGolconda\\nGreenville\\nHardin\\nHarrisburg\\nHavana\\nHennepin\\nHillsboro\\nJacksonville\\nJerseyville\\nJoliet\\nJonesboro\\nKankakee\\nLacon\\nLawrenceville\\nLewistown\\nLincoln\\nLouisville\\nMacomb\\nMarion\\nMarshall\\nMcLeansboro\\nMetropolis\\nMonmouth\\nMonticello\\nMorris\\nMorrison\\nMound City\\nMount Carmel\\nMount Carroll\\nMount Sterling\\nMount Vernon\\nMurphysboro\\nNashville\\nNewton\\nOlney\\nOquawka\\nOregon\\nOttawa\\nParis\\nPaxton\\nPekin\\nPeoria\\nPetersburg\\nPinkneyville\\nPittsfield\\nPontiac\\nPrinceton\\nQuincy\\nRobinson\\nRock Island\\nRockford\\nRushville\\nSalem\\nShawneetown\\nShelbyville\\nSpringfield\\nSullivan\\nSycamore\\nTaylorville\\nToledo\\nToulon\\nTuscola\\nUrbana\\nVandalia\\nVienna\\nVirginia\\nWaterloo\\nWatseka\\nWaukegan\\nWheaton\\nWinchester\\nWoodstock\\nYorkville\\n\\nPortals: Chicago Illinois Cities United StatesChicago at Wikipedia\\'s sister projects:Definitions from WiktionaryMedia from CommonsNews from WikinewsQuotations from WikiquoteTexts from WikisourceTextbooks from WikibooksResources from WikiversityTravel guides from WikivoyageData from Wikidata\\nAuthority control databases International\\nFAST\\nVIAF\\nNational\\nNorway\\nSpain\\nFrance\\nBnF data\\nArgentina\\nCatalonia\\nGermany\\nIsrael\\nUnited States\\nJapan\\nCzech Republic\\nAustralia\\nGreece\\nCroatia\\nGeographic\\nMusicBrainz area\\nPeople\\nTrove\\nOther\\nNARA\\nIdRef\\n\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Chicago&oldid=1239645454\"\\nCategories: Chicago1833 establishments in IllinoisCities in Cook County, IllinoisCities in DuPage County, IllinoisCities in the Chicago metropolitan areaCities in IllinoisCounty seats in IllinoisIllinois populated places on Lake MichiganInland port cities and towns of the United StatesMajority-minority cities and towns in Cook County, IllinoisMajority-minority cities and towns in DuPage County, IllinoisPopulated places established in 1833Populated places established in the 1780sRailway towns in IllinoisHidden categories: Pages using gadget WikiMiniAtlasPages with non-numeric formatnum argumentsPages using the Phonos extensionPages including recorded pronunciationsArticles containing Miami-Illinois-language textArticles containing Ojibwe-language textWebarchive template wayback linksWikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with Wikisource referenceCS1 maint: bot: original URL status unknownArticles with short descriptionShort description is different from WikidataWikipedia pages semi-protected against vandalismWikipedia indefinitely move-protected pagesUse American English from March 2019All Wikipedia articles written in American EnglishUse mdy dates from October 2023Pages using multiple image with auto scaled imagesArticles containing Latin-language textCoordinates on WikidataArticles containing French-language textArticles containing potentially dated statements from July 2019All articles containing potentially dated statementsAll articles with unsourced statementsArticles with unsourced statements from January 2024Articles with unsourced statements from March 2022Articles containing potentially dated statements from 2014Articles containing potentially dated statements from 2018Articles containing potentially dated statements from 2002Articles with hAudio microformatsSpoken articlesOfficial website different in Wikidata and WikipediaWikipedia articles incorporating a citation from the 1922 Encyclopaedia Britannica with Wikisource referenceArticles with FAST identifiersArticles with VIAF identifiersArticles with BIBSYS identifiersArticles with BNE identifiersArticles with BNF identifiersArticles with BNFdata identifiersArticles with BNMM identifiersArticles with CANTICN identifiersArticles with GND identifiersArticles with J9U identifiersArticles with LCCN identifiersArticles with NDL identifiersArticles with NKC identifiersArticles with NLA identifiersArticles with NLG identifiersArticles with NSK identifiersArticles with MusicBrainz area identifiersArticles with Trove identifiersArticles with NARA identifiersArticles with SUDOC identifiersArticles containing video clipsPages using the Kartographer extension\\n\\n')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading PDF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('random.pdf')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='TECHNOLOGY READINESS LEVELS\\nFOR MACHINE LEARNING SYSTEMS\\nAlexander Lavin∗\\nPasteur LabsCiarán M. Gilligan-Lee\\nSpotifyAlessya Visnjic\\nWhyLabsSiddha Ganju\\nNvidiaDava Newman\\nMIT\\nAtılım Güne¸ s Baydin\\nUniversity of OxfordSujoy Ganguly\\nUnity AIDanny Lange\\nUnity AIAmit Sharma\\nMicrosoft Research\\nStephan Zheng\\nSalesforce ResearchEric P. Xing\\nPetuumAdam Gibson\\nKonduitJames Parr\\nNASA Frontier Development Lab\\nChris Mattmann\\nNASA Jet Propulsion LabYarin Gal\\nAlan Turing Institute\\nABSTRACT\\nThe development and deployment of machine learning (ML) systems can be executed easily with\\nmodern tools, but the process is typically rushed and means-to-an-end. The lack of diligence can\\nlead to technical debt, scope creep and misaligned objectives, model misuse and failures, and\\nexpensive consequences. Engineering systems, on the other hand, follow well-deﬁned processes\\nand testing standards to streamline development for high-quality, reliable results. The extreme is\\nspacecraft systems, where mission critical measures and robustness are ingrained in the development\\nprocess. Drawing on experience in both spacecraft engineering and ML (from research through\\nproduct across domain areas), we have developed a proven systems engineering approach for machine\\nlearning development and deployment. Our Machine Learning Technology Readiness Levels (MLTRL)\\nframework deﬁnes a principled process to ensure robust, reliable, and responsible systems while\\nbeing streamlined for ML workﬂows, including key distinctions from traditional software engineering.\\nEven more, MLTRL deﬁnes a lingua franca for people across teams and organizations to work\\ncollaboratively on artiﬁcial intelligence and machine learning technologies. Here we describe the\\nframework and elucidate it with several real world use-cases of developing ML methods from basic\\nresearch through productization and deployment, in areas such as medical diagnostics, consumer\\ncomputer vision, satellite imagery, and particle physics.\\nKeywords: Machine Learning; Systems Engineering; Data Management; Medical AI; Space Sciences\\nIntroduction\\nThe accelerating use of artiﬁcial intelligence (AI) and machine learning (ML) technologies in systems of software,\\nhardware, data, and people introduces vulnerabilities and risks due to dynamic and unreliable behaviors; fundamentally,\\nML systems learn from data, introducing known and unknown challenges in how these systems behave and interact with\\ntheir environment. Currently the approach to building AI technologies is siloed: models and algorithms are developed\\nin testbeds isolated from real-world environments, and without the context of larger systems or broader products they’ll\\nbe integrated within for deployment. A main concern is models are typically trained and tested on only a handful of\\ncurated datasets, without measures and safeguards for future scenarios, and oblivious of the downstream tasks and\\nusers. Even more, models and algorithms are often integrated into a software stack without regard for the inherent\\nstochasticity –for instance, the massive effect random seeds have on deep reinforcement learning model performance\\n[1] – and failure modes of the ML components, which can be dangerously hidden in layers of software and abstraction.\\nOther domains of engineering, such as civil and aerospace, follow well-deﬁned processes and testing standards to\\nstreamline development for high-quality, reliable results. Technology Readiness Level (TRL) is a systems engineering\\nprotocol for deep tech[ 2] and scientiﬁc endeavors at scale, ideal for integrating many interdependent components\\n∗lavin@simulation.science\\nPreprint. Under review.arXiv:2101.03989v2  [cs.LG]  29 Nov 2021'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='andcross-functional teams of people. It is no surprise that TRL is standard process and parlance in NASA[ 3] and\\nDARPA[4].\\nFor a spaceﬂight project there are several deﬁned phases, from pre-concept to prototyping to deployed operations to\\nend-of-life, each with a series of exacting development cycles and reviews. This is in stark contrast to common machine\\nlearning and software workﬂows, which promote quick iteration, rapid deployment, and simple linear progressions. Yet\\nthe NASA technology readiness process for spacecraft systems is overkill; we need robust ML technologies integrated\\nwith larger systems of software, hardware, data, and humans, but not necessarily for missions to Mars. We aim to bring\\nsystems engineering to AI and ML by deﬁning and putting into action a lean Machine Learning Technology Readiness\\nLevels (MLTRL) framework. We draw on decades of AI and ML development, from research through production,\\nacross domains and diverse data scenarios: for example, computer vision in medical diagnostics and consumer apps,\\nautomation in self-driving vehicles and factory robotics, tools for scientiﬁc discovery and causal inference, streaming\\ntime-series in predictive maintenance and ﬁnance.\\nIn this paper we deﬁne our framework for developing and deploying robust, reliable, and responsible ML and data\\nsystems, with several real test cases of advancing models and algorithms from R&D through productization and\\ndeployment, including essential data considerations. Additionally, MLTRL prioritizes the role of AI ethics and\\nfairness, and our systems AI approach can help curb the large societal issues that can result from poorly deployed and\\nmaintained AI and ML technologies, such as the automation of systemic human bias, denial of individual autonomy,\\nand unjustiﬁable outcomes (see the Alan Turing Institute Report on Ethical AI [5]). The adoption and proliferation of\\nMLTRL provides a common nomenclature and metric across teams and industries. The standardization of MLTRL\\nacross the AI industry should help teams and organizations develop principled, safe, and trusted technologies.\\nFigure 1: MLTRL spans research through prototyping, productization, and deployment. Most ML workﬂows prescribe\\nan isolated, linear process of data processing, training, testing, and serving a model [ 6]. Those workﬂows fail to deﬁne\\nhow ML development must iterate over that basic process to become more mature and robust, and how to integrate with\\na much larger system of software, hardware, data, and people. Not to mention MLTRL continues beyond deployment:\\nmonitoring and feedback cycles are important for continuous reliability and improvement over the product lifetime.\\nResults\\nMLTRL deﬁnes technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment.\\nA TRL represents the maturity of a model or algorithmii, data pipelines, software module, or composition thereof; a\\ntypical ML system consists of many interconnected subsystems and components, and the TRL of the system is the\\nlowest level of its constituent parts [ 7]. The anatomy of a level is marked by gated reviews, evolving working groups,\\nrequirements documentation with risk calculations, progressive code and testing standards, and deliverables such as\\nTRL Cards (Figure 3) and ethics checklists.iiiThese components—which are crucial for implementing the levels in a\\niiNote we use “model” and “algorithm” somewhat interchangeably when referring to the technology under development. The\\nsame MLTRL process and methods apply for a machine translation model and for an A/B testing algorithm, for example.\\niiiTemplates and examples for MLTRL deliverables will be open-sourced upon publication at github.com/alan-turing-institute.\\n2'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='systematic fashion—as well as MLTRL metrics and methods are concretely described in examples and in the Methods\\nsection. Lastly, to emphasize the importance of data tasks in ML, from data curation [ 8] to data governance [ 9], we\\nstate several important data considerations at each MLTRL level.\\nMACHINE LEARNING TECHNOLOGY READINESS LEVELS\\nThe levels are brieﬂy deﬁned as follows and in Figure 1, and elucidated with real-world examples later.\\nLevel 0 - First Principles This is a stage for greenﬁeld AI research, initiated with a novel idea, guiding question, or\\npoking at a problem from new angles. The work mainly consists of literature review, building mathematical foundations,\\nwhite-boarding concepts and algorithms, and building an understanding of the data – for work in theoretical AI and ML,\\nhowever, there will not yet be data to work with (for example, a novel algorithm for Bayesian optimization[ 10], which\\ncould eventually be used for many domains and datasets). The outcome of Level 0 is a set of concrete ideas with sound\\nmathematical formulation, to pursue through low-level experimentation in the next stage. When relevant, this level\\nexpects conclusions about data readiness, including strategies for getting the data to be suitable for the speciﬁc ML task.\\nTo graduate, the basic principles, hypotheses, data readiness, and research plans need to be stated, referencing relevant\\nliterature. With graduation, a TRL Card should be started to succinctly document the methods and insights thus far –\\nthis key MLTRL deliverable is detailed in the Methods section and Figure 3.\\nLevel 0 data – Not a hard requirement at this stage because this is largely theoretical machine learning. That being said,\\ndata availability needs to be considered for deﬁning any research project to move past theory.\\nLevel 0 review – The reviewer here is solely the lead of the research lab or team, for instance a PhD supervisor. We\\nassess hypotheses and explorations for mathematical validity and potential novelty or utility, not necessarily code nor\\nend-to-end experiment results.\\nLevel 1 - Goal-Oriented Research To progress from basic principles to practical use, we design and run low-level\\nexperiments to analyze speciﬁc model or algorithm properties (rather than end-to-end runs for a performance benchmark\\nscore). This involves collection and processing of sample data to train and evaluate the model. This sample data need\\nnot be the full data; it may be a smaller sample that is currently available or more convenient to collect. In some\\ncases it may sufﬁce to use synthetic data as the representative sample – in the medical domain, for example, acquiring\\ndatasets can take many months due to security and privacy constraints, so generating sample data can mitigate this\\nblocker from early ML development. Further, working with the sample data provides a blueprint for the data collection\\nand processing pipeline (including answering whether it is even possible to collect all necessary data), that can be\\nscaled up for the for the next steps. The experiments, good results or not, and mathematical foundations need to pass a\\nreview process with fellow researchers before graduating to Level 2. The application is still speculative, but through\\ncomparison studies and analyses we start to understand if/how/where the technology offers potential improvements and\\nutility. Code is research-caliber : The aim here is to be quick and dirty, moving fast through iterations of experiments.\\nHacky code is okay, and full test coverage is actually discouraged, as long as the overall codebase is organized and\\nmaintainable. It is important to start semantic versioning practices early in the project lifecycle, which should cover\\ncode, models, anddatasets. This is crucial for retrospectives and reproducibility, issues with which can be costly and\\nsevere at later stages. This versioning information and additional progress should be reported on the TRL Card (see for\\nexample Figure 3).\\nLevel 1 data – At minimum we work with sample data that is representative of downstream real datasets, which can be\\na subset of real data, synthetic data, or both. Beyond driving low-level ML experiments, the sample data forces us to\\nconsider data acquisition and processing strategies at an early stage before it becomes a blocker later.\\nLevel 1 review – The panel for this gated review is entirely members of the research team, reviewing for scientiﬁc rigor\\nin early experimentation, and pointing to important concepts and prior work from their respective areas of expertise.\\nThere may be several iterations of feedback and additional experiments.\\nLevel 2 - Proof of Principle (PoP) Development Active R&D is initiated, mainly by developing and running in\\ntestbeds : simulated environments and/or simulated data that closely matches the conditions and data of real scenarios –\\nnote these are driven by model-speciﬁc technical goals, not necessarily application or product goals (yet). An important\\ndeliverable at this stage is the formal research requirements document (with well-speciﬁed veriﬁcation and validation\\n(V&V) steps)iv. Here is one of several key decision points in the broader process: The R&D team considers several\\npaths forward and sets the course: (A) prototype development towards Level 3, (B) continued R&D for longer-term\\nivArequirement is a singular documented physical or functional need that a particular design, product, or process aims to satisfy.\\nRequirements aim to specify all stakeholders’ needs while not specifying a speciﬁc solution. Deﬁnitions are incomplete without\\n3'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='research initiatives and/or publications, or some combination of A and B. We ﬁnd the culmination of this stage is often\\na bifurcation: some work moves to applied AI, while some circles back for more research. This common MLTRL cycle\\nis an instance of the non-monotonic discovery switchback mechanism (detailed in the Methods section).\\nLevel 2 data – Datasets at this stage may include publicly available benchmark datasets, semi-simulated data based\\non the data sample in Level 1, or fully simulated data based on certain assumptions about the potential deployment\\nenvironments. The data should allow researchers to characterize model properties, and highlight corner cases or\\nboundary conditions, in order to justify the utility of continuing R&D on the model.\\nLevel 2 review – To graduate from the PoP stage, the technology needs to satisfy research claims made in previous\\nstages (brought to be bare by the aforementioned PoP data in both quantitative and qualitative ways) with the analyses\\nwell-documented and reproducible.\\nLevel 3 - System Development Here we have checkpoints that push code development towards interoperability,\\nreliability, maintainability, extensibility, and scalability. Code becomes prototype-caliber : A signiﬁcant step up from\\nresearch code in robustness and cleanliness. This needs to be well-designed, well-architected for dataﬂow and interfaces,\\ngenerally covered by unit and integration tests, meet team style standards, and sufﬁciently-documented. Note the\\nprogrammers’ mentality remains that this code will someday be refactored/scrapped for productization; prototype code\\nis relatively primitive with regard to efﬁciency and reliability of the eventual system. With the transition to Level 4 and\\nproof-of-concept mode, the working group should evolve to include product engineering to help deﬁne service-level\\nagreements and objectives (SLAs and SLOs) of the eventual production system.\\nLevel 3 data – For the most part consistent with Level 2; in general, the previous level review can elucidate potential\\ngaps in data coverage and robustness to be addressed in the subsequent level. However, for test suites developed at this\\nstage, it is useful to deﬁne dedicated subsets of the experiment data as default testing sources, as well as setup mock\\ndata for speciﬁc functionalities and scenarios to be tested.\\nLevel 3 review – Teammates from applied AI and engineering are brought into the review to focus on sound software\\npractices, interfaces and documentation for future development, and version control for models and datasets. There are\\nlikely domain- or organization-speciﬁc data management considerations going forward that this review should point out\\n– e.g. standards for data tracking and compliance in healthcare [11].\\nLevel 4 - Proof of Concept (PoC) Development This stage is the seed of application-driven development; for many\\norganizations this is the ﬁrst touch-point with product managers and stakeholders beyond the R&D group. Thus TRL\\nCards and requirements documentation are instrumental in communicating the project status and onboarding new\\npeople. The aim is to demonstrate the technology in a real scenario: quick proof-of-concept examples are developed to\\nexplore candidate application areas and communicate the quantitative and qualitative results. It is essential to use real\\nand representative data for these potential applications. Thus data engineering for the PoC largely involves scaling up\\nthe data collection and processing from Level 1, which may include collecting new data or processing all available data\\nusing scaled experiment pipelines from Level 3. In some scenarios there will new datasets brought in for the PoC, for\\nexample, from an external research partner as a means of validation. Hand-in-hand with the evolution from sample to\\nreal data, the experiment metrics should evolve from ML research to the applied setting: proof-of-concept evaluations\\nshould quantify model and algorithm performance (e.g., precision and recall and various data splits), computational\\ncosts (e.g., CPU vs GPU runtimes), and also metrics that are more relevant to the eventual end-user (e.g., number\\nof false positives in the top-N predictions of a recommender system). We ﬁnd this PoC exploration reveals speciﬁc\\ndifferences between clean and controlled research data versus noisy and stochastic real-world data. The issues can\\nbe readily identiﬁed because of the well-deﬁned distinctions between those development stages in MLTRL, and then\\ntargeted for further development.\\nAI ethics processes vary across organizations, but all should engage in ethics conversations at this stage, including ethics\\nof data collection, and potential of any harm or discriminatory impacts due to the model (as the AI capabilities and\\ndatasets are known). MLTRL requires ethics considerations to be reported on TRL Cards at all stages, which generally\\nlink to an extended ethics checklist. The key decision point here is to push onward with application development or not.\\nIt is common to pause projects that pass Level 4 review, waiting for a better time to dedicate resources, and/or pull the\\ntechnology into a different project.\\nLevel 4 data – Unlike the previous stages, having real-world and representative data is critical for the PoC; even with\\nmethods for verifying that data distributions in synthetic data reliably mirror those of real data [], sufﬁcient conﬁdence\\nin the technology must be achieved with real-world data of the use-case. Further, one must consider how to obtain\\ncorresponding measures for veriﬁcation and validation (V&V). Veriﬁcation: Are we building the product right? Validation: Are we\\nbuilding the right product?\\n4'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='high-quality and consistent data required for the future model inference: generation of the data pipeline PoC that will\\nresemble the future inference pipeline that will take data from intended sources, transform it into features, and send it to\\nthe model for inference.\\nLevel 4 review – Demonstrate the utility towards one or more practical applications (each with multiple datasets), taking\\ncare to communicate assumptions and limitations, and again reviewing data-readiness: evaluating the real-world data\\nfor quality, validity, and availability. The review also evaluates security and privacy considerations – deﬁning these in\\nthe requirements document with risk quantiﬁcation is a useful mechanism for mitigating potential issues (discussed\\nfurther in the Methods section).\\nLevel 5 - Machine Learning “Capability” At this stage the technology is more than an isolated model or algorithm,\\nit is a speciﬁc capability . For instance, producing depth images from stereo vision sensors on a mobile robot is a\\nreal-world capability beyond the isolated ML technique of self-supervised learning for RGB stereo disparity estimation.\\nIn many organizations this represents a technology transition or handoff from R&D to productization. MLTRL\\nmakes this transition explicit, evolving the requisite work, guiding documentation, objectives and metrics, and team;\\nindeed, without MLTRL it is common for this stage to be erroneously leaped completely, as shown in Figure 2. An\\ninterdisciplinary working group is deﬁned, as we start developing the technology in the context of a larger real-world\\nprocess – i.e., transitioning the model or algorithm from an isolated solution to a module of a larger application. Just as\\nthe ML technology should no longer be owned entirely by ML experts, steps have been taken to share the technology\\nwith others in the organization via demos, example scripts, and/or an API; the knowledge and expertise cannot remain\\nwithin the R&D team, let alone an individual ML developer. Graduation from Level 5 should be difﬁcult, as it signiﬁes\\nthe dedication of resources to push this ML technology through productization. This transition is a common challenge\\nin deep-tech, sometimes referred to as “the valley of death” because project managers and decision-makers struggle\\nto allocate resources and align technology roadmaps to effectively move to Level 6, 7 and onward. MLTRL directly\\naddresses this challenge by stepping through the technology transition or handoff explicitly.\\nLevel 5 data – For the most part consistent with Level 4. However, considerations need to be taken for scaling of data\\npipelines: there will soon be more engineers accessing the existing data and adding more, and the data will be getting\\nmuch more use, including automated testing in later levels. With this scaling can come challenges with data governance.\\nThe data pipelines likely do not mirror the structure of the teams or broader organization. This can result in data silos,\\nduplications, unclear responsibilities, and missing control of data over its entire lifecycle. These challenges and several\\napproaches to data governance (planning and control, organizational, and risk-based) are detailed in Janssen et al. [9].\\nLevel 5 review – The veriﬁcation and validation (V&V) measures and steps deﬁned in earlier R&D stages (namely\\nLevel 2) must all be completed by now, and the product-driven requirements (and corresponding V&V) are drafted at\\nthis stage. We thoroughly review them here, and make sure there is stakeholder alignment (at the ﬁrst possible step of\\nproductization, well ahead of deployment).\\nLevel 6 - Application Development The main work here is signiﬁcant software engineering to bring the code up to\\nproduct-caliber : This code will be deployed to users and thus needs to follow precise speciﬁcations, have comprehensive\\ntest coverage, well-deﬁned APIs, etc. The resulting ML modules should be robustiﬁed towards one or more target\\nuse-cases. If those target use-cases call for model explanations, the methods need to be built and validated alongside\\nthe ML model, and tested for their efﬁcacy in faithfully interpreting the model’s decisions – crucially, this needs to be\\nin the context of downstream tasks and the end-users, as there is often a gap between ML explainability that serves\\nML engineers rather than external stakeholders[ 12]. Similarly, we need to develop the ML modules with known data\\nchallenges in mind, speciﬁcally to check the robustness of the model (and broader pipeline) to changes in the data\\ndistribution between development and deployment.\\nThe deployment setting(s) should be addressed thoroughly in the product requirements document, as ML serving (or\\ndeploying) is an overloaded term that needs careful consideration. First, there are two main types: internal, as APIs\\nfor experiments and other usage mainly by data science and ML teams, and external, meaning an ML model that\\nis embedded or consumed within a real application with real users. The serving constraints vary signiﬁcantly when\\nconsidering cloud deployment vs on-premise or hybrid, batch or streaming, open-source solution or containerized\\nexecutable, etc. Even more, the data at deployment may be limited due to compliance, or we may only have access to\\nencrypted data sources, some of which may only be accessible locally – these scenarios may call for advanced ML\\napproaches such as federated learning[ 13] and other privacy-oriented ML[ 14]. And depending on the application, an\\nML model may not be deployable without restrictions; this typically means being embedded in a rules engine workﬂow\\nwhere the ML model acts like an advisor that discovers edge cases in rules. These deployment factors are hardly\\nconsidered in model and algorithm development despite signiﬁcant inﬂuence on modeling and algorithmic choices;\\nthat said, hardware choices typically are considered early on, such as GPU versus edge devices. It is crucial to make\\nthese systems decisions at Level 6–not too early that serving scenarios and requirements are uncertain, and not too late\\n5'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='that corresponding changes to model or application development risk deployment delays or failures. This marks a key\\ndecision for the project lifecycle, as this expensive ML deployment risk is common without MLTRL (see Figure 2).\\nLevel 6 data – Additional data should be collected and operationalized at this stage towards robustifying the ML\\nmodels, algorithms, and surrounding components. These include adversarial examples to check local robustness [ 15],\\nsemantically-equivalent perturbations to check consistency of the model with respect to domain assumptions [16, 17],\\nand collecting data from different sources and checking how well the trained model generalizes to them. These\\nconsiderations are even more vital in the challenging deployment domains mentioned above with limited data access.\\nLevel 6 review – Focus is on the code quality, the set of newly deﬁned product requirements, system SLA and SLO\\nrequirements, data pipelines spec, and an AI ethics revisit now that we are closer to a real-world use-case. In particular,\\nregulatory compliance is mandated for this gated review; the data privacy and security laws are changing rapidly, and\\nmissteps with compliance can make or break the project.\\nLevel 7 - Integrations For integrating the technology into existing production systems, we recommend the working\\ngroup has a balance of infrastructure engineers andapplied AI engineers – this stage of development is vulnerable\\nto latent model assumptions and failure modes, and as such cannot be safely developed solely by software engineers.\\nImportant tools for them to build together include:\\n•Tests that run use-case speciﬁc critical scenarios and data-slices – a proper risk-quantiﬁcation table will\\nhighlight these.\\n•A “golden dataset” should be deﬁned to baseline the performance of each model and succession of models –see\\nthe computer vision app example in Figure 4–for use in the continuous integration and deployment (CI/CD)\\ntests.\\n•Metamorphic testing : a software engineering methodology for testing a speciﬁc set of relations between the\\noutputs of multiple inputs. When integrating ML modules into larger systems, a codiﬁed list of metamorphic\\nrelations[18] can provide valuable veriﬁcation and validation measures and steps.\\n•Data intervention tests that seek data bugs at various points in the pipelines, downstream to measure the\\npotential effects of data processing and ML on consumers or users of that data, as well as upstream at data\\ningestion or creation. Rather than using model performance as a proxy for data quality, it is crucial to use\\nintervention tests that instead catch data errors with mechanisms speciﬁc to data validation.\\nThese tests in particular help mitigate underspeciﬁcation in ML pipelines, a key obstacle to reliably training models that\\nbehave as expected in deployment[ 19]. On the note of reliability, it is important that quality assurance engineers (QA)\\nplay a key role here and through Level 9, overseeing data processes to ensure privacy and security, and covering audits\\nfor downstream accountability of AI methods.\\nLevel 7 data – In addition to the data for test suites discussed above, this level calls for QA to prioritize data governance :\\nhow data is obtained, managed, used, and secured by the organization. This was earlier suggested in level 5 (in order to\\npreempt related technical debt), and essential here at the main junction for integration, which may create additional\\ngovernance challenges in light of downstream effects and consumers.\\nLevel 7 review – The review should focus on the data pipelines and test suites; a scorecard like the ML Testing\\nRubric[ 20] is useful. The group should also emphasize ethical considerations at this stage, as they may be more\\nadequately addressed now (where there are many test suites put into place) rather than close to shipping later.\\nLevel 8 - Flight-ready The technology is demonstrated to work in its ﬁnal form and under expected conditions.\\nThere should be additional tests implemented at this stage covering deployment aspects, notably A/B tests, blue/green\\ndeployment tests, shadow testing, and canary testing, which enable proactive and gradual testing for changing ML\\nmethods and data. Ahead of deployment, the CI/CD system should be ready to regularly stress test the overall system\\nand ML components. In practice, problems stemming from real-world data are impossible to anticipate and design for –\\nan upstream data provider could change formats unexpectedly or a physical event could cause the customer behavior to\\nchange. Running models in shadow mode for a period of time would help stress test the infrastructure and evaluate how\\nsusceptible the ML model(s) will be to performance regressions caused by data. We observe that ML systems with\\ndata-oriented architectures are more readily tested in this manner, and better surface data quality issues, data drifts, and\\nconcept drifts – this is discussed later in the Beyond Software Engineering section. To close this stage, the key decision\\nis go or no-go for deployment, and when.\\nLevel 8 data – If not already in place, there absolutely needs to be mechanisms for automatically logging data\\ndistributions alongside model performance once deployed.\\n6'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='Level 8 review – A diligent walkthrough of every technical and product requirement, showing the corresponding\\nvalidations, and the review panel is representative of the full slate of stakeholders.\\nLevel 9 - Deployment In deploying AI and ML technologies, there is signiﬁcant need to monitor the current version,\\nand explicit considerations towards improving the next version. For instance, performance degradation can be hidden\\nand critical, and feature improvements often bring unintended consequences and constraints. Thus at this level, the\\nfocus is on maintenance engineering–i.e., methods and pipelines for ML monitoring and updating. Monitoring for data\\nquality, concept drift, and data drift is crucial; no AI system without thorough tests for these can reliably be deployed.\\nBy the same token there must be automated evaluation and reporting – if actuals[ 21] are available, continuous evaluation\\nshould be enabled, but in many cases actuals come with a delay, so it is essential to record model outputs to allow for\\nefﬁcient evaluation after the fact. To these ends, the ML pipeline should be instrumented to log system metadata, model\\nmetadata, and data itself.\\nMonitoring for data quality issues and data drifts is crucial to catch deviations in model behavior, particularly those that\\nare non-obvious in the model or product end-performance. Data logging is unique in the context of ML systems: data\\nlogs should capture statistical properties of input features and model predictions, and capture their anomalies. With\\nmonitoring for data, concept, and model drifts, the logs are to be sent to the relevant systems, applied, and research\\nengineers. The latter is often non-trivial, as the model server is not ideal for model “observability” because it does not\\nnecessarily have the right data points to link the complex layers needed to analyze and debug models. To this end,\\nMLTRL requires the drift tests to be implemented at stages well ahead of deployment, earlier than is standard practice.\\nAgain we advocate for data-ﬁrst architectures rather than the software industry-standard design by services (discussed\\nlater), which aids in surfacing and logging the relevant data types and slices when monitoring AI systems. For retraining\\nand improving models, monitoring must be enabled to catch training-serving skew and let the team know when to\\nretrain. Towards model improvements, adding or modifying features can often have unintended consequences, such as\\nintroducing latencies or even bias. To mitigate these risks, MLTRL has an embedded switchback here: any component\\nor module changes to the deployed version must cycle back to Level 7 (integrations stage) or earlier. Additionally,\\nfor quality ML products, we stress a deﬁned communication path for user feedback without roadblocks to R&D; we\\nencourage real-world feedback all the way to research, providing valuable problem constraints and perspectives.\\nLevel 9 data – Proper mechanisms for logging and inspecting data (alongside models) is critical for deploying reliable\\nAI and ML – systems that learn on data have unique monitoring requirements (detailed above). In addition to the\\ninfrastructure and test suites covering data and environment shifts, it’s important for product managers and other owners\\nto be on top of data policy shifts in domains such as ﬁnance and healthcare.\\nLevel 9 review – The review at this stage is unique, as it also helps in lifecycle management: at a regular cadence\\nthat depends on the deployed system and domain of use, owners and other stakeholders are to revisit this review and\\nrecommend switchbacks if needed (discussed in the Methods section). This additional oversight at deployment is\\nshown to help deﬁne regimented release cycles of updated versions, and provide another “eye” check for stale model\\nperformance or other system abnormalities.\\nNotice MLTRL is deﬁned as stages or levels, yet much of the value in practice is realized in the transitions: MLTRL\\nenables teams to move from one level to the next reliably and efﬁciently, and provides a guide for how teams and\\nobjectives evolve with the progressing technology.\\nDiscussion\\nMLTRL is designed to apply to many real-world use-cases involving data and ML, from simple regression models\\nused for predictive modeling energy demand or anomaly detection in datacenters, to real-time modeling in rideshare\\napplications and motion planning in warehouse robotics. For simple use-cases MLTRL may be overkill, and a subset\\nmay sufﬁce – for instance, model cards as demonstrated by Google for basic image classiﬁcation. Yet this is a ﬁne line,\\nas the same cards-only approach in the popular “Huggingface” codebases are too simplistic for the language models\\nthey represent, deployed in domains that carry signiﬁcant consequences. MLTRL becomes more valuable with more\\ncomplex, larger systems and environments, especially in risk averse domains. We thoroughly discuss this through\\nseveral real uses of MLTRL below.\\n7'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='Figure 2: Most ML and AI projects live in these sections of MLTRL, not concerned with fundamental R&D – that is,\\ncompletely using existing methods and implementations, and even pretrained models. In the left diagram, the arrows\\nshow a common development pattern with MLTRL in industry: projects go back to the ML toolbox to develop new\\nfeatures (dashed line), and frequent, incremental improvements are often a practice of jumping back a couple levels to\\nLevel 7 (which is the main systems integrations stage). At Levels 7 and 8 we stress the need for tests that run use-case\\nspeciﬁc critical scenarios and data-slices, which are highlighted by a proper risk-quantiﬁcation matrix [ 22]. Cycling\\nback to previous lower levels is not just a late-stage mechanism in MLTRL, but rather “switchbacks” occur throughout\\nthe process (as discussed in the Methods section and throughout the text). In the right diagram we show the more\\ncommon approach in industry ( without using our framework), which skips essential technology transition stages – ML\\nEngineers push straight through to deployment, ignoring important productization and systems integration factors. This\\nwill be discussed in more detail in the Methods section.\\nEXAMPLES\\nHuman-machine visual inspection\\nWhile most ML projects begin with a speciﬁc task and/or dataset, there are many that originate in ML theory without\\nany target application – i.e., projects starting MLTRL at level 0 or 1. These projects nicely demonstrate the utility of\\nMLTRL built-in switchbacks, bifurcating paths, and iteration with domain experts. An example we discuss here is a\\nnovel approach to representing data in generative vision models from Naud & Lavin[ 23], which was then developed into\\nstate-of-the-art unsupervised anomaly detection, and targeted for two human-machine visual inspection applications:\\nFirst, industrial anomaly detection, notably in precision manufacturing, to identify potential errors for human-expert\\nmanual inspection. Second, using the model to improve the accuracy and efﬁciency of neuropathology, the microscopic\\nexamination of neurosurgical specimens for cancerous tissue. In these human-machine teaming use-cases there are\\nspeciﬁc challenges impeding practical, reliable use:\\n•Hidden feedback loops can be common and problematic in real-world systems inﬂuencing their own training\\ndata: over time the behavior of users may evolve to select data inputs they prefer for the speciﬁc AI system,\\nrepresenting some skew from the training data. In this neuropathology case, selecting whole-slide images that\\nare uniquely difﬁcult for manual inspection, or even biased by that individual user. Similarly we see underlying\\nhealthcare processes can act as hidden confounders, resulting in unreliable decision support tools[26].\\n•Model availability can be limited in many deployment settings: for example, on-premises deployments\\n(common in privacy preserving domains like healthcare and banking), edge deployments (common in industrial\\nuse-cases such as manufacturing and agriculture), or from the infrastructure’s inability to scale to the volume\\nof requests. This can severely limit the team’s ability to monitor, debug, and improve deployed models.\\n•Uncertainty estimation is valuable in many AI scenarios, yet not straightforward to implement in practice.\\nThis is further complicated with multiple data sources and users, each injecting generally unknown amounts of\\nnoise and uncertainties. In medical applications it is of critical importance, to provide measures of conﬁdence\\nand sensitivity, and for AI researchers through end-users. In anomaly detection, various uncertainty measures\\ncan help calibrate the false-positive versus false-negative rates, which can be very domain speciﬁc.\\n8'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='Figure 3: The maturity of each ML technology is tracked via TRL Cards , which we describe in the Methods section.\\nHere is an example reﬂecting a neuropathology machine vision use-case[ 23], detailed in the Discussion Section. Note\\nthis is a subset of a full TRL Card, which in reality lives as a full document in an internal wiki. Notice the card\\nclearly communicates the data sources, versions, and assumptions. This helps mitigate invalid assumptions about\\nperformance and generalizability when moving from R&D to production, and promotes the use of real-world data\\nearlier in the project lifecycle. We recommend documenting datasets thoroughly with semantic versioning and tools\\nsuch as datasheets for datasets [24], and following data accountability best-practices as they evolve (see [25]).\\n•Costs of edge cases can be signiﬁcant, sometimes risking expensive machine downtime or medical failures.\\nThis is exacerbated in anomaly detection anomalies are by deﬁnition rare so they can be difﬁcult to train for,\\nespecially for the anomalies that are completely unseen until they arise in the wild.\\n•End-user trust can be difﬁcult to achieve, often preventing the adoption of ML applications, particularly in\\nthe healthcare domain and other highly regulated industries.\\nThese and additional ML challenges such as data privacy and interpretability can inhibit ML adoption in clinical practice\\nand industrial settings, but can be mitigated with MLTRL processes. We’ll describe how in the context of the Naud\\n& Lavin[ 23] example, which began at level 0 with theoretical ML work on manifold geometries, and at level 5 was\\ndirected towards specialized human-machine teaming applications utilizing the same ML method under-the-hood.\\n•Levels 0-1 – From open-ended exploration of data-representation properties in various Riemmanian manifold\\ncurvatures, we derived from ﬁrst principles and empirically identiﬁed a property with hyperbolic manifolds:\\nwhen used as a latent space for embedding data without labels, the geometry organizes the data by it’s implicit\\nhierarchical structure. Unsupervised computer vision was identiﬁed in reviews as a promising direction for\\nproof-of-principle work.\\n•Level 2 – One approach for validating the earlier theoretical developments was to generate synthetic data to\\nisolate very speciﬁc features in data we would expect represented in the latent manifold. The results showed\\npromise for anomaly detection – using the latent representation of data to automatically identify images that\\nare out-of-the-ordinary (anomalous), and also using the manifold to inspect how they are semantically different.\\nFurther, starting with an implicitly probabilistic modeling approach implied uncertainty estimation could be\\na valuable feature downstream. This made the level 2 key decision point clear: proceed with applied ML\\ndevelopment.\\n•Levels 3-5 – Proof-of-concept development and reviews demonstrated promise for several commercial appli-\\ncations relevant to the business, and also highlighted the need for several key features (deﬁned as R&D and\\nproduct requirements): interpretability (towards end-user trust), uncertainty quantiﬁcation (to show conﬁdence\\nscores), and human-in-the-loop (for domain expertise). Without the MLTRL PoC steps and review processes,\\nthese features can often be delayed until beta testing or overlooked completely – for example, the failures of\\napplying IBM Watson in medical applications [ 27]. For this technology, the applications to develop towards\\nare anomaly detection in histopathology and manufacturing, speciﬁcally inspecting whole-slide images of\\nneural tissue, and detecting defects in metallic surfaces, respectively.\\n9'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='From the systems perspective, we suggest quantifying the uncertainties of components and propagating them\\nthrough the system, which can improve safety and trust. Probabilistic ML methods, rooted in Bayesian\\nprobability theory, provide a principled approach to representing and manipulating uncertainty about models\\nand predictions[ 28]. For this reason we advocate strongly for probabilistic models and algorithms in AI\\nsystems. In this machine vision example, the MLTRL technical requirements speciﬁcally called for a\\nprobabilistic generative model to readily quantify various types of uncertainties and propagate them forward to\\nthe visualization component of the pipeline, and the product requirements called for the downstream conﬁdence\\nand sensitivity measures to be exposed to the end-user. Component uncertainties must be assembled in a\\nprincipled way to yield a meaningful measure of overall system uncertainty, based on which safe decisions can\\nbe made[29]. See the Methods section for more on uncertainty in AI systems.\\nThe early checks for data management and governance proved valuable here, as the application areas dealt\\nwith highly sensitive data that would signiﬁcantly inﬂuence the design of data pipelines and test suites. In\\nboth the neuropathology and manufacturing applications, the data management checks also raised concerns\\nabout hidden feedback loops, where users may unintentionally skew the data inputs when using the anomaly\\ndetection models in practice, for instance biasing the data towards speciﬁc subsets they subjectively need help\\nwith. Incorporating domain experts this early in the project lifecycle helped inform veriﬁcation and validation\\nsteps to help be robust to the hidden feedback loops. Not to mention their input guided us towards user-centric\\nmetrics for performance, which can often skew from ML metrics in important ways – for instance, the typical\\nacceptance ratio for false positives versus false negatives doesn’t apply to select edge cases, for which our\\nhierarchical anomaly classiﬁcation scheme was useful [23].\\nFrom prior reviews and TRL card documentation, we also identiﬁed the value of synthetic data generation\\ninto application development: anomalies are by deﬁnition rare so they are hard to come by in real datasets,\\nespecially with evolving environments in deployment settings, so the ability to generate synthetic datasets for\\nanomaly detection can accelerate the level 6-9 pipeline, and help ensure more reliable models in the wild.\\n•Level 6 (medical) – The medical inspection application experienced a bifurcation with product work proceed-\\ning while additional R&D was desired to explore improved data processing methods, while engaging with\\nclinicians and medical researchers for feedback. Proceeding through the levels in a non-linear, non-monotonic\\nway is common in MLTRL and encouraged by various switchback mechanisms (detailed in the Methods\\nsection). These practices – intentional switchbacks, frequent engagement with domain experts and users – can\\nhelp mitigate methodological ﬂaws and underlying biases that are common when applying ML to clinical\\napplications. For instance, recent work by Roberts et al. [ 30] investigated 2,122 studies applying ML to\\nCOVID-19 use-cases, ﬁnding that none of the models are sufﬁcient for clinical use due to methodological ﬂaws\\nand/or underlying biases. They go on to give many recommendations – some we’ve discussed in the context of\\nMLTRL, and more – which should be reviewed for higher quality medical-ML models and documentation.\\n•Level 6-9 (manufacturing) – Overall these stages proceeded regularly and efﬁciently for the defect detection\\nproduct. MLTRL’s embedded switchback from level 9 to 4 proved particularly useful in this lifecycle, both\\nfor incorporating feedback from the ﬁeld and for updating with research progress. On the former, the data\\ndistribution shifts from one deployment setting to another signiﬁcantly affected false-positive versus false-\\nnegative calibrations, so this was added as a feature to the CI/CD pipelines. On the latter, the built-in touch\\npoints for real-world feedback and data into the continued ML research provided valuable constraints to\\nhelp guide research, and product managers could readily understand what capabilities could be available for\\nproduct integration and when (readily communicated with TRL Cards) – for instance, later adding support for\\nvideo-based inspection for defects, and tooling for end-users to reason about uncertainty estimates (which\\nhelps establish trust).\\n•Level 7-9 (medical) – For productization the “neuropathology copilot” was handed off to a partner pharmaceu-\\ntical company to integrate into their existing software systems. The MLTRL documentation and communication\\nstreamlined the technology transfer, which can often by a time-consuming manual process. If not pursuing\\nthis path, the product would’ve likely faced many of the medical-ML deployment challenges with model\\navailability and data access; MLTRL cannot overcome the technical challenges of deploying on-premises, but\\nthe manifestation of those challenges as performance regressions, data shifts, privacy and ethics concerns, etc.\\ncan be mitigated by the system-level checks and strategies MLTRL puts forth.\\nComputer vision with real and synthetic data\\nAdvancements in physics engines and graphics processing have advanced AI environment and data-generation capabili-\\nties, putting increased emphasis on transitioning models across the simulation-to-reality gap [ 31,32,33]. To develop a\\ncomputer vision application for automated recycling, we leveraged the Unity Perception [ 34] package, a toolkit for\\ngenerating large-scale datasets for perception-based ML training and validation. We produced synthetic images to\\n10'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='\\x15A?U?HEJC\\x03?H=OOEBE?=PEKJ\\x03LELAHEJA\\x03\\n\\xa0=¡\\xa0>¡\\n,V\\x03PRGHO\\x03FRQğGHQFH\\x03\\x1f\\x03WKUHVKROG\",V\\x03GHWHFWHG\\x03REMHFW\\x03LQ\\x03WDUJHW\\x03VHW\"3URYLGH\\x03FRUUHVSRQGLQJ\\x03UHF\\\\FOLQJ\\x03LQVWUXFWLRQV,QLWLDWH\\x03KXPDQ\\x10\\x03LQ\\x10WKH\\x10ORRS\\x03SURWRFRO12<(6<(6Figure 4: Computer vision pipeline for an automated recycling application (a), which contains multiple ML models,\\nuser input, and image data from various sources. Complicated logic such as this can mask ML model performance lags\\nand failures, and also emphasized the need for R&D-to-product hand off described in MLTRL. Additional emphasis is\\nplaced on ML tests that consider the mix of real-world data with user annotations (b, right) and synthetic data generated\\nby Unity AI’s Perception tool and structured domain randomization (b, left).\\ncomplement real-world data sources (Figure 4). This application exempliﬁes three important challenges in ML product\\ndevelopment that MLTRL helps overcome:\\n•Multiple and disparate data sources are common in deployed ML pipelines yet often ignored in R&D.\\nFor instance, upstream data providers can change formats unexpectedly, or a physical event could cause the\\ncustomer behavior to change. It is nearly impossible to anticipate and design for all potential problems with\\nreal-world data and deployment. This computer vision system implemented pipelines and extended test suites\\nto cover open-source benchmark data, real user data, and synthetic data.\\n•Hidden performance degradation can be challenging to detect and debug in ML systems because gradual\\nchanges in performance may not be immediately visible. Common reasons for this challenge are that the\\nML component may be one step in a series. Additionally, local/isolated changes to an ML component’s\\nperformance may not directly affect the observed downstream performance. We can see both issues in the\\nillustrated logic diagram for the automated recycling app (Figure 4). A slight degradation in the initial CV\\nmodel may not heavily inﬂuence the following user input. However, when an uncommon input image appears\\nin the future, the app fails altogether.\\n•Model usage requirements can make or break an ML product. For example, the Netﬂix “$1M Prize” solution\\nwas never fully deployed because of signiﬁcant engineering costs in real-world scenariosv. For example,\\nengineering teams must communicate memory usage, compute power requirements, hardware availability,\\nnetwork privacy, and latency to the ML teams. ML teams often only understand the statistics or ML theory\\nbehind a model but not the system requirements or how it scales.\\nWe next elucidate these challenges and how MLTRL helps overcome them in the context of this project’s lifecycle. This\\nproject started at level 4, using largely existing ML methods with a target use-case.\\n•Level 4 – For this project, we validated most of the components in other projects. Speciﬁcally, the computer\\nvision (CV) model for object recognition and classiﬁcation was an off-the-shelf model. The synthetic data\\ngeneration method used Unity Perception, a well-established open-source project. Though this allowed us to\\nvnetﬂixtechblog.com/netﬂix-recommendations-beyond-the-5-stars-part-1\\n11'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='skip the earlier levels, many challenges arise when combining ML elements that were independently validated\\nand developed. The MLTRL prototype-caliber code checkpoint ensures that the existing code components\\nare validated and helps avoid poorly deﬁned borders and abstractions between components. ML pipelines\\noften grow out of glue code, and our regimented code checkpoints motivate well-architected software that\\nminimizes these danger spots.\\n•Level 5 – The problematic “valley of death”, mentioned earlier in the level 5 deﬁnitions, is less prevalent in use-\\ncases like this that start at a higher MLTRL level with a speciﬁc product deliverable. In this case, the product\\ndeliverable was a real-time object recognition and classiﬁcation of trash for a mobile recycling application.\\nStill, this stage is critical for the requirements and V&V transition. This stage mitigates failure risks due to the\\ndisparate data sources integrated at various steps in this CV system and accounted for the end-user compute\\nconstraints for mobile computing. Speciﬁcally, the TRL cards from earlier stages surfaced potential issues\\nwith imbalanced datasets and the need for speciﬁc synthetic images. These considerations are essential for the\\ndata readiness and testing V&V in the productization requirements. Data quality and availability issues often\\npresent huge blockers because teams discover them too late in the game. Data-readiness is one class of many\\nexample issues teams face without MLTRL, as depicted in Fig. 2.\\n•Level 6 – We were re-using a well-understood model and deployment pipeline in this use-case, meaning our\\nprimary challenge was around data reliability. For the problem of recognizing and classifying trash, building a\\nreliable data source using only real data is almost impossible due to diversity, class imbalance, and annotation\\nchallenges. Therefore we chose to develop a synthetic data generator to create training data. At this MLTRL\\nlevel, we needed to ensure that the synthetic data generator created sufﬁciently diverse data and exposed the\\ncontrols needed to alter the data distribution in production. Therefore, we carefully exposed APIs using the\\nUnity Perception package, which allowed us to control lighting, camera parameters, target and non-target\\nobject placements and counts, and background textures. Additionally, we ensured that the object labeling\\nmatched the real-world annotator instructions and that output data formats matched real-world counterparts.\\nLastly, we established a set of statistical tests to compare synthetic and real-world data distributions. The\\nMLTRL checks ensured that we understood, and in this case, adequately designed our data sources to meet\\nin-production requirements.\\n•Level 7 – From the previous level’s R&D TRL cards and observations, we knew relatively early in produc-\\ntization that we would need to assume bias for the real data sources due to class imbalance and imperfect\\nannotations. Therefore we designed tests to monitor these in the deployed application. MLTRL imposes these\\ncritical deployment tests well ahead of deployment, where we can easily overlook ML-speciﬁc failure modes.\\n•Level 8 – As we suggested earlier, problems that stem from real-world data are near impossible to anticipate\\nand design for, implying the need for level 8 ﬂight-readiness preparations. Given that we were generating\\nsynthetic images (with structured domain randomization) to complement the real data, we created tests for\\ndifferent data distribution shifts at multiple points in the classiﬁcation pipeline. We also implemented thorough\\nshadow tests ahead of deployment to evaluate how susceptible the ML model(s) to performance regressions\\ncaused by data. Additionally, we also implemented these as CI/CD tests over various deployment scenarios (or\\nmobile device computing speciﬁcations). Without these fully covered, documented, and automated, it would\\nbe impossible to pass level 8 review and deploy the technology.\\n•Level 9 – Post-deployment, the monitoring tests prescribed at Levels 8 and 9 and the three main code quality\\ncheckpoints in the MLTRL process help surface hidden performance degradation problems, common with\\ncomplex pipelines of data ﬂows and various models. The switchbacks depicted in Fig. 2 are typical in CV\\nuse-cases. For instance, miscalibrations in models pre-trained on synthetic data and ﬁne-tuned on newer real\\ndata can be common yet difﬁcult to catch. However, the level 7 to 4 switchback is designed precisely for these\\nchallenges and product improvements.\\nAccelerating scientiﬁc discovery with massive particle physics simulators\\nComputational models and simulation are key to scientiﬁc advances at all scales, from particle physics, to material\\ndesign and drug discovery, to weather and climate science, and to cosmology[ 35]. Many simulators model the forward\\nevolution of a system (coinciding with the arrow of time), such as the interaction of elementary particles, diffusion of\\ngasses, folding of proteins, or evolution of the universe in the largest scale. The task of inference refers to ﬁnding initial\\nconditions or global parameters of such systems that can lead to some observed data representing the ﬁnal outcome\\nof a simulation. In probabilistic programming[ 36], this inference task is performed by deﬁning prior distributions\\nover any latent quantities of interest, and obtaining posterior distributions over these latent quantities conditioned\\non observed outcomes (for example, experimental data) using Bayes rule. This process, in effect, corresponds to\\ninverting the simulator such that we go from the outcomes towards the inputs that caused the outcomes. In the\\n“Etalumis” project[ 37] (“simulate” spelled backwards), we are using probabilistic programming methods to invert\\n12'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='existing, large-scale simulators via Bayesian inference. The project is as an interdisciplinary collaboration of specialists\\nin probabilistic machine learning, particle physics, and high-performance computing, all essential elements to achieve\\nthe project outcomes. Even more, it is a multi-year project spanning multiple countries, companies, university labs, and\\ngovernment research organizations, bringing signiﬁcant challenges in project management, technology coordination\\nand validation. Aided by MLTRL, there were several key challenges to overcome in this project that are common in\\nscientiﬁc-ML projects:\\n•Integrating with legacy systems is common in scientiﬁc and industrial use-cases, where ML methods are\\napplied with existing sensor networks, infrastructure, and codebases. In this case, particle physics domain\\nexperts at CERN are using the SHERPA simulator[ 38], a 1 million line codebase developed over the last\\ntwo decades. Rewriting the simulator for ML use-cases is infeasible due to the codebase size and buried\\ndomain knowledge, and new ML experts would need signiﬁcant onboarding to gain working knowledge of\\nthe codebase. It is also common to work with legacy data infrastructure, which can be poorly organized for\\nmachine learning (let alone preprocessed and clean) and unlikely to have followed best practices such as\\ndataset versioning.\\n•Coupling hardware and software architectures is non-trivial when deploying ML at scale, as performance\\nconstraints are often considered in deployment tests well after model and algorithm development, not to\\nmention the expertise is often split across disjoint teams. This can be exacerbated in scientiﬁc-ML when\\nscaling to supercomputing infrastructure, and working with massive datasets that can be in the terabytes and\\npetabytes.\\n•Interpretability is often a desired feature yet difﬁcult to deliver and validate in practice. Particularly in\\nscientiﬁc ML applications such as this, mechanisms and tooling for domain experts to interpret predictions\\nand models are key for usability (integrating in workﬂows and building trust).\\nTo this end, we will go through the MLTRL levels one by one, demonstrating how they ensure the above scientiﬁc ML\\nchallenges are diligently addressed.\\n•Level 0 – The theoretical developments leading to Etalumis are immense and well discussed in Baydin et\\nal [37]. In particular the ML theory and methods are in a relatively nascent area of ML and mathematics,\\nprobabilistic programming. New territory can present more challenges compared to well-traveled research\\npaths, for instance in computer vision with neural networks. It is thus helpful to have a guiding framework\\nwhen making a new path in ML research, such as MLTRL where early reviews help theoretical ML projects\\nget legs.\\n•Level 1-2 – Running low-level experiments in simple testbeds is generally straightforward when working\\nwith probabilistic programming and simulation; in a sense, this easy iteration over experiments is what\\nPPL are designed for. It was additionally helpful in this project to have rich data grounded in physical\\nconstraints, allowing us to better isolate model behaviors (rather than data assumptions and noise). The\\nMLTRL requirements documentation is particularly useful for the standard PPL experimentation workﬂow:\\nmodel, infer, criticize, repeat (or Box’s loop) [ 39]. The evaluation step (i.e. criticizing the model) can be\\nmore nuanced than checking summary statistics as in deep learning and similar ML workﬂows. It is thus a\\nuseful practice to write down the criticism methods, metrics, and expected results as veriﬁcations for speciﬁc\\nresearch requirements, rather than iterating over Box’s loop without a priori targets. Further, because this\\nresearch project had a speciﬁc target application early in the process (the SHERPA simulator), the project\\ntimeline beneﬁted from recognizing simulator-integration constraints upfront as requirements, not to mention\\ndata availability concerns, which are often overlooked in early R&D levels. It was additionally useful to have\\nCERN scientists as domain experts in the reviews at these R&D levels.\\n•Level 3 – Systems development can be challenging with probabilistic programming, again because it is\\nrelatively nascent and much of the out-of-the-box tools and infrastructure are not there as in most ML and\\ndeep learning. Here in particular there’s a novel (unproven) approach for systems integration: a probabilistic\\nprogramming execution protocol was developed to reroute random number draws in the stochastic simulator\\ncodebase (SHERPA) to the probabilistic programming system, thus enabling the system to control stochastic\\nchoices in SHERPA and run inference on its execution traces, all while keeping the legacy codebase intact! A\\nmore invasive method that modiﬁes SHERPA would not have been acceptable. If it were not for MLTRL forcing\\nsystems considerations this early in the Etalumis project lifecycle, this could have been an insurmountable\\nhurdle later when multiple codebases and infrastructures come into play. By the same token, systems planning\\nhere helped enable the signiﬁcant HPC scaling later: the team deﬁned the need for HPC support well ahead\\nof actually running HPC, in order to build the prototype code in a way that would readily map to HPC (in\\naddition to local or cloud CPU and GPU). The data engineering challenges in this system’s development\\n13'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='nonetheless persist – that is, data pipelines and APIs that can integrate various sources and infrastructures, and\\nnormalize data from various databases – although MLTRL helps consider these at the an earlier stage that can\\nhelp inform architecture design.\\n•Level 4 – The natural “embedded switchback” from Level 4 to 2 (see the Methods section) provided an efﬁcient\\npath toward developing an improved, amortized inference method–i.e., using a computationally expensive\\ndeep learning based inference algorithm to train only once, in order to then do fast, repeated inference in the\\nSHERPA model. Leveraging cyclic R&D methods, the Etalumis project could iteratively improve inference\\nmethods without stalling the broader system development, ultimately producing the largest scale posterior\\ninference in a Turing-complete probabilistic programming system. Achieving this scale through iterative R&D\\nalong the main project lifecycle was additionally enabled by working with with NERSC engineers and their\\nCori supercomputer to progressively scale smaller R&D tests to the goal supercomputing deployment scenario.\\nTypical ML workﬂows that follow simple linear progressions[ 6,40] would not enable ramping up in this\\nfashion, and can actual prevent scaling R&D to production due to lack of systems engineering processes (like\\nMLTRL) connecting research to deployment.\\n•Level 5 – Multi-org international collaborations can be riddled with communication and teamwork issues,\\nin particular at this pivotal stage where teams transition from R&D to application and product development.\\nFirst, MLTRL as a lingua franca was key to the team effort bringing Etalumis proof-of-concept into the\\nlarger effort of applying it to massive high-energy physics simulators. It was also critical at this stage to\\nclearly communicate end-user requirements across the various teams and organizations, which must be deﬁned\\nin MLTRL requirements docs with V&V measures – the essential science-user requirements were mainly\\nfor model and prediction interpretability, uncertainty estimation, and code usability. If there are concerns\\nover these features, MLTRL switchbacks can help to quickly cycle back and improve modeling choices in a\\ntransparent, efﬁcient way – generally in ML projects, these fundamental issues with usability are caught too\\nlate, even after deployment. In the probabilistic generative model setting we’ve deﬁned in Etalumis, Bayesian\\ninference gives results that are interpretable because they include exact locations and processes in the model\\nthat are associated with each prediction. Working with ML methods that are inherently interpretable, we are\\nwell-positioned to deliver interpretable interfaces for the end-users later in the project lifecycle.\\n•Level 6-9 – The standard MLTRL protocol apply in these application-to-deployment stages, with several\\nEtalumis-speciﬁc highlights. First, given the signiﬁcant research contributions in both probabilistic pro-\\ngramming and scientiﬁc-ML, it’s important to share the code publicly. The development and deployment\\nof the open-source code repository PPXvibranched into a separate MLTRL path from the Etalumis path\\nfor deployment at CERN. It’s useful to have systems engineering enable clean separation of requirements,\\ndeployments, etc. when there are different development and product lifecycles originating from a common\\nparent project. For example, in this case it was useful to employ MLTRL switchbacks in the open-sourcing\\nprocess, isolated from the CERN application paths, in order to add support for additional programming\\nlanguages so PPX can apply to more scientiﬁc simulators – both directions beneﬁted signiﬁcantly the from\\nthe data pipelines considerations brought up levels earlier, where open-sourcing required different data APIs\\nand data transformations to enable broad usability. Second, related to the open-source code deliverable and\\nthe scientiﬁc ML user requirements we noted above, the late stages of MLTRL reviews include higher level\\nstakeholders and speciﬁc end-users, yet again enforcing these scientiﬁc usability requirements are met. An\\nexample result of this in Etalumis is the ability to output human-readable execution traces of the SHERPA\\nruns and inference, enabling never before possible step-by-step interpretability of the black-box simulator.\\nThe scientiﬁc ML perspective additionally brings to forefront an end-to-end data perspective that is pertinent in\\nessentially all ML use-cases: these systems are only useful to the extent they provide comprehensive data analyses that\\nintegrate the data consumed and generated in these workﬂows, from raw domain data to machine-learned models. These\\ndata analyses drive reproducibility, explainability, and experiment data understanding, which are critical requirements\\nin scientiﬁc endeavors and ML broadly.\\nCausal inference & ML in medicine\\nUnderstanding cause and effect relationships is crucial for accurate and actionable decision-making in many settings,\\nfrom healthcare and epidemiology, to economics and government policy development. Unfortunately, standard\\nmachine learning algorithms can only ﬁnd patterns and correlation in data, and as correlation is not causation, their\\npredictions cannot be conﬁdently used for understanding cause and effect. Indeed, relying on correlations extracted\\nfrom observational data to guide decision-making can lead to embarrassing, costly, and even dangerous mistakes,\\nsuch as concluding that asthma reduces pneumonia mortality risk [ 41], and that smoking reduces risk of developing\\nvigithub.com/pyprob/ppx\\n14'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='severe COVID-19 [ 42]. Fortunately, there has been much recent development in a ﬁeld known as causal inference that\\ncan quantitatively make sense of cause and effect from purely observational data[ 43]. The ability of causal inference\\nalgorithms to quantify causal impact rests on a number of important checks and assumptions–beyond those employed\\nin standard machine learning or purely statistical methodology–that must be carefully deliberated over during their\\ndevelopment and training. These speciﬁc checks and assumptions are as follows:\\n•Specifying cause-and-effect relationships between relevant variables– One of the most important assump-\\ntions underlying causal inference is the structure of the causal relations between quantities of interest. The\\ngold standard for determining causal relations is to perform a randomised controlled trial, but in most cases\\nthese cannot be employed due to ethical concerns, technological infeasibility, or prohibitive cost. In these\\nsituations, domain experts have to be consulted to determine the causal relationships. It is important in these\\nsituations to carefully address the manner in which such domain knowledge was extracted from experts, the\\nnumber and diversity of experts involved, the amount of consensus between experts, and so on. The need for\\ncareful documentation of this knowledge and its periodic review is made clear in the MLTRL framework, as\\nwe shall see below.\\n•Identiﬁability– Another vital component of building causal models is whether the causal question of interest\\nisidentiﬁable from the causal structure speciﬁed for the model together with observational (and sometimes\\nexperimental) data.\\n•Adjusting for and monitoring confounding bias– An important aspect of causal model performance, not\\npresent in standard machine learning algorithms, is confounding bias adjustment. The standard approach is to\\nemploy propensity score matching to remove such bias. However, the quality of bias adjustment achieved in\\nany speciﬁc instance with such propensity-based matching methods needs to be checked and documented,\\nwith alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[44].\\n•Sensitivity analysis– As causal estimates are based on generally untestable assumptions, such as observing all\\nrelevant confounders, it is vital to determine how sensitive the resulting predictions are to potential violations\\nof these assumptions.\\n•Consistency– It is crucial to understand if the learned causal estimate provably converges to the true causal\\neffect in the limit of inﬁnite sample size. However, causal models cannot be validated by standard held-out\\ntests, but rather require randomization or special data collection strategies to evaluate their predictions [ 45,46].\\nThe MLTRL framework makes transparent the need to carefully document and defend these assumptions, thus ensuring\\nthe safe and robust creation, deployment, and maintenance of causal models. We elucidate this with recent work by\\nRichens et al.[ 47], developing a causal approach to computer-assisted diagnosis which outperforms previous purely\\nmachine learning based methods. To this end, we will go through the MLTRL levels one by one, demonstrating how\\nthey ensure the above speciﬁc checks and assumptions are naturally accounted for. This should provide a blueprint for\\nhow to employ the MLTRL levels in other causal inference applications.\\n•Level 0 – When initially faced with a causal inference task, the ﬁrst step is always to understand the causal\\nrelationships between relevant variables. For instance, in Richens et al. [ 47], the ﬁrst step toward building\\nthe diagnostic model was specifying the causal relationships between the diverse set risk factors, diseases,\\nand symptoms included in the model. To learn these relations, doctors and healthcare professionals were\\nconsulted to employ their expansive medical domain knowledge which was robustly evaluated by additional\\nindependent groups of healthcare professionals. The MLTRL framework ensured this issue is dealt with and\\ndocumented correctly, as such knowledge is required to progress from Level 0; failure to do this has plagued\\nsimilar healthcare AI projects [48].\\nThe next step of any causal analysis is to understand whether the causal question of interest is uniquely\\nidentiﬁable from the causal structure speciﬁed for the model together with observational and experimental data.\\nIn this medical diagnosis example, identiﬁcation was crucial to establish, as the causal question of interest,\\n“would the observed symptoms not be present had a speciﬁc disease been cured?”, was highly non-trivial.\\nAgain, MLTRL ensures this vital aspect of model building is carefully considered, as a mathematical proof of\\nidentiﬁability would be required to graduate from Level 0.\\nWith both the causal structure and identiﬁability result in hand, one can progress to Level 1.\\n•Level 1 – At this level, the goal is to take the estimand for the identiﬁed causal question of interest and\\ndevise a way to estimate it from data. To do this one will need efﬁcient ways to adjust for confounﬁng bias.\\nThe standard approach is to employ propensity score-based methods to remove such bias when the target\\ndecision is binary, and use multi-stage ML models adhering to the assumed causal structure[ 49] for continuous\\ntarget decisions (and high-dimensional data in general). However, the quality of bias adjustment achieved in\\nany speciﬁc instance with propensity-based matching methods needs to be checked and documented, with\\n15'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[ 44]. As\\nabove, MLTRL ensures transparency and adherence to this important aspect of causal model development, as\\nwithout it a project cannot graduate from Level 1. Even more, MLTRL ensures tests for confounding bias\\nare developed early-on and maintained throughout later stages to deployment. Still, in many cases, it is not\\npossible to completely remove confounding in the observed data. TRL Cards offer a transparent way to declare\\nspeciﬁc limitations of a causal ML method.\\n•Level 2 – PoC-level tests for causal models must go beyond that of typical ML models. As discussed above,\\nto ensure the estimated causal effects are robust to the assumptions required for their derivation, sensitivity\\nto these assumptions must be analysed. Such sensitivity analysis is often limited to R&D experiments or\\na post-hoc feature of ML products. MLTRL on the other hand requires this throughout the lifecycle as\\ncomponents of ML test suites and gated reviews. In the case of causal ML, best practice is to employ sensitivity\\nanalysis for this robustness check[ 50]. MLTRL ensures this check is highlighted and adhered to, and no model\\nwill end up graduating Level 2–let alone being deployed–unless it is passed.\\n•Level 3 – Coding best practices, as in general ML applications.\\n•Level 4-5 – There are additional tests to consider when taking causal models from research to production,\\nin particular at Level 4–proof of concept demonstration in a real scenario. Consistency , for example, is an\\nimportant property of causal methods that informs us whether the method provably converges to the true\\ncausal graph in the limit of inﬁnite sample size. Quantifying consistency in the test suite is critical when\\ndatasets change from controlled laboratory settings to open-world, and when the application scales. And\\nPoC validation steps are more efﬁcient with MLTRL because the process facilitates early speciﬁcation of the\\nevaluation metric for a causal model in Level 2. Causal models cannot be validated by standard held-out tests,\\nbut rather require randomization or special data collection strategies to evaluate their predictions[ 45,46]. Any\\ndifﬁculty in evaluating the model’s predictions will be caught early and remedied.\\n•Level 6-9 – With the the causal ML components of this technology developed reliably in the previous levels,\\nthe rest of the levels developing this technology focused on general medical-ML deployment challenges. For\\nthe most part, data governance, privacy, and management that was detailed earlier in the neuropathology\\nMLTRL use-case, as well as on-premises deployment.\\nAI for open-source space sciences\\nThe CAMS (Cameras for Allsky Meteor Surveillance) project [ 51], established in 2010 by NASA, uses hundreds of\\noff-the-shelf CCTV cameras to capture the meteor activity in the night sky. Initially, resident scientists would retrieve\\nhard-disks containing video data captured each night and perform manual triangulation of tracks or streaks of light\\nin the night sky, and compute a meteor’s trajectory, orbit, and lightcurve. Each solution was manually classiﬁed as a\\nmeteor or not (i.e., planes, birds, clouds, etc). In 2017, a project run by the Frontier Development Labvii[52], the AI\\naccelerator for NASA and ESA, aimed to automate the data processing pipeline and replicate the scientists thought\\nprocess to build an ML model that identiﬁes meteors in the CAMS project [ 53,54]. The data automation led to\\norders of magnitude improvements in operational efﬁciency of the system, and allowed new contributors and amateur\\nastronomers to start contributing to meteor sightings. Additionally, a novel web tool allowed anybody anywhere to\\nview the meteors detected in the previous night. The CAMS camera system has had six-fold global expansion of the\\ndata capture network, discovered ten new meteor showers, contributed towards instrumental evidence of previously\\npredicted comets, and helped calculate parent bodies of various meteor showers. CAMS utilized the MLTRL framework\\nto progress as described:\\n•Level 1 – Understanding the domain and data is a prerequisite for any ML development. Extensive data\\nexploration elucidated visual differences between objects in the night sky such as meteors, satellites, clouds,\\ntail lights of planes, light from the eyes of cats peering into cameras, trees, and other tall objects visible in\\nthe moonlight. This step helped (1) understand visual properties of meteors that later deﬁned the ML model\\narchitecture, and (2) mitigate impact of data imbalance by proactively developing domain-oriented strategies.\\nThe results are well-documented on a datasheet associated with the TRL card, and discussed at the stage\\nreview. This MLTRL documentation forced us to consider data sharing and other privacy concerns at this early\\nconceptualization stage, which is certainly relevant considering CAMS is for open-source and gathering data\\nfrom myriad sources.\\n•Level 2-3 – The agile and non-monotonic (or non-linear) development prescribed by MLTRL allowed the\\nteam to ﬁrst develop an approximate end-to-end pipeline that offered a path to ML model deployment and\\nquick turnaround time to incorporate feedback from the regular gated reviews. Then, with relatively quicker\\nviiThe NASA Frontier Development Lab and partners open-source the code and data via the SpaceML platform: spaceml.org\\n16'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='experimentation, the team could improve on the quality of not just the ML model, but also scale up the systems\\ndevelopment simultaneously in a non-monotonic development cycle.\\n•Level 4 – With the initial pipeline in place, scalable training of baselines and initial models on real challenging\\ndatasets ensued. Throughout the levels, the MLTRL gated reviews were essential for making efﬁcient progress\\nwhile ensuring robustness and functionality that meets stakeholder needs. At this stage we highlight speciﬁc\\nadvantages of the MLTRL review processes that had instrumental effect on the project success: With the\\nrequired panel of mixed ML researchers and engineers, domain scientists, and product managers, the stage 4\\nreviews stressed the signiﬁcance of numerical improvements and comparison to existing baselines, and helped\\nidentify and overcome issues with data imbalance. The team likely would have overlooked these approaches\\nwithout the review from peers in diverse roles and teams. In general, the evolving panel of reviewers at\\ndifferent stages of the project was essential for covering a variety of veriﬁcation and validation measures –\\nfrom helping mitigate data challenges, to open-source code quality.\\n•Level 5 – To complete this R&D-to-productization level, a novel web tool called the NASA CAMS Meteor\\nShower Portalviiiwas created that allowed users to view meteor shower activity from the previous night and\\nverify meteor predictions generated by the ML model. This app development was valuable for A/B testing,\\nvalidating detected meteors and classiﬁed new meteor showers with human-AI interaction, and demonstrating\\nreal-world utility to stakeholders in review. ML processes without MLTRL miss out on these valuable\\ndevelopment by overlooking the need for such a demo tool.\\n•Level 6 – Application development was naturally driven by end-user feedback from the web app in level 5 –\\nwithout MLTRL it’s unlikely the team would be able to work with early productization feedback. With almost\\nreal time feedback coming in daily, newer methods for improving robustness of meteor identiﬁcation led to\\nresearching and developing a unique augmentation technique, resulting in the state of the art performance of\\nthe ML model. Further application development led to incorporating features that were in demand by users of\\nthe NASA CAMS Meteor Shower Portal: include celestial reference points through constellations, add ability\\nto zoom in/out and (un)cluster showers, and provide tooling for scientiﬁc communication. The coordination of\\nthese features into product-caliber codebase resulted in the release of the NASA CAMS Meteor Shower Portal\\n2.0 that was built by a team of citizen scientists – again we found the speciﬁc checkpoints in the MLTRL\\nreview were crucial for achieving these goals.\\n•Level 7 – Integration was particularly challenging in two ways. First, integrating the ML and data engineering\\ndeliverables with the existing infrastructure and tools of the larger CAMS system, which had started devel-\\nopment years earlier with other teams in partner organizations, required quantiﬁable progress for verifying\\nthe tech-readiness of ML models and modules. The use of technology readiness levels provided a clear and\\nconsistent metric for the maturity of the ML and data technologies, making for clear communication and\\nefﬁcient project integration. Without MLTRL it is difﬁcult to have a conversation, let alone make progress, to-\\nwards integrating AI/ML and data subsystems and components. Second, integrating open-source contributions\\ninto the main ML subsystem was a signiﬁcant challenge alleviated with diligent veriﬁcation and validation\\nmeasures from MLTRL, as well as quantifying robustness with ML testing suites (using scoring measures like\\nthat of the ML Testing Rubric[20], and devising a checklist based on metamorphic testing[18]).\\n•Level 8 – CAMS, like many datasets in practice, consisted of a smaller labeled subset and a much larger\\nunlabeled set. In an attempt to additionally increase robustness of the ML subsystem ahead of “ﬂight readiness”,\\nwe looked to active learning [ 55,56] techniques to leverage the unlabeled data. Models using an initial version\\nof this approach, where results of the active learning provided “weak” labels, resulted in consumption of the\\nentire decade long unlabelled data collected by CAMS and slightly higher scores on deployment tests. Active\\nlearning showed to be a promising feature and was switched back to level 7 for further development towards\\nthe next deployment version, so as not to delay the rest of the project.\\n•Level 9 – The ML components in CAMS require continual monitoring for model and data drifts, such as\\nchanges in weather, smoke, and cloud patterns that affect the view of the night sky. The data drifts may also be\\nspeciﬁc to locations, such as ﬁreﬂies and bugs in CAMS Australia and New Zealand stations which appear as\\nfalse positives. The ML pipeline is largely automated with CI/CD, runs regular regression tests, and production\\nof benchmarks. Manual intervention can be triggered when needed, such as sending low conﬁdence meteors for\\nveriﬁcation to scientists in the CAMS project. The team also regularly releases the code, models, and web tools\\non the open-source space sciences and exploration ML toolbox, SpaceMLix. Through the SpaceML community\\nand partner organizations, CAMS continually improves with feature requests, debugging, and improving data\\npractices, while tracking progress with standard software release cycles and MLTRL documentation.\\nviiimeteorshowers.seti.org\\nixspaceml.org\\n17'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='BEYOND SOFTWARE ENGINEERING\\nSoftware engineering (SWE) practices vary signiﬁcantly across domains and industries. Some domains, such as medical\\napplications, aerospace, or autonomous vehicles rely on a highly rigorous development process which is required\\nby regulations. Other domains, for example advertising and e-commerce are not regulated and can employ a lenient\\napproach to development. ML development should at minimum inherit the acceptable software engineering practices of\\nthe domain. There are, however, several key areas where ML development stands out from SWE, adding its own unique\\nchallenges which even most rigorous SWE practices are not able to overcome.\\nFor instance, the behavior of ML systems is learned from data, not speciﬁed directly in code. The data requirements\\naround ML (i.e., data discovery, management, and monitoring) adds signiﬁcant complexity not seen in other types\\nof SWE. There are many beneﬁts to using a data-oriented architecture (DOA) [48] with the data-ﬁrst workﬂows and\\nmanagement practices prescribed in MLTRL. DOA aims to make the data ﬂowing between elements of business logic\\nmore explicit and accessible with a streaming-based architecture rather than the micro-service architectures that are\\nstandard in software systems. One speciﬁc beneﬁt of DOA is making data available and traceable by design, which\\nhelps signiﬁcantly in the ML logging challenges and data governance needs we discussed in Levels 7-9. Moreover,\\nMLTRL highlights data-related requirements along every step to ensure that the development process considers data\\nreadiness and availability.\\nNot to mention an array of ML-speciﬁc failure modes; for example, models that become miscalibrated due to subtle\\ndata distributional shifts in the deployment setting, resulting in models that are more conﬁdent in predictions than they\\nshould be. MLTRL helps deﬁne ML-speciﬁc testing considerations (levels 5 and 7) to help surface these failure-modes\\nearly. ML opens up new threat vectors across the whole deployment workﬂow that otherwise aren’t risks in software\\nsystems: for example, a poisoning attack to contaminate the training phase of ML systems, or membership inference\\nto see if a given data record was part of the model’s training. MLTRL consider these threat vectors and suggests\\nrelevant risk-identiﬁcation during prototyping and productization phases. More generally, ML codebases have all the\\nproblems for regular code, plus ML-speciﬁc issues at the system level, mainly as a consequence of added complexity\\nand dynamism. The resulting entanglement, for instance, implies that the SWE practice of making isolated changes is\\noften not feasible – Scully et al.[ 57] refer to this as the “changing anything changes everything” principle. Given this\\nconsideration, typical SWE change-management is insufﬁcient. Furthermore, ML systems almost necessarily increase\\nthe technical debt; package-level refactoring is generally sufﬁcient for removing technical debt in software systems, but\\nthis is not the case in ML systems.\\nThese factors and others suggest that inherited software engineering and management practices of a given domain are\\ninsufﬁcient for the successful development of robust and reliable ML systems. But it is not trading off one for the other:\\nMLTRL can be used in synergy with the existing, industry-standard software engineering practices such as agile [ 58]\\nand waterfall [ 59] to handle unique challenges of ML development. Because ML applications are a category of software,\\nall best practices of building and operating software should be extended when possible to the ML application. Practices\\nlike version control, comprehensive testing, continuous integration and continuous deployment are all applicable to ML\\ndevelopment. MLTRL provides a framework that helps extend SWE building and operating practices that are acceptable\\nin a given domain to tackle the unique challenges of ML development.\\nRELATED WORKS\\nA recent case study from Microsoft Research [ 40] similarly identiﬁes a few themes describing how ML is not equal to\\nsoftware engineering, and recommends a linear ML workﬂow with steps for data preparation through modeling and\\ndeploying. They deﬁne an effective workﬂow for isolated development of an ML model, but this approach does not\\nensure the technology is actually improving in quality and robustness. Their process should be repeated at progressive\\nstages of development in the broader ML and data technology lifecycle. If applied in the MLTRL framework, the\\nspeciﬁc ingredients of the ML model workﬂow – that is, people, software, tests, objectives, etc. – evolve over time and\\nsubsequent stages as the technologies mature.\\nThere exist many recommended workﬂows for speciﬁc ML methods and areas of pipelines. For instance, a more\\niterative process for Bayesian ML [ 60] and even more speciﬁcally for probabilistic programming [ 39], a data mining\\nprocess deﬁned in 2000 that remains widely used [ 61], others for describing data iterations [ 62], and human-computer\\ninteraction cycles [ 63]. In these recommended workﬂows and others, there’s an important distinction between their\\ncycles and “switchback” mechanisms in MLTRL. Their cycles suggest to generically iterate over a data-modeling-\\nevaluation-deployment process. Switchbacks, on the other hand, are speciﬁc, purpose-driven workﬂows for dialing\\npart(s) of a project to an earlier stage – this doesn’t simply mean go back and train the model on more data, but rather\\nswitching back regresses the technology’s maturity level (e.g. from level 5 to level 3) such that it must again fulﬁll the\\nlevel-by-level requirements, evaluations and reviews. See the Methods section for more details on MLTRL switchbacks.\\n18'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='In general, iteration is an important part of data, ML, and software processes. MLTRL is unique from the other\\nrecommended processes in many ways, and perhaps most importantly because it considers data ﬂows and ML models\\nin the context of larger systems. These isolated processes (that are speciﬁc to e.g. modeling in prototype development\\nor data wrangling in application development) are synergistic with MLTRL because they can be used within each level\\nof the larger lifecycle or framework. For example, the Bayesian modeling processes [ 39,60] we mentioned above\\nare really useful to guide developers of probabilistic ML approaches. But there are important distinctions between\\nexecuting these modeling steps and cycles in a well-deﬁned prototyping environment with curated data and minimal\\nresponsibilities, versus a production environment riddled with sparse and noisy data, that interacts with the physical\\nworld in non-obvious ways, and can carry expensive (even hidden) consequences. MLTRL provides the necessary,\\nholistic context and structure to use these and other development processes reliably and responsibly.\\nAlso related to our work, Google teams have proposed ML testing recommendations [ 20] and validating the data fed\\ninto ML systems [ 64]. For NLP applications, typical ML testing practices struggle to translate to real-world settings,\\noften overestimating performance capabilities. An effective way to address this is devising a checklist of linguistic\\ncapabilities and test types, as in Ribeiro et al.[ 17]–interestingly their test suite was inspired by metamorphic testing,\\nwhich we suggested earlier in Level 7 for testing systems AI integrations. A survey by Paleyes et al. [ 48] go over\\nnumerous case studies to discuss challenges in ML deployment. They similarly pay special attention to the need for\\nethical considerations, end-user trust, and extra security in ML deployments. On the latter point, Kumar et al. [ 65]\\nprovide a table thoroughly breaking down new threat vectors across the whole ML deployment workﬂow (some of\\nwhich we mentioned above). These works, notably the ML security measures and the quantiﬁcation of an ML test suite\\nin a principled way – i.e., that does not use misguided heuristics such as code coverage – are valuable to include in any\\nML workﬂow including MLTRL, and are synergistic with the framework we’ve described in this paper. These analyses\\nprovide useful insights, but they do not provide a holistic, regimented process for the full ML lifecycle from R&D\\nthrough deployment. An end-to-end approach is suggested by Raji et al.[ 66], but only for the speciﬁc task of auditing\\nalgorithms; components of AI auditing are mentioned in Level 7, and covered throughout in the review processes.\\nSculley et al.[ 57] go into more ML debt topics such as undeclared consumers and data dependencies, and go on to\\nrecommend an ML Testing Rubric as a production checklist [ 20]. For example, testing models by a canary process\\nbefore serving them into production. This, along with similar shadow testing we mentioned earlier, are common in\\nautonomous ML systems, notably robotics and autonomous vehicles. They explicitly call out tests in four main areas\\n(ML infrastructure, model development, features and data, and monitoring of running ML systems), some of which we\\ndiscussed earlier. For example, tests that the training and serving features compute the same values; a model may train\\non logged processes or user input, but is then served on a live feed with different inputs. In addition to the Google ML\\nTesting Rubric, we advocate metamorphic testing : a SWE methodology for testing a speciﬁc set of relations between\\nthe outputs of multiple inputs. True to the checklists in the Google ML Testing Rubric and in MLTRL, metamorphic\\ntesting for ML can have a codiﬁed list of metamorphic relations[18].\\nIn domains such as healthcare there have been the introduction of similar checklists for data readiness – for example,\\nto ensure regulatory-grade real-world-evidence (RWE) data quality [ 67] – yet these are nascent and not yet widely\\naccepted. Applying AI in healthcare has led to developing guidance for regulatory protocol, which is still a work in\\nprogress. Larson et al.[ 68] provide a comprehensive analysis for medical imaging and AI, arriving at several regulatory\\nframework recommendations that mirror what we outline as important measures in MLTRL: e.g., detailed task elements\\nsuch as pitfalls and limitations (surfaced on TRL Cards), clear deﬁnition of an algorithm relative to the downstream\\ntask, deﬁning the algorithm “capability” (Level 5), real-world monitoring, and more.\\nD’amour et al.[ 19] dive into the problem we noted earlier about model miscalibration. They point to the trend in machine\\nlearning to develop models relatively isolated from the downstream use and larger system, resulting in underspeciﬁcation\\nthat handicaps practical ML pipelines. This is largely problematic in deep learning pipelines, but we’ve also noted this\\nrisk in the case of causal inference applications. Suggested remedies include stress tests –empirical evaluations that\\nprobe the model’s inductive biases on practically relevant dimensions–and in general the methods we deﬁne in Level 7.\\nLIMITATIONS, RESPONSIBILITIES, and ETHICS\\nMLTRL has been developed, deployed, iterated, and validated in myriad environments, as demonstrated by the previous\\nexamples and many others. Nonetheless we strongly suggest that MLTRL not be viewed as a cure-all for machine\\nlearning systems engineering. Rather, MLTRL provides mechanisms to better enable ML practitioners, teams, and\\nstakeholders to be diligent and responsible with these technologies and data. That is, one cannot implement MLTRL in\\nan organization and turn a blind eye to the many data, ML, and integration challenges we’ve discussed here. MLTRL is\\nanalogous to a pilot’s checklist, not autopilot.\\n19'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='MLTRL is intended to be complimentary to existing software development methodologies, not replace or alter them.\\nSpeciﬁcally, whether the team uses agile or waterfall methods, MLTRL can be adopted to help deﬁne and structure\\nphases of the project, as well as the success criteria of each stage. In context of the software development process, the\\npurpose of MLTRL is to help the team minimize the technical dept and risk associated with the delivery of an ML\\napplication by helping the development team ask necessary questions.\\nWe discussed many data challenges and approaches in the context of MLTRL, and should highlight again the importance\\nof data considerations in any ML initiative. The data availability and quality can severely limit the ability to develop and\\ndeploy ML, whether MLTRL is used or not. It is again the responsibility of the ML practitioners, teams, and stakeholders\\nto gather, use, and distribute data in safe, legal, ethical ways. MLTRL helps do so with rigor and transparency, but\\nagain is not a solution for data bias. We recommend these recent works on data bias in ML: [ 69,70,71,72,73].\\nFurther, AI/ML ethics is a continuously evolving, multidisciplinary space – see [ 5]. MLTRL aims to prioritize ethics\\nconsiderations at each level of the framework, and would do well to also evolve over time with the broader AI/ML\\nethics developments.\\nCONCLUSION\\nWe’ve described Machine Learning Technology Readiness Levels (MLTRL) , an industry-hardened systems engineering\\nframework for robust, reliable, and responsible machine learning. MLTRL is derived from the processes and testing\\nstandards of spacecraft development, yet lean and efﬁcient for ML, data, and software workﬂows. Examples from\\nseveral organizations across industries demonstrate the efﬁcacy of MLTRL for AI and ML technologies, from research\\nand development through productization and deployment, in important domains such as healthcare and physics, with\\nemphasis on data readiness amongst other critical challenges. Our aim is MLTRL works in synergy with recent\\napproaches in the community focused on diligent data-readiness, privacy and security, and ethics. Even more, MLTRL\\nestablishes a much-needed lingua franca for the AI ecosystem, and broadly for AI in the worlds of science, engineering,\\nand business. Our hope is that our systems framework is adopted broadly in AI and ML organizations, and that\\n“technology readiness levels” becomes common nomenclature across AI stakeholders – from researchers and engineers\\nto sales-people and executive decision-makers.\\nMethods\\nGated reviews\\nAt the end of each stage is a dedicated review period: (1) Present the technical developments along with the requirements\\nand their corresponding veriﬁcation measures and validation steps, (2) make key decisions on path(s) forward (or\\nbackward) and timing, and (3) debrief the processx. As in the gated reviews deﬁned by TRL used by NASA, DARPA, et\\nal., MLTRL stipulates speciﬁc criteria for review at each level, as well as calling out speciﬁc key decision points (noted\\nin the level descriptions above). The designated reviewers will “graduate” the technology to the next level, or provide a\\nlist of speciﬁc tasks that are still needed (ideally with quantitative remarks). After graduation at each level, the working\\ngroup does a brief post-mortem; we ﬁnd that a quick day or two pays dividends in cutting away technical debt and\\nimproving team processes. Regular gated reviews are essential for making efﬁcient progress while ensuring robustness\\nand functionality that meets stakeholder needs. There are several important mechanisms in MLTRL reviews that are\\nspeciﬁcally useful with AI and ML technologies: First, the review panels evolve over a project lifecycle, as noted\\nbelow. Second, MLTRL prescribes that each review runs through an AI ethics checklist deﬁned by the organization; it is\\nimportant to repeat this at each review, as the review panel and stakeholders evolve considerably over a project lifecycle.\\nAs previously described in the levels deﬁnitions, including ethics reviews as an integral part of early system development\\nis essential for informing model speciﬁcations and avoiding unintended biases or harm[74] after deployment.\\nTRL “Cards”\\nIn Figure 3 we succinctly showcase a key deliverable: TRL Cards . The model cards proposed by Google [ 75] are a useful\\ndevelopment for external user-readiness with ML. On the other hand, our TRL Cards aim to be more information-dense,\\nlike datasheets for medical devices and engineering tools – see the open-source TRL Card repo for examples and\\ntemplates (to be released at github.com/alan-turing-institute). These serve as “report cards” that grow and improve upon\\ngraduating levels, and provide a means of inter-team and cross-functional communication. The content of a TRL Card\\nis roughly in two categories: project info, and implicit knowledge. The former clearly states info such as project owners\\nxMLTRL should include regular debriefs and meta-evaluations such that process improvements can be made in a data-driven,\\nefﬁcient way (rather than an annual meta-review). MLTRL is a high-level framework that each organization should operationalize in\\na way that suits their speciﬁc capabilities and resources.\\n20'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='and reviewers, development status, and semantic versioning–not just for code, also for models and data. In the latter\\ncategory are speciﬁc insights that are typically siloed in the ML development team but should be communicated to\\nother stakeholders: modeling assumptions, dataset biases, corner cases, etc. With the spread of AI and ML in critical\\napplication areas, we are seeing domain expert consortiums deﬁning AI reporting guidelines – e.g., Rivera et al.[ 76]\\ncalling for clinical trials reports for interventions involving AI – which will greatly beneﬁt from the use of our TRL\\nreporting cards. We stress that these TRL Cards are key for the progression of projects, rather than documentation\\nafterthoughts. The TRL Cards thus promote transparency and trust, within teams and across organizations. TRL Card\\ntemplates will be open-sourced upon publication of this work, including methods for coordinating use with other\\nreporting tools such as “Datasheets for Datasets” [24].\\nRisk mitigation\\nIdentifying and addressing risks in a software project is not a new practice. However, akin to the MLTRL roots in\\nspacecraft engineering, risk is a “ﬁrst-class citizen” here. In the deﬁnition of technical and product requirements, each\\nentry has a calculation of the form risk =p(failure )×value , where the value of a component is an integer 1−10.\\nBeing diligent about quantifying risks across the technical requirements is a useful mechanism for ﬂagging ML-related\\nvulnerabilities that can sometimes be hidden by layers of other software. MLTRL also speciﬁes that risk quantiﬁcation\\nand testing strategies are required for sim-to-real development. That is, there is nearly always a non-trivial gap in\\ntransferring a model or algorithm from a simulation testbed to the real world. Requiring explicit sim-to-real testing\\nsteps in the workﬂow helps mitigate unforeseen (and often hazardous) failures. Additionally, comprehensive ML test\\ncoverage that we mention throughout this paper is a critical strategy for mitigating risks anduncertainties: ML-based\\nsystem behavior is not easily speciﬁed in advance, but rather depends on dynamic qualities of the data and on various\\nmodel conﬁguration choices[20].\\nNon-monotonic, non-linear paths\\nWe observe many projects beneﬁt from cyclic paths, dialing components of a technology back to a lower level. Our\\nframework not only encourages cycles, we make them explicit with “switchback mechanisms” to regress the maturity\\nof speciﬁc components in an AI system:\\n1.Discovery switchbacks occur as a natural mechanism – new technical gaps are discovered through systems\\nintegration, sparking later rounds of component development[ 77]. These are most common in the R&D levels,\\nfor example moving a component of a proof-of-concept technology (at Level 4) back to proof-of-principle\\ndevelopment (Level 2).\\n2.Review switchbacks result from gated reviews, where speciﬁc components or larger subsystems may be dialed\\nback to earlier levels. This switchback is one of the “key decision points” in the MLTRL project lifecycle\\n(as noted in the Levels deﬁnitions), and is often a decision driven by business-needs and timing rather than\\ntechnical concerns (for instance when mission priorities and funds shift). This mechanism is common from\\nLevel 6/7 to 4, which stresses the importance of this R&D to product transition phase (see Figure 2 (left)).\\n3.Embedded switchbacks are predeﬁned in the MLTRL process. For example, a predeﬁned path from 4 to 2, and\\nfrom 9 to 4. In complex systems, particularly with AI technologies, these built-in loops help mitigate technical\\ndebt and overcome other inefﬁciencies such as noncomprehensive V&V steps.\\nWithout these built-in mechanisms for cyclic development paths, it can be difﬁcult and inefﬁcient to build systems of\\nmodules and components at varying degrees of maturity. Contrary to traditional thought that switchback events should\\nbe suppressed and minimized, in fact they represent a natural and necessary part of the complex technology development\\nprocess – efforts to eliminate them may stiﬂe important innovations without necessarily improving efﬁciency. This is\\na fault of the standard monotonic approaches in AI/ML projects, stage-gate processes, and even the traditional TRL\\nframework.\\nIt is also important to note that most projects do not start at Level 0; very few ML companies engage in this low-level\\ntheoretical research. For example, a team looking to use an off-the-shelf object recognition model could start that\\ntechnology at Level 3, and proceed with thorough V&V for their speciﬁc datasets and use-cases. However, no technology\\ncan skip levels after the MLTRL process has been initiated. The industry default (that is, without implementing MLTRL)\\nis to ignorantly take pretrained models, run ﬁne tuning on their speciﬁc data, and jump to deployment, effectively\\nskipping Levels 5 to 7. Additionally, we ﬁnd it is advantageous to incorporate components from other high-TRL ranking\\nprojects while starting new projects; MLTRL makes the veriﬁcation and validation (V&V) steps straightforward for\\nintegrating previously developed ML components.\\n21'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='Evolving people, objectives, and measures\\nAs suggested earlier, much of the practical value of MLTRL comes at the transition between levels. More precisely,\\nMLTRL manages these oft neglected transitions explicitly as evolving teams, objectives, and deliverables. For instance,\\nthe team (or working group) at Level 3 is mostly AI Research Engineers, but at Level 6 is mixed Applied AI/SW\\nEngineers mixed with product managers and designers. Similarly, the review panels evolve from level to level, to match\\nthe changing technology development objectives. What the reviewers reference similarly evolves: notice in the level\\ndeﬁnitions that technical requirements and V&V guide early stages, but at and after Level 6 the product requirements\\nand V&V takeover – naturally, the risk quantiﬁcation and mitigation strategies evolve in parallel. Regarding the\\ndeliverables, notably TRL Cards and risk matrices[ 22] (to rank and prioritize various science, technical, and project\\nrisks), the information develops and evolves over time as the technology matures.\\nQuantiﬁable progress\\nBy deﬁning technology maturity in a quantitative way, MLTRL enables teams to accurately and consistently deﬁne\\ntheir ML progress metrics. Notably industry-standard “objectives and key results” (OKRs) and “key performance\\nindicators” (KPIs) [ 78] can be deﬁned as achieving certain readiness levels in a given period of time; this is a preferable\\nmetric in essentially all ML systems which consist of much more than a single performance score to measure progress.\\nEven more, meta-review of MLTRL progress over multiple projects can provide useful insights at the organization\\nlevel. For example, analysis of the time-per-level and the most frequent development paths/cycles can bring to light\\noperational bottlenecks. Compared to conventional software engineering metrics based on sprint stories and tickets, or\\ntime-tracking tools, MLTRL provides a more accurate analysis of ML workﬂows.\\nCommunication and explanation\\nA distinct advantage of MLTRL in practice is the nomenclature: an agreed upon grading scheme for the maturity of\\nan AI technology, and a framework for how/when that technology ﬁts within a product or system, enables everyone\\nto communicate effectively and transparently. MLTRL also acts as a gate for interpretability and explainability–at\\nthe granularity of individual models and algorithms, and more crucially from a holistic, systems standpoint. Notably\\nthe DARPA XAIxiprogram advocates for this advance in developing AI technologies; they suggest interpretability\\nand explainability are necessary at various locations in an AI system to be sufﬁcient for deployment as an AI product,\\notherwise leading to issues with ethics and bias.\\nRobustness via uncertainty-aware ML\\nHow to design a reliable system from unreliable components has been a guiding question in the ﬁelds of computing and\\nintelligence [79]. In the case of AI/ML systems, we aim to build reliable systems with myriad unreliable components:\\nnoisy and faulty sensors, human and AI error, and so on. There is thus signiﬁcant value to quantifying the myriad\\nuncertainties, propagating them throughout a system, and arriving at a notion or measure of reliability. For this reason,\\nalthough MLTRL applies generally to AI/ML methods and systems, we advocate for methods in the class of probabilistic\\nML, which naturally represent and manipulate uncertainty about models and predictions[ 28]. These are Bayesian\\nmethods that use probabilities to represent aleatoric uncertainty , measuring the noise inherent in the observations, and\\nepistemic uncertainty , accounting for uncertainty in the model itself (i.e., capturing our ignorance about which model\\ngenerated the data). In the simplest case, an uncertainty aware ML pipeline should quantify uncertainty at the points of\\nsensor inputs or perception, prediction or model output, and decision or end-user action – McAllister et al.[ 29] suggest\\nthis with Bayesian deep learning models for safer autonomous vehicle pipelines. We can achieve this sufﬁciently well\\nin practice for simple systems. However, we do not yet have a principled, theoretically grounded, and generalizable way\\nof propagating errors and uncertainties downstream and throughout more complex AI systems – i.e., how to integrate\\ndifferent software, hardware, data, and human components while considering how errors and uncertainties propagate\\nthrough the system. This is an important direction of our future work.\\nReferences\\n[1]Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning\\nthat matters. In AAAI , 2018.\\nxiDARPA Explainable Artiﬁcial Intelligence (XAI)\\n22'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='[2]Arnaud de la Tour, Massimo Portincaso, Kyle Blank, and Nicolas Goeldel. The dawn of the deep tech ecosystem. Technical\\nreport, The Boston Consulting Group, 2019.\\n[3] NASA. The NASA systems engineering handbook. 2003.\\n[4] United States Department of Defense. Defense acquisition guidebook. Technical report, U.S. Dept. of Defense, 2004.\\n[5] D. Leslie. Understanding artiﬁcial intelligence ethics and safety. ArXiv , abs/1906.05684, 2019.\\n[6]Google. Machine learning workﬂow. https://cloud.google.com/mlengine/docs/tensorflow/\\nml-solutions-overview . Accessed: 2020-12-13.\\n[7]Alexander Lavin and Gregory Renard. Technology readiness levels for AI & ML. ICML Workshop on Challenges Deploying\\nML Systems , 2020.\\n[8] T. Dasu and T. Johnson. Exploratory data mining and data cleaning. 2003.\\n[9]M. Janssen, P. Brous, Elsa Estevez, L. Barbosa, and T. Janowski. Data governance: Organizing data for trustworthy artiﬁcial\\nintelligence. Gov. Inf. Q. , 37:101493, 2020.\\n[10] B. Shahriari, Kevin Swersky, Ziyu Wang, R. Adams, and N. D. Freitas. Taking the human out of the loop: A review of bayesian\\noptimization. Proceedings of the IEEE , 104:148–175, 2016.\\n[11] Goutham Ramakrishnan, A. Nori, Hannah Murfet, and Pashmina Cameron. Towards compliant data management systems for\\nhealthcare ml. ArXiv , abs/2011.07555, 2020.\\n[12] Umang Bhatt, Alice Xiang, S. Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura,\\nand P. Eckersley. Explainable machine learning in deployment. Proceedings of the 2020 Conference on Fairness, Accountability,\\nand Transparency , 2020.\\n[13] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and V . Smith. Federated learning: Challenges, methods, and future directions.\\nIEEE Signal Processing Magazine , 37:50–60, 2020.\\n[14] T. Ryffel, Andrew Trask, M. Dahl, Bobby Wagner, J. Mancuso, D. Rueckert, and J. Passerat-Palmbach. A generic framework\\nfor privacy preserving deep learning. ArXiv , abs/1811.04017, 2018.\\n[15] A. Madry, Aleksandar Makelov, Ludwig Schmidt, D. Tsipras, and Adrian Vladu. Towards deep learning models resistant to\\nadversarial attacks. ArXiv , abs/1706.06083, 2018.\\n[16] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. ArXiv , abs/1710.11342, 2018.\\n[17] Marco Túlio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models\\nwith checklist. In ACL, 2020.\\n[18] Xiaoyuan Xie, Joshua W. K. Ho, C. Murphy, G. Kaiser, B. Xu, and T. Chen. Testing and validating machine learning classiﬁers\\nby metamorphic testing. The Journal of systems and software , 84 4:544–558, 2011.\\n[19] Alexander D’Amour, K. Heller, D. Moldovan, Ben Adlam, B. Alipanahi, Alex Beutel, C. Chen, Jonathan Deaton, Jacob\\nEisenstein, M. Hoffman, Farhad Hormozdiari, N. Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, M. Lucic,\\nY . Ma, Cory Y . McLean, Diana Mincu, Akinori Mitani, A. Montanari, Zachary Nado, V . Natarajan, C. Nielson, Thomas F.\\nOsborne, R. Raman, K. Ramasamy, Rory Sayres, J. Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, V . Veitch,\\nMax Vladymyrov, Xuezhi Wang, K. Webster, S. Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley. Underspeciﬁcation\\npresents challenges for credibility in modern machine learning. ArXiv , abs/2011.03395, 2020.\\n[20] Eric Breck, Shanqing Cai, E. Nielsen, M. Salib, and D. Sculley. The ml test score: A rubric for ml production readiness and\\ntechnical debt reduction. 2017 IEEE International Conference on Big Data (Big Data) , pages 1123–1132, 2017.\\n[21] A. Botchkarev. A new typology design of performance metrics to measure errors in machine learning regression algorithms.\\nInterdisciplinary Journal of Information, Knowledge, and Management , 14:045–076, 2019.\\n[22] N. Duijm. Recommendations on the use and design of risk matrices. Safety Science , 76:21–31, 2015.\\n[23] Louise Naud and Alexander Lavin. Manifolds for unsupervised visual anomaly detection. ArXiv , abs/2006.11364, 2020.\\n[24] Timnit Gebru, J. Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, H. Wallach, Hal Daumé, and K. Crawford.\\nDatasheets for datasets. ArXiv , abs/1803.09010, 2018.\\n[25] B. Hutchinson, A. Smart, A. Hanna, Emily L. Denton, Christina Greer, Oddur Kjartansson, P. Barnes, and Margaret Mitchell.\\nTowards accountability for machine learning datasets: Practices from software engineering and infrastructure. Proceedings of\\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency , 2021.\\n23'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='[26] P. Schulam and S. Saria. Reliable decision support using counterfactual models. In NIPS 2017 , 2017.\\n[27] Towards trustable machine learning. Nature Biomedical Engineering , 2:709–710, 2018.\\n[28] Zoubin Ghahramani. Probabilistic machine learning and artiﬁcial intelligence. Nature , 521:452–459, 2015.\\n[29] Rowan McAllister, Yarin Gal, Alex Kendall, Mark van der Wilk, A. Shah, R. Cipolla, and Adrian Weller. Concrete problems\\nfor autonomous vehicle safety: Advantages of bayesian deep learning. In IJCAI , 2017.\\n[30] Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I. Avilés-Rivero,\\nChristian Etmann, Cathal McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, Effrossyni Gkrania-Klotsas,\\nJames H. F. Rudd, Evis Sala, and Carola-Bibiane Schönlieb. Common pitfalls and recommendations for using machine learning\\nto detect and prognosticate for covid-19 using chest radiographs and ct scans. Nature Machine Intelligence , 3:199–217, 2021.\\n[31] J. Tobin, Rachel H Fong, Alex Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep\\nneural networks from simulation to the real world. 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems\\n(IROS) , pages 23–30, 2017.\\n[32] Arthur Juliani, Vincent-Pierre Berges, Esh Vckay, Yuan Gao, Hunter Henry, M. Mattar, and D. Lange. Unity: A general\\nplatform for intelligent agents. ArXiv , abs/1809.02627, 2018.\\n[33] Stefan Hinterstoißer, Olivier Pauly, Tim Hauke Heibel, Martina Marek, and Martin Bokeloh. An annotation saved is an\\nannotation earned: Using fully synthetic training for object instance detection. ArXiv , abs/1902.09967, 2019.\\n[34] Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan Hogins, You-Cyuan Jhang, Mohsen Kamalzadeh,\\nBowen Li, Steven Leal, Pete Parisi, Cesar Romero, Wesley Smith, Alex Thaman, Samuel Warren, and Nupur Yadav. Unity\\nperception: Generate synthetic data for computer vision. CoRR , abs/2107.04259, 2021.\\n[35] K. Cranmer, J. Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of the National Academy\\nof Sciences , 117:30055 – 30062, 2020.\\n[36] Jan-Willem van de Meent, Brooks Paige, H. Yang, and Frank Wood. An introduction to probabilistic programming. ArXiv ,\\nabs/1809.10756, 2018.\\n[37] Atilim Günes Baydin, Lei Shao, W. Bhimji, L. Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi,\\nBradley Gram-Hansen, Gilles Louppe, Mingfei Ma, X. Zhao, P. Torr, V . Lee, K. Cranmer, Prabhat, and F. Wood. Etalumis:\\nbringing probabilistic programming to scientiﬁc simulators at scale. Proceedings of the International Conference for High\\nPerformance Computing, Networking, Storage and Analysis , 2019.\\n[38] T. Gleisberg, S. Höche, F. Krauss, M. Schönherr, S. Schumann, F. Siegert, and J. Winter. Event generation with sherpa 1.1.\\nJournal of High Energy Physics , 2009:007–007, 2009.\\n[39] David M. Blei. Build, compute, critique, repeat: Data analysis with latent variable models. 2014.\\n[40] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald C. Gall, Ece Kamar, Nachiappan Nagappan, Besmira\\nNushi, and Thomas Zimmermann. Software engineering for machine learning: A case study. 2019 IEEE/ACM 41st International\\nConference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) , 2019.\\n[41] R. Ambrosino, B. Buchanan, G. Cooper, and Marvin J. Fine. The use of misclassiﬁcation costs to learn rule-based decision\\nsupport models for cost-effective hospital admission strategies. Proceedings. Symposium on Computer Applications in Medical\\nCare , pages 304–8, 1995.\\n[42] Gareth J Grifﬁth, Tim T Morris, Matthew J Tudball, Annie Herbert, Giulia Mancano, Lindsey Pike, Gemma C Sharp, Jonathan\\nSterne, Tom M Palmer, George Davey Smith, et al. Collider bias undermines our understanding of covid-19 disease risk and\\nseverity. Nature communications , 11(1):1–12, 2020.\\n[43] J. Pearl. Theoretical impediments to machine learning with seven sparks from the causal revolution. Proceedings of the\\nEleventh ACM International Conference on Web Search and Data Mining , 2018.\\n[44] T. Nguyen, G. Collins, J. Spence, J. Daurès, P. Devereaux, P. Landais, and Y . Le Manach. Double-adjustment in propensity\\nscore matching analysis: choosing a threshold for considering residual imbalance. BMC Medical Research Methodology , 17,\\n2017.\\n[45] D. Eckles and E. Bakshy. Bias and high-dimensional adjustment in observational studies of peer effects. ArXiv , abs/1706.04692,\\n2017.\\n[46] Yanbo Xu, Divyat Mahajan, Liz Manrao, A. Sharma, and E. Kiciman. Split-treatment analysis to rank heterogeneous causal\\neffects for prospective interventions. ArXiv , abs/2011.05877, 2020.\\n24'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='[47] Jonathan G Richens, C. M. Lee, and Saurabh Johri. Improving the accuracy of medical diagnosis with causal machine learning.\\nNature Communications , 11, 2020.\\n[48] Andrei Paleyes, Raoul-Gabriel Urma, and N. Lawrence. Challenges in deploying machine learning: a survey of case studies.\\nArXiv , abs/2011.09926, 2020.\\n[49] V . Chernozhukov, D. Chetverikov, M. Demirer, E. Duﬂo, Christian L. Hansen, Whitney K. Newey, and J. Robins. Dou-\\nble/debiased machine learning for treatment and structural parameters. Econometrics: Econometric & Statistical Methods -\\nSpecial Topics eJournal , 2018.\\n[50] Victor Veitch and Anisha Zaveri. Sense and sensitivity analysis: Simple post-hoc analysis of bias due to unobserved confounding.\\nNeurIPS 2020, arXiv preprint arXiv:2003.01747 , 2020.\\n[51] P. Jenniskens, P.S. Gural, L. Dynneson, B.J. Grigsby, K.E. Newman, M. Borden, M. Koop, and D. Holman. Cams: Cameras for\\nallsky meteor surveillance to establish minor meteor showers. Icarus , 216(1):40 – 61, 2011.\\n[52] Siddha Ganju, Anirudh Koul, Alexander Lavin, J. Veitch-Michaelis, Meher Kasam, and J. Parr. Learnings from frontier\\ndevelopment lab and spaceml - ai accelerators for nasa and esa. ArXiv , abs/2011.04776, 2020.\\n[53] S. Zoghbi, M. Cicco, A. P. Stapper, A. J. Ordonez, J. Collison, P. S. Gural, S. Ganju, J.-L. Galache, and P. Jenniskens. Searching\\nfor long-period comets with deep learning tools. In Deep Learning for Physical Science Workshop, NeurIPS , 2017.\\n[54] Peter Jenniskens, Jack Baggaley, Ian Crumpton, Peter Aldous, Petr Pokorny, Diego Janches, Peter S. Gural, Dave Samuels, Jim\\nAlbers, Andreas Howell, Carl Johannink, Martin Breukers, Mohammad Odeh, Nicholas Moskovitz, Jack Collison, and Siddha\\nGanju. A survey of southern hemisphere meteor showers. Planetary and Space Science , 154:21 – 29, 2018.\\n[55] D. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active learning with statistical models. In NIPS , 1994.\\n[56] Y . Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. ArXiv , abs/1703.02910, 2017.\\n[57] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young,\\nJean-François Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. In NIPS , 2015.\\n[58] P. Abrahamsson, Outi Salo, Jussi Ronkainen, and Juhani Warsta. Agile software development methods: Review and analysis.\\nArXiv , abs/1709.08439, 2017.\\n[59] Marco Kuhrmann, Philipp Diebold, Jürgen Münch, Paolo Tell, Vahid Garousi, Michael Felderer, Kitija Trektere, Fergal\\nMcCaffery, Oliver Linssen, Eckhart Hanser, and Christian R. Prause. Hybrid software and system development in practice:\\nwaterfall, scrum, and beyond. Proceedings of the 2017 International Conference on Software and System Process , 2017.\\n[60] Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry,\\nPaul-Christian Burkner, and Martin Modrak. Bayesian workﬂow. ArXiv , abs/2011.01808, 2020.\\n[61] P. Chapman, J. Clinton, R. Kerber, T. Khabaza, T. Reinartz, C. Shearer, and R. Wirth. Crisp-dm 1.0: Step-by-step data mining\\nguide. 2000.\\n[62] Fred Hohman, Kanit Wongsuphasawat, Mary Beth Kery, and Kayur Patel. Understanding and visualizing data iteration in\\nmachine learning. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems , 2020.\\n[63] Saleema Amershi, M. Cakmak, W. B. Knox, and T. Kulesza. Power to the people: The role of humans in interactive machine\\nlearning. AI Mag. , 35:105–120, 2014.\\n[64] Eric Breck, Marty Zinkevich, Neoklis Polyzotis, Steven Euijong Whang, and Sudip Roy. Data validation for machine learning.\\n2019.\\n[65] R. Kumar, David R. O’Brien, Kendra Albert, Salomé Viljöen, and Jeffrey Snover. Failure modes in machine learning systems.\\nArXiv , abs/1911.11034, 2019.\\n[66] Inioluwa Deborah Raji, Andrew Smart, Rebecca White, M. Mitchell, Timnit Gebru, B. Hutchinson, Jamila Smith-Loud, Daniel\\nTheron, and P. Barnes. Closing the ai accountability gap: deﬁning an end-to-end framework for internal algorithmic auditing.\\nProceedings of the 2020 Conference on Fairness, Accountability, and Transparency , 2020.\\n[67] R. Miksad and A. Abernethy. Harnessing the power of real-world evidence (rwe): A checklist to ensure regulatory-grade data\\nquality. Clinical Pharmacology and Therapeutics , 103:202 – 205, 2018.\\n[68] D. B. Larson, Hugh Harvey, D. Rubin, Neville Irani, J. R. Tse, and C. Langlotz. Regulatory frameworks for development and\\nevaluation of artiﬁcial intelligence–based diagnostic imaging algorithms: Summary and recommendations. Journal of the\\nAmerican College of Radiology , 2020.\\n25'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='[69] Ninareh Mehrabi, Fred Morstatter, N. Saxena, Kristina Lerman, and A. Galstyan. A survey on bias and fairness in machine\\nlearning. ACM Computing Surveys (CSUR) , 54:1 – 35, 2019.\\n[70] Eirini Ntoutsi, P. Fafalios, U. Gadiraju, Vasileios Iosiﬁdis, W. Nejdl, Maria-Esther Vidal, S. Ruggieri, F. Turini, S. Papadopoulos,\\nEmmanouil Krasanakis, I. Kompatsiaris, K. Kinder-Kurlanda, Claudia Wagner, F. Karimi, Miriam Fernández, Harith Alani,\\nB. Berendt, Tina Kruegel, C. Heinze, Klaus Broelemann, Gjergji Kasneci, T. Tiropanis, and Steffen Staab. Bias in data-driven\\nai systems - an introductory survey. ArXiv , abs/2001.09762, 2020.\\n[71] E. Jo and Timnit Gebru. Lessons from archives: strategies for collecting sociocultural data in machine learning. Proceedings of\\nthe 2020 Conference on Fairness, Accountability, and Transparency , 2020.\\n[72] J. Wiens, W. Price, and M. Sjoding. Diagnosing bias in data-driven algorithms for healthcare. Nature Medicine , 26:25–26,\\n2020.\\n[73] R. Challen, J. Denny, M. Pitt, L. Gompels, T. Edwards, and K. Tsaneva-Atanasova. Artiﬁcial intelligence, bias and clinical\\nsafety. BMJ Quality & Safety , 28:231 – 237, 2019.\\n[74] Z. Obermeyer, B. Powers, C. V ogeli, and S. Mullainathan. Dissecting racial bias in an algorithm used to manage the health of\\npopulations. Science , 366:447 – 453, 2019.\\n[75] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, In-\\nioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. Proceedings of the Conference on Fairness,\\nAccountability, and Transparency , 2019.\\n[76] Samantha Cruz Rivera, Xiaoxuan Liu, A. Chan, A. K. Denniston, and M. Calvert. Guidelines for clinical trial protocols for\\ninterventions involving artiﬁcial intelligence: the spirit-ai extension. Nature Medicine , 26:1351 – 1363, 2020.\\n[77] Z. Szajnfarber. Managing innovation in architecturally hierarchical systems: Three switchback mechanisms that impact practice.\\nIEEE Transactions on Engineering Management , 61:633–645, 2014.\\n[78] H. Zhou and Y . He. Comparative study of okr and kpi. DEStech Transactions on Economics, Business and Management , 2018.\\n[79] J. Neumann. Probabilistic logic and the synthesis of reliable organisms from unreliable components. 1956.\\nAcknowledgements\\nThe authors would like to thank Gur Kimchi, Carl Henrik Ek and Neil Lawrence for valuable discussions about this\\nproject.\\nAuthor contributions statement\\nA.L. conceived of the original ideas and framework, with signiﬁcant contributions towards improving the framework\\nfrom all co-authors. A.L. initiated the use of MLTRL in practice, including the neuropathology test case discussed here.\\nC.G-L. contributed insight regarding causal AI, including the section on counterfactual diagnosis. C.G-L. also made\\nsigniﬁcant contributions broadly in the paper, notably in the Methods descriptions and paper revisions. Si.G. contributed\\nthe spacecraft test case, along with early insights in the framework deﬁnitions. A.V . contributed to the deﬁnition of\\nlater stages involving deployment (as did A.G.), and comparison with traditional software workﬂows. Both E.X. and\\nY .G. provided insights regarding AI in academia, and Y .G. additionally contributed to the uncertainty quantiﬁcation\\nmethods. Su.G. and D.L. contributed the computer vision test case. A.G.B. contributed the particle physics test case,\\nand signiﬁcant reviews of the writeup. A.S. contributed insights related to causal ML and AI ethics. D.N. provided\\nvaluable feedback on the overall framework, and contributed signiﬁcantly with the details on “switchback mechanisms”.\\nS.Z. contributed to multiple paper revisions, with emphasis on clarity and applicability to broad ML users and teams.\\nJ.P. contributed to multiple paper revisions, and to deploying the systems ML methods broadly in practice for Earth and\\nspace sciences. –same goes for C.M., with additional feedback overall on the methods. All co-authors discussed the\\ncontent and contributed to editing the manuscript.\\nCompeting interests\\nThe authors declare no competing interests.\\nAdditional information\\nCorrespondence and requests for materials should be addressed to A.L.\\n26')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Text Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recursive Character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='TECHNOLOGY READINESS LEVELS\\nFOR MACHINE LEARNING SYSTEMS\\nAlexander Lavin∗\\nPasteur LabsCiarán M. Gilligan-Lee\\nSpotifyAlessya Visnjic\\nWhyLabsSiddha Ganju\\nNvidiaDava Newman\\nMIT\\nAtılım Güne¸ s Baydin\\nUniversity of OxfordSujoy Ganguly\\nUnity AIDanny Lange\\nUnity AIAmit Sharma\\nMicrosoft Research\\nStephan Zheng\\nSalesforce ResearchEric P. Xing\\nPetuumAdam Gibson\\nKonduitJames Parr\\nNASA Frontier Development Lab\\nChris Mattmann\\nNASA Jet Propulsion LabYarin Gal\\nAlan Turing Institute\\nABSTRACT'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='Alan Turing Institute\\nABSTRACT\\nThe development and deployment of machine learning (ML) systems can be executed easily with\\nmodern tools, but the process is typically rushed and means-to-an-end. The lack of diligence can\\nlead to technical debt, scope creep and misaligned objectives, model misuse and failures, and\\nexpensive consequences. Engineering systems, on the other hand, follow well-deﬁned processes'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='and testing standards to streamline development for high-quality, reliable results. The extreme is\\nspacecraft systems, where mission critical measures and robustness are ingrained in the development\\nprocess. Drawing on experience in both spacecraft engineering and ML (from research through\\nproduct across domain areas), we have developed a proven systems engineering approach for machine\\nlearning development and deployment. Our Machine Learning Technology Readiness Levels (MLTRL)'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='framework deﬁnes a principled process to ensure robust, reliable, and responsible systems while\\nbeing streamlined for ML workﬂows, including key distinctions from traditional software engineering.\\nEven more, MLTRL deﬁnes a lingua franca for people across teams and organizations to work\\ncollaboratively on artiﬁcial intelligence and machine learning technologies. Here we describe the\\nframework and elucidate it with several real world use-cases of developing ML methods from basic'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='research through productization and deployment, in areas such as medical diagnostics, consumer\\ncomputer vision, satellite imagery, and particle physics.\\nKeywords: Machine Learning; Systems Engineering; Data Management; Medical AI; Space Sciences\\nIntroduction\\nThe accelerating use of artiﬁcial intelligence (AI) and machine learning (ML) technologies in systems of software,\\nhardware, data, and people introduces vulnerabilities and risks due to dynamic and unreliable behaviors; fundamentally,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='ML systems learn from data, introducing known and unknown challenges in how these systems behave and interact with\\ntheir environment. Currently the approach to building AI technologies is siloed: models and algorithms are developed\\nin testbeds isolated from real-world environments, and without the context of larger systems or broader products they’ll\\nbe integrated within for deployment. A main concern is models are typically trained and tested on only a handful of'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='curated datasets, without measures and safeguards for future scenarios, and oblivious of the downstream tasks and\\nusers. Even more, models and algorithms are often integrated into a software stack without regard for the inherent\\nstochasticity –for instance, the massive effect random seeds have on deep reinforcement learning model performance\\n[1] – and failure modes of the ML components, which can be dangerously hidden in layers of software and abstraction.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='Other domains of engineering, such as civil and aerospace, follow well-deﬁned processes and testing standards to\\nstreamline development for high-quality, reliable results. Technology Readiness Level (TRL) is a systems engineering\\nprotocol for deep tech[ 2] and scientiﬁc endeavors at scale, ideal for integrating many interdependent components\\n∗lavin@simulation.science\\nPreprint. Under review.arXiv:2101.03989v2  [cs.LG]  29 Nov 2021'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='andcross-functional teams of people. It is no surprise that TRL is standard process and parlance in NASA[ 3] and\\nDARPA[4].\\nFor a spaceﬂight project there are several deﬁned phases, from pre-concept to prototyping to deployed operations to\\nend-of-life, each with a series of exacting development cycles and reviews. This is in stark contrast to common machine\\nlearning and software workﬂows, which promote quick iteration, rapid deployment, and simple linear progressions. Yet'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='the NASA technology readiness process for spacecraft systems is overkill; we need robust ML technologies integrated\\nwith larger systems of software, hardware, data, and humans, but not necessarily for missions to Mars. We aim to bring\\nsystems engineering to AI and ML by deﬁning and putting into action a lean Machine Learning Technology Readiness\\nLevels (MLTRL) framework. We draw on decades of AI and ML development, from research through production,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='across domains and diverse data scenarios: for example, computer vision in medical diagnostics and consumer apps,\\nautomation in self-driving vehicles and factory robotics, tools for scientiﬁc discovery and causal inference, streaming\\ntime-series in predictive maintenance and ﬁnance.\\nIn this paper we deﬁne our framework for developing and deploying robust, reliable, and responsible ML and data'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='systems, with several real test cases of advancing models and algorithms from R&D through productization and\\ndeployment, including essential data considerations. Additionally, MLTRL prioritizes the role of AI ethics and\\nfairness, and our systems AI approach can help curb the large societal issues that can result from poorly deployed and\\nmaintained AI and ML technologies, such as the automation of systemic human bias, denial of individual autonomy,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='and unjustiﬁable outcomes (see the Alan Turing Institute Report on Ethical AI [5]). The adoption and proliferation of\\nMLTRL provides a common nomenclature and metric across teams and industries. The standardization of MLTRL\\nacross the AI industry should help teams and organizations develop principled, safe, and trusted technologies.\\nFigure 1: MLTRL spans research through prototyping, productization, and deployment. Most ML workﬂows prescribe'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='an isolated, linear process of data processing, training, testing, and serving a model [ 6]. Those workﬂows fail to deﬁne\\nhow ML development must iterate over that basic process to become more mature and robust, and how to integrate with\\na much larger system of software, hardware, data, and people. Not to mention MLTRL continues beyond deployment:\\nmonitoring and feedback cycles are important for continuous reliability and improvement over the product lifetime.\\nResults'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='Results\\nMLTRL deﬁnes technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment.\\nA TRL represents the maturity of a model or algorithmii, data pipelines, software module, or composition thereof; a\\ntypical ML system consists of many interconnected subsystems and components, and the TRL of the system is the\\nlowest level of its constituent parts [ 7]. The anatomy of a level is marked by gated reviews, evolving working groups,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='requirements documentation with risk calculations, progressive code and testing standards, and deliverables such as\\nTRL Cards (Figure 3) and ethics checklists.iiiThese components—which are crucial for implementing the levels in a\\niiNote we use “model” and “algorithm” somewhat interchangeably when referring to the technology under development. The\\nsame MLTRL process and methods apply for a machine translation model and for an A/B testing algorithm, for example.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='iiiTemplates and examples for MLTRL deliverables will be open-sourced upon publication at github.com/alan-turing-institute.\\n2'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='systematic fashion—as well as MLTRL metrics and methods are concretely described in examples and in the Methods\\nsection. Lastly, to emphasize the importance of data tasks in ML, from data curation [ 8] to data governance [ 9], we\\nstate several important data considerations at each MLTRL level.\\nMACHINE LEARNING TECHNOLOGY READINESS LEVELS\\nThe levels are brieﬂy deﬁned as follows and in Figure 1, and elucidated with real-world examples later.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='Level 0 - First Principles This is a stage for greenﬁeld AI research, initiated with a novel idea, guiding question, or\\npoking at a problem from new angles. The work mainly consists of literature review, building mathematical foundations,\\nwhite-boarding concepts and algorithms, and building an understanding of the data – for work in theoretical AI and ML,\\nhowever, there will not yet be data to work with (for example, a novel algorithm for Bayesian optimization[ 10], which'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='could eventually be used for many domains and datasets). The outcome of Level 0 is a set of concrete ideas with sound\\nmathematical formulation, to pursue through low-level experimentation in the next stage. When relevant, this level\\nexpects conclusions about data readiness, including strategies for getting the data to be suitable for the speciﬁc ML task.\\nTo graduate, the basic principles, hypotheses, data readiness, and research plans need to be stated, referencing relevant'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='literature. With graduation, a TRL Card should be started to succinctly document the methods and insights thus far –\\nthis key MLTRL deliverable is detailed in the Methods section and Figure 3.\\nLevel 0 data – Not a hard requirement at this stage because this is largely theoretical machine learning. That being said,\\ndata availability needs to be considered for deﬁning any research project to move past theory.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='Level 0 review – The reviewer here is solely the lead of the research lab or team, for instance a PhD supervisor. We\\nassess hypotheses and explorations for mathematical validity and potential novelty or utility, not necessarily code nor\\nend-to-end experiment results.\\nLevel 1 - Goal-Oriented Research To progress from basic principles to practical use, we design and run low-level\\nexperiments to analyze speciﬁc model or algorithm properties (rather than end-to-end runs for a performance benchmark'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='score). This involves collection and processing of sample data to train and evaluate the model. This sample data need\\nnot be the full data; it may be a smaller sample that is currently available or more convenient to collect. In some\\ncases it may sufﬁce to use synthetic data as the representative sample – in the medical domain, for example, acquiring\\ndatasets can take many months due to security and privacy constraints, so generating sample data can mitigate this'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='blocker from early ML development. Further, working with the sample data provides a blueprint for the data collection\\nand processing pipeline (including answering whether it is even possible to collect all necessary data), that can be\\nscaled up for the for the next steps. The experiments, good results or not, and mathematical foundations need to pass a\\nreview process with fellow researchers before graduating to Level 2. The application is still speculative, but through'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='comparison studies and analyses we start to understand if/how/where the technology offers potential improvements and\\nutility. Code is research-caliber : The aim here is to be quick and dirty, moving fast through iterations of experiments.\\nHacky code is okay, and full test coverage is actually discouraged, as long as the overall codebase is organized and\\nmaintainable. It is important to start semantic versioning practices early in the project lifecycle, which should cover'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='code, models, anddatasets. This is crucial for retrospectives and reproducibility, issues with which can be costly and\\nsevere at later stages. This versioning information and additional progress should be reported on the TRL Card (see for\\nexample Figure 3).\\nLevel 1 data – At minimum we work with sample data that is representative of downstream real datasets, which can be\\na subset of real data, synthetic data, or both. Beyond driving low-level ML experiments, the sample data forces us to'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='consider data acquisition and processing strategies at an early stage before it becomes a blocker later.\\nLevel 1 review – The panel for this gated review is entirely members of the research team, reviewing for scientiﬁc rigor\\nin early experimentation, and pointing to important concepts and prior work from their respective areas of expertise.\\nThere may be several iterations of feedback and additional experiments.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='Level 2 - Proof of Principle (PoP) Development Active R&D is initiated, mainly by developing and running in\\ntestbeds : simulated environments and/or simulated data that closely matches the conditions and data of real scenarios –\\nnote these are driven by model-speciﬁc technical goals, not necessarily application or product goals (yet). An important\\ndeliverable at this stage is the formal research requirements document (with well-speciﬁed veriﬁcation and validation'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='(V&V) steps)iv. Here is one of several key decision points in the broader process: The R&D team considers several\\npaths forward and sets the course: (A) prototype development towards Level 3, (B) continued R&D for longer-term\\nivArequirement is a singular documented physical or functional need that a particular design, product, or process aims to satisfy.\\nRequirements aim to specify all stakeholders’ needs while not specifying a speciﬁc solution. Deﬁnitions are incomplete without\\n3'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='research initiatives and/or publications, or some combination of A and B. We ﬁnd the culmination of this stage is often\\na bifurcation: some work moves to applied AI, while some circles back for more research. This common MLTRL cycle\\nis an instance of the non-monotonic discovery switchback mechanism (detailed in the Methods section).\\nLevel 2 data – Datasets at this stage may include publicly available benchmark datasets, semi-simulated data based'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='on the data sample in Level 1, or fully simulated data based on certain assumptions about the potential deployment\\nenvironments. The data should allow researchers to characterize model properties, and highlight corner cases or\\nboundary conditions, in order to justify the utility of continuing R&D on the model.\\nLevel 2 review – To graduate from the PoP stage, the technology needs to satisfy research claims made in previous'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='stages (brought to be bare by the aforementioned PoP data in both quantitative and qualitative ways) with the analyses\\nwell-documented and reproducible.\\nLevel 3 - System Development Here we have checkpoints that push code development towards interoperability,\\nreliability, maintainability, extensibility, and scalability. Code becomes prototype-caliber : A signiﬁcant step up from'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='research code in robustness and cleanliness. This needs to be well-designed, well-architected for dataﬂow and interfaces,\\ngenerally covered by unit and integration tests, meet team style standards, and sufﬁciently-documented. Note the\\nprogrammers’ mentality remains that this code will someday be refactored/scrapped for productization; prototype code\\nis relatively primitive with regard to efﬁciency and reliability of the eventual system. With the transition to Level 4 and'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='proof-of-concept mode, the working group should evolve to include product engineering to help deﬁne service-level\\nagreements and objectives (SLAs and SLOs) of the eventual production system.\\nLevel 3 data – For the most part consistent with Level 2; in general, the previous level review can elucidate potential\\ngaps in data coverage and robustness to be addressed in the subsequent level. However, for test suites developed at this'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='stage, it is useful to deﬁne dedicated subsets of the experiment data as default testing sources, as well as setup mock\\ndata for speciﬁc functionalities and scenarios to be tested.\\nLevel 3 review – Teammates from applied AI and engineering are brought into the review to focus on sound software\\npractices, interfaces and documentation for future development, and version control for models and datasets. There are'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='likely domain- or organization-speciﬁc data management considerations going forward that this review should point out\\n– e.g. standards for data tracking and compliance in healthcare [11].\\nLevel 4 - Proof of Concept (PoC) Development This stage is the seed of application-driven development; for many\\norganizations this is the ﬁrst touch-point with product managers and stakeholders beyond the R&D group. Thus TRL'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='Cards and requirements documentation are instrumental in communicating the project status and onboarding new\\npeople. The aim is to demonstrate the technology in a real scenario: quick proof-of-concept examples are developed to\\nexplore candidate application areas and communicate the quantitative and qualitative results. It is essential to use real\\nand representative data for these potential applications. Thus data engineering for the PoC largely involves scaling up'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='the data collection and processing from Level 1, which may include collecting new data or processing all available data\\nusing scaled experiment pipelines from Level 3. In some scenarios there will new datasets brought in for the PoC, for\\nexample, from an external research partner as a means of validation. Hand-in-hand with the evolution from sample to\\nreal data, the experiment metrics should evolve from ML research to the applied setting: proof-of-concept evaluations'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='should quantify model and algorithm performance (e.g., precision and recall and various data splits), computational\\ncosts (e.g., CPU vs GPU runtimes), and also metrics that are more relevant to the eventual end-user (e.g., number\\nof false positives in the top-N predictions of a recommender system). We ﬁnd this PoC exploration reveals speciﬁc\\ndifferences between clean and controlled research data versus noisy and stochastic real-world data. The issues can'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='be readily identiﬁed because of the well-deﬁned distinctions between those development stages in MLTRL, and then\\ntargeted for further development.\\nAI ethics processes vary across organizations, but all should engage in ethics conversations at this stage, including ethics\\nof data collection, and potential of any harm or discriminatory impacts due to the model (as the AI capabilities and'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='datasets are known). MLTRL requires ethics considerations to be reported on TRL Cards at all stages, which generally\\nlink to an extended ethics checklist. The key decision point here is to push onward with application development or not.\\nIt is common to pause projects that pass Level 4 review, waiting for a better time to dedicate resources, and/or pull the\\ntechnology into a different project.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='technology into a different project.\\nLevel 4 data – Unlike the previous stages, having real-world and representative data is critical for the PoC; even with\\nmethods for verifying that data distributions in synthetic data reliably mirror those of real data [], sufﬁcient conﬁdence\\nin the technology must be achieved with real-world data of the use-case. Further, one must consider how to obtain'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='corresponding measures for veriﬁcation and validation (V&V). Veriﬁcation: Are we building the product right? Validation: Are we\\nbuilding the right product?\\n4'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='high-quality and consistent data required for the future model inference: generation of the data pipeline PoC that will\\nresemble the future inference pipeline that will take data from intended sources, transform it into features, and send it to\\nthe model for inference.\\nLevel 4 review – Demonstrate the utility towards one or more practical applications (each with multiple datasets), taking'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='care to communicate assumptions and limitations, and again reviewing data-readiness: evaluating the real-world data\\nfor quality, validity, and availability. The review also evaluates security and privacy considerations – deﬁning these in\\nthe requirements document with risk quantiﬁcation is a useful mechanism for mitigating potential issues (discussed\\nfurther in the Methods section).\\nLevel 5 - Machine Learning “Capability” At this stage the technology is more than an isolated model or algorithm,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='it is a speciﬁc capability . For instance, producing depth images from stereo vision sensors on a mobile robot is a\\nreal-world capability beyond the isolated ML technique of self-supervised learning for RGB stereo disparity estimation.\\nIn many organizations this represents a technology transition or handoff from R&D to productization. MLTRL\\nmakes this transition explicit, evolving the requisite work, guiding documentation, objectives and metrics, and team;'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='indeed, without MLTRL it is common for this stage to be erroneously leaped completely, as shown in Figure 2. An\\ninterdisciplinary working group is deﬁned, as we start developing the technology in the context of a larger real-world\\nprocess – i.e., transitioning the model or algorithm from an isolated solution to a module of a larger application. Just as\\nthe ML technology should no longer be owned entirely by ML experts, steps have been taken to share the technology'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='with others in the organization via demos, example scripts, and/or an API; the knowledge and expertise cannot remain\\nwithin the R&D team, let alone an individual ML developer. Graduation from Level 5 should be difﬁcult, as it signiﬁes\\nthe dedication of resources to push this ML technology through productization. This transition is a common challenge\\nin deep-tech, sometimes referred to as “the valley of death” because project managers and decision-makers struggle'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='to allocate resources and align technology roadmaps to effectively move to Level 6, 7 and onward. MLTRL directly\\naddresses this challenge by stepping through the technology transition or handoff explicitly.\\nLevel 5 data – For the most part consistent with Level 4. However, considerations need to be taken for scaling of data\\npipelines: there will soon be more engineers accessing the existing data and adding more, and the data will be getting'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='much more use, including automated testing in later levels. With this scaling can come challenges with data governance.\\nThe data pipelines likely do not mirror the structure of the teams or broader organization. This can result in data silos,\\nduplications, unclear responsibilities, and missing control of data over its entire lifecycle. These challenges and several\\napproaches to data governance (planning and control, organizational, and risk-based) are detailed in Janssen et al. [9].'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='Level 5 review – The veriﬁcation and validation (V&V) measures and steps deﬁned in earlier R&D stages (namely\\nLevel 2) must all be completed by now, and the product-driven requirements (and corresponding V&V) are drafted at\\nthis stage. We thoroughly review them here, and make sure there is stakeholder alignment (at the ﬁrst possible step of\\nproductization, well ahead of deployment).\\nLevel 6 - Application Development The main work here is signiﬁcant software engineering to bring the code up to'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='product-caliber : This code will be deployed to users and thus needs to follow precise speciﬁcations, have comprehensive\\ntest coverage, well-deﬁned APIs, etc. The resulting ML modules should be robustiﬁed towards one or more target\\nuse-cases. If those target use-cases call for model explanations, the methods need to be built and validated alongside\\nthe ML model, and tested for their efﬁcacy in faithfully interpreting the model’s decisions – crucially, this needs to be'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='in the context of downstream tasks and the end-users, as there is often a gap between ML explainability that serves\\nML engineers rather than external stakeholders[ 12]. Similarly, we need to develop the ML modules with known data\\nchallenges in mind, speciﬁcally to check the robustness of the model (and broader pipeline) to changes in the data\\ndistribution between development and deployment.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='distribution between development and deployment.\\nThe deployment setting(s) should be addressed thoroughly in the product requirements document, as ML serving (or\\ndeploying) is an overloaded term that needs careful consideration. First, there are two main types: internal, as APIs\\nfor experiments and other usage mainly by data science and ML teams, and external, meaning an ML model that'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='is embedded or consumed within a real application with real users. The serving constraints vary signiﬁcantly when\\nconsidering cloud deployment vs on-premise or hybrid, batch or streaming, open-source solution or containerized\\nexecutable, etc. Even more, the data at deployment may be limited due to compliance, or we may only have access to\\nencrypted data sources, some of which may only be accessible locally – these scenarios may call for advanced ML'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='approaches such as federated learning[ 13] and other privacy-oriented ML[ 14]. And depending on the application, an\\nML model may not be deployable without restrictions; this typically means being embedded in a rules engine workﬂow\\nwhere the ML model acts like an advisor that discovers edge cases in rules. These deployment factors are hardly\\nconsidered in model and algorithm development despite signiﬁcant inﬂuence on modeling and algorithmic choices;'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='that said, hardware choices typically are considered early on, such as GPU versus edge devices. It is crucial to make\\nthese systems decisions at Level 6–not too early that serving scenarios and requirements are uncertain, and not too late\\n5'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='that corresponding changes to model or application development risk deployment delays or failures. This marks a key\\ndecision for the project lifecycle, as this expensive ML deployment risk is common without MLTRL (see Figure 2).\\nLevel 6 data – Additional data should be collected and operationalized at this stage towards robustifying the ML\\nmodels, algorithms, and surrounding components. These include adversarial examples to check local robustness [ 15],'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='semantically-equivalent perturbations to check consistency of the model with respect to domain assumptions [16, 17],\\nand collecting data from different sources and checking how well the trained model generalizes to them. These\\nconsiderations are even more vital in the challenging deployment domains mentioned above with limited data access.\\nLevel 6 review – Focus is on the code quality, the set of newly deﬁned product requirements, system SLA and SLO'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='requirements, data pipelines spec, and an AI ethics revisit now that we are closer to a real-world use-case. In particular,\\nregulatory compliance is mandated for this gated review; the data privacy and security laws are changing rapidly, and\\nmissteps with compliance can make or break the project.\\nLevel 7 - Integrations For integrating the technology into existing production systems, we recommend the working'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='group has a balance of infrastructure engineers andapplied AI engineers – this stage of development is vulnerable\\nto latent model assumptions and failure modes, and as such cannot be safely developed solely by software engineers.\\nImportant tools for them to build together include:\\n•Tests that run use-case speciﬁc critical scenarios and data-slices – a proper risk-quantiﬁcation table will\\nhighlight these.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='highlight these.\\n•A “golden dataset” should be deﬁned to baseline the performance of each model and succession of models –see\\nthe computer vision app example in Figure 4–for use in the continuous integration and deployment (CI/CD)\\ntests.\\n•Metamorphic testing : a software engineering methodology for testing a speciﬁc set of relations between the\\noutputs of multiple inputs. When integrating ML modules into larger systems, a codiﬁed list of metamorphic'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='relations[18] can provide valuable veriﬁcation and validation measures and steps.\\n•Data intervention tests that seek data bugs at various points in the pipelines, downstream to measure the\\npotential effects of data processing and ML on consumers or users of that data, as well as upstream at data\\ningestion or creation. Rather than using model performance as a proxy for data quality, it is crucial to use\\nintervention tests that instead catch data errors with mechanisms speciﬁc to data validation.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='These tests in particular help mitigate underspeciﬁcation in ML pipelines, a key obstacle to reliably training models that\\nbehave as expected in deployment[ 19]. On the note of reliability, it is important that quality assurance engineers (QA)\\nplay a key role here and through Level 9, overseeing data processes to ensure privacy and security, and covering audits\\nfor downstream accountability of AI methods.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='for downstream accountability of AI methods.\\nLevel 7 data – In addition to the data for test suites discussed above, this level calls for QA to prioritize data governance :\\nhow data is obtained, managed, used, and secured by the organization. This was earlier suggested in level 5 (in order to\\npreempt related technical debt), and essential here at the main junction for integration, which may create additional\\ngovernance challenges in light of downstream effects and consumers.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='Level 7 review – The review should focus on the data pipelines and test suites; a scorecard like the ML Testing\\nRubric[ 20] is useful. The group should also emphasize ethical considerations at this stage, as they may be more\\nadequately addressed now (where there are many test suites put into place) rather than close to shipping later.\\nLevel 8 - Flight-ready The technology is demonstrated to work in its ﬁnal form and under expected conditions.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='There should be additional tests implemented at this stage covering deployment aspects, notably A/B tests, blue/green\\ndeployment tests, shadow testing, and canary testing, which enable proactive and gradual testing for changing ML\\nmethods and data. Ahead of deployment, the CI/CD system should be ready to regularly stress test the overall system\\nand ML components. In practice, problems stemming from real-world data are impossible to anticipate and design for –'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='an upstream data provider could change formats unexpectedly or a physical event could cause the customer behavior to\\nchange. Running models in shadow mode for a period of time would help stress test the infrastructure and evaluate how\\nsusceptible the ML model(s) will be to performance regressions caused by data. We observe that ML systems with\\ndata-oriented architectures are more readily tested in this manner, and better surface data quality issues, data drifts, and'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='concept drifts – this is discussed later in the Beyond Software Engineering section. To close this stage, the key decision\\nis go or no-go for deployment, and when.\\nLevel 8 data – If not already in place, there absolutely needs to be mechanisms for automatically logging data\\ndistributions alongside model performance once deployed.\\n6'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='Level 8 review – A diligent walkthrough of every technical and product requirement, showing the corresponding\\nvalidations, and the review panel is representative of the full slate of stakeholders.\\nLevel 9 - Deployment In deploying AI and ML technologies, there is signiﬁcant need to monitor the current version,\\nand explicit considerations towards improving the next version. For instance, performance degradation can be hidden'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='and critical, and feature improvements often bring unintended consequences and constraints. Thus at this level, the\\nfocus is on maintenance engineering–i.e., methods and pipelines for ML monitoring and updating. Monitoring for data\\nquality, concept drift, and data drift is crucial; no AI system without thorough tests for these can reliably be deployed.\\nBy the same token there must be automated evaluation and reporting – if actuals[ 21] are available, continuous evaluation'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='should be enabled, but in many cases actuals come with a delay, so it is essential to record model outputs to allow for\\nefﬁcient evaluation after the fact. To these ends, the ML pipeline should be instrumented to log system metadata, model\\nmetadata, and data itself.\\nMonitoring for data quality issues and data drifts is crucial to catch deviations in model behavior, particularly those that'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='are non-obvious in the model or product end-performance. Data logging is unique in the context of ML systems: data\\nlogs should capture statistical properties of input features and model predictions, and capture their anomalies. With\\nmonitoring for data, concept, and model drifts, the logs are to be sent to the relevant systems, applied, and research\\nengineers. The latter is often non-trivial, as the model server is not ideal for model “observability” because it does not'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='necessarily have the right data points to link the complex layers needed to analyze and debug models. To this end,\\nMLTRL requires the drift tests to be implemented at stages well ahead of deployment, earlier than is standard practice.\\nAgain we advocate for data-ﬁrst architectures rather than the software industry-standard design by services (discussed\\nlater), which aids in surfacing and logging the relevant data types and slices when monitoring AI systems. For retraining'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='and improving models, monitoring must be enabled to catch training-serving skew and let the team know when to\\nretrain. Towards model improvements, adding or modifying features can often have unintended consequences, such as\\nintroducing latencies or even bias. To mitigate these risks, MLTRL has an embedded switchback here: any component\\nor module changes to the deployed version must cycle back to Level 7 (integrations stage) or earlier. Additionally,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='for quality ML products, we stress a deﬁned communication path for user feedback without roadblocks to R&D; we\\nencourage real-world feedback all the way to research, providing valuable problem constraints and perspectives.\\nLevel 9 data – Proper mechanisms for logging and inspecting data (alongside models) is critical for deploying reliable\\nAI and ML – systems that learn on data have unique monitoring requirements (detailed above). In addition to the'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='infrastructure and test suites covering data and environment shifts, it’s important for product managers and other owners\\nto be on top of data policy shifts in domains such as ﬁnance and healthcare.\\nLevel 9 review – The review at this stage is unique, as it also helps in lifecycle management: at a regular cadence\\nthat depends on the deployed system and domain of use, owners and other stakeholders are to revisit this review and'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='recommend switchbacks if needed (discussed in the Methods section). This additional oversight at deployment is\\nshown to help deﬁne regimented release cycles of updated versions, and provide another “eye” check for stale model\\nperformance or other system abnormalities.\\nNotice MLTRL is deﬁned as stages or levels, yet much of the value in practice is realized in the transitions: MLTRL\\nenables teams to move from one level to the next reliably and efﬁciently, and provides a guide for how teams and'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='objectives evolve with the progressing technology.\\nDiscussion\\nMLTRL is designed to apply to many real-world use-cases involving data and ML, from simple regression models\\nused for predictive modeling energy demand or anomaly detection in datacenters, to real-time modeling in rideshare\\napplications and motion planning in warehouse robotics. For simple use-cases MLTRL may be overkill, and a subset'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='may sufﬁce – for instance, model cards as demonstrated by Google for basic image classiﬁcation. Yet this is a ﬁne line,\\nas the same cards-only approach in the popular “Huggingface” codebases are too simplistic for the language models\\nthey represent, deployed in domains that carry signiﬁcant consequences. MLTRL becomes more valuable with more\\ncomplex, larger systems and environments, especially in risk averse domains. We thoroughly discuss this through\\nseveral real uses of MLTRL below.\\n7'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='Figure 2: Most ML and AI projects live in these sections of MLTRL, not concerned with fundamental R&D – that is,\\ncompletely using existing methods and implementations, and even pretrained models. In the left diagram, the arrows\\nshow a common development pattern with MLTRL in industry: projects go back to the ML toolbox to develop new\\nfeatures (dashed line), and frequent, incremental improvements are often a practice of jumping back a couple levels to'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='Level 7 (which is the main systems integrations stage). At Levels 7 and 8 we stress the need for tests that run use-case\\nspeciﬁc critical scenarios and data-slices, which are highlighted by a proper risk-quantiﬁcation matrix [ 22]. Cycling\\nback to previous lower levels is not just a late-stage mechanism in MLTRL, but rather “switchbacks” occur throughout\\nthe process (as discussed in the Methods section and throughout the text). In the right diagram we show the more'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='common approach in industry ( without using our framework), which skips essential technology transition stages – ML\\nEngineers push straight through to deployment, ignoring important productization and systems integration factors. This\\nwill be discussed in more detail in the Methods section.\\nEXAMPLES\\nHuman-machine visual inspection\\nWhile most ML projects begin with a speciﬁc task and/or dataset, there are many that originate in ML theory without'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='any target application – i.e., projects starting MLTRL at level 0 or 1. These projects nicely demonstrate the utility of\\nMLTRL built-in switchbacks, bifurcating paths, and iteration with domain experts. An example we discuss here is a\\nnovel approach to representing data in generative vision models from Naud & Lavin[ 23], which was then developed into\\nstate-of-the-art unsupervised anomaly detection, and targeted for two human-machine visual inspection applications:'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='First, industrial anomaly detection, notably in precision manufacturing, to identify potential errors for human-expert\\nmanual inspection. Second, using the model to improve the accuracy and efﬁciency of neuropathology, the microscopic\\nexamination of neurosurgical specimens for cancerous tissue. In these human-machine teaming use-cases there are\\nspeciﬁc challenges impeding practical, reliable use:'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='•Hidden feedback loops can be common and problematic in real-world systems inﬂuencing their own training\\ndata: over time the behavior of users may evolve to select data inputs they prefer for the speciﬁc AI system,\\nrepresenting some skew from the training data. In this neuropathology case, selecting whole-slide images that\\nare uniquely difﬁcult for manual inspection, or even biased by that individual user. Similarly we see underlying'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='healthcare processes can act as hidden confounders, resulting in unreliable decision support tools[26].\\n•Model availability can be limited in many deployment settings: for example, on-premises deployments\\n(common in privacy preserving domains like healthcare and banking), edge deployments (common in industrial\\nuse-cases such as manufacturing and agriculture), or from the infrastructure’s inability to scale to the volume'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='of requests. This can severely limit the team’s ability to monitor, debug, and improve deployed models.\\n•Uncertainty estimation is valuable in many AI scenarios, yet not straightforward to implement in practice.\\nThis is further complicated with multiple data sources and users, each injecting generally unknown amounts of\\nnoise and uncertainties. In medical applications it is of critical importance, to provide measures of conﬁdence'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='and sensitivity, and for AI researchers through end-users. In anomaly detection, various uncertainty measures\\ncan help calibrate the false-positive versus false-negative rates, which can be very domain speciﬁc.\\n8'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='Figure 3: The maturity of each ML technology is tracked via TRL Cards , which we describe in the Methods section.\\nHere is an example reﬂecting a neuropathology machine vision use-case[ 23], detailed in the Discussion Section. Note\\nthis is a subset of a full TRL Card, which in reality lives as a full document in an internal wiki. Notice the card\\nclearly communicates the data sources, versions, and assumptions. This helps mitigate invalid assumptions about'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='performance and generalizability when moving from R&D to production, and promotes the use of real-world data\\nearlier in the project lifecycle. We recommend documenting datasets thoroughly with semantic versioning and tools\\nsuch as datasheets for datasets [24], and following data accountability best-practices as they evolve (see [25]).\\n•Costs of edge cases can be signiﬁcant, sometimes risking expensive machine downtime or medical failures.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='This is exacerbated in anomaly detection anomalies are by deﬁnition rare so they can be difﬁcult to train for,\\nespecially for the anomalies that are completely unseen until they arise in the wild.\\n•End-user trust can be difﬁcult to achieve, often preventing the adoption of ML applications, particularly in\\nthe healthcare domain and other highly regulated industries.\\nThese and additional ML challenges such as data privacy and interpretability can inhibit ML adoption in clinical practice'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='and industrial settings, but can be mitigated with MLTRL processes. We’ll describe how in the context of the Naud\\n& Lavin[ 23] example, which began at level 0 with theoretical ML work on manifold geometries, and at level 5 was\\ndirected towards specialized human-machine teaming applications utilizing the same ML method under-the-hood.\\n•Levels 0-1 – From open-ended exploration of data-representation properties in various Riemmanian manifold'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='curvatures, we derived from ﬁrst principles and empirically identiﬁed a property with hyperbolic manifolds:\\nwhen used as a latent space for embedding data without labels, the geometry organizes the data by it’s implicit\\nhierarchical structure. Unsupervised computer vision was identiﬁed in reviews as a promising direction for\\nproof-of-principle work.\\n•Level 2 – One approach for validating the earlier theoretical developments was to generate synthetic data to'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='isolate very speciﬁc features in data we would expect represented in the latent manifold. The results showed\\npromise for anomaly detection – using the latent representation of data to automatically identify images that\\nare out-of-the-ordinary (anomalous), and also using the manifold to inspect how they are semantically different.\\nFurther, starting with an implicitly probabilistic modeling approach implied uncertainty estimation could be'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='a valuable feature downstream. This made the level 2 key decision point clear: proceed with applied ML\\ndevelopment.\\n•Levels 3-5 – Proof-of-concept development and reviews demonstrated promise for several commercial appli-\\ncations relevant to the business, and also highlighted the need for several key features (deﬁned as R&D and\\nproduct requirements): interpretability (towards end-user trust), uncertainty quantiﬁcation (to show conﬁdence'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='scores), and human-in-the-loop (for domain expertise). Without the MLTRL PoC steps and review processes,\\nthese features can often be delayed until beta testing or overlooked completely – for example, the failures of\\napplying IBM Watson in medical applications [ 27]. For this technology, the applications to develop towards\\nare anomaly detection in histopathology and manufacturing, speciﬁcally inspecting whole-slide images of'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='neural tissue, and detecting defects in metallic surfaces, respectively.\\n9'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='From the systems perspective, we suggest quantifying the uncertainties of components and propagating them\\nthrough the system, which can improve safety and trust. Probabilistic ML methods, rooted in Bayesian\\nprobability theory, provide a principled approach to representing and manipulating uncertainty about models\\nand predictions[ 28]. For this reason we advocate strongly for probabilistic models and algorithms in AI'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='systems. In this machine vision example, the MLTRL technical requirements speciﬁcally called for a\\nprobabilistic generative model to readily quantify various types of uncertainties and propagate them forward to\\nthe visualization component of the pipeline, and the product requirements called for the downstream conﬁdence\\nand sensitivity measures to be exposed to the end-user. Component uncertainties must be assembled in a'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='principled way to yield a meaningful measure of overall system uncertainty, based on which safe decisions can\\nbe made[29]. See the Methods section for more on uncertainty in AI systems.\\nThe early checks for data management and governance proved valuable here, as the application areas dealt\\nwith highly sensitive data that would signiﬁcantly inﬂuence the design of data pipelines and test suites. In'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='both the neuropathology and manufacturing applications, the data management checks also raised concerns\\nabout hidden feedback loops, where users may unintentionally skew the data inputs when using the anomaly\\ndetection models in practice, for instance biasing the data towards speciﬁc subsets they subjectively need help\\nwith. Incorporating domain experts this early in the project lifecycle helped inform veriﬁcation and validation'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='steps to help be robust to the hidden feedback loops. Not to mention their input guided us towards user-centric\\nmetrics for performance, which can often skew from ML metrics in important ways – for instance, the typical\\nacceptance ratio for false positives versus false negatives doesn’t apply to select edge cases, for which our\\nhierarchical anomaly classiﬁcation scheme was useful [23].\\nFrom prior reviews and TRL card documentation, we also identiﬁed the value of synthetic data generation'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='into application development: anomalies are by deﬁnition rare so they are hard to come by in real datasets,\\nespecially with evolving environments in deployment settings, so the ability to generate synthetic datasets for\\nanomaly detection can accelerate the level 6-9 pipeline, and help ensure more reliable models in the wild.\\n•Level 6 (medical) – The medical inspection application experienced a bifurcation with product work proceed-'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='ing while additional R&D was desired to explore improved data processing methods, while engaging with\\nclinicians and medical researchers for feedback. Proceeding through the levels in a non-linear, non-monotonic\\nway is common in MLTRL and encouraged by various switchback mechanisms (detailed in the Methods\\nsection). These practices – intentional switchbacks, frequent engagement with domain experts and users – can'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='help mitigate methodological ﬂaws and underlying biases that are common when applying ML to clinical\\napplications. For instance, recent work by Roberts et al. [ 30] investigated 2,122 studies applying ML to\\nCOVID-19 use-cases, ﬁnding that none of the models are sufﬁcient for clinical use due to methodological ﬂaws\\nand/or underlying biases. They go on to give many recommendations – some we’ve discussed in the context of'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='MLTRL, and more – which should be reviewed for higher quality medical-ML models and documentation.\\n•Level 6-9 (manufacturing) – Overall these stages proceeded regularly and efﬁciently for the defect detection\\nproduct. MLTRL’s embedded switchback from level 9 to 4 proved particularly useful in this lifecycle, both\\nfor incorporating feedback from the ﬁeld and for updating with research progress. On the former, the data'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='distribution shifts from one deployment setting to another signiﬁcantly affected false-positive versus false-\\nnegative calibrations, so this was added as a feature to the CI/CD pipelines. On the latter, the built-in touch\\npoints for real-world feedback and data into the continued ML research provided valuable constraints to\\nhelp guide research, and product managers could readily understand what capabilities could be available for'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='product integration and when (readily communicated with TRL Cards) – for instance, later adding support for\\nvideo-based inspection for defects, and tooling for end-users to reason about uncertainty estimates (which\\nhelps establish trust).\\n•Level 7-9 (medical) – For productization the “neuropathology copilot” was handed off to a partner pharmaceu-\\ntical company to integrate into their existing software systems. The MLTRL documentation and communication'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='streamlined the technology transfer, which can often by a time-consuming manual process. If not pursuing\\nthis path, the product would’ve likely faced many of the medical-ML deployment challenges with model\\navailability and data access; MLTRL cannot overcome the technical challenges of deploying on-premises, but\\nthe manifestation of those challenges as performance regressions, data shifts, privacy and ethics concerns, etc.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='can be mitigated by the system-level checks and strategies MLTRL puts forth.\\nComputer vision with real and synthetic data\\nAdvancements in physics engines and graphics processing have advanced AI environment and data-generation capabili-\\nties, putting increased emphasis on transitioning models across the simulation-to-reality gap [ 31,32,33]. To develop a\\ncomputer vision application for automated recycling, we leveraged the Unity Perception [ 34] package, a toolkit for'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='generating large-scale datasets for perception-based ML training and validation. We produced synthetic images to\\n10'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='\\x15A?U?HEJC\\x03?H=OOEBE?=PEKJ\\x03LELAHEJA\\x03\\n\\xa0=¡\\xa0>¡\\n,V\\x03PRGHO\\x03FRQğGHQFH\\x03\\x1f\\x03WKUHVKROG\",V\\x03GHWHFWHG\\x03REMHFW\\x03LQ\\x03WDUJHW\\x03VHW\"3URYLGH\\x03FRUUHVSRQGLQJ\\x03UHF\\\\FOLQJ\\x03LQVWUXFWLRQV,QLWLDWH\\x03KXPDQ\\x10\\x03LQ\\x10WKH\\x10ORRS\\x03SURWRFRO12<(6<(6Figure 4: Computer vision pipeline for an automated recycling application (a), which contains multiple ML models,\\nuser input, and image data from various sources. Complicated logic such as this can mask ML model performance lags'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='and failures, and also emphasized the need for R&D-to-product hand off described in MLTRL. Additional emphasis is\\nplaced on ML tests that consider the mix of real-world data with user annotations (b, right) and synthetic data generated\\nby Unity AI’s Perception tool and structured domain randomization (b, left).\\ncomplement real-world data sources (Figure 4). This application exempliﬁes three important challenges in ML product\\ndevelopment that MLTRL helps overcome:'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='development that MLTRL helps overcome:\\n•Multiple and disparate data sources are common in deployed ML pipelines yet often ignored in R&D.\\nFor instance, upstream data providers can change formats unexpectedly, or a physical event could cause the\\ncustomer behavior to change. It is nearly impossible to anticipate and design for all potential problems with\\nreal-world data and deployment. This computer vision system implemented pipelines and extended test suites'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='to cover open-source benchmark data, real user data, and synthetic data.\\n•Hidden performance degradation can be challenging to detect and debug in ML systems because gradual\\nchanges in performance may not be immediately visible. Common reasons for this challenge are that the\\nML component may be one step in a series. Additionally, local/isolated changes to an ML component’s\\nperformance may not directly affect the observed downstream performance. We can see both issues in the'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='illustrated logic diagram for the automated recycling app (Figure 4). A slight degradation in the initial CV\\nmodel may not heavily inﬂuence the following user input. However, when an uncommon input image appears\\nin the future, the app fails altogether.\\n•Model usage requirements can make or break an ML product. For example, the Netﬂix “$1M Prize” solution\\nwas never fully deployed because of signiﬁcant engineering costs in real-world scenariosv. For example,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='engineering teams must communicate memory usage, compute power requirements, hardware availability,\\nnetwork privacy, and latency to the ML teams. ML teams often only understand the statistics or ML theory\\nbehind a model but not the system requirements or how it scales.\\nWe next elucidate these challenges and how MLTRL helps overcome them in the context of this project’s lifecycle. This\\nproject started at level 4, using largely existing ML methods with a target use-case.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='•Level 4 – For this project, we validated most of the components in other projects. Speciﬁcally, the computer\\nvision (CV) model for object recognition and classiﬁcation was an off-the-shelf model. The synthetic data\\ngeneration method used Unity Perception, a well-established open-source project. Though this allowed us to\\nvnetﬂixtechblog.com/netﬂix-recommendations-beyond-the-5-stars-part-1\\n11'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='skip the earlier levels, many challenges arise when combining ML elements that were independently validated\\nand developed. The MLTRL prototype-caliber code checkpoint ensures that the existing code components\\nare validated and helps avoid poorly deﬁned borders and abstractions between components. ML pipelines\\noften grow out of glue code, and our regimented code checkpoints motivate well-architected software that\\nminimizes these danger spots.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='minimizes these danger spots.\\n•Level 5 – The problematic “valley of death”, mentioned earlier in the level 5 deﬁnitions, is less prevalent in use-\\ncases like this that start at a higher MLTRL level with a speciﬁc product deliverable. In this case, the product\\ndeliverable was a real-time object recognition and classiﬁcation of trash for a mobile recycling application.\\nStill, this stage is critical for the requirements and V&V transition. This stage mitigates failure risks due to the'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='disparate data sources integrated at various steps in this CV system and accounted for the end-user compute\\nconstraints for mobile computing. Speciﬁcally, the TRL cards from earlier stages surfaced potential issues\\nwith imbalanced datasets and the need for speciﬁc synthetic images. These considerations are essential for the\\ndata readiness and testing V&V in the productization requirements. Data quality and availability issues often'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='present huge blockers because teams discover them too late in the game. Data-readiness is one class of many\\nexample issues teams face without MLTRL, as depicted in Fig. 2.\\n•Level 6 – We were re-using a well-understood model and deployment pipeline in this use-case, meaning our\\nprimary challenge was around data reliability. For the problem of recognizing and classifying trash, building a'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='reliable data source using only real data is almost impossible due to diversity, class imbalance, and annotation\\nchallenges. Therefore we chose to develop a synthetic data generator to create training data. At this MLTRL\\nlevel, we needed to ensure that the synthetic data generator created sufﬁciently diverse data and exposed the\\ncontrols needed to alter the data distribution in production. Therefore, we carefully exposed APIs using the'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='Unity Perception package, which allowed us to control lighting, camera parameters, target and non-target\\nobject placements and counts, and background textures. Additionally, we ensured that the object labeling\\nmatched the real-world annotator instructions and that output data formats matched real-world counterparts.\\nLastly, we established a set of statistical tests to compare synthetic and real-world data distributions. The'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='MLTRL checks ensured that we understood, and in this case, adequately designed our data sources to meet\\nin-production requirements.\\n•Level 7 – From the previous level’s R&D TRL cards and observations, we knew relatively early in produc-\\ntization that we would need to assume bias for the real data sources due to class imbalance and imperfect\\nannotations. Therefore we designed tests to monitor these in the deployed application. MLTRL imposes these'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='critical deployment tests well ahead of deployment, where we can easily overlook ML-speciﬁc failure modes.\\n•Level 8 – As we suggested earlier, problems that stem from real-world data are near impossible to anticipate\\nand design for, implying the need for level 8 ﬂight-readiness preparations. Given that we were generating\\nsynthetic images (with structured domain randomization) to complement the real data, we created tests for'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='different data distribution shifts at multiple points in the classiﬁcation pipeline. We also implemented thorough\\nshadow tests ahead of deployment to evaluate how susceptible the ML model(s) to performance regressions\\ncaused by data. Additionally, we also implemented these as CI/CD tests over various deployment scenarios (or\\nmobile device computing speciﬁcations). Without these fully covered, documented, and automated, it would\\nbe impossible to pass level 8 review and deploy the technology.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='•Level 9 – Post-deployment, the monitoring tests prescribed at Levels 8 and 9 and the three main code quality\\ncheckpoints in the MLTRL process help surface hidden performance degradation problems, common with\\ncomplex pipelines of data ﬂows and various models. The switchbacks depicted in Fig. 2 are typical in CV\\nuse-cases. For instance, miscalibrations in models pre-trained on synthetic data and ﬁne-tuned on newer real'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='data can be common yet difﬁcult to catch. However, the level 7 to 4 switchback is designed precisely for these\\nchallenges and product improvements.\\nAccelerating scientiﬁc discovery with massive particle physics simulators\\nComputational models and simulation are key to scientiﬁc advances at all scales, from particle physics, to material\\ndesign and drug discovery, to weather and climate science, and to cosmology[ 35]. Many simulators model the forward'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='evolution of a system (coinciding with the arrow of time), such as the interaction of elementary particles, diffusion of\\ngasses, folding of proteins, or evolution of the universe in the largest scale. The task of inference refers to ﬁnding initial\\nconditions or global parameters of such systems that can lead to some observed data representing the ﬁnal outcome\\nof a simulation. In probabilistic programming[ 36], this inference task is performed by deﬁning prior distributions'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='over any latent quantities of interest, and obtaining posterior distributions over these latent quantities conditioned\\non observed outcomes (for example, experimental data) using Bayes rule. This process, in effect, corresponds to\\ninverting the simulator such that we go from the outcomes towards the inputs that caused the outcomes. In the\\n“Etalumis” project[ 37] (“simulate” spelled backwards), we are using probabilistic programming methods to invert\\n12'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='existing, large-scale simulators via Bayesian inference. The project is as an interdisciplinary collaboration of specialists\\nin probabilistic machine learning, particle physics, and high-performance computing, all essential elements to achieve\\nthe project outcomes. Even more, it is a multi-year project spanning multiple countries, companies, university labs, and\\ngovernment research organizations, bringing signiﬁcant challenges in project management, technology coordination'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='and validation. Aided by MLTRL, there were several key challenges to overcome in this project that are common in\\nscientiﬁc-ML projects:\\n•Integrating with legacy systems is common in scientiﬁc and industrial use-cases, where ML methods are\\napplied with existing sensor networks, infrastructure, and codebases. In this case, particle physics domain\\nexperts at CERN are using the SHERPA simulator[ 38], a 1 million line codebase developed over the last'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='two decades. Rewriting the simulator for ML use-cases is infeasible due to the codebase size and buried\\ndomain knowledge, and new ML experts would need signiﬁcant onboarding to gain working knowledge of\\nthe codebase. It is also common to work with legacy data infrastructure, which can be poorly organized for\\nmachine learning (let alone preprocessed and clean) and unlikely to have followed best practices such as\\ndataset versioning.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='dataset versioning.\\n•Coupling hardware and software architectures is non-trivial when deploying ML at scale, as performance\\nconstraints are often considered in deployment tests well after model and algorithm development, not to\\nmention the expertise is often split across disjoint teams. This can be exacerbated in scientiﬁc-ML when\\nscaling to supercomputing infrastructure, and working with massive datasets that can be in the terabytes and\\npetabytes.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='petabytes.\\n•Interpretability is often a desired feature yet difﬁcult to deliver and validate in practice. Particularly in\\nscientiﬁc ML applications such as this, mechanisms and tooling for domain experts to interpret predictions\\nand models are key for usability (integrating in workﬂows and building trust).\\nTo this end, we will go through the MLTRL levels one by one, demonstrating how they ensure the above scientiﬁc ML\\nchallenges are diligently addressed.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='challenges are diligently addressed.\\n•Level 0 – The theoretical developments leading to Etalumis are immense and well discussed in Baydin et\\nal [37]. In particular the ML theory and methods are in a relatively nascent area of ML and mathematics,\\nprobabilistic programming. New territory can present more challenges compared to well-traveled research\\npaths, for instance in computer vision with neural networks. It is thus helpful to have a guiding framework'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='when making a new path in ML research, such as MLTRL where early reviews help theoretical ML projects\\nget legs.\\n•Level 1-2 – Running low-level experiments in simple testbeds is generally straightforward when working\\nwith probabilistic programming and simulation; in a sense, this easy iteration over experiments is what\\nPPL are designed for. It was additionally helpful in this project to have rich data grounded in physical'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='constraints, allowing us to better isolate model behaviors (rather than data assumptions and noise). The\\nMLTRL requirements documentation is particularly useful for the standard PPL experimentation workﬂow:\\nmodel, infer, criticize, repeat (or Box’s loop) [ 39]. The evaluation step (i.e. criticizing the model) can be\\nmore nuanced than checking summary statistics as in deep learning and similar ML workﬂows. It is thus a'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='useful practice to write down the criticism methods, metrics, and expected results as veriﬁcations for speciﬁc\\nresearch requirements, rather than iterating over Box’s loop without a priori targets. Further, because this\\nresearch project had a speciﬁc target application early in the process (the SHERPA simulator), the project\\ntimeline beneﬁted from recognizing simulator-integration constraints upfront as requirements, not to mention'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='data availability concerns, which are often overlooked in early R&D levels. It was additionally useful to have\\nCERN scientists as domain experts in the reviews at these R&D levels.\\n•Level 3 – Systems development can be challenging with probabilistic programming, again because it is\\nrelatively nascent and much of the out-of-the-box tools and infrastructure are not there as in most ML and'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='deep learning. Here in particular there’s a novel (unproven) approach for systems integration: a probabilistic\\nprogramming execution protocol was developed to reroute random number draws in the stochastic simulator\\ncodebase (SHERPA) to the probabilistic programming system, thus enabling the system to control stochastic\\nchoices in SHERPA and run inference on its execution traces, all while keeping the legacy codebase intact! A'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='more invasive method that modiﬁes SHERPA would not have been acceptable. If it were not for MLTRL forcing\\nsystems considerations this early in the Etalumis project lifecycle, this could have been an insurmountable\\nhurdle later when multiple codebases and infrastructures come into play. By the same token, systems planning\\nhere helped enable the signiﬁcant HPC scaling later: the team deﬁned the need for HPC support well ahead'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='of actually running HPC, in order to build the prototype code in a way that would readily map to HPC (in\\naddition to local or cloud CPU and GPU). The data engineering challenges in this system’s development\\n13'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='nonetheless persist – that is, data pipelines and APIs that can integrate various sources and infrastructures, and\\nnormalize data from various databases – although MLTRL helps consider these at the an earlier stage that can\\nhelp inform architecture design.\\n•Level 4 – The natural “embedded switchback” from Level 4 to 2 (see the Methods section) provided an efﬁcient\\npath toward developing an improved, amortized inference method–i.e., using a computationally expensive'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='deep learning based inference algorithm to train only once, in order to then do fast, repeated inference in the\\nSHERPA model. Leveraging cyclic R&D methods, the Etalumis project could iteratively improve inference\\nmethods without stalling the broader system development, ultimately producing the largest scale posterior\\ninference in a Turing-complete probabilistic programming system. Achieving this scale through iterative R&D'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='along the main project lifecycle was additionally enabled by working with with NERSC engineers and their\\nCori supercomputer to progressively scale smaller R&D tests to the goal supercomputing deployment scenario.\\nTypical ML workﬂows that follow simple linear progressions[ 6,40] would not enable ramping up in this\\nfashion, and can actual prevent scaling R&D to production due to lack of systems engineering processes (like\\nMLTRL) connecting research to deployment.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='MLTRL) connecting research to deployment.\\n•Level 5 – Multi-org international collaborations can be riddled with communication and teamwork issues,\\nin particular at this pivotal stage where teams transition from R&D to application and product development.\\nFirst, MLTRL as a lingua franca was key to the team effort bringing Etalumis proof-of-concept into the\\nlarger effort of applying it to massive high-energy physics simulators. It was also critical at this stage to'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='clearly communicate end-user requirements across the various teams and organizations, which must be deﬁned\\nin MLTRL requirements docs with V&V measures – the essential science-user requirements were mainly\\nfor model and prediction interpretability, uncertainty estimation, and code usability. If there are concerns\\nover these features, MLTRL switchbacks can help to quickly cycle back and improve modeling choices in a'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='transparent, efﬁcient way – generally in ML projects, these fundamental issues with usability are caught too\\nlate, even after deployment. In the probabilistic generative model setting we’ve deﬁned in Etalumis, Bayesian\\ninference gives results that are interpretable because they include exact locations and processes in the model\\nthat are associated with each prediction. Working with ML methods that are inherently interpretable, we are'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='well-positioned to deliver interpretable interfaces for the end-users later in the project lifecycle.\\n•Level 6-9 – The standard MLTRL protocol apply in these application-to-deployment stages, with several\\nEtalumis-speciﬁc highlights. First, given the signiﬁcant research contributions in both probabilistic pro-\\ngramming and scientiﬁc-ML, it’s important to share the code publicly. The development and deployment'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='of the open-source code repository PPXvibranched into a separate MLTRL path from the Etalumis path\\nfor deployment at CERN. It’s useful to have systems engineering enable clean separation of requirements,\\ndeployments, etc. when there are different development and product lifecycles originating from a common\\nparent project. For example, in this case it was useful to employ MLTRL switchbacks in the open-sourcing'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='process, isolated from the CERN application paths, in order to add support for additional programming\\nlanguages so PPX can apply to more scientiﬁc simulators – both directions beneﬁted signiﬁcantly the from\\nthe data pipelines considerations brought up levels earlier, where open-sourcing required different data APIs\\nand data transformations to enable broad usability. Second, related to the open-source code deliverable and'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='the scientiﬁc ML user requirements we noted above, the late stages of MLTRL reviews include higher level\\nstakeholders and speciﬁc end-users, yet again enforcing these scientiﬁc usability requirements are met. An\\nexample result of this in Etalumis is the ability to output human-readable execution traces of the SHERPA\\nruns and inference, enabling never before possible step-by-step interpretability of the black-box simulator.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='The scientiﬁc ML perspective additionally brings to forefront an end-to-end data perspective that is pertinent in\\nessentially all ML use-cases: these systems are only useful to the extent they provide comprehensive data analyses that\\nintegrate the data consumed and generated in these workﬂows, from raw domain data to machine-learned models. These\\ndata analyses drive reproducibility, explainability, and experiment data understanding, which are critical requirements'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='in scientiﬁc endeavors and ML broadly.\\nCausal inference & ML in medicine\\nUnderstanding cause and effect relationships is crucial for accurate and actionable decision-making in many settings,\\nfrom healthcare and epidemiology, to economics and government policy development. Unfortunately, standard\\nmachine learning algorithms can only ﬁnd patterns and correlation in data, and as correlation is not causation, their'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='predictions cannot be conﬁdently used for understanding cause and effect. Indeed, relying on correlations extracted\\nfrom observational data to guide decision-making can lead to embarrassing, costly, and even dangerous mistakes,\\nsuch as concluding that asthma reduces pneumonia mortality risk [ 41], and that smoking reduces risk of developing\\nvigithub.com/pyprob/ppx\\n14'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='severe COVID-19 [ 42]. Fortunately, there has been much recent development in a ﬁeld known as causal inference that\\ncan quantitatively make sense of cause and effect from purely observational data[ 43]. The ability of causal inference\\nalgorithms to quantify causal impact rests on a number of important checks and assumptions–beyond those employed\\nin standard machine learning or purely statistical methodology–that must be carefully deliberated over during their'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='development and training. These speciﬁc checks and assumptions are as follows:\\n•Specifying cause-and-effect relationships between relevant variables– One of the most important assump-\\ntions underlying causal inference is the structure of the causal relations between quantities of interest. The\\ngold standard for determining causal relations is to perform a randomised controlled trial, but in most cases'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='these cannot be employed due to ethical concerns, technological infeasibility, or prohibitive cost. In these\\nsituations, domain experts have to be consulted to determine the causal relationships. It is important in these\\nsituations to carefully address the manner in which such domain knowledge was extracted from experts, the\\nnumber and diversity of experts involved, the amount of consensus between experts, and so on. The need for'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='careful documentation of this knowledge and its periodic review is made clear in the MLTRL framework, as\\nwe shall see below.\\n•Identiﬁability– Another vital component of building causal models is whether the causal question of interest\\nisidentiﬁable from the causal structure speciﬁed for the model together with observational (and sometimes\\nexperimental) data.\\n•Adjusting for and monitoring confounding bias– An important aspect of causal model performance, not'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='present in standard machine learning algorithms, is confounding bias adjustment. The standard approach is to\\nemploy propensity score matching to remove such bias. However, the quality of bias adjustment achieved in\\nany speciﬁc instance with such propensity-based matching methods needs to be checked and documented,\\nwith alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[44].'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='•Sensitivity analysis– As causal estimates are based on generally untestable assumptions, such as observing all\\nrelevant confounders, it is vital to determine how sensitive the resulting predictions are to potential violations\\nof these assumptions.\\n•Consistency– It is crucial to understand if the learned causal estimate provably converges to the true causal\\neffect in the limit of inﬁnite sample size. However, causal models cannot be validated by standard held-out'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='tests, but rather require randomization or special data collection strategies to evaluate their predictions [ 45,46].\\nThe MLTRL framework makes transparent the need to carefully document and defend these assumptions, thus ensuring\\nthe safe and robust creation, deployment, and maintenance of causal models. We elucidate this with recent work by\\nRichens et al.[ 47], developing a causal approach to computer-assisted diagnosis which outperforms previous purely'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='machine learning based methods. To this end, we will go through the MLTRL levels one by one, demonstrating how\\nthey ensure the above speciﬁc checks and assumptions are naturally accounted for. This should provide a blueprint for\\nhow to employ the MLTRL levels in other causal inference applications.\\n•Level 0 – When initially faced with a causal inference task, the ﬁrst step is always to understand the causal'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='relationships between relevant variables. For instance, in Richens et al. [ 47], the ﬁrst step toward building\\nthe diagnostic model was specifying the causal relationships between the diverse set risk factors, diseases,\\nand symptoms included in the model. To learn these relations, doctors and healthcare professionals were\\nconsulted to employ their expansive medical domain knowledge which was robustly evaluated by additional'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='independent groups of healthcare professionals. The MLTRL framework ensured this issue is dealt with and\\ndocumented correctly, as such knowledge is required to progress from Level 0; failure to do this has plagued\\nsimilar healthcare AI projects [48].\\nThe next step of any causal analysis is to understand whether the causal question of interest is uniquely\\nidentiﬁable from the causal structure speciﬁed for the model together with observational and experimental data.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='In this medical diagnosis example, identiﬁcation was crucial to establish, as the causal question of interest,\\n“would the observed symptoms not be present had a speciﬁc disease been cured?”, was highly non-trivial.\\nAgain, MLTRL ensures this vital aspect of model building is carefully considered, as a mathematical proof of\\nidentiﬁability would be required to graduate from Level 0.\\nWith both the causal structure and identiﬁability result in hand, one can progress to Level 1.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='•Level 1 – At this level, the goal is to take the estimand for the identiﬁed causal question of interest and\\ndevise a way to estimate it from data. To do this one will need efﬁcient ways to adjust for confounﬁng bias.\\nThe standard approach is to employ propensity score-based methods to remove such bias when the target\\ndecision is binary, and use multi-stage ML models adhering to the assumed causal structure[ 49] for continuous'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='target decisions (and high-dimensional data in general). However, the quality of bias adjustment achieved in\\nany speciﬁc instance with propensity-based matching methods needs to be checked and documented, with\\n15'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[ 44]. As\\nabove, MLTRL ensures transparency and adherence to this important aspect of causal model development, as\\nwithout it a project cannot graduate from Level 1. Even more, MLTRL ensures tests for confounding bias\\nare developed early-on and maintained throughout later stages to deployment. Still, in many cases, it is not'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='possible to completely remove confounding in the observed data. TRL Cards offer a transparent way to declare\\nspeciﬁc limitations of a causal ML method.\\n•Level 2 – PoC-level tests for causal models must go beyond that of typical ML models. As discussed above,\\nto ensure the estimated causal effects are robust to the assumptions required for their derivation, sensitivity\\nto these assumptions must be analysed. Such sensitivity analysis is often limited to R&D experiments or'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='a post-hoc feature of ML products. MLTRL on the other hand requires this throughout the lifecycle as\\ncomponents of ML test suites and gated reviews. In the case of causal ML, best practice is to employ sensitivity\\nanalysis for this robustness check[ 50]. MLTRL ensures this check is highlighted and adhered to, and no model\\nwill end up graduating Level 2–let alone being deployed–unless it is passed.\\n•Level 3 – Coding best practices, as in general ML applications.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='•Level 4-5 – There are additional tests to consider when taking causal models from research to production,\\nin particular at Level 4–proof of concept demonstration in a real scenario. Consistency , for example, is an\\nimportant property of causal methods that informs us whether the method provably converges to the true\\ncausal graph in the limit of inﬁnite sample size. Quantifying consistency in the test suite is critical when'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='datasets change from controlled laboratory settings to open-world, and when the application scales. And\\nPoC validation steps are more efﬁcient with MLTRL because the process facilitates early speciﬁcation of the\\nevaluation metric for a causal model in Level 2. Causal models cannot be validated by standard held-out tests,\\nbut rather require randomization or special data collection strategies to evaluate their predictions[ 45,46]. Any'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='difﬁculty in evaluating the model’s predictions will be caught early and remedied.\\n•Level 6-9 – With the the causal ML components of this technology developed reliably in the previous levels,\\nthe rest of the levels developing this technology focused on general medical-ML deployment challenges. For\\nthe most part, data governance, privacy, and management that was detailed earlier in the neuropathology\\nMLTRL use-case, as well as on-premises deployment.\\nAI for open-source space sciences'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='AI for open-source space sciences\\nThe CAMS (Cameras for Allsky Meteor Surveillance) project [ 51], established in 2010 by NASA, uses hundreds of\\noff-the-shelf CCTV cameras to capture the meteor activity in the night sky. Initially, resident scientists would retrieve\\nhard-disks containing video data captured each night and perform manual triangulation of tracks or streaks of light'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='in the night sky, and compute a meteor’s trajectory, orbit, and lightcurve. Each solution was manually classiﬁed as a\\nmeteor or not (i.e., planes, birds, clouds, etc). In 2017, a project run by the Frontier Development Labvii[52], the AI\\naccelerator for NASA and ESA, aimed to automate the data processing pipeline and replicate the scientists thought\\nprocess to build an ML model that identiﬁes meteors in the CAMS project [ 53,54]. The data automation led to'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='orders of magnitude improvements in operational efﬁciency of the system, and allowed new contributors and amateur\\nastronomers to start contributing to meteor sightings. Additionally, a novel web tool allowed anybody anywhere to\\nview the meteors detected in the previous night. The CAMS camera system has had six-fold global expansion of the\\ndata capture network, discovered ten new meteor showers, contributed towards instrumental evidence of previously'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='predicted comets, and helped calculate parent bodies of various meteor showers. CAMS utilized the MLTRL framework\\nto progress as described:\\n•Level 1 – Understanding the domain and data is a prerequisite for any ML development. Extensive data\\nexploration elucidated visual differences between objects in the night sky such as meteors, satellites, clouds,\\ntail lights of planes, light from the eyes of cats peering into cameras, trees, and other tall objects visible in'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='the moonlight. This step helped (1) understand visual properties of meteors that later deﬁned the ML model\\narchitecture, and (2) mitigate impact of data imbalance by proactively developing domain-oriented strategies.\\nThe results are well-documented on a datasheet associated with the TRL card, and discussed at the stage\\nreview. This MLTRL documentation forced us to consider data sharing and other privacy concerns at this early'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='conceptualization stage, which is certainly relevant considering CAMS is for open-source and gathering data\\nfrom myriad sources.\\n•Level 2-3 – The agile and non-monotonic (or non-linear) development prescribed by MLTRL allowed the\\nteam to ﬁrst develop an approximate end-to-end pipeline that offered a path to ML model deployment and\\nquick turnaround time to incorporate feedback from the regular gated reviews. Then, with relatively quicker'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='viiThe NASA Frontier Development Lab and partners open-source the code and data via the SpaceML platform: spaceml.org\\n16'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='experimentation, the team could improve on the quality of not just the ML model, but also scale up the systems\\ndevelopment simultaneously in a non-monotonic development cycle.\\n•Level 4 – With the initial pipeline in place, scalable training of baselines and initial models on real challenging\\ndatasets ensued. Throughout the levels, the MLTRL gated reviews were essential for making efﬁcient progress'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='while ensuring robustness and functionality that meets stakeholder needs. At this stage we highlight speciﬁc\\nadvantages of the MLTRL review processes that had instrumental effect on the project success: With the\\nrequired panel of mixed ML researchers and engineers, domain scientists, and product managers, the stage 4\\nreviews stressed the signiﬁcance of numerical improvements and comparison to existing baselines, and helped'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='identify and overcome issues with data imbalance. The team likely would have overlooked these approaches\\nwithout the review from peers in diverse roles and teams. In general, the evolving panel of reviewers at\\ndifferent stages of the project was essential for covering a variety of veriﬁcation and validation measures –\\nfrom helping mitigate data challenges, to open-source code quality.\\n•Level 5 – To complete this R&D-to-productization level, a novel web tool called the NASA CAMS Meteor'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='Shower Portalviiiwas created that allowed users to view meteor shower activity from the previous night and\\nverify meteor predictions generated by the ML model. This app development was valuable for A/B testing,\\nvalidating detected meteors and classiﬁed new meteor showers with human-AI interaction, and demonstrating\\nreal-world utility to stakeholders in review. ML processes without MLTRL miss out on these valuable\\ndevelopment by overlooking the need for such a demo tool.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='•Level 6 – Application development was naturally driven by end-user feedback from the web app in level 5 –\\nwithout MLTRL it’s unlikely the team would be able to work with early productization feedback. With almost\\nreal time feedback coming in daily, newer methods for improving robustness of meteor identiﬁcation led to\\nresearching and developing a unique augmentation technique, resulting in the state of the art performance of'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='the ML model. Further application development led to incorporating features that were in demand by users of\\nthe NASA CAMS Meteor Shower Portal: include celestial reference points through constellations, add ability\\nto zoom in/out and (un)cluster showers, and provide tooling for scientiﬁc communication. The coordination of\\nthese features into product-caliber codebase resulted in the release of the NASA CAMS Meteor Shower Portal'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='2.0 that was built by a team of citizen scientists – again we found the speciﬁc checkpoints in the MLTRL\\nreview were crucial for achieving these goals.\\n•Level 7 – Integration was particularly challenging in two ways. First, integrating the ML and data engineering\\ndeliverables with the existing infrastructure and tools of the larger CAMS system, which had started devel-\\nopment years earlier with other teams in partner organizations, required quantiﬁable progress for verifying'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='the tech-readiness of ML models and modules. The use of technology readiness levels provided a clear and\\nconsistent metric for the maturity of the ML and data technologies, making for clear communication and\\nefﬁcient project integration. Without MLTRL it is difﬁcult to have a conversation, let alone make progress, to-\\nwards integrating AI/ML and data subsystems and components. Second, integrating open-source contributions'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='into the main ML subsystem was a signiﬁcant challenge alleviated with diligent veriﬁcation and validation\\nmeasures from MLTRL, as well as quantifying robustness with ML testing suites (using scoring measures like\\nthat of the ML Testing Rubric[20], and devising a checklist based on metamorphic testing[18]).\\n•Level 8 – CAMS, like many datasets in practice, consisted of a smaller labeled subset and a much larger'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='unlabeled set. In an attempt to additionally increase robustness of the ML subsystem ahead of “ﬂight readiness”,\\nwe looked to active learning [ 55,56] techniques to leverage the unlabeled data. Models using an initial version\\nof this approach, where results of the active learning provided “weak” labels, resulted in consumption of the\\nentire decade long unlabelled data collected by CAMS and slightly higher scores on deployment tests. Active'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='learning showed to be a promising feature and was switched back to level 7 for further development towards\\nthe next deployment version, so as not to delay the rest of the project.\\n•Level 9 – The ML components in CAMS require continual monitoring for model and data drifts, such as\\nchanges in weather, smoke, and cloud patterns that affect the view of the night sky. The data drifts may also be\\nspeciﬁc to locations, such as ﬁreﬂies and bugs in CAMS Australia and New Zealand stations which appear as'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='false positives. The ML pipeline is largely automated with CI/CD, runs regular regression tests, and production\\nof benchmarks. Manual intervention can be triggered when needed, such as sending low conﬁdence meteors for\\nveriﬁcation to scientists in the CAMS project. The team also regularly releases the code, models, and web tools\\non the open-source space sciences and exploration ML toolbox, SpaceMLix. Through the SpaceML community'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='and partner organizations, CAMS continually improves with feature requests, debugging, and improving data\\npractices, while tracking progress with standard software release cycles and MLTRL documentation.\\nviiimeteorshowers.seti.org\\nixspaceml.org\\n17'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='BEYOND SOFTWARE ENGINEERING\\nSoftware engineering (SWE) practices vary signiﬁcantly across domains and industries. Some domains, such as medical\\napplications, aerospace, or autonomous vehicles rely on a highly rigorous development process which is required\\nby regulations. Other domains, for example advertising and e-commerce are not regulated and can employ a lenient\\napproach to development. ML development should at minimum inherit the acceptable software engineering practices of'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='the domain. There are, however, several key areas where ML development stands out from SWE, adding its own unique\\nchallenges which even most rigorous SWE practices are not able to overcome.\\nFor instance, the behavior of ML systems is learned from data, not speciﬁed directly in code. The data requirements\\naround ML (i.e., data discovery, management, and monitoring) adds signiﬁcant complexity not seen in other types'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='of SWE. There are many beneﬁts to using a data-oriented architecture (DOA) [48] with the data-ﬁrst workﬂows and\\nmanagement practices prescribed in MLTRL. DOA aims to make the data ﬂowing between elements of business logic\\nmore explicit and accessible with a streaming-based architecture rather than the micro-service architectures that are\\nstandard in software systems. One speciﬁc beneﬁt of DOA is making data available and traceable by design, which'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='helps signiﬁcantly in the ML logging challenges and data governance needs we discussed in Levels 7-9. Moreover,\\nMLTRL highlights data-related requirements along every step to ensure that the development process considers data\\nreadiness and availability.\\nNot to mention an array of ML-speciﬁc failure modes; for example, models that become miscalibrated due to subtle\\ndata distributional shifts in the deployment setting, resulting in models that are more conﬁdent in predictions than they'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='should be. MLTRL helps deﬁne ML-speciﬁc testing considerations (levels 5 and 7) to help surface these failure-modes\\nearly. ML opens up new threat vectors across the whole deployment workﬂow that otherwise aren’t risks in software\\nsystems: for example, a poisoning attack to contaminate the training phase of ML systems, or membership inference\\nto see if a given data record was part of the model’s training. MLTRL consider these threat vectors and suggests'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='relevant risk-identiﬁcation during prototyping and productization phases. More generally, ML codebases have all the\\nproblems for regular code, plus ML-speciﬁc issues at the system level, mainly as a consequence of added complexity\\nand dynamism. The resulting entanglement, for instance, implies that the SWE practice of making isolated changes is\\noften not feasible – Scully et al.[ 57] refer to this as the “changing anything changes everything” principle. Given this'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='consideration, typical SWE change-management is insufﬁcient. Furthermore, ML systems almost necessarily increase\\nthe technical debt; package-level refactoring is generally sufﬁcient for removing technical debt in software systems, but\\nthis is not the case in ML systems.\\nThese factors and others suggest that inherited software engineering and management practices of a given domain are'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='insufﬁcient for the successful development of robust and reliable ML systems. But it is not trading off one for the other:\\nMLTRL can be used in synergy with the existing, industry-standard software engineering practices such as agile [ 58]\\nand waterfall [ 59] to handle unique challenges of ML development. Because ML applications are a category of software,\\nall best practices of building and operating software should be extended when possible to the ML application. Practices'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='like version control, comprehensive testing, continuous integration and continuous deployment are all applicable to ML\\ndevelopment. MLTRL provides a framework that helps extend SWE building and operating practices that are acceptable\\nin a given domain to tackle the unique challenges of ML development.\\nRELATED WORKS\\nA recent case study from Microsoft Research [ 40] similarly identiﬁes a few themes describing how ML is not equal to'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='software engineering, and recommends a linear ML workﬂow with steps for data preparation through modeling and\\ndeploying. They deﬁne an effective workﬂow for isolated development of an ML model, but this approach does not\\nensure the technology is actually improving in quality and robustness. Their process should be repeated at progressive\\nstages of development in the broader ML and data technology lifecycle. If applied in the MLTRL framework, the'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='speciﬁc ingredients of the ML model workﬂow – that is, people, software, tests, objectives, etc. – evolve over time and\\nsubsequent stages as the technologies mature.\\nThere exist many recommended workﬂows for speciﬁc ML methods and areas of pipelines. For instance, a more\\niterative process for Bayesian ML [ 60] and even more speciﬁcally for probabilistic programming [ 39], a data mining'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='process deﬁned in 2000 that remains widely used [ 61], others for describing data iterations [ 62], and human-computer\\ninteraction cycles [ 63]. In these recommended workﬂows and others, there’s an important distinction between their\\ncycles and “switchback” mechanisms in MLTRL. Their cycles suggest to generically iterate over a data-modeling-\\nevaluation-deployment process. Switchbacks, on the other hand, are speciﬁc, purpose-driven workﬂows for dialing'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='part(s) of a project to an earlier stage – this doesn’t simply mean go back and train the model on more data, but rather\\nswitching back regresses the technology’s maturity level (e.g. from level 5 to level 3) such that it must again fulﬁll the\\nlevel-by-level requirements, evaluations and reviews. See the Methods section for more details on MLTRL switchbacks.\\n18'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='In general, iteration is an important part of data, ML, and software processes. MLTRL is unique from the other\\nrecommended processes in many ways, and perhaps most importantly because it considers data ﬂows and ML models\\nin the context of larger systems. These isolated processes (that are speciﬁc to e.g. modeling in prototype development\\nor data wrangling in application development) are synergistic with MLTRL because they can be used within each level'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='of the larger lifecycle or framework. For example, the Bayesian modeling processes [ 39,60] we mentioned above\\nare really useful to guide developers of probabilistic ML approaches. But there are important distinctions between\\nexecuting these modeling steps and cycles in a well-deﬁned prototyping environment with curated data and minimal\\nresponsibilities, versus a production environment riddled with sparse and noisy data, that interacts with the physical'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='world in non-obvious ways, and can carry expensive (even hidden) consequences. MLTRL provides the necessary,\\nholistic context and structure to use these and other development processes reliably and responsibly.\\nAlso related to our work, Google teams have proposed ML testing recommendations [ 20] and validating the data fed\\ninto ML systems [ 64]. For NLP applications, typical ML testing practices struggle to translate to real-world settings,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='often overestimating performance capabilities. An effective way to address this is devising a checklist of linguistic\\ncapabilities and test types, as in Ribeiro et al.[ 17]–interestingly their test suite was inspired by metamorphic testing,\\nwhich we suggested earlier in Level 7 for testing systems AI integrations. A survey by Paleyes et al. [ 48] go over\\nnumerous case studies to discuss challenges in ML deployment. They similarly pay special attention to the need for'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='ethical considerations, end-user trust, and extra security in ML deployments. On the latter point, Kumar et al. [ 65]\\nprovide a table thoroughly breaking down new threat vectors across the whole ML deployment workﬂow (some of\\nwhich we mentioned above). These works, notably the ML security measures and the quantiﬁcation of an ML test suite\\nin a principled way – i.e., that does not use misguided heuristics such as code coverage – are valuable to include in any'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='ML workﬂow including MLTRL, and are synergistic with the framework we’ve described in this paper. These analyses\\nprovide useful insights, but they do not provide a holistic, regimented process for the full ML lifecycle from R&D\\nthrough deployment. An end-to-end approach is suggested by Raji et al.[ 66], but only for the speciﬁc task of auditing\\nalgorithms; components of AI auditing are mentioned in Level 7, and covered throughout in the review processes.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='Sculley et al.[ 57] go into more ML debt topics such as undeclared consumers and data dependencies, and go on to\\nrecommend an ML Testing Rubric as a production checklist [ 20]. For example, testing models by a canary process\\nbefore serving them into production. This, along with similar shadow testing we mentioned earlier, are common in\\nautonomous ML systems, notably robotics and autonomous vehicles. They explicitly call out tests in four main areas'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='(ML infrastructure, model development, features and data, and monitoring of running ML systems), some of which we\\ndiscussed earlier. For example, tests that the training and serving features compute the same values; a model may train\\non logged processes or user input, but is then served on a live feed with different inputs. In addition to the Google ML\\nTesting Rubric, we advocate metamorphic testing : a SWE methodology for testing a speciﬁc set of relations between'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='the outputs of multiple inputs. True to the checklists in the Google ML Testing Rubric and in MLTRL, metamorphic\\ntesting for ML can have a codiﬁed list of metamorphic relations[18].\\nIn domains such as healthcare there have been the introduction of similar checklists for data readiness – for example,\\nto ensure regulatory-grade real-world-evidence (RWE) data quality [ 67] – yet these are nascent and not yet widely'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='accepted. Applying AI in healthcare has led to developing guidance for regulatory protocol, which is still a work in\\nprogress. Larson et al.[ 68] provide a comprehensive analysis for medical imaging and AI, arriving at several regulatory\\nframework recommendations that mirror what we outline as important measures in MLTRL: e.g., detailed task elements\\nsuch as pitfalls and limitations (surfaced on TRL Cards), clear deﬁnition of an algorithm relative to the downstream'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='task, deﬁning the algorithm “capability” (Level 5), real-world monitoring, and more.\\nD’amour et al.[ 19] dive into the problem we noted earlier about model miscalibration. They point to the trend in machine\\nlearning to develop models relatively isolated from the downstream use and larger system, resulting in underspeciﬁcation\\nthat handicaps practical ML pipelines. This is largely problematic in deep learning pipelines, but we’ve also noted this'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='risk in the case of causal inference applications. Suggested remedies include stress tests –empirical evaluations that\\nprobe the model’s inductive biases on practically relevant dimensions–and in general the methods we deﬁne in Level 7.\\nLIMITATIONS, RESPONSIBILITIES, and ETHICS\\nMLTRL has been developed, deployed, iterated, and validated in myriad environments, as demonstrated by the previous'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='examples and many others. Nonetheless we strongly suggest that MLTRL not be viewed as a cure-all for machine\\nlearning systems engineering. Rather, MLTRL provides mechanisms to better enable ML practitioners, teams, and\\nstakeholders to be diligent and responsible with these technologies and data. That is, one cannot implement MLTRL in\\nan organization and turn a blind eye to the many data, ML, and integration challenges we’ve discussed here. MLTRL is'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='analogous to a pilot’s checklist, not autopilot.\\n19'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='MLTRL is intended to be complimentary to existing software development methodologies, not replace or alter them.\\nSpeciﬁcally, whether the team uses agile or waterfall methods, MLTRL can be adopted to help deﬁne and structure\\nphases of the project, as well as the success criteria of each stage. In context of the software development process, the\\npurpose of MLTRL is to help the team minimize the technical dept and risk associated with the delivery of an ML'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='application by helping the development team ask necessary questions.\\nWe discussed many data challenges and approaches in the context of MLTRL, and should highlight again the importance\\nof data considerations in any ML initiative. The data availability and quality can severely limit the ability to develop and\\ndeploy ML, whether MLTRL is used or not. It is again the responsibility of the ML practitioners, teams, and stakeholders'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='to gather, use, and distribute data in safe, legal, ethical ways. MLTRL helps do so with rigor and transparency, but\\nagain is not a solution for data bias. We recommend these recent works on data bias in ML: [ 69,70,71,72,73].\\nFurther, AI/ML ethics is a continuously evolving, multidisciplinary space – see [ 5]. MLTRL aims to prioritize ethics\\nconsiderations at each level of the framework, and would do well to also evolve over time with the broader AI/ML\\nethics developments.\\nCONCLUSION'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='ethics developments.\\nCONCLUSION\\nWe’ve described Machine Learning Technology Readiness Levels (MLTRL) , an industry-hardened systems engineering\\nframework for robust, reliable, and responsible machine learning. MLTRL is derived from the processes and testing\\nstandards of spacecraft development, yet lean and efﬁcient for ML, data, and software workﬂows. Examples from\\nseveral organizations across industries demonstrate the efﬁcacy of MLTRL for AI and ML technologies, from research'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='and development through productization and deployment, in important domains such as healthcare and physics, with\\nemphasis on data readiness amongst other critical challenges. Our aim is MLTRL works in synergy with recent\\napproaches in the community focused on diligent data-readiness, privacy and security, and ethics. Even more, MLTRL\\nestablishes a much-needed lingua franca for the AI ecosystem, and broadly for AI in the worlds of science, engineering,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='and business. Our hope is that our systems framework is adopted broadly in AI and ML organizations, and that\\n“technology readiness levels” becomes common nomenclature across AI stakeholders – from researchers and engineers\\nto sales-people and executive decision-makers.\\nMethods\\nGated reviews\\nAt the end of each stage is a dedicated review period: (1) Present the technical developments along with the requirements'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='and their corresponding veriﬁcation measures and validation steps, (2) make key decisions on path(s) forward (or\\nbackward) and timing, and (3) debrief the processx. As in the gated reviews deﬁned by TRL used by NASA, DARPA, et\\nal., MLTRL stipulates speciﬁc criteria for review at each level, as well as calling out speciﬁc key decision points (noted\\nin the level descriptions above). The designated reviewers will “graduate” the technology to the next level, or provide a'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='list of speciﬁc tasks that are still needed (ideally with quantitative remarks). After graduation at each level, the working\\ngroup does a brief post-mortem; we ﬁnd that a quick day or two pays dividends in cutting away technical debt and\\nimproving team processes. Regular gated reviews are essential for making efﬁcient progress while ensuring robustness\\nand functionality that meets stakeholder needs. There are several important mechanisms in MLTRL reviews that are'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='speciﬁcally useful with AI and ML technologies: First, the review panels evolve over a project lifecycle, as noted\\nbelow. Second, MLTRL prescribes that each review runs through an AI ethics checklist deﬁned by the organization; it is\\nimportant to repeat this at each review, as the review panel and stakeholders evolve considerably over a project lifecycle.\\nAs previously described in the levels deﬁnitions, including ethics reviews as an integral part of early system development'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='is essential for informing model speciﬁcations and avoiding unintended biases or harm[74] after deployment.\\nTRL “Cards”\\nIn Figure 3 we succinctly showcase a key deliverable: TRL Cards . The model cards proposed by Google [ 75] are a useful\\ndevelopment for external user-readiness with ML. On the other hand, our TRL Cards aim to be more information-dense,\\nlike datasheets for medical devices and engineering tools – see the open-source TRL Card repo for examples and'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='templates (to be released at github.com/alan-turing-institute). These serve as “report cards” that grow and improve upon\\ngraduating levels, and provide a means of inter-team and cross-functional communication. The content of a TRL Card\\nis roughly in two categories: project info, and implicit knowledge. The former clearly states info such as project owners\\nxMLTRL should include regular debriefs and meta-evaluations such that process improvements can be made in a data-driven,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='efﬁcient way (rather than an annual meta-review). MLTRL is a high-level framework that each organization should operationalize in\\na way that suits their speciﬁc capabilities and resources.\\n20'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='and reviewers, development status, and semantic versioning–not just for code, also for models and data. In the latter\\ncategory are speciﬁc insights that are typically siloed in the ML development team but should be communicated to\\nother stakeholders: modeling assumptions, dataset biases, corner cases, etc. With the spread of AI and ML in critical\\napplication areas, we are seeing domain expert consortiums deﬁning AI reporting guidelines – e.g., Rivera et al.[ 76]'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='calling for clinical trials reports for interventions involving AI – which will greatly beneﬁt from the use of our TRL\\nreporting cards. We stress that these TRL Cards are key for the progression of projects, rather than documentation\\nafterthoughts. The TRL Cards thus promote transparency and trust, within teams and across organizations. TRL Card\\ntemplates will be open-sourced upon publication of this work, including methods for coordinating use with other'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='reporting tools such as “Datasheets for Datasets” [24].\\nRisk mitigation\\nIdentifying and addressing risks in a software project is not a new practice. However, akin to the MLTRL roots in\\nspacecraft engineering, risk is a “ﬁrst-class citizen” here. In the deﬁnition of technical and product requirements, each\\nentry has a calculation of the form risk =p(failure )×value , where the value of a component is an integer 1−10.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='Being diligent about quantifying risks across the technical requirements is a useful mechanism for ﬂagging ML-related\\nvulnerabilities that can sometimes be hidden by layers of other software. MLTRL also speciﬁes that risk quantiﬁcation\\nand testing strategies are required for sim-to-real development. That is, there is nearly always a non-trivial gap in\\ntransferring a model or algorithm from a simulation testbed to the real world. Requiring explicit sim-to-real testing'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='steps in the workﬂow helps mitigate unforeseen (and often hazardous) failures. Additionally, comprehensive ML test\\ncoverage that we mention throughout this paper is a critical strategy for mitigating risks anduncertainties: ML-based\\nsystem behavior is not easily speciﬁed in advance, but rather depends on dynamic qualities of the data and on various\\nmodel conﬁguration choices[20].\\nNon-monotonic, non-linear paths'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='Non-monotonic, non-linear paths\\nWe observe many projects beneﬁt from cyclic paths, dialing components of a technology back to a lower level. Our\\nframework not only encourages cycles, we make them explicit with “switchback mechanisms” to regress the maturity\\nof speciﬁc components in an AI system:\\n1.Discovery switchbacks occur as a natural mechanism – new technical gaps are discovered through systems'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='integration, sparking later rounds of component development[ 77]. These are most common in the R&D levels,\\nfor example moving a component of a proof-of-concept technology (at Level 4) back to proof-of-principle\\ndevelopment (Level 2).\\n2.Review switchbacks result from gated reviews, where speciﬁc components or larger subsystems may be dialed\\nback to earlier levels. This switchback is one of the “key decision points” in the MLTRL project lifecycle'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='(as noted in the Levels deﬁnitions), and is often a decision driven by business-needs and timing rather than\\ntechnical concerns (for instance when mission priorities and funds shift). This mechanism is common from\\nLevel 6/7 to 4, which stresses the importance of this R&D to product transition phase (see Figure 2 (left)).\\n3.Embedded switchbacks are predeﬁned in the MLTRL process. For example, a predeﬁned path from 4 to 2, and'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='from 9 to 4. In complex systems, particularly with AI technologies, these built-in loops help mitigate technical\\ndebt and overcome other inefﬁciencies such as noncomprehensive V&V steps.\\nWithout these built-in mechanisms for cyclic development paths, it can be difﬁcult and inefﬁcient to build systems of\\nmodules and components at varying degrees of maturity. Contrary to traditional thought that switchback events should'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='be suppressed and minimized, in fact they represent a natural and necessary part of the complex technology development\\nprocess – efforts to eliminate them may stiﬂe important innovations without necessarily improving efﬁciency. This is\\na fault of the standard monotonic approaches in AI/ML projects, stage-gate processes, and even the traditional TRL\\nframework.\\nIt is also important to note that most projects do not start at Level 0; very few ML companies engage in this low-level'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='theoretical research. For example, a team looking to use an off-the-shelf object recognition model could start that\\ntechnology at Level 3, and proceed with thorough V&V for their speciﬁc datasets and use-cases. However, no technology\\ncan skip levels after the MLTRL process has been initiated. The industry default (that is, without implementing MLTRL)\\nis to ignorantly take pretrained models, run ﬁne tuning on their speciﬁc data, and jump to deployment, effectively'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='skipping Levels 5 to 7. Additionally, we ﬁnd it is advantageous to incorporate components from other high-TRL ranking\\nprojects while starting new projects; MLTRL makes the veriﬁcation and validation (V&V) steps straightforward for\\nintegrating previously developed ML components.\\n21'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='Evolving people, objectives, and measures\\nAs suggested earlier, much of the practical value of MLTRL comes at the transition between levels. More precisely,\\nMLTRL manages these oft neglected transitions explicitly as evolving teams, objectives, and deliverables. For instance,\\nthe team (or working group) at Level 3 is mostly AI Research Engineers, but at Level 6 is mixed Applied AI/SW'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='Engineers mixed with product managers and designers. Similarly, the review panels evolve from level to level, to match\\nthe changing technology development objectives. What the reviewers reference similarly evolves: notice in the level\\ndeﬁnitions that technical requirements and V&V guide early stages, but at and after Level 6 the product requirements\\nand V&V takeover – naturally, the risk quantiﬁcation and mitigation strategies evolve in parallel. Regarding the'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='deliverables, notably TRL Cards and risk matrices[ 22] (to rank and prioritize various science, technical, and project\\nrisks), the information develops and evolves over time as the technology matures.\\nQuantiﬁable progress\\nBy deﬁning technology maturity in a quantitative way, MLTRL enables teams to accurately and consistently deﬁne\\ntheir ML progress metrics. Notably industry-standard “objectives and key results” (OKRs) and “key performance'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='indicators” (KPIs) [ 78] can be deﬁned as achieving certain readiness levels in a given period of time; this is a preferable\\nmetric in essentially all ML systems which consist of much more than a single performance score to measure progress.\\nEven more, meta-review of MLTRL progress over multiple projects can provide useful insights at the organization\\nlevel. For example, analysis of the time-per-level and the most frequent development paths/cycles can bring to light'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='operational bottlenecks. Compared to conventional software engineering metrics based on sprint stories and tickets, or\\ntime-tracking tools, MLTRL provides a more accurate analysis of ML workﬂows.\\nCommunication and explanation\\nA distinct advantage of MLTRL in practice is the nomenclature: an agreed upon grading scheme for the maturity of\\nan AI technology, and a framework for how/when that technology ﬁts within a product or system, enables everyone'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='to communicate effectively and transparently. MLTRL also acts as a gate for interpretability and explainability–at\\nthe granularity of individual models and algorithms, and more crucially from a holistic, systems standpoint. Notably\\nthe DARPA XAIxiprogram advocates for this advance in developing AI technologies; they suggest interpretability\\nand explainability are necessary at various locations in an AI system to be sufﬁcient for deployment as an AI product,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='otherwise leading to issues with ethics and bias.\\nRobustness via uncertainty-aware ML\\nHow to design a reliable system from unreliable components has been a guiding question in the ﬁelds of computing and\\nintelligence [79]. In the case of AI/ML systems, we aim to build reliable systems with myriad unreliable components:\\nnoisy and faulty sensors, human and AI error, and so on. There is thus signiﬁcant value to quantifying the myriad'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='uncertainties, propagating them throughout a system, and arriving at a notion or measure of reliability. For this reason,\\nalthough MLTRL applies generally to AI/ML methods and systems, we advocate for methods in the class of probabilistic\\nML, which naturally represent and manipulate uncertainty about models and predictions[ 28]. These are Bayesian\\nmethods that use probabilities to represent aleatoric uncertainty , measuring the noise inherent in the observations, and'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='epistemic uncertainty , accounting for uncertainty in the model itself (i.e., capturing our ignorance about which model\\ngenerated the data). In the simplest case, an uncertainty aware ML pipeline should quantify uncertainty at the points of\\nsensor inputs or perception, prediction or model output, and decision or end-user action – McAllister et al.[ 29] suggest\\nthis with Bayesian deep learning models for safer autonomous vehicle pipelines. We can achieve this sufﬁciently well'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='in practice for simple systems. However, we do not yet have a principled, theoretically grounded, and generalizable way\\nof propagating errors and uncertainties downstream and throughout more complex AI systems – i.e., how to integrate\\ndifferent software, hardware, data, and human components while considering how errors and uncertainties propagate\\nthrough the system. This is an important direction of our future work.\\nReferences'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='References\\n[1]Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning\\nthat matters. In AAAI , 2018.\\nxiDARPA Explainable Artiﬁcial Intelligence (XAI)\\n22'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='[2]Arnaud de la Tour, Massimo Portincaso, Kyle Blank, and Nicolas Goeldel. The dawn of the deep tech ecosystem. Technical\\nreport, The Boston Consulting Group, 2019.\\n[3] NASA. The NASA systems engineering handbook. 2003.\\n[4] United States Department of Defense. Defense acquisition guidebook. Technical report, U.S. Dept. of Defense, 2004.\\n[5] D. Leslie. Understanding artiﬁcial intelligence ethics and safety. ArXiv , abs/1906.05684, 2019.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='[6]Google. Machine learning workﬂow. https://cloud.google.com/mlengine/docs/tensorflow/\\nml-solutions-overview . Accessed: 2020-12-13.\\n[7]Alexander Lavin and Gregory Renard. Technology readiness levels for AI & ML. ICML Workshop on Challenges Deploying\\nML Systems , 2020.\\n[8] T. Dasu and T. Johnson. Exploratory data mining and data cleaning. 2003.\\n[9]M. Janssen, P. Brous, Elsa Estevez, L. Barbosa, and T. Janowski. Data governance: Organizing data for trustworthy artiﬁcial'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='intelligence. Gov. Inf. Q. , 37:101493, 2020.\\n[10] B. Shahriari, Kevin Swersky, Ziyu Wang, R. Adams, and N. D. Freitas. Taking the human out of the loop: A review of bayesian\\noptimization. Proceedings of the IEEE , 104:148–175, 2016.\\n[11] Goutham Ramakrishnan, A. Nori, Hannah Murfet, and Pashmina Cameron. Towards compliant data management systems for\\nhealthcare ml. ArXiv , abs/2011.07555, 2020.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='healthcare ml. ArXiv , abs/2011.07555, 2020.\\n[12] Umang Bhatt, Alice Xiang, S. Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura,\\nand P. Eckersley. Explainable machine learning in deployment. Proceedings of the 2020 Conference on Fairness, Accountability,\\nand Transparency , 2020.\\n[13] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and V . Smith. Federated learning: Challenges, methods, and future directions.\\nIEEE Signal Processing Magazine , 37:50–60, 2020.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='IEEE Signal Processing Magazine , 37:50–60, 2020.\\n[14] T. Ryffel, Andrew Trask, M. Dahl, Bobby Wagner, J. Mancuso, D. Rueckert, and J. Passerat-Palmbach. A generic framework\\nfor privacy preserving deep learning. ArXiv , abs/1811.04017, 2018.\\n[15] A. Madry, Aleksandar Makelov, Ludwig Schmidt, D. Tsipras, and Adrian Vladu. Towards deep learning models resistant to\\nadversarial attacks. ArXiv , abs/1706.06083, 2018.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='[16] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. ArXiv , abs/1710.11342, 2018.\\n[17] Marco Túlio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models\\nwith checklist. In ACL, 2020.\\n[18] Xiaoyuan Xie, Joshua W. K. Ho, C. Murphy, G. Kaiser, B. Xu, and T. Chen. Testing and validating machine learning classiﬁers\\nby metamorphic testing. The Journal of systems and software , 84 4:544–558, 2011.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='[19] Alexander D’Amour, K. Heller, D. Moldovan, Ben Adlam, B. Alipanahi, Alex Beutel, C. Chen, Jonathan Deaton, Jacob\\nEisenstein, M. Hoffman, Farhad Hormozdiari, N. Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, M. Lucic,\\nY . Ma, Cory Y . McLean, Diana Mincu, Akinori Mitani, A. Montanari, Zachary Nado, V . Natarajan, C. Nielson, Thomas F.\\nOsborne, R. Raman, K. Ramasamy, Rory Sayres, J. Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, V . Veitch,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='Max Vladymyrov, Xuezhi Wang, K. Webster, S. Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley. Underspeciﬁcation\\npresents challenges for credibility in modern machine learning. ArXiv , abs/2011.03395, 2020.\\n[20] Eric Breck, Shanqing Cai, E. Nielsen, M. Salib, and D. Sculley. The ml test score: A rubric for ml production readiness and\\ntechnical debt reduction. 2017 IEEE International Conference on Big Data (Big Data) , pages 1123–1132, 2017.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='[21] A. Botchkarev. A new typology design of performance metrics to measure errors in machine learning regression algorithms.\\nInterdisciplinary Journal of Information, Knowledge, and Management , 14:045–076, 2019.\\n[22] N. Duijm. Recommendations on the use and design of risk matrices. Safety Science , 76:21–31, 2015.\\n[23] Louise Naud and Alexander Lavin. Manifolds for unsupervised visual anomaly detection. ArXiv , abs/2006.11364, 2020.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='[24] Timnit Gebru, J. Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, H. Wallach, Hal Daumé, and K. Crawford.\\nDatasheets for datasets. ArXiv , abs/1803.09010, 2018.\\n[25] B. Hutchinson, A. Smart, A. Hanna, Emily L. Denton, Christina Greer, Oddur Kjartansson, P. Barnes, and Margaret Mitchell.\\nTowards accountability for machine learning datasets: Practices from software engineering and infrastructure. Proceedings of'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='the 2021 ACM Conference on Fairness, Accountability, and Transparency , 2021.\\n23'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='[26] P. Schulam and S. Saria. Reliable decision support using counterfactual models. In NIPS 2017 , 2017.\\n[27] Towards trustable machine learning. Nature Biomedical Engineering , 2:709–710, 2018.\\n[28] Zoubin Ghahramani. Probabilistic machine learning and artiﬁcial intelligence. Nature , 521:452–459, 2015.\\n[29] Rowan McAllister, Yarin Gal, Alex Kendall, Mark van der Wilk, A. Shah, R. Cipolla, and Adrian Weller. Concrete problems'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='for autonomous vehicle safety: Advantages of bayesian deep learning. In IJCAI , 2017.\\n[30] Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I. Avilés-Rivero,\\nChristian Etmann, Cathal McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, Effrossyni Gkrania-Klotsas,\\nJames H. F. Rudd, Evis Sala, and Carola-Bibiane Schönlieb. Common pitfalls and recommendations for using machine learning'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='to detect and prognosticate for covid-19 using chest radiographs and ct scans. Nature Machine Intelligence , 3:199–217, 2021.\\n[31] J. Tobin, Rachel H Fong, Alex Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep\\nneural networks from simulation to the real world. 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems\\n(IROS) , pages 23–30, 2017.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='(IROS) , pages 23–30, 2017.\\n[32] Arthur Juliani, Vincent-Pierre Berges, Esh Vckay, Yuan Gao, Hunter Henry, M. Mattar, and D. Lange. Unity: A general\\nplatform for intelligent agents. ArXiv , abs/1809.02627, 2018.\\n[33] Stefan Hinterstoißer, Olivier Pauly, Tim Hauke Heibel, Martina Marek, and Martin Bokeloh. An annotation saved is an\\nannotation earned: Using fully synthetic training for object instance detection. ArXiv , abs/1902.09967, 2019.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='[34] Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan Hogins, You-Cyuan Jhang, Mohsen Kamalzadeh,\\nBowen Li, Steven Leal, Pete Parisi, Cesar Romero, Wesley Smith, Alex Thaman, Samuel Warren, and Nupur Yadav. Unity\\nperception: Generate synthetic data for computer vision. CoRR , abs/2107.04259, 2021.\\n[35] K. Cranmer, J. Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of the National Academy\\nof Sciences , 117:30055 – 30062, 2020.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='of Sciences , 117:30055 – 30062, 2020.\\n[36] Jan-Willem van de Meent, Brooks Paige, H. Yang, and Frank Wood. An introduction to probabilistic programming. ArXiv ,\\nabs/1809.10756, 2018.\\n[37] Atilim Günes Baydin, Lei Shao, W. Bhimji, L. Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi,\\nBradley Gram-Hansen, Gilles Louppe, Mingfei Ma, X. Zhao, P. Torr, V . Lee, K. Cranmer, Prabhat, and F. Wood. Etalumis:'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='bringing probabilistic programming to scientiﬁc simulators at scale. Proceedings of the International Conference for High\\nPerformance Computing, Networking, Storage and Analysis , 2019.\\n[38] T. Gleisberg, S. Höche, F. Krauss, M. Schönherr, S. Schumann, F. Siegert, and J. Winter. Event generation with sherpa 1.1.\\nJournal of High Energy Physics , 2009:007–007, 2009.\\n[39] David M. Blei. Build, compute, critique, repeat: Data analysis with latent variable models. 2014.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='[40] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald C. Gall, Ece Kamar, Nachiappan Nagappan, Besmira\\nNushi, and Thomas Zimmermann. Software engineering for machine learning: A case study. 2019 IEEE/ACM 41st International\\nConference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) , 2019.\\n[41] R. Ambrosino, B. Buchanan, G. Cooper, and Marvin J. Fine. The use of misclassiﬁcation costs to learn rule-based decision'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='support models for cost-effective hospital admission strategies. Proceedings. Symposium on Computer Applications in Medical\\nCare , pages 304–8, 1995.\\n[42] Gareth J Grifﬁth, Tim T Morris, Matthew J Tudball, Annie Herbert, Giulia Mancano, Lindsey Pike, Gemma C Sharp, Jonathan\\nSterne, Tom M Palmer, George Davey Smith, et al. Collider bias undermines our understanding of covid-19 disease risk and\\nseverity. Nature communications , 11(1):1–12, 2020.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='[43] J. Pearl. Theoretical impediments to machine learning with seven sparks from the causal revolution. Proceedings of the\\nEleventh ACM International Conference on Web Search and Data Mining , 2018.\\n[44] T. Nguyen, G. Collins, J. Spence, J. Daurès, P. Devereaux, P. Landais, and Y . Le Manach. Double-adjustment in propensity\\nscore matching analysis: choosing a threshold for considering residual imbalance. BMC Medical Research Methodology , 17,\\n2017.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='2017.\\n[45] D. Eckles and E. Bakshy. Bias and high-dimensional adjustment in observational studies of peer effects. ArXiv , abs/1706.04692,\\n2017.\\n[46] Yanbo Xu, Divyat Mahajan, Liz Manrao, A. Sharma, and E. Kiciman. Split-treatment analysis to rank heterogeneous causal\\neffects for prospective interventions. ArXiv , abs/2011.05877, 2020.\\n24'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='[47] Jonathan G Richens, C. M. Lee, and Saurabh Johri. Improving the accuracy of medical diagnosis with causal machine learning.\\nNature Communications , 11, 2020.\\n[48] Andrei Paleyes, Raoul-Gabriel Urma, and N. Lawrence. Challenges in deploying machine learning: a survey of case studies.\\nArXiv , abs/2011.09926, 2020.\\n[49] V . Chernozhukov, D. Chetverikov, M. Demirer, E. Duﬂo, Christian L. Hansen, Whitney K. Newey, and J. Robins. Dou-'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='ble/debiased machine learning for treatment and structural parameters. Econometrics: Econometric & Statistical Methods -\\nSpecial Topics eJournal , 2018.\\n[50] Victor Veitch and Anisha Zaveri. Sense and sensitivity analysis: Simple post-hoc analysis of bias due to unobserved confounding.\\nNeurIPS 2020, arXiv preprint arXiv:2003.01747 , 2020.\\n[51] P. Jenniskens, P.S. Gural, L. Dynneson, B.J. Grigsby, K.E. Newman, M. Borden, M. Koop, and D. Holman. Cams: Cameras for'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='allsky meteor surveillance to establish minor meteor showers. Icarus , 216(1):40 – 61, 2011.\\n[52] Siddha Ganju, Anirudh Koul, Alexander Lavin, J. Veitch-Michaelis, Meher Kasam, and J. Parr. Learnings from frontier\\ndevelopment lab and spaceml - ai accelerators for nasa and esa. ArXiv , abs/2011.04776, 2020.\\n[53] S. Zoghbi, M. Cicco, A. P. Stapper, A. J. Ordonez, J. Collison, P. S. Gural, S. Ganju, J.-L. Galache, and P. Jenniskens. Searching'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='for long-period comets with deep learning tools. In Deep Learning for Physical Science Workshop, NeurIPS , 2017.\\n[54] Peter Jenniskens, Jack Baggaley, Ian Crumpton, Peter Aldous, Petr Pokorny, Diego Janches, Peter S. Gural, Dave Samuels, Jim\\nAlbers, Andreas Howell, Carl Johannink, Martin Breukers, Mohammad Odeh, Nicholas Moskovitz, Jack Collison, and Siddha\\nGanju. A survey of southern hemisphere meteor showers. Planetary and Space Science , 154:21 – 29, 2018.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='[55] D. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active learning with statistical models. In NIPS , 1994.\\n[56] Y . Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. ArXiv , abs/1703.02910, 2017.\\n[57] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young,\\nJean-François Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. In NIPS , 2015.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='[58] P. Abrahamsson, Outi Salo, Jussi Ronkainen, and Juhani Warsta. Agile software development methods: Review and analysis.\\nArXiv , abs/1709.08439, 2017.\\n[59] Marco Kuhrmann, Philipp Diebold, Jürgen Münch, Paolo Tell, Vahid Garousi, Michael Felderer, Kitija Trektere, Fergal\\nMcCaffery, Oliver Linssen, Eckhart Hanser, and Christian R. Prause. Hybrid software and system development in practice:'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='waterfall, scrum, and beyond. Proceedings of the 2017 International Conference on Software and System Process , 2017.\\n[60] Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry,\\nPaul-Christian Burkner, and Martin Modrak. Bayesian workﬂow. ArXiv , abs/2011.01808, 2020.\\n[61] P. Chapman, J. Clinton, R. Kerber, T. Khabaza, T. Reinartz, C. Shearer, and R. Wirth. Crisp-dm 1.0: Step-by-step data mining\\nguide. 2000.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='guide. 2000.\\n[62] Fred Hohman, Kanit Wongsuphasawat, Mary Beth Kery, and Kayur Patel. Understanding and visualizing data iteration in\\nmachine learning. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems , 2020.\\n[63] Saleema Amershi, M. Cakmak, W. B. Knox, and T. Kulesza. Power to the people: The role of humans in interactive machine\\nlearning. AI Mag. , 35:105–120, 2014.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='learning. AI Mag. , 35:105–120, 2014.\\n[64] Eric Breck, Marty Zinkevich, Neoklis Polyzotis, Steven Euijong Whang, and Sudip Roy. Data validation for machine learning.\\n2019.\\n[65] R. Kumar, David R. O’Brien, Kendra Albert, Salomé Viljöen, and Jeffrey Snover. Failure modes in machine learning systems.\\nArXiv , abs/1911.11034, 2019.\\n[66] Inioluwa Deborah Raji, Andrew Smart, Rebecca White, M. Mitchell, Timnit Gebru, B. Hutchinson, Jamila Smith-Loud, Daniel'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='Theron, and P. Barnes. Closing the ai accountability gap: deﬁning an end-to-end framework for internal algorithmic auditing.\\nProceedings of the 2020 Conference on Fairness, Accountability, and Transparency , 2020.\\n[67] R. Miksad and A. Abernethy. Harnessing the power of real-world evidence (rwe): A checklist to ensure regulatory-grade data\\nquality. Clinical Pharmacology and Therapeutics , 103:202 – 205, 2018.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='[68] D. B. Larson, Hugh Harvey, D. Rubin, Neville Irani, J. R. Tse, and C. Langlotz. Regulatory frameworks for development and\\nevaluation of artiﬁcial intelligence–based diagnostic imaging algorithms: Summary and recommendations. Journal of the\\nAmerican College of Radiology , 2020.\\n25'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='[69] Ninareh Mehrabi, Fred Morstatter, N. Saxena, Kristina Lerman, and A. Galstyan. A survey on bias and fairness in machine\\nlearning. ACM Computing Surveys (CSUR) , 54:1 – 35, 2019.\\n[70] Eirini Ntoutsi, P. Fafalios, U. Gadiraju, Vasileios Iosiﬁdis, W. Nejdl, Maria-Esther Vidal, S. Ruggieri, F. Turini, S. Papadopoulos,\\nEmmanouil Krasanakis, I. Kompatsiaris, K. Kinder-Kurlanda, Claudia Wagner, F. Karimi, Miriam Fernández, Harith Alani,'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='B. Berendt, Tina Kruegel, C. Heinze, Klaus Broelemann, Gjergji Kasneci, T. Tiropanis, and Steffen Staab. Bias in data-driven\\nai systems - an introductory survey. ArXiv , abs/2001.09762, 2020.\\n[71] E. Jo and Timnit Gebru. Lessons from archives: strategies for collecting sociocultural data in machine learning. Proceedings of\\nthe 2020 Conference on Fairness, Accountability, and Transparency , 2020.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='[72] J. Wiens, W. Price, and M. Sjoding. Diagnosing bias in data-driven algorithms for healthcare. Nature Medicine , 26:25–26,\\n2020.\\n[73] R. Challen, J. Denny, M. Pitt, L. Gompels, T. Edwards, and K. Tsaneva-Atanasova. Artiﬁcial intelligence, bias and clinical\\nsafety. BMJ Quality & Safety , 28:231 – 237, 2019.\\n[74] Z. Obermeyer, B. Powers, C. V ogeli, and S. Mullainathan. Dissecting racial bias in an algorithm used to manage the health of\\npopulations. Science , 366:447 – 453, 2019.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='populations. Science , 366:447 – 453, 2019.\\n[75] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, In-\\nioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. Proceedings of the Conference on Fairness,\\nAccountability, and Transparency , 2019.\\n[76] Samantha Cruz Rivera, Xiaoxuan Liu, A. Chan, A. K. Denniston, and M. Calvert. Guidelines for clinical trial protocols for'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='interventions involving artiﬁcial intelligence: the spirit-ai extension. Nature Medicine , 26:1351 – 1363, 2020.\\n[77] Z. Szajnfarber. Managing innovation in architecturally hierarchical systems: Three switchback mechanisms that impact practice.\\nIEEE Transactions on Engineering Management , 61:633–645, 2014.\\n[78] H. Zhou and Y . He. Comparative study of okr and kpi. DEStech Transactions on Economics, Business and Management , 2018.'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='[79] J. Neumann. Probabilistic logic and the synthesis of reliable organisms from unreliable components. 1956.\\nAcknowledgements\\nThe authors would like to thank Gur Kimchi, Carl Henrik Ek and Neil Lawrence for valuable discussions about this\\nproject.\\nAuthor contributions statement\\nA.L. conceived of the original ideas and framework, with signiﬁcant contributions towards improving the framework'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='from all co-authors. A.L. initiated the use of MLTRL in practice, including the neuropathology test case discussed here.\\nC.G-L. contributed insight regarding causal AI, including the section on counterfactual diagnosis. C.G-L. also made\\nsigniﬁcant contributions broadly in the paper, notably in the Methods descriptions and paper revisions. Si.G. contributed\\nthe spacecraft test case, along with early insights in the framework deﬁnitions. A.V . contributed to the deﬁnition of'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='later stages involving deployment (as did A.G.), and comparison with traditional software workﬂows. Both E.X. and\\nY .G. provided insights regarding AI in academia, and Y .G. additionally contributed to the uncertainty quantiﬁcation\\nmethods. Su.G. and D.L. contributed the computer vision test case. A.G.B. contributed the particle physics test case,\\nand signiﬁcant reviews of the writeup. A.S. contributed insights related to causal ML and AI ethics. D.N. provided'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='valuable feedback on the overall framework, and contributed signiﬁcantly with the details on “switchback mechanisms”.\\nS.Z. contributed to multiple paper revisions, with emphasis on clarity and applicability to broad ML users and teams.\\nJ.P. contributed to multiple paper revisions, and to deploying the systems ML methods broadly in practice for Earth and\\nspace sciences. –same goes for C.M., with additional feedback overall on the methods. All co-authors discussed the'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='content and contributed to editing the manuscript.\\nCompeting interests\\nThe authors declare no competing interests.\\nAdditional information\\nCorrespondence and requests for materials should be addressed to A.L.\\n26')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "splitter = CharacterTextSplitter(chunk_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'random.pdf', 'page': 0}, page_content='TECHNOLOGY READINESS LEVELS\\nFOR MACHINE LEARNING SYSTEMS\\nAlexander Lavin∗\\nPasteur LabsCiarán M. Gilligan-Lee\\nSpotifyAlessya Visnjic\\nWhyLabsSiddha Ganju\\nNvidiaDava Newman\\nMIT\\nAtılım Güne¸ s Baydin\\nUniversity of OxfordSujoy Ganguly\\nUnity AIDanny Lange\\nUnity AIAmit Sharma\\nMicrosoft Research\\nStephan Zheng\\nSalesforce ResearchEric P. Xing\\nPetuumAdam Gibson\\nKonduitJames Parr\\nNASA Frontier Development Lab\\nChris Mattmann\\nNASA Jet Propulsion LabYarin Gal\\nAlan Turing Institute\\nABSTRACT\\nThe development and deployment of machine learning (ML) systems can be executed easily with\\nmodern tools, but the process is typically rushed and means-to-an-end. The lack of diligence can\\nlead to technical debt, scope creep and misaligned objectives, model misuse and failures, and\\nexpensive consequences. Engineering systems, on the other hand, follow well-deﬁned processes\\nand testing standards to streamline development for high-quality, reliable results. The extreme is\\nspacecraft systems, where mission critical measures and robustness are ingrained in the development\\nprocess. Drawing on experience in both spacecraft engineering and ML (from research through\\nproduct across domain areas), we have developed a proven systems engineering approach for machine\\nlearning development and deployment. Our Machine Learning Technology Readiness Levels (MLTRL)\\nframework deﬁnes a principled process to ensure robust, reliable, and responsible systems while\\nbeing streamlined for ML workﬂows, including key distinctions from traditional software engineering.\\nEven more, MLTRL deﬁnes a lingua franca for people across teams and organizations to work\\ncollaboratively on artiﬁcial intelligence and machine learning technologies. Here we describe the\\nframework and elucidate it with several real world use-cases of developing ML methods from basic\\nresearch through productization and deployment, in areas such as medical diagnostics, consumer\\ncomputer vision, satellite imagery, and particle physics.\\nKeywords: Machine Learning; Systems Engineering; Data Management; Medical AI; Space Sciences\\nIntroduction\\nThe accelerating use of artiﬁcial intelligence (AI) and machine learning (ML) technologies in systems of software,\\nhardware, data, and people introduces vulnerabilities and risks due to dynamic and unreliable behaviors; fundamentally,\\nML systems learn from data, introducing known and unknown challenges in how these systems behave and interact with\\ntheir environment. Currently the approach to building AI technologies is siloed: models and algorithms are developed\\nin testbeds isolated from real-world environments, and without the context of larger systems or broader products they’ll\\nbe integrated within for deployment. A main concern is models are typically trained and tested on only a handful of\\ncurated datasets, without measures and safeguards for future scenarios, and oblivious of the downstream tasks and\\nusers. Even more, models and algorithms are often integrated into a software stack without regard for the inherent\\nstochasticity –for instance, the massive effect random seeds have on deep reinforcement learning model performance\\n[1] – and failure modes of the ML components, which can be dangerously hidden in layers of software and abstraction.\\nOther domains of engineering, such as civil and aerospace, follow well-deﬁned processes and testing standards to\\nstreamline development for high-quality, reliable results. Technology Readiness Level (TRL) is a systems engineering\\nprotocol for deep tech[ 2] and scientiﬁc endeavors at scale, ideal for integrating many interdependent components\\n∗lavin@simulation.science\\nPreprint. Under review.arXiv:2101.03989v2  [cs.LG]  29 Nov 2021'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 1}, page_content='andcross-functional teams of people. It is no surprise that TRL is standard process and parlance in NASA[ 3] and\\nDARPA[4].\\nFor a spaceﬂight project there are several deﬁned phases, from pre-concept to prototyping to deployed operations to\\nend-of-life, each with a series of exacting development cycles and reviews. This is in stark contrast to common machine\\nlearning and software workﬂows, which promote quick iteration, rapid deployment, and simple linear progressions. Yet\\nthe NASA technology readiness process for spacecraft systems is overkill; we need robust ML technologies integrated\\nwith larger systems of software, hardware, data, and humans, but not necessarily for missions to Mars. We aim to bring\\nsystems engineering to AI and ML by deﬁning and putting into action a lean Machine Learning Technology Readiness\\nLevels (MLTRL) framework. We draw on decades of AI and ML development, from research through production,\\nacross domains and diverse data scenarios: for example, computer vision in medical diagnostics and consumer apps,\\nautomation in self-driving vehicles and factory robotics, tools for scientiﬁc discovery and causal inference, streaming\\ntime-series in predictive maintenance and ﬁnance.\\nIn this paper we deﬁne our framework for developing and deploying robust, reliable, and responsible ML and data\\nsystems, with several real test cases of advancing models and algorithms from R&D through productization and\\ndeployment, including essential data considerations. Additionally, MLTRL prioritizes the role of AI ethics and\\nfairness, and our systems AI approach can help curb the large societal issues that can result from poorly deployed and\\nmaintained AI and ML technologies, such as the automation of systemic human bias, denial of individual autonomy,\\nand unjustiﬁable outcomes (see the Alan Turing Institute Report on Ethical AI [5]). The adoption and proliferation of\\nMLTRL provides a common nomenclature and metric across teams and industries. The standardization of MLTRL\\nacross the AI industry should help teams and organizations develop principled, safe, and trusted technologies.\\nFigure 1: MLTRL spans research through prototyping, productization, and deployment. Most ML workﬂows prescribe\\nan isolated, linear process of data processing, training, testing, and serving a model [ 6]. Those workﬂows fail to deﬁne\\nhow ML development must iterate over that basic process to become more mature and robust, and how to integrate with\\na much larger system of software, hardware, data, and people. Not to mention MLTRL continues beyond deployment:\\nmonitoring and feedback cycles are important for continuous reliability and improvement over the product lifetime.\\nResults\\nMLTRL deﬁnes technology readiness levels (TRLs) to guide and communicate AI and ML development and deployment.\\nA TRL represents the maturity of a model or algorithmii, data pipelines, software module, or composition thereof; a\\ntypical ML system consists of many interconnected subsystems and components, and the TRL of the system is the\\nlowest level of its constituent parts [ 7]. The anatomy of a level is marked by gated reviews, evolving working groups,\\nrequirements documentation with risk calculations, progressive code and testing standards, and deliverables such as\\nTRL Cards (Figure 3) and ethics checklists.iiiThese components—which are crucial for implementing the levels in a\\niiNote we use “model” and “algorithm” somewhat interchangeably when referring to the technology under development. The\\nsame MLTRL process and methods apply for a machine translation model and for an A/B testing algorithm, for example.\\niiiTemplates and examples for MLTRL deliverables will be open-sourced upon publication at github.com/alan-turing-institute.\\n2'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 2}, page_content='systematic fashion—as well as MLTRL metrics and methods are concretely described in examples and in the Methods\\nsection. Lastly, to emphasize the importance of data tasks in ML, from data curation [ 8] to data governance [ 9], we\\nstate several important data considerations at each MLTRL level.\\nMACHINE LEARNING TECHNOLOGY READINESS LEVELS\\nThe levels are brieﬂy deﬁned as follows and in Figure 1, and elucidated with real-world examples later.\\nLevel 0 - First Principles This is a stage for greenﬁeld AI research, initiated with a novel idea, guiding question, or\\npoking at a problem from new angles. The work mainly consists of literature review, building mathematical foundations,\\nwhite-boarding concepts and algorithms, and building an understanding of the data – for work in theoretical AI and ML,\\nhowever, there will not yet be data to work with (for example, a novel algorithm for Bayesian optimization[ 10], which\\ncould eventually be used for many domains and datasets). The outcome of Level 0 is a set of concrete ideas with sound\\nmathematical formulation, to pursue through low-level experimentation in the next stage. When relevant, this level\\nexpects conclusions about data readiness, including strategies for getting the data to be suitable for the speciﬁc ML task.\\nTo graduate, the basic principles, hypotheses, data readiness, and research plans need to be stated, referencing relevant\\nliterature. With graduation, a TRL Card should be started to succinctly document the methods and insights thus far –\\nthis key MLTRL deliverable is detailed in the Methods section and Figure 3.\\nLevel 0 data – Not a hard requirement at this stage because this is largely theoretical machine learning. That being said,\\ndata availability needs to be considered for deﬁning any research project to move past theory.\\nLevel 0 review – The reviewer here is solely the lead of the research lab or team, for instance a PhD supervisor. We\\nassess hypotheses and explorations for mathematical validity and potential novelty or utility, not necessarily code nor\\nend-to-end experiment results.\\nLevel 1 - Goal-Oriented Research To progress from basic principles to practical use, we design and run low-level\\nexperiments to analyze speciﬁc model or algorithm properties (rather than end-to-end runs for a performance benchmark\\nscore). This involves collection and processing of sample data to train and evaluate the model. This sample data need\\nnot be the full data; it may be a smaller sample that is currently available or more convenient to collect. In some\\ncases it may sufﬁce to use synthetic data as the representative sample – in the medical domain, for example, acquiring\\ndatasets can take many months due to security and privacy constraints, so generating sample data can mitigate this\\nblocker from early ML development. Further, working with the sample data provides a blueprint for the data collection\\nand processing pipeline (including answering whether it is even possible to collect all necessary data), that can be\\nscaled up for the for the next steps. The experiments, good results or not, and mathematical foundations need to pass a\\nreview process with fellow researchers before graduating to Level 2. The application is still speculative, but through\\ncomparison studies and analyses we start to understand if/how/where the technology offers potential improvements and\\nutility. Code is research-caliber : The aim here is to be quick and dirty, moving fast through iterations of experiments.\\nHacky code is okay, and full test coverage is actually discouraged, as long as the overall codebase is organized and\\nmaintainable. It is important to start semantic versioning practices early in the project lifecycle, which should cover\\ncode, models, anddatasets. This is crucial for retrospectives and reproducibility, issues with which can be costly and\\nsevere at later stages. This versioning information and additional progress should be reported on the TRL Card (see for\\nexample Figure 3).\\nLevel 1 data – At minimum we work with sample data that is representative of downstream real datasets, which can be\\na subset of real data, synthetic data, or both. Beyond driving low-level ML experiments, the sample data forces us to\\nconsider data acquisition and processing strategies at an early stage before it becomes a blocker later.\\nLevel 1 review – The panel for this gated review is entirely members of the research team, reviewing for scientiﬁc rigor\\nin early experimentation, and pointing to important concepts and prior work from their respective areas of expertise.\\nThere may be several iterations of feedback and additional experiments.\\nLevel 2 - Proof of Principle (PoP) Development Active R&D is initiated, mainly by developing and running in\\ntestbeds : simulated environments and/or simulated data that closely matches the conditions and data of real scenarios –\\nnote these are driven by model-speciﬁc technical goals, not necessarily application or product goals (yet). An important\\ndeliverable at this stage is the formal research requirements document (with well-speciﬁed veriﬁcation and validation\\n(V&V) steps)iv. Here is one of several key decision points in the broader process: The R&D team considers several\\npaths forward and sets the course: (A) prototype development towards Level 3, (B) continued R&D for longer-term\\nivArequirement is a singular documented physical or functional need that a particular design, product, or process aims to satisfy.\\nRequirements aim to specify all stakeholders’ needs while not specifying a speciﬁc solution. Deﬁnitions are incomplete without\\n3'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 3}, page_content='research initiatives and/or publications, or some combination of A and B. We ﬁnd the culmination of this stage is often\\na bifurcation: some work moves to applied AI, while some circles back for more research. This common MLTRL cycle\\nis an instance of the non-monotonic discovery switchback mechanism (detailed in the Methods section).\\nLevel 2 data – Datasets at this stage may include publicly available benchmark datasets, semi-simulated data based\\non the data sample in Level 1, or fully simulated data based on certain assumptions about the potential deployment\\nenvironments. The data should allow researchers to characterize model properties, and highlight corner cases or\\nboundary conditions, in order to justify the utility of continuing R&D on the model.\\nLevel 2 review – To graduate from the PoP stage, the technology needs to satisfy research claims made in previous\\nstages (brought to be bare by the aforementioned PoP data in both quantitative and qualitative ways) with the analyses\\nwell-documented and reproducible.\\nLevel 3 - System Development Here we have checkpoints that push code development towards interoperability,\\nreliability, maintainability, extensibility, and scalability. Code becomes prototype-caliber : A signiﬁcant step up from\\nresearch code in robustness and cleanliness. This needs to be well-designed, well-architected for dataﬂow and interfaces,\\ngenerally covered by unit and integration tests, meet team style standards, and sufﬁciently-documented. Note the\\nprogrammers’ mentality remains that this code will someday be refactored/scrapped for productization; prototype code\\nis relatively primitive with regard to efﬁciency and reliability of the eventual system. With the transition to Level 4 and\\nproof-of-concept mode, the working group should evolve to include product engineering to help deﬁne service-level\\nagreements and objectives (SLAs and SLOs) of the eventual production system.\\nLevel 3 data – For the most part consistent with Level 2; in general, the previous level review can elucidate potential\\ngaps in data coverage and robustness to be addressed in the subsequent level. However, for test suites developed at this\\nstage, it is useful to deﬁne dedicated subsets of the experiment data as default testing sources, as well as setup mock\\ndata for speciﬁc functionalities and scenarios to be tested.\\nLevel 3 review – Teammates from applied AI and engineering are brought into the review to focus on sound software\\npractices, interfaces and documentation for future development, and version control for models and datasets. There are\\nlikely domain- or organization-speciﬁc data management considerations going forward that this review should point out\\n– e.g. standards for data tracking and compliance in healthcare [11].\\nLevel 4 - Proof of Concept (PoC) Development This stage is the seed of application-driven development; for many\\norganizations this is the ﬁrst touch-point with product managers and stakeholders beyond the R&D group. Thus TRL\\nCards and requirements documentation are instrumental in communicating the project status and onboarding new\\npeople. The aim is to demonstrate the technology in a real scenario: quick proof-of-concept examples are developed to\\nexplore candidate application areas and communicate the quantitative and qualitative results. It is essential to use real\\nand representative data for these potential applications. Thus data engineering for the PoC largely involves scaling up\\nthe data collection and processing from Level 1, which may include collecting new data or processing all available data\\nusing scaled experiment pipelines from Level 3. In some scenarios there will new datasets brought in for the PoC, for\\nexample, from an external research partner as a means of validation. Hand-in-hand with the evolution from sample to\\nreal data, the experiment metrics should evolve from ML research to the applied setting: proof-of-concept evaluations\\nshould quantify model and algorithm performance (e.g., precision and recall and various data splits), computational\\ncosts (e.g., CPU vs GPU runtimes), and also metrics that are more relevant to the eventual end-user (e.g., number\\nof false positives in the top-N predictions of a recommender system). We ﬁnd this PoC exploration reveals speciﬁc\\ndifferences between clean and controlled research data versus noisy and stochastic real-world data. The issues can\\nbe readily identiﬁed because of the well-deﬁned distinctions between those development stages in MLTRL, and then\\ntargeted for further development.\\nAI ethics processes vary across organizations, but all should engage in ethics conversations at this stage, including ethics\\nof data collection, and potential of any harm or discriminatory impacts due to the model (as the AI capabilities and\\ndatasets are known). MLTRL requires ethics considerations to be reported on TRL Cards at all stages, which generally\\nlink to an extended ethics checklist. The key decision point here is to push onward with application development or not.\\nIt is common to pause projects that pass Level 4 review, waiting for a better time to dedicate resources, and/or pull the\\ntechnology into a different project.\\nLevel 4 data – Unlike the previous stages, having real-world and representative data is critical for the PoC; even with\\nmethods for verifying that data distributions in synthetic data reliably mirror those of real data [], sufﬁcient conﬁdence\\nin the technology must be achieved with real-world data of the use-case. Further, one must consider how to obtain\\ncorresponding measures for veriﬁcation and validation (V&V). Veriﬁcation: Are we building the product right? Validation: Are we\\nbuilding the right product?\\n4'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 4}, page_content='high-quality and consistent data required for the future model inference: generation of the data pipeline PoC that will\\nresemble the future inference pipeline that will take data from intended sources, transform it into features, and send it to\\nthe model for inference.\\nLevel 4 review – Demonstrate the utility towards one or more practical applications (each with multiple datasets), taking\\ncare to communicate assumptions and limitations, and again reviewing data-readiness: evaluating the real-world data\\nfor quality, validity, and availability. The review also evaluates security and privacy considerations – deﬁning these in\\nthe requirements document with risk quantiﬁcation is a useful mechanism for mitigating potential issues (discussed\\nfurther in the Methods section).\\nLevel 5 - Machine Learning “Capability” At this stage the technology is more than an isolated model or algorithm,\\nit is a speciﬁc capability . For instance, producing depth images from stereo vision sensors on a mobile robot is a\\nreal-world capability beyond the isolated ML technique of self-supervised learning for RGB stereo disparity estimation.\\nIn many organizations this represents a technology transition or handoff from R&D to productization. MLTRL\\nmakes this transition explicit, evolving the requisite work, guiding documentation, objectives and metrics, and team;\\nindeed, without MLTRL it is common for this stage to be erroneously leaped completely, as shown in Figure 2. An\\ninterdisciplinary working group is deﬁned, as we start developing the technology in the context of a larger real-world\\nprocess – i.e., transitioning the model or algorithm from an isolated solution to a module of a larger application. Just as\\nthe ML technology should no longer be owned entirely by ML experts, steps have been taken to share the technology\\nwith others in the organization via demos, example scripts, and/or an API; the knowledge and expertise cannot remain\\nwithin the R&D team, let alone an individual ML developer. Graduation from Level 5 should be difﬁcult, as it signiﬁes\\nthe dedication of resources to push this ML technology through productization. This transition is a common challenge\\nin deep-tech, sometimes referred to as “the valley of death” because project managers and decision-makers struggle\\nto allocate resources and align technology roadmaps to effectively move to Level 6, 7 and onward. MLTRL directly\\naddresses this challenge by stepping through the technology transition or handoff explicitly.\\nLevel 5 data – For the most part consistent with Level 4. However, considerations need to be taken for scaling of data\\npipelines: there will soon be more engineers accessing the existing data and adding more, and the data will be getting\\nmuch more use, including automated testing in later levels. With this scaling can come challenges with data governance.\\nThe data pipelines likely do not mirror the structure of the teams or broader organization. This can result in data silos,\\nduplications, unclear responsibilities, and missing control of data over its entire lifecycle. These challenges and several\\napproaches to data governance (planning and control, organizational, and risk-based) are detailed in Janssen et al. [9].\\nLevel 5 review – The veriﬁcation and validation (V&V) measures and steps deﬁned in earlier R&D stages (namely\\nLevel 2) must all be completed by now, and the product-driven requirements (and corresponding V&V) are drafted at\\nthis stage. We thoroughly review them here, and make sure there is stakeholder alignment (at the ﬁrst possible step of\\nproductization, well ahead of deployment).\\nLevel 6 - Application Development The main work here is signiﬁcant software engineering to bring the code up to\\nproduct-caliber : This code will be deployed to users and thus needs to follow precise speciﬁcations, have comprehensive\\ntest coverage, well-deﬁned APIs, etc. The resulting ML modules should be robustiﬁed towards one or more target\\nuse-cases. If those target use-cases call for model explanations, the methods need to be built and validated alongside\\nthe ML model, and tested for their efﬁcacy in faithfully interpreting the model’s decisions – crucially, this needs to be\\nin the context of downstream tasks and the end-users, as there is often a gap between ML explainability that serves\\nML engineers rather than external stakeholders[ 12]. Similarly, we need to develop the ML modules with known data\\nchallenges in mind, speciﬁcally to check the robustness of the model (and broader pipeline) to changes in the data\\ndistribution between development and deployment.\\nThe deployment setting(s) should be addressed thoroughly in the product requirements document, as ML serving (or\\ndeploying) is an overloaded term that needs careful consideration. First, there are two main types: internal, as APIs\\nfor experiments and other usage mainly by data science and ML teams, and external, meaning an ML model that\\nis embedded or consumed within a real application with real users. The serving constraints vary signiﬁcantly when\\nconsidering cloud deployment vs on-premise or hybrid, batch or streaming, open-source solution or containerized\\nexecutable, etc. Even more, the data at deployment may be limited due to compliance, or we may only have access to\\nencrypted data sources, some of which may only be accessible locally – these scenarios may call for advanced ML\\napproaches such as federated learning[ 13] and other privacy-oriented ML[ 14]. And depending on the application, an\\nML model may not be deployable without restrictions; this typically means being embedded in a rules engine workﬂow\\nwhere the ML model acts like an advisor that discovers edge cases in rules. These deployment factors are hardly\\nconsidered in model and algorithm development despite signiﬁcant inﬂuence on modeling and algorithmic choices;\\nthat said, hardware choices typically are considered early on, such as GPU versus edge devices. It is crucial to make\\nthese systems decisions at Level 6–not too early that serving scenarios and requirements are uncertain, and not too late\\n5'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 5}, page_content='that corresponding changes to model or application development risk deployment delays or failures. This marks a key\\ndecision for the project lifecycle, as this expensive ML deployment risk is common without MLTRL (see Figure 2).\\nLevel 6 data – Additional data should be collected and operationalized at this stage towards robustifying the ML\\nmodels, algorithms, and surrounding components. These include adversarial examples to check local robustness [ 15],\\nsemantically-equivalent perturbations to check consistency of the model with respect to domain assumptions [16, 17],\\nand collecting data from different sources and checking how well the trained model generalizes to them. These\\nconsiderations are even more vital in the challenging deployment domains mentioned above with limited data access.\\nLevel 6 review – Focus is on the code quality, the set of newly deﬁned product requirements, system SLA and SLO\\nrequirements, data pipelines spec, and an AI ethics revisit now that we are closer to a real-world use-case. In particular,\\nregulatory compliance is mandated for this gated review; the data privacy and security laws are changing rapidly, and\\nmissteps with compliance can make or break the project.\\nLevel 7 - Integrations For integrating the technology into existing production systems, we recommend the working\\ngroup has a balance of infrastructure engineers andapplied AI engineers – this stage of development is vulnerable\\nto latent model assumptions and failure modes, and as such cannot be safely developed solely by software engineers.\\nImportant tools for them to build together include:\\n•Tests that run use-case speciﬁc critical scenarios and data-slices – a proper risk-quantiﬁcation table will\\nhighlight these.\\n•A “golden dataset” should be deﬁned to baseline the performance of each model and succession of models –see\\nthe computer vision app example in Figure 4–for use in the continuous integration and deployment (CI/CD)\\ntests.\\n•Metamorphic testing : a software engineering methodology for testing a speciﬁc set of relations between the\\noutputs of multiple inputs. When integrating ML modules into larger systems, a codiﬁed list of metamorphic\\nrelations[18] can provide valuable veriﬁcation and validation measures and steps.\\n•Data intervention tests that seek data bugs at various points in the pipelines, downstream to measure the\\npotential effects of data processing and ML on consumers or users of that data, as well as upstream at data\\ningestion or creation. Rather than using model performance as a proxy for data quality, it is crucial to use\\nintervention tests that instead catch data errors with mechanisms speciﬁc to data validation.\\nThese tests in particular help mitigate underspeciﬁcation in ML pipelines, a key obstacle to reliably training models that\\nbehave as expected in deployment[ 19]. On the note of reliability, it is important that quality assurance engineers (QA)\\nplay a key role here and through Level 9, overseeing data processes to ensure privacy and security, and covering audits\\nfor downstream accountability of AI methods.\\nLevel 7 data – In addition to the data for test suites discussed above, this level calls for QA to prioritize data governance :\\nhow data is obtained, managed, used, and secured by the organization. This was earlier suggested in level 5 (in order to\\npreempt related technical debt), and essential here at the main junction for integration, which may create additional\\ngovernance challenges in light of downstream effects and consumers.\\nLevel 7 review – The review should focus on the data pipelines and test suites; a scorecard like the ML Testing\\nRubric[ 20] is useful. The group should also emphasize ethical considerations at this stage, as they may be more\\nadequately addressed now (where there are many test suites put into place) rather than close to shipping later.\\nLevel 8 - Flight-ready The technology is demonstrated to work in its ﬁnal form and under expected conditions.\\nThere should be additional tests implemented at this stage covering deployment aspects, notably A/B tests, blue/green\\ndeployment tests, shadow testing, and canary testing, which enable proactive and gradual testing for changing ML\\nmethods and data. Ahead of deployment, the CI/CD system should be ready to regularly stress test the overall system\\nand ML components. In practice, problems stemming from real-world data are impossible to anticipate and design for –\\nan upstream data provider could change formats unexpectedly or a physical event could cause the customer behavior to\\nchange. Running models in shadow mode for a period of time would help stress test the infrastructure and evaluate how\\nsusceptible the ML model(s) will be to performance regressions caused by data. We observe that ML systems with\\ndata-oriented architectures are more readily tested in this manner, and better surface data quality issues, data drifts, and\\nconcept drifts – this is discussed later in the Beyond Software Engineering section. To close this stage, the key decision\\nis go or no-go for deployment, and when.\\nLevel 8 data – If not already in place, there absolutely needs to be mechanisms for automatically logging data\\ndistributions alongside model performance once deployed.\\n6'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 6}, page_content='Level 8 review – A diligent walkthrough of every technical and product requirement, showing the corresponding\\nvalidations, and the review panel is representative of the full slate of stakeholders.\\nLevel 9 - Deployment In deploying AI and ML technologies, there is signiﬁcant need to monitor the current version,\\nand explicit considerations towards improving the next version. For instance, performance degradation can be hidden\\nand critical, and feature improvements often bring unintended consequences and constraints. Thus at this level, the\\nfocus is on maintenance engineering–i.e., methods and pipelines for ML monitoring and updating. Monitoring for data\\nquality, concept drift, and data drift is crucial; no AI system without thorough tests for these can reliably be deployed.\\nBy the same token there must be automated evaluation and reporting – if actuals[ 21] are available, continuous evaluation\\nshould be enabled, but in many cases actuals come with a delay, so it is essential to record model outputs to allow for\\nefﬁcient evaluation after the fact. To these ends, the ML pipeline should be instrumented to log system metadata, model\\nmetadata, and data itself.\\nMonitoring for data quality issues and data drifts is crucial to catch deviations in model behavior, particularly those that\\nare non-obvious in the model or product end-performance. Data logging is unique in the context of ML systems: data\\nlogs should capture statistical properties of input features and model predictions, and capture their anomalies. With\\nmonitoring for data, concept, and model drifts, the logs are to be sent to the relevant systems, applied, and research\\nengineers. The latter is often non-trivial, as the model server is not ideal for model “observability” because it does not\\nnecessarily have the right data points to link the complex layers needed to analyze and debug models. To this end,\\nMLTRL requires the drift tests to be implemented at stages well ahead of deployment, earlier than is standard practice.\\nAgain we advocate for data-ﬁrst architectures rather than the software industry-standard design by services (discussed\\nlater), which aids in surfacing and logging the relevant data types and slices when monitoring AI systems. For retraining\\nand improving models, monitoring must be enabled to catch training-serving skew and let the team know when to\\nretrain. Towards model improvements, adding or modifying features can often have unintended consequences, such as\\nintroducing latencies or even bias. To mitigate these risks, MLTRL has an embedded switchback here: any component\\nor module changes to the deployed version must cycle back to Level 7 (integrations stage) or earlier. Additionally,\\nfor quality ML products, we stress a deﬁned communication path for user feedback without roadblocks to R&D; we\\nencourage real-world feedback all the way to research, providing valuable problem constraints and perspectives.\\nLevel 9 data – Proper mechanisms for logging and inspecting data (alongside models) is critical for deploying reliable\\nAI and ML – systems that learn on data have unique monitoring requirements (detailed above). In addition to the\\ninfrastructure and test suites covering data and environment shifts, it’s important for product managers and other owners\\nto be on top of data policy shifts in domains such as ﬁnance and healthcare.\\nLevel 9 review – The review at this stage is unique, as it also helps in lifecycle management: at a regular cadence\\nthat depends on the deployed system and domain of use, owners and other stakeholders are to revisit this review and\\nrecommend switchbacks if needed (discussed in the Methods section). This additional oversight at deployment is\\nshown to help deﬁne regimented release cycles of updated versions, and provide another “eye” check for stale model\\nperformance or other system abnormalities.\\nNotice MLTRL is deﬁned as stages or levels, yet much of the value in practice is realized in the transitions: MLTRL\\nenables teams to move from one level to the next reliably and efﬁciently, and provides a guide for how teams and\\nobjectives evolve with the progressing technology.\\nDiscussion\\nMLTRL is designed to apply to many real-world use-cases involving data and ML, from simple regression models\\nused for predictive modeling energy demand or anomaly detection in datacenters, to real-time modeling in rideshare\\napplications and motion planning in warehouse robotics. For simple use-cases MLTRL may be overkill, and a subset\\nmay sufﬁce – for instance, model cards as demonstrated by Google for basic image classiﬁcation. Yet this is a ﬁne line,\\nas the same cards-only approach in the popular “Huggingface” codebases are too simplistic for the language models\\nthey represent, deployed in domains that carry signiﬁcant consequences. MLTRL becomes more valuable with more\\ncomplex, larger systems and environments, especially in risk averse domains. We thoroughly discuss this through\\nseveral real uses of MLTRL below.\\n7'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 7}, page_content='Figure 2: Most ML and AI projects live in these sections of MLTRL, not concerned with fundamental R&D – that is,\\ncompletely using existing methods and implementations, and even pretrained models. In the left diagram, the arrows\\nshow a common development pattern with MLTRL in industry: projects go back to the ML toolbox to develop new\\nfeatures (dashed line), and frequent, incremental improvements are often a practice of jumping back a couple levels to\\nLevel 7 (which is the main systems integrations stage). At Levels 7 and 8 we stress the need for tests that run use-case\\nspeciﬁc critical scenarios and data-slices, which are highlighted by a proper risk-quantiﬁcation matrix [ 22]. Cycling\\nback to previous lower levels is not just a late-stage mechanism in MLTRL, but rather “switchbacks” occur throughout\\nthe process (as discussed in the Methods section and throughout the text). In the right diagram we show the more\\ncommon approach in industry ( without using our framework), which skips essential technology transition stages – ML\\nEngineers push straight through to deployment, ignoring important productization and systems integration factors. This\\nwill be discussed in more detail in the Methods section.\\nEXAMPLES\\nHuman-machine visual inspection\\nWhile most ML projects begin with a speciﬁc task and/or dataset, there are many that originate in ML theory without\\nany target application – i.e., projects starting MLTRL at level 0 or 1. These projects nicely demonstrate the utility of\\nMLTRL built-in switchbacks, bifurcating paths, and iteration with domain experts. An example we discuss here is a\\nnovel approach to representing data in generative vision models from Naud & Lavin[ 23], which was then developed into\\nstate-of-the-art unsupervised anomaly detection, and targeted for two human-machine visual inspection applications:\\nFirst, industrial anomaly detection, notably in precision manufacturing, to identify potential errors for human-expert\\nmanual inspection. Second, using the model to improve the accuracy and efﬁciency of neuropathology, the microscopic\\nexamination of neurosurgical specimens for cancerous tissue. In these human-machine teaming use-cases there are\\nspeciﬁc challenges impeding practical, reliable use:\\n•Hidden feedback loops can be common and problematic in real-world systems inﬂuencing their own training\\ndata: over time the behavior of users may evolve to select data inputs they prefer for the speciﬁc AI system,\\nrepresenting some skew from the training data. In this neuropathology case, selecting whole-slide images that\\nare uniquely difﬁcult for manual inspection, or even biased by that individual user. Similarly we see underlying\\nhealthcare processes can act as hidden confounders, resulting in unreliable decision support tools[26].\\n•Model availability can be limited in many deployment settings: for example, on-premises deployments\\n(common in privacy preserving domains like healthcare and banking), edge deployments (common in industrial\\nuse-cases such as manufacturing and agriculture), or from the infrastructure’s inability to scale to the volume\\nof requests. This can severely limit the team’s ability to monitor, debug, and improve deployed models.\\n•Uncertainty estimation is valuable in many AI scenarios, yet not straightforward to implement in practice.\\nThis is further complicated with multiple data sources and users, each injecting generally unknown amounts of\\nnoise and uncertainties. In medical applications it is of critical importance, to provide measures of conﬁdence\\nand sensitivity, and for AI researchers through end-users. In anomaly detection, various uncertainty measures\\ncan help calibrate the false-positive versus false-negative rates, which can be very domain speciﬁc.\\n8'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 8}, page_content='Figure 3: The maturity of each ML technology is tracked via TRL Cards , which we describe in the Methods section.\\nHere is an example reﬂecting a neuropathology machine vision use-case[ 23], detailed in the Discussion Section. Note\\nthis is a subset of a full TRL Card, which in reality lives as a full document in an internal wiki. Notice the card\\nclearly communicates the data sources, versions, and assumptions. This helps mitigate invalid assumptions about\\nperformance and generalizability when moving from R&D to production, and promotes the use of real-world data\\nearlier in the project lifecycle. We recommend documenting datasets thoroughly with semantic versioning and tools\\nsuch as datasheets for datasets [24], and following data accountability best-practices as they evolve (see [25]).\\n•Costs of edge cases can be signiﬁcant, sometimes risking expensive machine downtime or medical failures.\\nThis is exacerbated in anomaly detection anomalies are by deﬁnition rare so they can be difﬁcult to train for,\\nespecially for the anomalies that are completely unseen until they arise in the wild.\\n•End-user trust can be difﬁcult to achieve, often preventing the adoption of ML applications, particularly in\\nthe healthcare domain and other highly regulated industries.\\nThese and additional ML challenges such as data privacy and interpretability can inhibit ML adoption in clinical practice\\nand industrial settings, but can be mitigated with MLTRL processes. We’ll describe how in the context of the Naud\\n& Lavin[ 23] example, which began at level 0 with theoretical ML work on manifold geometries, and at level 5 was\\ndirected towards specialized human-machine teaming applications utilizing the same ML method under-the-hood.\\n•Levels 0-1 – From open-ended exploration of data-representation properties in various Riemmanian manifold\\ncurvatures, we derived from ﬁrst principles and empirically identiﬁed a property with hyperbolic manifolds:\\nwhen used as a latent space for embedding data without labels, the geometry organizes the data by it’s implicit\\nhierarchical structure. Unsupervised computer vision was identiﬁed in reviews as a promising direction for\\nproof-of-principle work.\\n•Level 2 – One approach for validating the earlier theoretical developments was to generate synthetic data to\\nisolate very speciﬁc features in data we would expect represented in the latent manifold. The results showed\\npromise for anomaly detection – using the latent representation of data to automatically identify images that\\nare out-of-the-ordinary (anomalous), and also using the manifold to inspect how they are semantically different.\\nFurther, starting with an implicitly probabilistic modeling approach implied uncertainty estimation could be\\na valuable feature downstream. This made the level 2 key decision point clear: proceed with applied ML\\ndevelopment.\\n•Levels 3-5 – Proof-of-concept development and reviews demonstrated promise for several commercial appli-\\ncations relevant to the business, and also highlighted the need for several key features (deﬁned as R&D and\\nproduct requirements): interpretability (towards end-user trust), uncertainty quantiﬁcation (to show conﬁdence\\nscores), and human-in-the-loop (for domain expertise). Without the MLTRL PoC steps and review processes,\\nthese features can often be delayed until beta testing or overlooked completely – for example, the failures of\\napplying IBM Watson in medical applications [ 27]. For this technology, the applications to develop towards\\nare anomaly detection in histopathology and manufacturing, speciﬁcally inspecting whole-slide images of\\nneural tissue, and detecting defects in metallic surfaces, respectively.\\n9'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 9}, page_content='From the systems perspective, we suggest quantifying the uncertainties of components and propagating them\\nthrough the system, which can improve safety and trust. Probabilistic ML methods, rooted in Bayesian\\nprobability theory, provide a principled approach to representing and manipulating uncertainty about models\\nand predictions[ 28]. For this reason we advocate strongly for probabilistic models and algorithms in AI\\nsystems. In this machine vision example, the MLTRL technical requirements speciﬁcally called for a\\nprobabilistic generative model to readily quantify various types of uncertainties and propagate them forward to\\nthe visualization component of the pipeline, and the product requirements called for the downstream conﬁdence\\nand sensitivity measures to be exposed to the end-user. Component uncertainties must be assembled in a\\nprincipled way to yield a meaningful measure of overall system uncertainty, based on which safe decisions can\\nbe made[29]. See the Methods section for more on uncertainty in AI systems.\\nThe early checks for data management and governance proved valuable here, as the application areas dealt\\nwith highly sensitive data that would signiﬁcantly inﬂuence the design of data pipelines and test suites. In\\nboth the neuropathology and manufacturing applications, the data management checks also raised concerns\\nabout hidden feedback loops, where users may unintentionally skew the data inputs when using the anomaly\\ndetection models in practice, for instance biasing the data towards speciﬁc subsets they subjectively need help\\nwith. Incorporating domain experts this early in the project lifecycle helped inform veriﬁcation and validation\\nsteps to help be robust to the hidden feedback loops. Not to mention their input guided us towards user-centric\\nmetrics for performance, which can often skew from ML metrics in important ways – for instance, the typical\\nacceptance ratio for false positives versus false negatives doesn’t apply to select edge cases, for which our\\nhierarchical anomaly classiﬁcation scheme was useful [23].\\nFrom prior reviews and TRL card documentation, we also identiﬁed the value of synthetic data generation\\ninto application development: anomalies are by deﬁnition rare so they are hard to come by in real datasets,\\nespecially with evolving environments in deployment settings, so the ability to generate synthetic datasets for\\nanomaly detection can accelerate the level 6-9 pipeline, and help ensure more reliable models in the wild.\\n•Level 6 (medical) – The medical inspection application experienced a bifurcation with product work proceed-\\ning while additional R&D was desired to explore improved data processing methods, while engaging with\\nclinicians and medical researchers for feedback. Proceeding through the levels in a non-linear, non-monotonic\\nway is common in MLTRL and encouraged by various switchback mechanisms (detailed in the Methods\\nsection). These practices – intentional switchbacks, frequent engagement with domain experts and users – can\\nhelp mitigate methodological ﬂaws and underlying biases that are common when applying ML to clinical\\napplications. For instance, recent work by Roberts et al. [ 30] investigated 2,122 studies applying ML to\\nCOVID-19 use-cases, ﬁnding that none of the models are sufﬁcient for clinical use due to methodological ﬂaws\\nand/or underlying biases. They go on to give many recommendations – some we’ve discussed in the context of\\nMLTRL, and more – which should be reviewed for higher quality medical-ML models and documentation.\\n•Level 6-9 (manufacturing) – Overall these stages proceeded regularly and efﬁciently for the defect detection\\nproduct. MLTRL’s embedded switchback from level 9 to 4 proved particularly useful in this lifecycle, both\\nfor incorporating feedback from the ﬁeld and for updating with research progress. On the former, the data\\ndistribution shifts from one deployment setting to another signiﬁcantly affected false-positive versus false-\\nnegative calibrations, so this was added as a feature to the CI/CD pipelines. On the latter, the built-in touch\\npoints for real-world feedback and data into the continued ML research provided valuable constraints to\\nhelp guide research, and product managers could readily understand what capabilities could be available for\\nproduct integration and when (readily communicated with TRL Cards) – for instance, later adding support for\\nvideo-based inspection for defects, and tooling for end-users to reason about uncertainty estimates (which\\nhelps establish trust).\\n•Level 7-9 (medical) – For productization the “neuropathology copilot” was handed off to a partner pharmaceu-\\ntical company to integrate into their existing software systems. The MLTRL documentation and communication\\nstreamlined the technology transfer, which can often by a time-consuming manual process. If not pursuing\\nthis path, the product would’ve likely faced many of the medical-ML deployment challenges with model\\navailability and data access; MLTRL cannot overcome the technical challenges of deploying on-premises, but\\nthe manifestation of those challenges as performance regressions, data shifts, privacy and ethics concerns, etc.\\ncan be mitigated by the system-level checks and strategies MLTRL puts forth.\\nComputer vision with real and synthetic data\\nAdvancements in physics engines and graphics processing have advanced AI environment and data-generation capabili-\\nties, putting increased emphasis on transitioning models across the simulation-to-reality gap [ 31,32,33]. To develop a\\ncomputer vision application for automated recycling, we leveraged the Unity Perception [ 34] package, a toolkit for\\ngenerating large-scale datasets for perception-based ML training and validation. We produced synthetic images to\\n10'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 10}, page_content='\\x15A?U?HEJC\\x03?H=OOEBE?=PEKJ\\x03LELAHEJA\\x03\\n\\xa0=¡\\xa0>¡\\n,V\\x03PRGHO\\x03FRQğGHQFH\\x03\\x1f\\x03WKUHVKROG\",V\\x03GHWHFWHG\\x03REMHFW\\x03LQ\\x03WDUJHW\\x03VHW\"3URYLGH\\x03FRUUHVSRQGLQJ\\x03UHF\\\\FOLQJ\\x03LQVWUXFWLRQV,QLWLDWH\\x03KXPDQ\\x10\\x03LQ\\x10WKH\\x10ORRS\\x03SURWRFRO12<(6<(6Figure 4: Computer vision pipeline for an automated recycling application (a), which contains multiple ML models,\\nuser input, and image data from various sources. Complicated logic such as this can mask ML model performance lags\\nand failures, and also emphasized the need for R&D-to-product hand off described in MLTRL. Additional emphasis is\\nplaced on ML tests that consider the mix of real-world data with user annotations (b, right) and synthetic data generated\\nby Unity AI’s Perception tool and structured domain randomization (b, left).\\ncomplement real-world data sources (Figure 4). This application exempliﬁes three important challenges in ML product\\ndevelopment that MLTRL helps overcome:\\n•Multiple and disparate data sources are common in deployed ML pipelines yet often ignored in R&D.\\nFor instance, upstream data providers can change formats unexpectedly, or a physical event could cause the\\ncustomer behavior to change. It is nearly impossible to anticipate and design for all potential problems with\\nreal-world data and deployment. This computer vision system implemented pipelines and extended test suites\\nto cover open-source benchmark data, real user data, and synthetic data.\\n•Hidden performance degradation can be challenging to detect and debug in ML systems because gradual\\nchanges in performance may not be immediately visible. Common reasons for this challenge are that the\\nML component may be one step in a series. Additionally, local/isolated changes to an ML component’s\\nperformance may not directly affect the observed downstream performance. We can see both issues in the\\nillustrated logic diagram for the automated recycling app (Figure 4). A slight degradation in the initial CV\\nmodel may not heavily inﬂuence the following user input. However, when an uncommon input image appears\\nin the future, the app fails altogether.\\n•Model usage requirements can make or break an ML product. For example, the Netﬂix “$1M Prize” solution\\nwas never fully deployed because of signiﬁcant engineering costs in real-world scenariosv. For example,\\nengineering teams must communicate memory usage, compute power requirements, hardware availability,\\nnetwork privacy, and latency to the ML teams. ML teams often only understand the statistics or ML theory\\nbehind a model but not the system requirements or how it scales.\\nWe next elucidate these challenges and how MLTRL helps overcome them in the context of this project’s lifecycle. This\\nproject started at level 4, using largely existing ML methods with a target use-case.\\n•Level 4 – For this project, we validated most of the components in other projects. Speciﬁcally, the computer\\nvision (CV) model for object recognition and classiﬁcation was an off-the-shelf model. The synthetic data\\ngeneration method used Unity Perception, a well-established open-source project. Though this allowed us to\\nvnetﬂixtechblog.com/netﬂix-recommendations-beyond-the-5-stars-part-1\\n11'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 11}, page_content='skip the earlier levels, many challenges arise when combining ML elements that were independently validated\\nand developed. The MLTRL prototype-caliber code checkpoint ensures that the existing code components\\nare validated and helps avoid poorly deﬁned borders and abstractions between components. ML pipelines\\noften grow out of glue code, and our regimented code checkpoints motivate well-architected software that\\nminimizes these danger spots.\\n•Level 5 – The problematic “valley of death”, mentioned earlier in the level 5 deﬁnitions, is less prevalent in use-\\ncases like this that start at a higher MLTRL level with a speciﬁc product deliverable. In this case, the product\\ndeliverable was a real-time object recognition and classiﬁcation of trash for a mobile recycling application.\\nStill, this stage is critical for the requirements and V&V transition. This stage mitigates failure risks due to the\\ndisparate data sources integrated at various steps in this CV system and accounted for the end-user compute\\nconstraints for mobile computing. Speciﬁcally, the TRL cards from earlier stages surfaced potential issues\\nwith imbalanced datasets and the need for speciﬁc synthetic images. These considerations are essential for the\\ndata readiness and testing V&V in the productization requirements. Data quality and availability issues often\\npresent huge blockers because teams discover them too late in the game. Data-readiness is one class of many\\nexample issues teams face without MLTRL, as depicted in Fig. 2.\\n•Level 6 – We were re-using a well-understood model and deployment pipeline in this use-case, meaning our\\nprimary challenge was around data reliability. For the problem of recognizing and classifying trash, building a\\nreliable data source using only real data is almost impossible due to diversity, class imbalance, and annotation\\nchallenges. Therefore we chose to develop a synthetic data generator to create training data. At this MLTRL\\nlevel, we needed to ensure that the synthetic data generator created sufﬁciently diverse data and exposed the\\ncontrols needed to alter the data distribution in production. Therefore, we carefully exposed APIs using the\\nUnity Perception package, which allowed us to control lighting, camera parameters, target and non-target\\nobject placements and counts, and background textures. Additionally, we ensured that the object labeling\\nmatched the real-world annotator instructions and that output data formats matched real-world counterparts.\\nLastly, we established a set of statistical tests to compare synthetic and real-world data distributions. The\\nMLTRL checks ensured that we understood, and in this case, adequately designed our data sources to meet\\nin-production requirements.\\n•Level 7 – From the previous level’s R&D TRL cards and observations, we knew relatively early in produc-\\ntization that we would need to assume bias for the real data sources due to class imbalance and imperfect\\nannotations. Therefore we designed tests to monitor these in the deployed application. MLTRL imposes these\\ncritical deployment tests well ahead of deployment, where we can easily overlook ML-speciﬁc failure modes.\\n•Level 8 – As we suggested earlier, problems that stem from real-world data are near impossible to anticipate\\nand design for, implying the need for level 8 ﬂight-readiness preparations. Given that we were generating\\nsynthetic images (with structured domain randomization) to complement the real data, we created tests for\\ndifferent data distribution shifts at multiple points in the classiﬁcation pipeline. We also implemented thorough\\nshadow tests ahead of deployment to evaluate how susceptible the ML model(s) to performance regressions\\ncaused by data. Additionally, we also implemented these as CI/CD tests over various deployment scenarios (or\\nmobile device computing speciﬁcations). Without these fully covered, documented, and automated, it would\\nbe impossible to pass level 8 review and deploy the technology.\\n•Level 9 – Post-deployment, the monitoring tests prescribed at Levels 8 and 9 and the three main code quality\\ncheckpoints in the MLTRL process help surface hidden performance degradation problems, common with\\ncomplex pipelines of data ﬂows and various models. The switchbacks depicted in Fig. 2 are typical in CV\\nuse-cases. For instance, miscalibrations in models pre-trained on synthetic data and ﬁne-tuned on newer real\\ndata can be common yet difﬁcult to catch. However, the level 7 to 4 switchback is designed precisely for these\\nchallenges and product improvements.\\nAccelerating scientiﬁc discovery with massive particle physics simulators\\nComputational models and simulation are key to scientiﬁc advances at all scales, from particle physics, to material\\ndesign and drug discovery, to weather and climate science, and to cosmology[ 35]. Many simulators model the forward\\nevolution of a system (coinciding with the arrow of time), such as the interaction of elementary particles, diffusion of\\ngasses, folding of proteins, or evolution of the universe in the largest scale. The task of inference refers to ﬁnding initial\\nconditions or global parameters of such systems that can lead to some observed data representing the ﬁnal outcome\\nof a simulation. In probabilistic programming[ 36], this inference task is performed by deﬁning prior distributions\\nover any latent quantities of interest, and obtaining posterior distributions over these latent quantities conditioned\\non observed outcomes (for example, experimental data) using Bayes rule. This process, in effect, corresponds to\\ninverting the simulator such that we go from the outcomes towards the inputs that caused the outcomes. In the\\n“Etalumis” project[ 37] (“simulate” spelled backwards), we are using probabilistic programming methods to invert\\n12'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 12}, page_content='existing, large-scale simulators via Bayesian inference. The project is as an interdisciplinary collaboration of specialists\\nin probabilistic machine learning, particle physics, and high-performance computing, all essential elements to achieve\\nthe project outcomes. Even more, it is a multi-year project spanning multiple countries, companies, university labs, and\\ngovernment research organizations, bringing signiﬁcant challenges in project management, technology coordination\\nand validation. Aided by MLTRL, there were several key challenges to overcome in this project that are common in\\nscientiﬁc-ML projects:\\n•Integrating with legacy systems is common in scientiﬁc and industrial use-cases, where ML methods are\\napplied with existing sensor networks, infrastructure, and codebases. In this case, particle physics domain\\nexperts at CERN are using the SHERPA simulator[ 38], a 1 million line codebase developed over the last\\ntwo decades. Rewriting the simulator for ML use-cases is infeasible due to the codebase size and buried\\ndomain knowledge, and new ML experts would need signiﬁcant onboarding to gain working knowledge of\\nthe codebase. It is also common to work with legacy data infrastructure, which can be poorly organized for\\nmachine learning (let alone preprocessed and clean) and unlikely to have followed best practices such as\\ndataset versioning.\\n•Coupling hardware and software architectures is non-trivial when deploying ML at scale, as performance\\nconstraints are often considered in deployment tests well after model and algorithm development, not to\\nmention the expertise is often split across disjoint teams. This can be exacerbated in scientiﬁc-ML when\\nscaling to supercomputing infrastructure, and working with massive datasets that can be in the terabytes and\\npetabytes.\\n•Interpretability is often a desired feature yet difﬁcult to deliver and validate in practice. Particularly in\\nscientiﬁc ML applications such as this, mechanisms and tooling for domain experts to interpret predictions\\nand models are key for usability (integrating in workﬂows and building trust).\\nTo this end, we will go through the MLTRL levels one by one, demonstrating how they ensure the above scientiﬁc ML\\nchallenges are diligently addressed.\\n•Level 0 – The theoretical developments leading to Etalumis are immense and well discussed in Baydin et\\nal [37]. In particular the ML theory and methods are in a relatively nascent area of ML and mathematics,\\nprobabilistic programming. New territory can present more challenges compared to well-traveled research\\npaths, for instance in computer vision with neural networks. It is thus helpful to have a guiding framework\\nwhen making a new path in ML research, such as MLTRL where early reviews help theoretical ML projects\\nget legs.\\n•Level 1-2 – Running low-level experiments in simple testbeds is generally straightforward when working\\nwith probabilistic programming and simulation; in a sense, this easy iteration over experiments is what\\nPPL are designed for. It was additionally helpful in this project to have rich data grounded in physical\\nconstraints, allowing us to better isolate model behaviors (rather than data assumptions and noise). The\\nMLTRL requirements documentation is particularly useful for the standard PPL experimentation workﬂow:\\nmodel, infer, criticize, repeat (or Box’s loop) [ 39]. The evaluation step (i.e. criticizing the model) can be\\nmore nuanced than checking summary statistics as in deep learning and similar ML workﬂows. It is thus a\\nuseful practice to write down the criticism methods, metrics, and expected results as veriﬁcations for speciﬁc\\nresearch requirements, rather than iterating over Box’s loop without a priori targets. Further, because this\\nresearch project had a speciﬁc target application early in the process (the SHERPA simulator), the project\\ntimeline beneﬁted from recognizing simulator-integration constraints upfront as requirements, not to mention\\ndata availability concerns, which are often overlooked in early R&D levels. It was additionally useful to have\\nCERN scientists as domain experts in the reviews at these R&D levels.\\n•Level 3 – Systems development can be challenging with probabilistic programming, again because it is\\nrelatively nascent and much of the out-of-the-box tools and infrastructure are not there as in most ML and\\ndeep learning. Here in particular there’s a novel (unproven) approach for systems integration: a probabilistic\\nprogramming execution protocol was developed to reroute random number draws in the stochastic simulator\\ncodebase (SHERPA) to the probabilistic programming system, thus enabling the system to control stochastic\\nchoices in SHERPA and run inference on its execution traces, all while keeping the legacy codebase intact! A\\nmore invasive method that modiﬁes SHERPA would not have been acceptable. If it were not for MLTRL forcing\\nsystems considerations this early in the Etalumis project lifecycle, this could have been an insurmountable\\nhurdle later when multiple codebases and infrastructures come into play. By the same token, systems planning\\nhere helped enable the signiﬁcant HPC scaling later: the team deﬁned the need for HPC support well ahead\\nof actually running HPC, in order to build the prototype code in a way that would readily map to HPC (in\\naddition to local or cloud CPU and GPU). The data engineering challenges in this system’s development\\n13'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 13}, page_content='nonetheless persist – that is, data pipelines and APIs that can integrate various sources and infrastructures, and\\nnormalize data from various databases – although MLTRL helps consider these at the an earlier stage that can\\nhelp inform architecture design.\\n•Level 4 – The natural “embedded switchback” from Level 4 to 2 (see the Methods section) provided an efﬁcient\\npath toward developing an improved, amortized inference method–i.e., using a computationally expensive\\ndeep learning based inference algorithm to train only once, in order to then do fast, repeated inference in the\\nSHERPA model. Leveraging cyclic R&D methods, the Etalumis project could iteratively improve inference\\nmethods without stalling the broader system development, ultimately producing the largest scale posterior\\ninference in a Turing-complete probabilistic programming system. Achieving this scale through iterative R&D\\nalong the main project lifecycle was additionally enabled by working with with NERSC engineers and their\\nCori supercomputer to progressively scale smaller R&D tests to the goal supercomputing deployment scenario.\\nTypical ML workﬂows that follow simple linear progressions[ 6,40] would not enable ramping up in this\\nfashion, and can actual prevent scaling R&D to production due to lack of systems engineering processes (like\\nMLTRL) connecting research to deployment.\\n•Level 5 – Multi-org international collaborations can be riddled with communication and teamwork issues,\\nin particular at this pivotal stage where teams transition from R&D to application and product development.\\nFirst, MLTRL as a lingua franca was key to the team effort bringing Etalumis proof-of-concept into the\\nlarger effort of applying it to massive high-energy physics simulators. It was also critical at this stage to\\nclearly communicate end-user requirements across the various teams and organizations, which must be deﬁned\\nin MLTRL requirements docs with V&V measures – the essential science-user requirements were mainly\\nfor model and prediction interpretability, uncertainty estimation, and code usability. If there are concerns\\nover these features, MLTRL switchbacks can help to quickly cycle back and improve modeling choices in a\\ntransparent, efﬁcient way – generally in ML projects, these fundamental issues with usability are caught too\\nlate, even after deployment. In the probabilistic generative model setting we’ve deﬁned in Etalumis, Bayesian\\ninference gives results that are interpretable because they include exact locations and processes in the model\\nthat are associated with each prediction. Working with ML methods that are inherently interpretable, we are\\nwell-positioned to deliver interpretable interfaces for the end-users later in the project lifecycle.\\n•Level 6-9 – The standard MLTRL protocol apply in these application-to-deployment stages, with several\\nEtalumis-speciﬁc highlights. First, given the signiﬁcant research contributions in both probabilistic pro-\\ngramming and scientiﬁc-ML, it’s important to share the code publicly. The development and deployment\\nof the open-source code repository PPXvibranched into a separate MLTRL path from the Etalumis path\\nfor deployment at CERN. It’s useful to have systems engineering enable clean separation of requirements,\\ndeployments, etc. when there are different development and product lifecycles originating from a common\\nparent project. For example, in this case it was useful to employ MLTRL switchbacks in the open-sourcing\\nprocess, isolated from the CERN application paths, in order to add support for additional programming\\nlanguages so PPX can apply to more scientiﬁc simulators – both directions beneﬁted signiﬁcantly the from\\nthe data pipelines considerations brought up levels earlier, where open-sourcing required different data APIs\\nand data transformations to enable broad usability. Second, related to the open-source code deliverable and\\nthe scientiﬁc ML user requirements we noted above, the late stages of MLTRL reviews include higher level\\nstakeholders and speciﬁc end-users, yet again enforcing these scientiﬁc usability requirements are met. An\\nexample result of this in Etalumis is the ability to output human-readable execution traces of the SHERPA\\nruns and inference, enabling never before possible step-by-step interpretability of the black-box simulator.\\nThe scientiﬁc ML perspective additionally brings to forefront an end-to-end data perspective that is pertinent in\\nessentially all ML use-cases: these systems are only useful to the extent they provide comprehensive data analyses that\\nintegrate the data consumed and generated in these workﬂows, from raw domain data to machine-learned models. These\\ndata analyses drive reproducibility, explainability, and experiment data understanding, which are critical requirements\\nin scientiﬁc endeavors and ML broadly.\\nCausal inference & ML in medicine\\nUnderstanding cause and effect relationships is crucial for accurate and actionable decision-making in many settings,\\nfrom healthcare and epidemiology, to economics and government policy development. Unfortunately, standard\\nmachine learning algorithms can only ﬁnd patterns and correlation in data, and as correlation is not causation, their\\npredictions cannot be conﬁdently used for understanding cause and effect. Indeed, relying on correlations extracted\\nfrom observational data to guide decision-making can lead to embarrassing, costly, and even dangerous mistakes,\\nsuch as concluding that asthma reduces pneumonia mortality risk [ 41], and that smoking reduces risk of developing\\nvigithub.com/pyprob/ppx\\n14'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 14}, page_content='severe COVID-19 [ 42]. Fortunately, there has been much recent development in a ﬁeld known as causal inference that\\ncan quantitatively make sense of cause and effect from purely observational data[ 43]. The ability of causal inference\\nalgorithms to quantify causal impact rests on a number of important checks and assumptions–beyond those employed\\nin standard machine learning or purely statistical methodology–that must be carefully deliberated over during their\\ndevelopment and training. These speciﬁc checks and assumptions are as follows:\\n•Specifying cause-and-effect relationships between relevant variables– One of the most important assump-\\ntions underlying causal inference is the structure of the causal relations between quantities of interest. The\\ngold standard for determining causal relations is to perform a randomised controlled trial, but in most cases\\nthese cannot be employed due to ethical concerns, technological infeasibility, or prohibitive cost. In these\\nsituations, domain experts have to be consulted to determine the causal relationships. It is important in these\\nsituations to carefully address the manner in which such domain knowledge was extracted from experts, the\\nnumber and diversity of experts involved, the amount of consensus between experts, and so on. The need for\\ncareful documentation of this knowledge and its periodic review is made clear in the MLTRL framework, as\\nwe shall see below.\\n•Identiﬁability– Another vital component of building causal models is whether the causal question of interest\\nisidentiﬁable from the causal structure speciﬁed for the model together with observational (and sometimes\\nexperimental) data.\\n•Adjusting for and monitoring confounding bias– An important aspect of causal model performance, not\\npresent in standard machine learning algorithms, is confounding bias adjustment. The standard approach is to\\nemploy propensity score matching to remove such bias. However, the quality of bias adjustment achieved in\\nany speciﬁc instance with such propensity-based matching methods needs to be checked and documented,\\nwith alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[44].\\n•Sensitivity analysis– As causal estimates are based on generally untestable assumptions, such as observing all\\nrelevant confounders, it is vital to determine how sensitive the resulting predictions are to potential violations\\nof these assumptions.\\n•Consistency– It is crucial to understand if the learned causal estimate provably converges to the true causal\\neffect in the limit of inﬁnite sample size. However, causal models cannot be validated by standard held-out\\ntests, but rather require randomization or special data collection strategies to evaluate their predictions [ 45,46].\\nThe MLTRL framework makes transparent the need to carefully document and defend these assumptions, thus ensuring\\nthe safe and robust creation, deployment, and maintenance of causal models. We elucidate this with recent work by\\nRichens et al.[ 47], developing a causal approach to computer-assisted diagnosis which outperforms previous purely\\nmachine learning based methods. To this end, we will go through the MLTRL levels one by one, demonstrating how\\nthey ensure the above speciﬁc checks and assumptions are naturally accounted for. This should provide a blueprint for\\nhow to employ the MLTRL levels in other causal inference applications.\\n•Level 0 – When initially faced with a causal inference task, the ﬁrst step is always to understand the causal\\nrelationships between relevant variables. For instance, in Richens et al. [ 47], the ﬁrst step toward building\\nthe diagnostic model was specifying the causal relationships between the diverse set risk factors, diseases,\\nand symptoms included in the model. To learn these relations, doctors and healthcare professionals were\\nconsulted to employ their expansive medical domain knowledge which was robustly evaluated by additional\\nindependent groups of healthcare professionals. The MLTRL framework ensured this issue is dealt with and\\ndocumented correctly, as such knowledge is required to progress from Level 0; failure to do this has plagued\\nsimilar healthcare AI projects [48].\\nThe next step of any causal analysis is to understand whether the causal question of interest is uniquely\\nidentiﬁable from the causal structure speciﬁed for the model together with observational and experimental data.\\nIn this medical diagnosis example, identiﬁcation was crucial to establish, as the causal question of interest,\\n“would the observed symptoms not be present had a speciﬁc disease been cured?”, was highly non-trivial.\\nAgain, MLTRL ensures this vital aspect of model building is carefully considered, as a mathematical proof of\\nidentiﬁability would be required to graduate from Level 0.\\nWith both the causal structure and identiﬁability result in hand, one can progress to Level 1.\\n•Level 1 – At this level, the goal is to take the estimand for the identiﬁed causal question of interest and\\ndevise a way to estimate it from data. To do this one will need efﬁcient ways to adjust for confounﬁng bias.\\nThe standard approach is to employ propensity score-based methods to remove such bias when the target\\ndecision is binary, and use multi-stage ML models adhering to the assumed causal structure[ 49] for continuous\\ntarget decisions (and high-dimensional data in general). However, the quality of bias adjustment achieved in\\nany speciﬁc instance with propensity-based matching methods needs to be checked and documented, with\\n15'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 15}, page_content='alternate bias adjusting procedure required if appropriate levels of bias adjustment are not achieved[ 44]. As\\nabove, MLTRL ensures transparency and adherence to this important aspect of causal model development, as\\nwithout it a project cannot graduate from Level 1. Even more, MLTRL ensures tests for confounding bias\\nare developed early-on and maintained throughout later stages to deployment. Still, in many cases, it is not\\npossible to completely remove confounding in the observed data. TRL Cards offer a transparent way to declare\\nspeciﬁc limitations of a causal ML method.\\n•Level 2 – PoC-level tests for causal models must go beyond that of typical ML models. As discussed above,\\nto ensure the estimated causal effects are robust to the assumptions required for their derivation, sensitivity\\nto these assumptions must be analysed. Such sensitivity analysis is often limited to R&D experiments or\\na post-hoc feature of ML products. MLTRL on the other hand requires this throughout the lifecycle as\\ncomponents of ML test suites and gated reviews. In the case of causal ML, best practice is to employ sensitivity\\nanalysis for this robustness check[ 50]. MLTRL ensures this check is highlighted and adhered to, and no model\\nwill end up graduating Level 2–let alone being deployed–unless it is passed.\\n•Level 3 – Coding best practices, as in general ML applications.\\n•Level 4-5 – There are additional tests to consider when taking causal models from research to production,\\nin particular at Level 4–proof of concept demonstration in a real scenario. Consistency , for example, is an\\nimportant property of causal methods that informs us whether the method provably converges to the true\\ncausal graph in the limit of inﬁnite sample size. Quantifying consistency in the test suite is critical when\\ndatasets change from controlled laboratory settings to open-world, and when the application scales. And\\nPoC validation steps are more efﬁcient with MLTRL because the process facilitates early speciﬁcation of the\\nevaluation metric for a causal model in Level 2. Causal models cannot be validated by standard held-out tests,\\nbut rather require randomization or special data collection strategies to evaluate their predictions[ 45,46]. Any\\ndifﬁculty in evaluating the model’s predictions will be caught early and remedied.\\n•Level 6-9 – With the the causal ML components of this technology developed reliably in the previous levels,\\nthe rest of the levels developing this technology focused on general medical-ML deployment challenges. For\\nthe most part, data governance, privacy, and management that was detailed earlier in the neuropathology\\nMLTRL use-case, as well as on-premises deployment.\\nAI for open-source space sciences\\nThe CAMS (Cameras for Allsky Meteor Surveillance) project [ 51], established in 2010 by NASA, uses hundreds of\\noff-the-shelf CCTV cameras to capture the meteor activity in the night sky. Initially, resident scientists would retrieve\\nhard-disks containing video data captured each night and perform manual triangulation of tracks or streaks of light\\nin the night sky, and compute a meteor’s trajectory, orbit, and lightcurve. Each solution was manually classiﬁed as a\\nmeteor or not (i.e., planes, birds, clouds, etc). In 2017, a project run by the Frontier Development Labvii[52], the AI\\naccelerator for NASA and ESA, aimed to automate the data processing pipeline and replicate the scientists thought\\nprocess to build an ML model that identiﬁes meteors in the CAMS project [ 53,54]. The data automation led to\\norders of magnitude improvements in operational efﬁciency of the system, and allowed new contributors and amateur\\nastronomers to start contributing to meteor sightings. Additionally, a novel web tool allowed anybody anywhere to\\nview the meteors detected in the previous night. The CAMS camera system has had six-fold global expansion of the\\ndata capture network, discovered ten new meteor showers, contributed towards instrumental evidence of previously\\npredicted comets, and helped calculate parent bodies of various meteor showers. CAMS utilized the MLTRL framework\\nto progress as described:\\n•Level 1 – Understanding the domain and data is a prerequisite for any ML development. Extensive data\\nexploration elucidated visual differences between objects in the night sky such as meteors, satellites, clouds,\\ntail lights of planes, light from the eyes of cats peering into cameras, trees, and other tall objects visible in\\nthe moonlight. This step helped (1) understand visual properties of meteors that later deﬁned the ML model\\narchitecture, and (2) mitigate impact of data imbalance by proactively developing domain-oriented strategies.\\nThe results are well-documented on a datasheet associated with the TRL card, and discussed at the stage\\nreview. This MLTRL documentation forced us to consider data sharing and other privacy concerns at this early\\nconceptualization stage, which is certainly relevant considering CAMS is for open-source and gathering data\\nfrom myriad sources.\\n•Level 2-3 – The agile and non-monotonic (or non-linear) development prescribed by MLTRL allowed the\\nteam to ﬁrst develop an approximate end-to-end pipeline that offered a path to ML model deployment and\\nquick turnaround time to incorporate feedback from the regular gated reviews. Then, with relatively quicker\\nviiThe NASA Frontier Development Lab and partners open-source the code and data via the SpaceML platform: spaceml.org\\n16'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 16}, page_content='experimentation, the team could improve on the quality of not just the ML model, but also scale up the systems\\ndevelopment simultaneously in a non-monotonic development cycle.\\n•Level 4 – With the initial pipeline in place, scalable training of baselines and initial models on real challenging\\ndatasets ensued. Throughout the levels, the MLTRL gated reviews were essential for making efﬁcient progress\\nwhile ensuring robustness and functionality that meets stakeholder needs. At this stage we highlight speciﬁc\\nadvantages of the MLTRL review processes that had instrumental effect on the project success: With the\\nrequired panel of mixed ML researchers and engineers, domain scientists, and product managers, the stage 4\\nreviews stressed the signiﬁcance of numerical improvements and comparison to existing baselines, and helped\\nidentify and overcome issues with data imbalance. The team likely would have overlooked these approaches\\nwithout the review from peers in diverse roles and teams. In general, the evolving panel of reviewers at\\ndifferent stages of the project was essential for covering a variety of veriﬁcation and validation measures –\\nfrom helping mitigate data challenges, to open-source code quality.\\n•Level 5 – To complete this R&D-to-productization level, a novel web tool called the NASA CAMS Meteor\\nShower Portalviiiwas created that allowed users to view meteor shower activity from the previous night and\\nverify meteor predictions generated by the ML model. This app development was valuable for A/B testing,\\nvalidating detected meteors and classiﬁed new meteor showers with human-AI interaction, and demonstrating\\nreal-world utility to stakeholders in review. ML processes without MLTRL miss out on these valuable\\ndevelopment by overlooking the need for such a demo tool.\\n•Level 6 – Application development was naturally driven by end-user feedback from the web app in level 5 –\\nwithout MLTRL it’s unlikely the team would be able to work with early productization feedback. With almost\\nreal time feedback coming in daily, newer methods for improving robustness of meteor identiﬁcation led to\\nresearching and developing a unique augmentation technique, resulting in the state of the art performance of\\nthe ML model. Further application development led to incorporating features that were in demand by users of\\nthe NASA CAMS Meteor Shower Portal: include celestial reference points through constellations, add ability\\nto zoom in/out and (un)cluster showers, and provide tooling for scientiﬁc communication. The coordination of\\nthese features into product-caliber codebase resulted in the release of the NASA CAMS Meteor Shower Portal\\n2.0 that was built by a team of citizen scientists – again we found the speciﬁc checkpoints in the MLTRL\\nreview were crucial for achieving these goals.\\n•Level 7 – Integration was particularly challenging in two ways. First, integrating the ML and data engineering\\ndeliverables with the existing infrastructure and tools of the larger CAMS system, which had started devel-\\nopment years earlier with other teams in partner organizations, required quantiﬁable progress for verifying\\nthe tech-readiness of ML models and modules. The use of technology readiness levels provided a clear and\\nconsistent metric for the maturity of the ML and data technologies, making for clear communication and\\nefﬁcient project integration. Without MLTRL it is difﬁcult to have a conversation, let alone make progress, to-\\nwards integrating AI/ML and data subsystems and components. Second, integrating open-source contributions\\ninto the main ML subsystem was a signiﬁcant challenge alleviated with diligent veriﬁcation and validation\\nmeasures from MLTRL, as well as quantifying robustness with ML testing suites (using scoring measures like\\nthat of the ML Testing Rubric[20], and devising a checklist based on metamorphic testing[18]).\\n•Level 8 – CAMS, like many datasets in practice, consisted of a smaller labeled subset and a much larger\\nunlabeled set. In an attempt to additionally increase robustness of the ML subsystem ahead of “ﬂight readiness”,\\nwe looked to active learning [ 55,56] techniques to leverage the unlabeled data. Models using an initial version\\nof this approach, where results of the active learning provided “weak” labels, resulted in consumption of the\\nentire decade long unlabelled data collected by CAMS and slightly higher scores on deployment tests. Active\\nlearning showed to be a promising feature and was switched back to level 7 for further development towards\\nthe next deployment version, so as not to delay the rest of the project.\\n•Level 9 – The ML components in CAMS require continual monitoring for model and data drifts, such as\\nchanges in weather, smoke, and cloud patterns that affect the view of the night sky. The data drifts may also be\\nspeciﬁc to locations, such as ﬁreﬂies and bugs in CAMS Australia and New Zealand stations which appear as\\nfalse positives. The ML pipeline is largely automated with CI/CD, runs regular regression tests, and production\\nof benchmarks. Manual intervention can be triggered when needed, such as sending low conﬁdence meteors for\\nveriﬁcation to scientists in the CAMS project. The team also regularly releases the code, models, and web tools\\non the open-source space sciences and exploration ML toolbox, SpaceMLix. Through the SpaceML community\\nand partner organizations, CAMS continually improves with feature requests, debugging, and improving data\\npractices, while tracking progress with standard software release cycles and MLTRL documentation.\\nviiimeteorshowers.seti.org\\nixspaceml.org\\n17'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 17}, page_content='BEYOND SOFTWARE ENGINEERING\\nSoftware engineering (SWE) practices vary signiﬁcantly across domains and industries. Some domains, such as medical\\napplications, aerospace, or autonomous vehicles rely on a highly rigorous development process which is required\\nby regulations. Other domains, for example advertising and e-commerce are not regulated and can employ a lenient\\napproach to development. ML development should at minimum inherit the acceptable software engineering practices of\\nthe domain. There are, however, several key areas where ML development stands out from SWE, adding its own unique\\nchallenges which even most rigorous SWE practices are not able to overcome.\\nFor instance, the behavior of ML systems is learned from data, not speciﬁed directly in code. The data requirements\\naround ML (i.e., data discovery, management, and monitoring) adds signiﬁcant complexity not seen in other types\\nof SWE. There are many beneﬁts to using a data-oriented architecture (DOA) [48] with the data-ﬁrst workﬂows and\\nmanagement practices prescribed in MLTRL. DOA aims to make the data ﬂowing between elements of business logic\\nmore explicit and accessible with a streaming-based architecture rather than the micro-service architectures that are\\nstandard in software systems. One speciﬁc beneﬁt of DOA is making data available and traceable by design, which\\nhelps signiﬁcantly in the ML logging challenges and data governance needs we discussed in Levels 7-9. Moreover,\\nMLTRL highlights data-related requirements along every step to ensure that the development process considers data\\nreadiness and availability.\\nNot to mention an array of ML-speciﬁc failure modes; for example, models that become miscalibrated due to subtle\\ndata distributional shifts in the deployment setting, resulting in models that are more conﬁdent in predictions than they\\nshould be. MLTRL helps deﬁne ML-speciﬁc testing considerations (levels 5 and 7) to help surface these failure-modes\\nearly. ML opens up new threat vectors across the whole deployment workﬂow that otherwise aren’t risks in software\\nsystems: for example, a poisoning attack to contaminate the training phase of ML systems, or membership inference\\nto see if a given data record was part of the model’s training. MLTRL consider these threat vectors and suggests\\nrelevant risk-identiﬁcation during prototyping and productization phases. More generally, ML codebases have all the\\nproblems for regular code, plus ML-speciﬁc issues at the system level, mainly as a consequence of added complexity\\nand dynamism. The resulting entanglement, for instance, implies that the SWE practice of making isolated changes is\\noften not feasible – Scully et al.[ 57] refer to this as the “changing anything changes everything” principle. Given this\\nconsideration, typical SWE change-management is insufﬁcient. Furthermore, ML systems almost necessarily increase\\nthe technical debt; package-level refactoring is generally sufﬁcient for removing technical debt in software systems, but\\nthis is not the case in ML systems.\\nThese factors and others suggest that inherited software engineering and management practices of a given domain are\\ninsufﬁcient for the successful development of robust and reliable ML systems. But it is not trading off one for the other:\\nMLTRL can be used in synergy with the existing, industry-standard software engineering practices such as agile [ 58]\\nand waterfall [ 59] to handle unique challenges of ML development. Because ML applications are a category of software,\\nall best practices of building and operating software should be extended when possible to the ML application. Practices\\nlike version control, comprehensive testing, continuous integration and continuous deployment are all applicable to ML\\ndevelopment. MLTRL provides a framework that helps extend SWE building and operating practices that are acceptable\\nin a given domain to tackle the unique challenges of ML development.\\nRELATED WORKS\\nA recent case study from Microsoft Research [ 40] similarly identiﬁes a few themes describing how ML is not equal to\\nsoftware engineering, and recommends a linear ML workﬂow with steps for data preparation through modeling and\\ndeploying. They deﬁne an effective workﬂow for isolated development of an ML model, but this approach does not\\nensure the technology is actually improving in quality and robustness. Their process should be repeated at progressive\\nstages of development in the broader ML and data technology lifecycle. If applied in the MLTRL framework, the\\nspeciﬁc ingredients of the ML model workﬂow – that is, people, software, tests, objectives, etc. – evolve over time and\\nsubsequent stages as the technologies mature.\\nThere exist many recommended workﬂows for speciﬁc ML methods and areas of pipelines. For instance, a more\\niterative process for Bayesian ML [ 60] and even more speciﬁcally for probabilistic programming [ 39], a data mining\\nprocess deﬁned in 2000 that remains widely used [ 61], others for describing data iterations [ 62], and human-computer\\ninteraction cycles [ 63]. In these recommended workﬂows and others, there’s an important distinction between their\\ncycles and “switchback” mechanisms in MLTRL. Their cycles suggest to generically iterate over a data-modeling-\\nevaluation-deployment process. Switchbacks, on the other hand, are speciﬁc, purpose-driven workﬂows for dialing\\npart(s) of a project to an earlier stage – this doesn’t simply mean go back and train the model on more data, but rather\\nswitching back regresses the technology’s maturity level (e.g. from level 5 to level 3) such that it must again fulﬁll the\\nlevel-by-level requirements, evaluations and reviews. See the Methods section for more details on MLTRL switchbacks.\\n18'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 18}, page_content='In general, iteration is an important part of data, ML, and software processes. MLTRL is unique from the other\\nrecommended processes in many ways, and perhaps most importantly because it considers data ﬂows and ML models\\nin the context of larger systems. These isolated processes (that are speciﬁc to e.g. modeling in prototype development\\nor data wrangling in application development) are synergistic with MLTRL because they can be used within each level\\nof the larger lifecycle or framework. For example, the Bayesian modeling processes [ 39,60] we mentioned above\\nare really useful to guide developers of probabilistic ML approaches. But there are important distinctions between\\nexecuting these modeling steps and cycles in a well-deﬁned prototyping environment with curated data and minimal\\nresponsibilities, versus a production environment riddled with sparse and noisy data, that interacts with the physical\\nworld in non-obvious ways, and can carry expensive (even hidden) consequences. MLTRL provides the necessary,\\nholistic context and structure to use these and other development processes reliably and responsibly.\\nAlso related to our work, Google teams have proposed ML testing recommendations [ 20] and validating the data fed\\ninto ML systems [ 64]. For NLP applications, typical ML testing practices struggle to translate to real-world settings,\\noften overestimating performance capabilities. An effective way to address this is devising a checklist of linguistic\\ncapabilities and test types, as in Ribeiro et al.[ 17]–interestingly their test suite was inspired by metamorphic testing,\\nwhich we suggested earlier in Level 7 for testing systems AI integrations. A survey by Paleyes et al. [ 48] go over\\nnumerous case studies to discuss challenges in ML deployment. They similarly pay special attention to the need for\\nethical considerations, end-user trust, and extra security in ML deployments. On the latter point, Kumar et al. [ 65]\\nprovide a table thoroughly breaking down new threat vectors across the whole ML deployment workﬂow (some of\\nwhich we mentioned above). These works, notably the ML security measures and the quantiﬁcation of an ML test suite\\nin a principled way – i.e., that does not use misguided heuristics such as code coverage – are valuable to include in any\\nML workﬂow including MLTRL, and are synergistic with the framework we’ve described in this paper. These analyses\\nprovide useful insights, but they do not provide a holistic, regimented process for the full ML lifecycle from R&D\\nthrough deployment. An end-to-end approach is suggested by Raji et al.[ 66], but only for the speciﬁc task of auditing\\nalgorithms; components of AI auditing are mentioned in Level 7, and covered throughout in the review processes.\\nSculley et al.[ 57] go into more ML debt topics such as undeclared consumers and data dependencies, and go on to\\nrecommend an ML Testing Rubric as a production checklist [ 20]. For example, testing models by a canary process\\nbefore serving them into production. This, along with similar shadow testing we mentioned earlier, are common in\\nautonomous ML systems, notably robotics and autonomous vehicles. They explicitly call out tests in four main areas\\n(ML infrastructure, model development, features and data, and monitoring of running ML systems), some of which we\\ndiscussed earlier. For example, tests that the training and serving features compute the same values; a model may train\\non logged processes or user input, but is then served on a live feed with different inputs. In addition to the Google ML\\nTesting Rubric, we advocate metamorphic testing : a SWE methodology for testing a speciﬁc set of relations between\\nthe outputs of multiple inputs. True to the checklists in the Google ML Testing Rubric and in MLTRL, metamorphic\\ntesting for ML can have a codiﬁed list of metamorphic relations[18].\\nIn domains such as healthcare there have been the introduction of similar checklists for data readiness – for example,\\nto ensure regulatory-grade real-world-evidence (RWE) data quality [ 67] – yet these are nascent and not yet widely\\naccepted. Applying AI in healthcare has led to developing guidance for regulatory protocol, which is still a work in\\nprogress. Larson et al.[ 68] provide a comprehensive analysis for medical imaging and AI, arriving at several regulatory\\nframework recommendations that mirror what we outline as important measures in MLTRL: e.g., detailed task elements\\nsuch as pitfalls and limitations (surfaced on TRL Cards), clear deﬁnition of an algorithm relative to the downstream\\ntask, deﬁning the algorithm “capability” (Level 5), real-world monitoring, and more.\\nD’amour et al.[ 19] dive into the problem we noted earlier about model miscalibration. They point to the trend in machine\\nlearning to develop models relatively isolated from the downstream use and larger system, resulting in underspeciﬁcation\\nthat handicaps practical ML pipelines. This is largely problematic in deep learning pipelines, but we’ve also noted this\\nrisk in the case of causal inference applications. Suggested remedies include stress tests –empirical evaluations that\\nprobe the model’s inductive biases on practically relevant dimensions–and in general the methods we deﬁne in Level 7.\\nLIMITATIONS, RESPONSIBILITIES, and ETHICS\\nMLTRL has been developed, deployed, iterated, and validated in myriad environments, as demonstrated by the previous\\nexamples and many others. Nonetheless we strongly suggest that MLTRL not be viewed as a cure-all for machine\\nlearning systems engineering. Rather, MLTRL provides mechanisms to better enable ML practitioners, teams, and\\nstakeholders to be diligent and responsible with these technologies and data. That is, one cannot implement MLTRL in\\nan organization and turn a blind eye to the many data, ML, and integration challenges we’ve discussed here. MLTRL is\\nanalogous to a pilot’s checklist, not autopilot.\\n19'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 19}, page_content='MLTRL is intended to be complimentary to existing software development methodologies, not replace or alter them.\\nSpeciﬁcally, whether the team uses agile or waterfall methods, MLTRL can be adopted to help deﬁne and structure\\nphases of the project, as well as the success criteria of each stage. In context of the software development process, the\\npurpose of MLTRL is to help the team minimize the technical dept and risk associated with the delivery of an ML\\napplication by helping the development team ask necessary questions.\\nWe discussed many data challenges and approaches in the context of MLTRL, and should highlight again the importance\\nof data considerations in any ML initiative. The data availability and quality can severely limit the ability to develop and\\ndeploy ML, whether MLTRL is used or not. It is again the responsibility of the ML practitioners, teams, and stakeholders\\nto gather, use, and distribute data in safe, legal, ethical ways. MLTRL helps do so with rigor and transparency, but\\nagain is not a solution for data bias. We recommend these recent works on data bias in ML: [ 69,70,71,72,73].\\nFurther, AI/ML ethics is a continuously evolving, multidisciplinary space – see [ 5]. MLTRL aims to prioritize ethics\\nconsiderations at each level of the framework, and would do well to also evolve over time with the broader AI/ML\\nethics developments.\\nCONCLUSION\\nWe’ve described Machine Learning Technology Readiness Levels (MLTRL) , an industry-hardened systems engineering\\nframework for robust, reliable, and responsible machine learning. MLTRL is derived from the processes and testing\\nstandards of spacecraft development, yet lean and efﬁcient for ML, data, and software workﬂows. Examples from\\nseveral organizations across industries demonstrate the efﬁcacy of MLTRL for AI and ML technologies, from research\\nand development through productization and deployment, in important domains such as healthcare and physics, with\\nemphasis on data readiness amongst other critical challenges. Our aim is MLTRL works in synergy with recent\\napproaches in the community focused on diligent data-readiness, privacy and security, and ethics. Even more, MLTRL\\nestablishes a much-needed lingua franca for the AI ecosystem, and broadly for AI in the worlds of science, engineering,\\nand business. Our hope is that our systems framework is adopted broadly in AI and ML organizations, and that\\n“technology readiness levels” becomes common nomenclature across AI stakeholders – from researchers and engineers\\nto sales-people and executive decision-makers.\\nMethods\\nGated reviews\\nAt the end of each stage is a dedicated review period: (1) Present the technical developments along with the requirements\\nand their corresponding veriﬁcation measures and validation steps, (2) make key decisions on path(s) forward (or\\nbackward) and timing, and (3) debrief the processx. As in the gated reviews deﬁned by TRL used by NASA, DARPA, et\\nal., MLTRL stipulates speciﬁc criteria for review at each level, as well as calling out speciﬁc key decision points (noted\\nin the level descriptions above). The designated reviewers will “graduate” the technology to the next level, or provide a\\nlist of speciﬁc tasks that are still needed (ideally with quantitative remarks). After graduation at each level, the working\\ngroup does a brief post-mortem; we ﬁnd that a quick day or two pays dividends in cutting away technical debt and\\nimproving team processes. Regular gated reviews are essential for making efﬁcient progress while ensuring robustness\\nand functionality that meets stakeholder needs. There are several important mechanisms in MLTRL reviews that are\\nspeciﬁcally useful with AI and ML technologies: First, the review panels evolve over a project lifecycle, as noted\\nbelow. Second, MLTRL prescribes that each review runs through an AI ethics checklist deﬁned by the organization; it is\\nimportant to repeat this at each review, as the review panel and stakeholders evolve considerably over a project lifecycle.\\nAs previously described in the levels deﬁnitions, including ethics reviews as an integral part of early system development\\nis essential for informing model speciﬁcations and avoiding unintended biases or harm[74] after deployment.\\nTRL “Cards”\\nIn Figure 3 we succinctly showcase a key deliverable: TRL Cards . The model cards proposed by Google [ 75] are a useful\\ndevelopment for external user-readiness with ML. On the other hand, our TRL Cards aim to be more information-dense,\\nlike datasheets for medical devices and engineering tools – see the open-source TRL Card repo for examples and\\ntemplates (to be released at github.com/alan-turing-institute). These serve as “report cards” that grow and improve upon\\ngraduating levels, and provide a means of inter-team and cross-functional communication. The content of a TRL Card\\nis roughly in two categories: project info, and implicit knowledge. The former clearly states info such as project owners\\nxMLTRL should include regular debriefs and meta-evaluations such that process improvements can be made in a data-driven,\\nefﬁcient way (rather than an annual meta-review). MLTRL is a high-level framework that each organization should operationalize in\\na way that suits their speciﬁc capabilities and resources.\\n20'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 20}, page_content='and reviewers, development status, and semantic versioning–not just for code, also for models and data. In the latter\\ncategory are speciﬁc insights that are typically siloed in the ML development team but should be communicated to\\nother stakeholders: modeling assumptions, dataset biases, corner cases, etc. With the spread of AI and ML in critical\\napplication areas, we are seeing domain expert consortiums deﬁning AI reporting guidelines – e.g., Rivera et al.[ 76]\\ncalling for clinical trials reports for interventions involving AI – which will greatly beneﬁt from the use of our TRL\\nreporting cards. We stress that these TRL Cards are key for the progression of projects, rather than documentation\\nafterthoughts. The TRL Cards thus promote transparency and trust, within teams and across organizations. TRL Card\\ntemplates will be open-sourced upon publication of this work, including methods for coordinating use with other\\nreporting tools such as “Datasheets for Datasets” [24].\\nRisk mitigation\\nIdentifying and addressing risks in a software project is not a new practice. However, akin to the MLTRL roots in\\nspacecraft engineering, risk is a “ﬁrst-class citizen” here. In the deﬁnition of technical and product requirements, each\\nentry has a calculation of the form risk =p(failure )×value , where the value of a component is an integer 1−10.\\nBeing diligent about quantifying risks across the technical requirements is a useful mechanism for ﬂagging ML-related\\nvulnerabilities that can sometimes be hidden by layers of other software. MLTRL also speciﬁes that risk quantiﬁcation\\nand testing strategies are required for sim-to-real development. That is, there is nearly always a non-trivial gap in\\ntransferring a model or algorithm from a simulation testbed to the real world. Requiring explicit sim-to-real testing\\nsteps in the workﬂow helps mitigate unforeseen (and often hazardous) failures. Additionally, comprehensive ML test\\ncoverage that we mention throughout this paper is a critical strategy for mitigating risks anduncertainties: ML-based\\nsystem behavior is not easily speciﬁed in advance, but rather depends on dynamic qualities of the data and on various\\nmodel conﬁguration choices[20].\\nNon-monotonic, non-linear paths\\nWe observe many projects beneﬁt from cyclic paths, dialing components of a technology back to a lower level. Our\\nframework not only encourages cycles, we make them explicit with “switchback mechanisms” to regress the maturity\\nof speciﬁc components in an AI system:\\n1.Discovery switchbacks occur as a natural mechanism – new technical gaps are discovered through systems\\nintegration, sparking later rounds of component development[ 77]. These are most common in the R&D levels,\\nfor example moving a component of a proof-of-concept technology (at Level 4) back to proof-of-principle\\ndevelopment (Level 2).\\n2.Review switchbacks result from gated reviews, where speciﬁc components or larger subsystems may be dialed\\nback to earlier levels. This switchback is one of the “key decision points” in the MLTRL project lifecycle\\n(as noted in the Levels deﬁnitions), and is often a decision driven by business-needs and timing rather than\\ntechnical concerns (for instance when mission priorities and funds shift). This mechanism is common from\\nLevel 6/7 to 4, which stresses the importance of this R&D to product transition phase (see Figure 2 (left)).\\n3.Embedded switchbacks are predeﬁned in the MLTRL process. For example, a predeﬁned path from 4 to 2, and\\nfrom 9 to 4. In complex systems, particularly with AI technologies, these built-in loops help mitigate technical\\ndebt and overcome other inefﬁciencies such as noncomprehensive V&V steps.\\nWithout these built-in mechanisms for cyclic development paths, it can be difﬁcult and inefﬁcient to build systems of\\nmodules and components at varying degrees of maturity. Contrary to traditional thought that switchback events should\\nbe suppressed and minimized, in fact they represent a natural and necessary part of the complex technology development\\nprocess – efforts to eliminate them may stiﬂe important innovations without necessarily improving efﬁciency. This is\\na fault of the standard monotonic approaches in AI/ML projects, stage-gate processes, and even the traditional TRL\\nframework.\\nIt is also important to note that most projects do not start at Level 0; very few ML companies engage in this low-level\\ntheoretical research. For example, a team looking to use an off-the-shelf object recognition model could start that\\ntechnology at Level 3, and proceed with thorough V&V for their speciﬁc datasets and use-cases. However, no technology\\ncan skip levels after the MLTRL process has been initiated. The industry default (that is, without implementing MLTRL)\\nis to ignorantly take pretrained models, run ﬁne tuning on their speciﬁc data, and jump to deployment, effectively\\nskipping Levels 5 to 7. Additionally, we ﬁnd it is advantageous to incorporate components from other high-TRL ranking\\nprojects while starting new projects; MLTRL makes the veriﬁcation and validation (V&V) steps straightforward for\\nintegrating previously developed ML components.\\n21'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 21}, page_content='Evolving people, objectives, and measures\\nAs suggested earlier, much of the practical value of MLTRL comes at the transition between levels. More precisely,\\nMLTRL manages these oft neglected transitions explicitly as evolving teams, objectives, and deliverables. For instance,\\nthe team (or working group) at Level 3 is mostly AI Research Engineers, but at Level 6 is mixed Applied AI/SW\\nEngineers mixed with product managers and designers. Similarly, the review panels evolve from level to level, to match\\nthe changing technology development objectives. What the reviewers reference similarly evolves: notice in the level\\ndeﬁnitions that technical requirements and V&V guide early stages, but at and after Level 6 the product requirements\\nand V&V takeover – naturally, the risk quantiﬁcation and mitigation strategies evolve in parallel. Regarding the\\ndeliverables, notably TRL Cards and risk matrices[ 22] (to rank and prioritize various science, technical, and project\\nrisks), the information develops and evolves over time as the technology matures.\\nQuantiﬁable progress\\nBy deﬁning technology maturity in a quantitative way, MLTRL enables teams to accurately and consistently deﬁne\\ntheir ML progress metrics. Notably industry-standard “objectives and key results” (OKRs) and “key performance\\nindicators” (KPIs) [ 78] can be deﬁned as achieving certain readiness levels in a given period of time; this is a preferable\\nmetric in essentially all ML systems which consist of much more than a single performance score to measure progress.\\nEven more, meta-review of MLTRL progress over multiple projects can provide useful insights at the organization\\nlevel. For example, analysis of the time-per-level and the most frequent development paths/cycles can bring to light\\noperational bottlenecks. Compared to conventional software engineering metrics based on sprint stories and tickets, or\\ntime-tracking tools, MLTRL provides a more accurate analysis of ML workﬂows.\\nCommunication and explanation\\nA distinct advantage of MLTRL in practice is the nomenclature: an agreed upon grading scheme for the maturity of\\nan AI technology, and a framework for how/when that technology ﬁts within a product or system, enables everyone\\nto communicate effectively and transparently. MLTRL also acts as a gate for interpretability and explainability–at\\nthe granularity of individual models and algorithms, and more crucially from a holistic, systems standpoint. Notably\\nthe DARPA XAIxiprogram advocates for this advance in developing AI technologies; they suggest interpretability\\nand explainability are necessary at various locations in an AI system to be sufﬁcient for deployment as an AI product,\\notherwise leading to issues with ethics and bias.\\nRobustness via uncertainty-aware ML\\nHow to design a reliable system from unreliable components has been a guiding question in the ﬁelds of computing and\\nintelligence [79]. In the case of AI/ML systems, we aim to build reliable systems with myriad unreliable components:\\nnoisy and faulty sensors, human and AI error, and so on. There is thus signiﬁcant value to quantifying the myriad\\nuncertainties, propagating them throughout a system, and arriving at a notion or measure of reliability. For this reason,\\nalthough MLTRL applies generally to AI/ML methods and systems, we advocate for methods in the class of probabilistic\\nML, which naturally represent and manipulate uncertainty about models and predictions[ 28]. These are Bayesian\\nmethods that use probabilities to represent aleatoric uncertainty , measuring the noise inherent in the observations, and\\nepistemic uncertainty , accounting for uncertainty in the model itself (i.e., capturing our ignorance about which model\\ngenerated the data). In the simplest case, an uncertainty aware ML pipeline should quantify uncertainty at the points of\\nsensor inputs or perception, prediction or model output, and decision or end-user action – McAllister et al.[ 29] suggest\\nthis with Bayesian deep learning models for safer autonomous vehicle pipelines. We can achieve this sufﬁciently well\\nin practice for simple systems. However, we do not yet have a principled, theoretically grounded, and generalizable way\\nof propagating errors and uncertainties downstream and throughout more complex AI systems – i.e., how to integrate\\ndifferent software, hardware, data, and human components while considering how errors and uncertainties propagate\\nthrough the system. This is an important direction of our future work.\\nReferences\\n[1]Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning\\nthat matters. In AAAI , 2018.\\nxiDARPA Explainable Artiﬁcial Intelligence (XAI)\\n22'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 22}, page_content='[2]Arnaud de la Tour, Massimo Portincaso, Kyle Blank, and Nicolas Goeldel. The dawn of the deep tech ecosystem. Technical\\nreport, The Boston Consulting Group, 2019.\\n[3] NASA. The NASA systems engineering handbook. 2003.\\n[4] United States Department of Defense. Defense acquisition guidebook. Technical report, U.S. Dept. of Defense, 2004.\\n[5] D. Leslie. Understanding artiﬁcial intelligence ethics and safety. ArXiv , abs/1906.05684, 2019.\\n[6]Google. Machine learning workﬂow. https://cloud.google.com/mlengine/docs/tensorflow/\\nml-solutions-overview . Accessed: 2020-12-13.\\n[7]Alexander Lavin and Gregory Renard. Technology readiness levels for AI & ML. ICML Workshop on Challenges Deploying\\nML Systems , 2020.\\n[8] T. Dasu and T. Johnson. Exploratory data mining and data cleaning. 2003.\\n[9]M. Janssen, P. Brous, Elsa Estevez, L. Barbosa, and T. Janowski. Data governance: Organizing data for trustworthy artiﬁcial\\nintelligence. Gov. Inf. Q. , 37:101493, 2020.\\n[10] B. Shahriari, Kevin Swersky, Ziyu Wang, R. Adams, and N. D. Freitas. Taking the human out of the loop: A review of bayesian\\noptimization. Proceedings of the IEEE , 104:148–175, 2016.\\n[11] Goutham Ramakrishnan, A. Nori, Hannah Murfet, and Pashmina Cameron. Towards compliant data management systems for\\nhealthcare ml. ArXiv , abs/2011.07555, 2020.\\n[12] Umang Bhatt, Alice Xiang, S. Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura,\\nand P. Eckersley. Explainable machine learning in deployment. Proceedings of the 2020 Conference on Fairness, Accountability,\\nand Transparency , 2020.\\n[13] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and V . Smith. Federated learning: Challenges, methods, and future directions.\\nIEEE Signal Processing Magazine , 37:50–60, 2020.\\n[14] T. Ryffel, Andrew Trask, M. Dahl, Bobby Wagner, J. Mancuso, D. Rueckert, and J. Passerat-Palmbach. A generic framework\\nfor privacy preserving deep learning. ArXiv , abs/1811.04017, 2018.\\n[15] A. Madry, Aleksandar Makelov, Ludwig Schmidt, D. Tsipras, and Adrian Vladu. Towards deep learning models resistant to\\nadversarial attacks. ArXiv , abs/1706.06083, 2018.\\n[16] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. ArXiv , abs/1710.11342, 2018.\\n[17] Marco Túlio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models\\nwith checklist. In ACL, 2020.\\n[18] Xiaoyuan Xie, Joshua W. K. Ho, C. Murphy, G. Kaiser, B. Xu, and T. Chen. Testing and validating machine learning classiﬁers\\nby metamorphic testing. The Journal of systems and software , 84 4:544–558, 2011.\\n[19] Alexander D’Amour, K. Heller, D. Moldovan, Ben Adlam, B. Alipanahi, Alex Beutel, C. Chen, Jonathan Deaton, Jacob\\nEisenstein, M. Hoffman, Farhad Hormozdiari, N. Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, M. Lucic,\\nY . Ma, Cory Y . McLean, Diana Mincu, Akinori Mitani, A. Montanari, Zachary Nado, V . Natarajan, C. Nielson, Thomas F.\\nOsborne, R. Raman, K. Ramasamy, Rory Sayres, J. Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, V . Veitch,\\nMax Vladymyrov, Xuezhi Wang, K. Webster, S. Yadlowsky, Taedong Yun, Xiaohua Zhai, and D. Sculley. Underspeciﬁcation\\npresents challenges for credibility in modern machine learning. ArXiv , abs/2011.03395, 2020.\\n[20] Eric Breck, Shanqing Cai, E. Nielsen, M. Salib, and D. Sculley. The ml test score: A rubric for ml production readiness and\\ntechnical debt reduction. 2017 IEEE International Conference on Big Data (Big Data) , pages 1123–1132, 2017.\\n[21] A. Botchkarev. A new typology design of performance metrics to measure errors in machine learning regression algorithms.\\nInterdisciplinary Journal of Information, Knowledge, and Management , 14:045–076, 2019.\\n[22] N. Duijm. Recommendations on the use and design of risk matrices. Safety Science , 76:21–31, 2015.\\n[23] Louise Naud and Alexander Lavin. Manifolds for unsupervised visual anomaly detection. ArXiv , abs/2006.11364, 2020.\\n[24] Timnit Gebru, J. Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, H. Wallach, Hal Daumé, and K. Crawford.\\nDatasheets for datasets. ArXiv , abs/1803.09010, 2018.\\n[25] B. Hutchinson, A. Smart, A. Hanna, Emily L. Denton, Christina Greer, Oddur Kjartansson, P. Barnes, and Margaret Mitchell.\\nTowards accountability for machine learning datasets: Practices from software engineering and infrastructure. Proceedings of\\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency , 2021.\\n23'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 23}, page_content='[26] P. Schulam and S. Saria. Reliable decision support using counterfactual models. In NIPS 2017 , 2017.\\n[27] Towards trustable machine learning. Nature Biomedical Engineering , 2:709–710, 2018.\\n[28] Zoubin Ghahramani. Probabilistic machine learning and artiﬁcial intelligence. Nature , 521:452–459, 2015.\\n[29] Rowan McAllister, Yarin Gal, Alex Kendall, Mark van der Wilk, A. Shah, R. Cipolla, and Adrian Weller. Concrete problems\\nfor autonomous vehicle safety: Advantages of bayesian deep learning. In IJCAI , 2017.\\n[30] Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica I. Avilés-Rivero,\\nChristian Etmann, Cathal McCague, Lucian Beer, Jonathan R. Weir-McCall, Zhongzhao Teng, Effrossyni Gkrania-Klotsas,\\nJames H. F. Rudd, Evis Sala, and Carola-Bibiane Schönlieb. Common pitfalls and recommendations for using machine learning\\nto detect and prognosticate for covid-19 using chest radiographs and ct scans. Nature Machine Intelligence , 3:199–217, 2021.\\n[31] J. Tobin, Rachel H Fong, Alex Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep\\nneural networks from simulation to the real world. 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems\\n(IROS) , pages 23–30, 2017.\\n[32] Arthur Juliani, Vincent-Pierre Berges, Esh Vckay, Yuan Gao, Hunter Henry, M. Mattar, and D. Lange. Unity: A general\\nplatform for intelligent agents. ArXiv , abs/1809.02627, 2018.\\n[33] Stefan Hinterstoißer, Olivier Pauly, Tim Hauke Heibel, Martina Marek, and Martin Bokeloh. An annotation saved is an\\nannotation earned: Using fully synthetic training for object instance detection. ArXiv , abs/1902.09967, 2019.\\n[34] Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan Hogins, You-Cyuan Jhang, Mohsen Kamalzadeh,\\nBowen Li, Steven Leal, Pete Parisi, Cesar Romero, Wesley Smith, Alex Thaman, Samuel Warren, and Nupur Yadav. Unity\\nperception: Generate synthetic data for computer vision. CoRR , abs/2107.04259, 2021.\\n[35] K. Cranmer, J. Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings of the National Academy\\nof Sciences , 117:30055 – 30062, 2020.\\n[36] Jan-Willem van de Meent, Brooks Paige, H. Yang, and Frank Wood. An introduction to probabilistic programming. ArXiv ,\\nabs/1809.10756, 2018.\\n[37] Atilim Günes Baydin, Lei Shao, W. Bhimji, L. Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi,\\nBradley Gram-Hansen, Gilles Louppe, Mingfei Ma, X. Zhao, P. Torr, V . Lee, K. Cranmer, Prabhat, and F. Wood. Etalumis:\\nbringing probabilistic programming to scientiﬁc simulators at scale. Proceedings of the International Conference for High\\nPerformance Computing, Networking, Storage and Analysis , 2019.\\n[38] T. Gleisberg, S. Höche, F. Krauss, M. Schönherr, S. Schumann, F. Siegert, and J. Winter. Event generation with sherpa 1.1.\\nJournal of High Energy Physics , 2009:007–007, 2009.\\n[39] David M. Blei. Build, compute, critique, repeat: Data analysis with latent variable models. 2014.\\n[40] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald C. Gall, Ece Kamar, Nachiappan Nagappan, Besmira\\nNushi, and Thomas Zimmermann. Software engineering for machine learning: A case study. 2019 IEEE/ACM 41st International\\nConference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) , 2019.\\n[41] R. Ambrosino, B. Buchanan, G. Cooper, and Marvin J. Fine. The use of misclassiﬁcation costs to learn rule-based decision\\nsupport models for cost-effective hospital admission strategies. Proceedings. Symposium on Computer Applications in Medical\\nCare , pages 304–8, 1995.\\n[42] Gareth J Grifﬁth, Tim T Morris, Matthew J Tudball, Annie Herbert, Giulia Mancano, Lindsey Pike, Gemma C Sharp, Jonathan\\nSterne, Tom M Palmer, George Davey Smith, et al. Collider bias undermines our understanding of covid-19 disease risk and\\nseverity. Nature communications , 11(1):1–12, 2020.\\n[43] J. Pearl. Theoretical impediments to machine learning with seven sparks from the causal revolution. Proceedings of the\\nEleventh ACM International Conference on Web Search and Data Mining , 2018.\\n[44] T. Nguyen, G. Collins, J. Spence, J. Daurès, P. Devereaux, P. Landais, and Y . Le Manach. Double-adjustment in propensity\\nscore matching analysis: choosing a threshold for considering residual imbalance. BMC Medical Research Methodology , 17,\\n2017.\\n[45] D. Eckles and E. Bakshy. Bias and high-dimensional adjustment in observational studies of peer effects. ArXiv , abs/1706.04692,\\n2017.\\n[46] Yanbo Xu, Divyat Mahajan, Liz Manrao, A. Sharma, and E. Kiciman. Split-treatment analysis to rank heterogeneous causal\\neffects for prospective interventions. ArXiv , abs/2011.05877, 2020.\\n24'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 24}, page_content='[47] Jonathan G Richens, C. M. Lee, and Saurabh Johri. Improving the accuracy of medical diagnosis with causal machine learning.\\nNature Communications , 11, 2020.\\n[48] Andrei Paleyes, Raoul-Gabriel Urma, and N. Lawrence. Challenges in deploying machine learning: a survey of case studies.\\nArXiv , abs/2011.09926, 2020.\\n[49] V . Chernozhukov, D. Chetverikov, M. Demirer, E. Duﬂo, Christian L. Hansen, Whitney K. Newey, and J. Robins. Dou-\\nble/debiased machine learning for treatment and structural parameters. Econometrics: Econometric & Statistical Methods -\\nSpecial Topics eJournal , 2018.\\n[50] Victor Veitch and Anisha Zaveri. Sense and sensitivity analysis: Simple post-hoc analysis of bias due to unobserved confounding.\\nNeurIPS 2020, arXiv preprint arXiv:2003.01747 , 2020.\\n[51] P. Jenniskens, P.S. Gural, L. Dynneson, B.J. Grigsby, K.E. Newman, M. Borden, M. Koop, and D. Holman. Cams: Cameras for\\nallsky meteor surveillance to establish minor meteor showers. Icarus , 216(1):40 – 61, 2011.\\n[52] Siddha Ganju, Anirudh Koul, Alexander Lavin, J. Veitch-Michaelis, Meher Kasam, and J. Parr. Learnings from frontier\\ndevelopment lab and spaceml - ai accelerators for nasa and esa. ArXiv , abs/2011.04776, 2020.\\n[53] S. Zoghbi, M. Cicco, A. P. Stapper, A. J. Ordonez, J. Collison, P. S. Gural, S. Ganju, J.-L. Galache, and P. Jenniskens. Searching\\nfor long-period comets with deep learning tools. In Deep Learning for Physical Science Workshop, NeurIPS , 2017.\\n[54] Peter Jenniskens, Jack Baggaley, Ian Crumpton, Peter Aldous, Petr Pokorny, Diego Janches, Peter S. Gural, Dave Samuels, Jim\\nAlbers, Andreas Howell, Carl Johannink, Martin Breukers, Mohammad Odeh, Nicholas Moskovitz, Jack Collison, and Siddha\\nGanju. A survey of southern hemisphere meteor showers. Planetary and Space Science , 154:21 – 29, 2018.\\n[55] D. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active learning with statistical models. In NIPS , 1994.\\n[56] Y . Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. ArXiv , abs/1703.02910, 2017.\\n[57] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young,\\nJean-François Crespo, and Dan Dennison. Hidden technical debt in machine learning systems. In NIPS , 2015.\\n[58] P. Abrahamsson, Outi Salo, Jussi Ronkainen, and Juhani Warsta. Agile software development methods: Review and analysis.\\nArXiv , abs/1709.08439, 2017.\\n[59] Marco Kuhrmann, Philipp Diebold, Jürgen Münch, Paolo Tell, Vahid Garousi, Michael Felderer, Kitija Trektere, Fergal\\nMcCaffery, Oliver Linssen, Eckhart Hanser, and Christian R. Prause. Hybrid software and system development in practice:\\nwaterfall, scrum, and beyond. Proceedings of the 2017 International Conference on Software and System Process , 2017.\\n[60] Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry,\\nPaul-Christian Burkner, and Martin Modrak. Bayesian workﬂow. ArXiv , abs/2011.01808, 2020.\\n[61] P. Chapman, J. Clinton, R. Kerber, T. Khabaza, T. Reinartz, C. Shearer, and R. Wirth. Crisp-dm 1.0: Step-by-step data mining\\nguide. 2000.\\n[62] Fred Hohman, Kanit Wongsuphasawat, Mary Beth Kery, and Kayur Patel. Understanding and visualizing data iteration in\\nmachine learning. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems , 2020.\\n[63] Saleema Amershi, M. Cakmak, W. B. Knox, and T. Kulesza. Power to the people: The role of humans in interactive machine\\nlearning. AI Mag. , 35:105–120, 2014.\\n[64] Eric Breck, Marty Zinkevich, Neoklis Polyzotis, Steven Euijong Whang, and Sudip Roy. Data validation for machine learning.\\n2019.\\n[65] R. Kumar, David R. O’Brien, Kendra Albert, Salomé Viljöen, and Jeffrey Snover. Failure modes in machine learning systems.\\nArXiv , abs/1911.11034, 2019.\\n[66] Inioluwa Deborah Raji, Andrew Smart, Rebecca White, M. Mitchell, Timnit Gebru, B. Hutchinson, Jamila Smith-Loud, Daniel\\nTheron, and P. Barnes. Closing the ai accountability gap: deﬁning an end-to-end framework for internal algorithmic auditing.\\nProceedings of the 2020 Conference on Fairness, Accountability, and Transparency , 2020.\\n[67] R. Miksad and A. Abernethy. Harnessing the power of real-world evidence (rwe): A checklist to ensure regulatory-grade data\\nquality. Clinical Pharmacology and Therapeutics , 103:202 – 205, 2018.\\n[68] D. B. Larson, Hugh Harvey, D. Rubin, Neville Irani, J. R. Tse, and C. Langlotz. Regulatory frameworks for development and\\nevaluation of artiﬁcial intelligence–based diagnostic imaging algorithms: Summary and recommendations. Journal of the\\nAmerican College of Radiology , 2020.\\n25'),\n",
       " Document(metadata={'source': 'random.pdf', 'page': 25}, page_content='[69] Ninareh Mehrabi, Fred Morstatter, N. Saxena, Kristina Lerman, and A. Galstyan. A survey on bias and fairness in machine\\nlearning. ACM Computing Surveys (CSUR) , 54:1 – 35, 2019.\\n[70] Eirini Ntoutsi, P. Fafalios, U. Gadiraju, Vasileios Iosiﬁdis, W. Nejdl, Maria-Esther Vidal, S. Ruggieri, F. Turini, S. Papadopoulos,\\nEmmanouil Krasanakis, I. Kompatsiaris, K. Kinder-Kurlanda, Claudia Wagner, F. Karimi, Miriam Fernández, Harith Alani,\\nB. Berendt, Tina Kruegel, C. Heinze, Klaus Broelemann, Gjergji Kasneci, T. Tiropanis, and Steffen Staab. Bias in data-driven\\nai systems - an introductory survey. ArXiv , abs/2001.09762, 2020.\\n[71] E. Jo and Timnit Gebru. Lessons from archives: strategies for collecting sociocultural data in machine learning. Proceedings of\\nthe 2020 Conference on Fairness, Accountability, and Transparency , 2020.\\n[72] J. Wiens, W. Price, and M. Sjoding. Diagnosing bias in data-driven algorithms for healthcare. Nature Medicine , 26:25–26,\\n2020.\\n[73] R. Challen, J. Denny, M. Pitt, L. Gompels, T. Edwards, and K. Tsaneva-Atanasova. Artiﬁcial intelligence, bias and clinical\\nsafety. BMJ Quality & Safety , 28:231 – 237, 2019.\\n[74] Z. Obermeyer, B. Powers, C. V ogeli, and S. Mullainathan. Dissecting racial bias in an algorithm used to manage the health of\\npopulations. Science , 366:447 – 453, 2019.\\n[75] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, In-\\nioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. Proceedings of the Conference on Fairness,\\nAccountability, and Transparency , 2019.\\n[76] Samantha Cruz Rivera, Xiaoxuan Liu, A. Chan, A. K. Denniston, and M. Calvert. Guidelines for clinical trial protocols for\\ninterventions involving artiﬁcial intelligence: the spirit-ai extension. Nature Medicine , 26:1351 – 1363, 2020.\\n[77] Z. Szajnfarber. Managing innovation in architecturally hierarchical systems: Three switchback mechanisms that impact practice.\\nIEEE Transactions on Engineering Management , 61:633–645, 2014.\\n[78] H. Zhou and Y . He. Comparative study of okr and kpi. DEStech Transactions on Economics, Business and Management , 2018.\\n[79] J. Neumann. Probabilistic logic and the synthesis of reliable organisms from unreliable components. 1956.\\nAcknowledgements\\nThe authors would like to thank Gur Kimchi, Carl Henrik Ek and Neil Lawrence for valuable discussions about this\\nproject.\\nAuthor contributions statement\\nA.L. conceived of the original ideas and framework, with signiﬁcant contributions towards improving the framework\\nfrom all co-authors. A.L. initiated the use of MLTRL in practice, including the neuropathology test case discussed here.\\nC.G-L. contributed insight regarding causal AI, including the section on counterfactual diagnosis. C.G-L. also made\\nsigniﬁcant contributions broadly in the paper, notably in the Methods descriptions and paper revisions. Si.G. contributed\\nthe spacecraft test case, along with early insights in the framework deﬁnitions. A.V . contributed to the deﬁnition of\\nlater stages involving deployment (as did A.G.), and comparison with traditional software workﬂows. Both E.X. and\\nY .G. provided insights regarding AI in academia, and Y .G. additionally contributed to the uncertainty quantiﬁcation\\nmethods. Su.G. and D.L. contributed the computer vision test case. A.G.B. contributed the particle physics test case,\\nand signiﬁcant reviews of the writeup. A.S. contributed insights related to causal ML and AI ethics. D.N. provided\\nvaluable feedback on the overall framework, and contributed signiﬁcantly with the details on “switchback mechanisms”.\\nS.Z. contributed to multiple paper revisions, with emphasis on clarity and applicability to broad ML users and teams.\\nJ.P. contributed to multiple paper revisions, and to deploying the systems ML methods broadly in practice for Earth and\\nspace sciences. –same goes for C.M., with additional feedback overall on the methods. All co-authors discussed the\\ncontent and contributed to editing the manuscript.\\nCompeting interests\\nThe authors declare no competing interests.\\nAdditional information\\nCorrespondence and requests for materials should be addressed to A.L.\\n26')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HTML Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "# html_string = \"\"\"\n",
    "# <article><nav class=\"theme-doc-breadcrumbs breadcrumbsContainer_Z_bl\" aria-label=\"Breadcrumbs\"><ul class=\"breadcrumbs\" itemscope=\"\" itemtype=\"https://schema.org/BreadcrumbList\"><li class=\"breadcrumbs__item\"><a aria-label=\"Home page\" class=\"breadcrumbs__link\" href=\"/v0.2/\"><svg viewBox=\"0 0 24 24\" class=\"breadcrumbHomeIcon_YNFT\"><path d=\"M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z\" fill=\"currentColor\"></path></svg></a></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item\"><a class=\"breadcrumbs__link\" itemprop=\"item\" href=\"/v0.2/docs/integrations/components/\"><span itemprop=\"name\">Components</span></a><meta itemprop=\"position\" content=\"1\"></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item\"><a class=\"breadcrumbs__link\" itemprop=\"item\" href=\"/v0.2/docs/integrations/llms/\"><span itemprop=\"name\">LLMs</span></a><meta itemprop=\"position\" content=\"2\"></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item breadcrumbs__item--active\"><span class=\"breadcrumbs__link\" itemprop=\"name\">PipelineAI</span><meta itemprop=\"position\" content=\"3\"></li></ul></nav><div class=\"tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo\"><button type=\"button\" class=\"clean-btn tocCollapsibleButton_TO0P\">On this page</button></div><div class=\"theme-doc-markdown markdown\"><h1>PipelineAI</h1><blockquote><p><a href=\"https://pipeline.ai\" target=\"_blank\" rel=\"noopener noreferrer\">PipelineAI</a> allows you to run your ML models at scale in the cloud. It also provides API access to <a href=\"https://pipeline.ai\" target=\"_blank\" rel=\"noopener noreferrer\">several LLM models</a>.</p></blockquote><p>This notebook goes over how to use Langchain with <a href=\"https://docs.pipeline.ai/docs\" target=\"_blank\" rel=\"noopener noreferrer\">PipelineAI</a>.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"pipelineai-example\">PipelineAI example<a href=\"#pipelineai-example\" class=\"hash-link\" aria-label=\"Direct link to PipelineAI example\" title=\"Direct link to PipelineAI example\">​</a></h2><p><a href=\"https://docs.pipeline.ai/docs/langchain\" target=\"_blank\" rel=\"noopener noreferrer\">This example shows how PipelineAI integrated with LangChain</a> and it is created by PipelineAI.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"setup\">Setup<a href=\"#setup\" class=\"hash-link\" aria-label=\"Direct link to Setup\" title=\"Direct link to Setup\">​</a></h2><p>The <code>pipeline-ai</code> library is required to use the <code>PipelineAI</code> API, AKA <code>Pipeline Cloud</code>. Install <code>pipeline-ai</code> using <code>pip install pipeline-ai</code>.</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color: #9CDCFE; --prism-background-color: #222222;\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color: rgb(156, 220, 254);\"><span class=\"token comment\" style=\"color: rgb(106, 153, 85);\"># Install the package</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color: rgb(156, 220, 254);\"><span class=\"token plain\"></span><span class=\"token operator\" style=\"color: rgb(212, 212, 212);\">%</span><span class=\"token plain\">pip install </span><span class=\"token operator\" style=\"color: rgb(212, 212, 212);\">-</span><span class=\"token operator\" style=\"color: rgb(212, 212, 212);\">-</span><span class=\"token plain\">upgrade </span><span class=\"token operator\" style=\"color: rgb(212, 212, 212);\">-</span><span class=\"token operator\" style=\"color: rgb(212, 212, 212);\">-</span><span class=\"token plain\">quiet  pipeline</span><span class=\"token operator\" style=\"color: rgb(212, 212, 212);\">-</span><span class=\"token plain\">ai</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button type=\"button\" aria-label=\"Copy code to clipboard\" title=\"Copy\" class=\"clean-btn\"><span class=\"copyButtonIcons_eSgA\" aria-hidden=\"true\"><svg viewBox=\"0 0 24 24\" class=\"copyButtonIcon_y97N\"><path fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"></path></svg><svg viewBox=\"0 0 24 24\" class=\"copyButtonSuccessIcon_LjdS\"><path fill=\"currentColor\" d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\"></path></svg></span></button></div></div></div><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"example\">Example<a href=\"#example\" class=\"hash-link\" aria-label=\"Direct link to Example\" title=\"Direct link to Example\">​</a></h2><h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"imports\">Imports<a href=\"#imports\" class=\"hash-link\" aria-label=\"Direct link to Imports\" title=\"Direct link to Imports\">​</a></h3><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color: #9CDCFE; --prism-background-color: #222222;\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color: rgb(156, 220, 254);\"><span class=\"token keyword\" style=\"color: rgb(86, 156, 214);\">import</span><span class=\"token plain\"> os</span><br></span><span class=\"token-line\" style=\"color: rgb(156, 220, 254);\"><span class=\"token plain\" style=\"display: inline-block;\"></span><br></span><span class=\"token-line\" style=\"color: rgb(156, 220, 254);\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color: rgb(86, 156, 214);\">from</span><span class=\"token plain\"> langchain_community</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">.</span><span class=\"token plain\">llms </span><span class=\"token keyword\" style=\"color: rgb(86, 156, 214);\">import</span><span class=\"token plain\"> PipelineAI</span><br></span><span class=\"token-line\" style=\"color: rgb(156, 220, 254);\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color: rgb(86, 156, 214);\">from</span><span class=\"token plain\"> langchain_core</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">.</span><span class=\"token plain\">output_parsers </span><span class=\"token keyword\" style=\"color: rgb(86, 156, 214);\">import</span><span class=\"token plain\"> StrOutputParser</span><br></span><span class=\"token-line\" style=\"color: rgb(156, 220, 254);\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color: rgb(86, 156, 214);\">from</span><span class=\"token plain\"> langchain_core</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">.</span><span class=\"token plain\">prompts </span><span class=\"token keyword\" style=\"color: rgb(86, 156, 214);\">import</span><span class=\"token plain\"> PromptTemplate</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button type=\"button\" aria-label=\"Copy code to clipboard\" title=\"Copy\" class=\"clean-btn\"><span class=\"copyButtonIcons_eSgA\" aria-hidden=\"true\"><svg viewBox=\"0 0 24 24\" class=\"copyButtonIcon_y97N\"><path fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"></path></svg><svg viewBox=\"0 0 24 24\" class=\"copyButtonSuccessIcon_LjdS\"><path fill=\"currentColor\" d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\"></path></svg></span></button></div></div></div><div style=\"padding-top:1.3rem;background:var(--prism-background-color);color:var(--prism-color);margin-top:calc(-1 * var(--ifm-leading) - 5px);margin-bottom:var(--ifm-leading);box-shadow:var(--ifm-global-shadow-lw);border-bottom-left-radius:var(--ifm-code-border-radius);border-bottom-right-radius:var(--ifm-code-border-radius)\"><b style=\"padding-left:0.65rem;margin-bottom:0.45rem;margin-right:0.5rem\">API Reference:</b><span><a href=\"https://api.python.langchain.com/en/latest/llms/langchain_community.llms.pipelineai.PipelineAI.html\">PipelineAI</a> | </span><span><a href=\"https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html\">StrOutputParser</a> | </span><span><a href=\"https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html\">PromptTemplate</a></span></div><h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"set-the-environment-api-key\">Set the Environment API Key<a href=\"#set-the-environment-api-key\" class=\"hash-link\" aria-label=\"Direct link to Set the Environment API Key\" title=\"Direct link to Set the Environment API Key\">​</a></h3><p>Make sure to get your API key from PipelineAI. Check out the <a href=\"https://docs.pipeline.ai/docs/cloud-quickstart\" target=\"_blank\" rel=\"noopener noreferrer\">cloud quickstart guide</a>. You'll be given a 30 day free trial with 10 hours of serverless GPU compute to test different models.</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color: #9CDCFE; --prism-background-color: #222222;\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color: rgb(156, 220, 254);\"><span class=\"token plain\">os</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">.</span><span class=\"token plain\">environ</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">[</span><span class=\"token string\" style=\"color: rgb(206, 145, 120);\">\"PIPELINE_API_KEY\"</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">]</span><span class=\"token plain\"> </span><span class=\"token operator\" style=\"color: rgb(212, 212, 212);\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color: rgb(206, 145, 120);\">\"YOUR_API_KEY_HERE\"</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button type=\"button\" aria-label=\"Copy code to clipboard\" title=\"Copy\" class=\"clean-btn\"><span class=\"copyButtonIcons_eSgA\" aria-hidden=\"true\"><svg viewBox=\"0 0 24 24\" class=\"copyButtonIcon_y97N\"><path fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"></path></svg><svg viewBox=\"0 0 24 24\" class=\"copyButtonSuccessIcon_LjdS\"><path fill=\"currentColor\" d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\"></path></svg></span></button></div></div></div><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"create-the-pipelineai-instance\">Create the PipelineAI instance<a href=\"#create-the-pipelineai-instance\" class=\"hash-link\" aria-label=\"Direct link to Create the PipelineAI instance\" title=\"Direct link to Create the PipelineAI instance\">​</a></h2><p>When instantiating PipelineAI, you need to specify the id or tag of the pipeline you want to use, e.g. <code>pipeline_key = \"public/gpt-j:base\"</code>. You then have the option of passing additional pipeline-specific keyword arguments:</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color: #9CDCFE; --prism-background-color: #222222;\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color: rgb(156, 220, 254);\"><span class=\"token plain\">llm </span><span class=\"token operator\" style=\"color: rgb(212, 212, 212);\">=</span><span class=\"token plain\"> PipelineAI</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">(</span><span class=\"token plain\">pipeline_key</span><span class=\"token operator\" style=\"color: rgb(212, 212, 212);\">=</span><span class=\"token string\" style=\"color: rgb(206, 145, 120);\">\"YOUR_PIPELINE_KEY\"</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">,</span><span class=\"token plain\"> pipeline_kwargs</span><span class=\"token operator\" style=\"color: rgb(212, 212, 212);\">=</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">{</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">.</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">.</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">.</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">}</span><span class=\"token punctuation\" style=\"color: rgb(212, 212, 212);\">)</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button type=\"button\" class=\"clean-btn\" aria-label=\"Toggle word wrap\" title=\"Toggle word wrap\"><svg viewBox=\"0 0 24 24\" class=\"wordWrapButtonIcon_Bwma\" aria-hidden=\"true\"><path fill=\"currentColor\" d=\"M4 19h6v-2H4v2zM20 5H4v2h16V5zm-3 6H4v2h13.25c1.1 0 2 .9 2 2s-.9 2-2 2H15v-2l-3 3l3 3v-2h2c2.21 0 4-1.79 4-4s-1.79-4-4-4z\"></path></svg></button><button type=\"button\" aria-label=\"Copy code to clipboard\" title=\"Copy\" class=\"clean-btn\"><span class=\"copyButtonIcons_eSgA\" aria-hidden=\"true\"><svg viewBox=\"0 0 24 24\" class=\"copyButtonIcon_y97N\"><path fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"></path></svg><svg viewBox=\"0 0 24 24\" class=\"copyButtonSuccessIcon_LjdS\"><path fill=\"currentColor\" d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\"></path></svg></span></button></div></div></div><h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"create-a-prompt-template\">Create a Prompt Template<a href=\"#create-a-prompt-template\" class=\"hash-link\" aria-label=\"Direct link to Create a Prompt Template\" title=\"Direct link to Create a Prompt Template\">​</a></h3><p>We will create a prompt template for Question and Answer.</p><div class=\"language-python codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color: #9CDCFE; --prism-background-color: #222222;\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-python codeBlock_bY9V thin-scrollbar\"><code class=\"codeBlockLines_e6Vv\">\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitter = HTMLHeaderTextSplitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recursive JSON Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_json = {\n",
    "    \"user\": {\n",
    "        \"id\": 12345,\n",
    "        \"name\": \"John Doe\",\n",
    "        \"contact\": {\n",
    "            \"email\": \"john.doe@example.com\",\n",
    "            \"phone\": \"555-1234\",\n",
    "            \"address\": {\n",
    "                \"street\": \"123 Elm St\",\n",
    "                \"city\": \"Somewhere\",\n",
    "                \"state\": \"CA\",\n",
    "                \"zip\": \"90210\"\n",
    "            }\n",
    "        },\n",
    "        \"preferences\": {\n",
    "            \"notifications\": {\n",
    "                \"email\": True,\n",
    "                \"sms\": False,\n",
    "                \"push\": {\n",
    "                    \"enabled\": True,\n",
    "                    \"sound\": \"default\"\n",
    "                }\n",
    "            },\n",
    "            \"language\": \"en\",\n",
    "            \"timezone\": \"PST\"\n",
    "        },\n",
    "        \"subscriptions\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"Newsletter\",\n",
    "                \"type\": \"email\",\n",
    "                \"status\": \"subscribed\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"name\": \"Promo Alerts\",\n",
    "                \"type\": \"sms\",\n",
    "                \"status\": \"unsubscribed\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"session\": {\n",
    "        \"token\": \"abc123xyz456\",\n",
    "        \"expires\": \"2024-12-31T23:59:59Z\",\n",
    "        \"last_login\": {\n",
    "            \"ip\": \"192.168.1.1\",\n",
    "            \"location\": {\n",
    "                \"country\": \"USA\",\n",
    "                \"city\": \"Somewhere\",\n",
    "                \"coordinates\": {\n",
    "                    \"lat\": 34.0522,\n",
    "                    \"long\": -118.2437\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"shopping_cart\": {\n",
    "        \"items\": [\n",
    "            {\n",
    "                \"id\": \"9876\",\n",
    "                \"name\": \"Widget A\",\n",
    "                \"quantity\": 2,\n",
    "                \"price\": 19.99,\n",
    "                \"attributes\": {\n",
    "                    \"color\": \"red\",\n",
    "                    \"size\": \"M\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"5432\",\n",
    "                \"name\": \"Gadget B\",\n",
    "                \"quantity\": 1,\n",
    "                \"price\": 99.95,\n",
    "                \"attributes\": {\n",
    "                    \"warranty\": \"2 years\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"total_price\": 139.93,\n",
    "        \"currency\": \"USD\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "json_splitter = RecursiveJsonSplitter(max_chunk_size=100)\n",
    "json_chunks = json_splitter.split_json(random_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user': {'id': 12345,\n",
       "   'name': 'John Doe',\n",
       "   'contact': {'email': 'john.doe@example.com'}}},\n",
       " {'user': {'contact': {'phone': '555-1234',\n",
       "    'address': {'street': '123 Elm St'}}}},\n",
       " {'user': {'contact': {'address': {'city': 'Somewhere',\n",
       "     'state': 'CA',\n",
       "     'zip': '90210'}}}},\n",
       " {'user': {'preferences': {'notifications': {'email': True,\n",
       "     'sms': False,\n",
       "     'push': {'enabled': True, 'sound': 'default'}}}}},\n",
       " {'user': {'preferences': {'language': 'en', 'timezone': 'PST'}}},\n",
       " {'user': {'subscriptions': [{'id': 1,\n",
       "     'name': 'Newsletter',\n",
       "     'type': 'email',\n",
       "     'status': 'subscribed'},\n",
       "    {'id': 2,\n",
       "     'name': 'Promo Alerts',\n",
       "     'type': 'sms',\n",
       "     'status': 'unsubscribed'}]}},\n",
       " {'session': {'token': 'abc123xyz456', 'expires': '2024-12-31T23:59:59Z'}},\n",
       " {'session': {'last_login': {'ip': '192.168.1.1'}}},\n",
       " {'session': {'last_login': {'location': {'country': 'USA',\n",
       "     'city': 'Somewhere'}}}},\n",
       " {'session': {'last_login': {'location': {'coordinates': {'lat': 34.0522,\n",
       "      'long': -118.2437}}}}},\n",
       " {'shopping_cart': {'items': [{'id': '9876',\n",
       "     'name': 'Widget A',\n",
       "     'quantity': 2,\n",
       "     'price': 19.99,\n",
       "     'attributes': {'color': 'red', 'size': 'M'}},\n",
       "    {'id': '5432',\n",
       "     'name': 'Gadget B',\n",
       "     'quantity': 1,\n",
       "     'price': 99.95,\n",
       "     'attributes': {'warranty': '2 years'}}]}},\n",
       " {'shopping_cart': {'total_price': 139.93, 'currency': 'USD'}}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPEN AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is a toutorial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('random.pdf')\n",
    "final_documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1295814b0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(final_documents, embeddings)\n",
    "db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 7, 'source': 'random.pdf'}, page_content='Figure 2: Most ML and AI projects live in these sections of MLTRL, not concerned with fundamental R&D – that is,\\ncompletely using existing methods and implementations, and even pretrained models. In the left diagram, the arrows\\nshow a common development pattern with MLTRL in industry: projects go back to the ML toolbox to develop new\\nfeatures (dashed line), and frequent, incremental improvements are often a practice of jumping back a couple levels to\\nLevel 7 (which is the main systems integrations stage). At Levels 7 and 8 we stress the need for tests that run use-case\\nspeciﬁc critical scenarios and data-slices, which are highlighted by a proper risk-quantiﬁcation matrix [ 22]. Cycling\\nback to previous lower levels is not just a late-stage mechanism in MLTRL, but rather “switchbacks” occur throughout\\nthe process (as discussed in the Methods section and throughout the text). In the right diagram we show the more\\ncommon approach in industry ( without using our framework), which skips essential technology transition stages – ML\\nEngineers push straight through to deployment, ignoring important productization and systems integration factors. This\\nwill be discussed in more detail in the Methods section.\\nEXAMPLES\\nHuman-machine visual inspection\\nWhile most ML projects begin with a speciﬁc task and/or dataset, there are many that originate in ML theory without\\nany target application – i.e., projects starting MLTRL at level 0 or 1. These projects nicely demonstrate the utility of\\nMLTRL built-in switchbacks, bifurcating paths, and iteration with domain experts. An example we discuss here is a\\nnovel approach to representing data in generative vision models from Naud & Lavin[ 23], which was then developed into\\nstate-of-the-art unsupervised anomaly detection, and targeted for two human-machine visual inspection applications:\\nFirst, industrial anomaly detection, notably in precision manufacturing, to identify potential errors for human-expert\\nmanual inspection. Second, using the model to improve the accuracy and efﬁciency of neuropathology, the microscopic\\nexamination of neurosurgical specimens for cancerous tissue. In these human-machine teaming use-cases there are\\nspeciﬁc challenges impeding practical, reliable use:\\n•Hidden feedback loops can be common and problematic in real-world systems inﬂuencing their own training\\ndata: over time the behavior of users may evolve to select data inputs they prefer for the speciﬁc AI system,\\nrepresenting some skew from the training data. In this neuropathology case, selecting whole-slide images that\\nare uniquely difﬁcult for manual inspection, or even biased by that individual user. Similarly we see underlying\\nhealthcare processes can act as hidden confounders, resulting in unreliable decision support tools[26].\\n•Model availability can be limited in many deployment settings: for example, on-premises deployments\\n(common in privacy preserving domains like healthcare and banking), edge deployments (common in industrial\\nuse-cases such as manufacturing and agriculture), or from the infrastructure’s inability to scale to the volume\\nof requests. This can severely limit the team’s ability to monitor, debug, and improve deployed models.\\n•Uncertainty estimation is valuable in many AI scenarios, yet not straightforward to implement in practice.\\nThis is further complicated with multiple data sources and users, each injecting generally unknown amounts of\\nnoise and uncertainties. In medical applications it is of critical importance, to provide measures of conﬁdence\\nand sensitivity, and for AI researchers through end-users. In anomaly detection, various uncertainty measures\\ncan help calibrate the false-positive versus false-negative rates, which can be very domain speciﬁc.\\n8'),\n",
       " Document(metadata={'page': 5, 'source': 'random.pdf'}, page_content='that corresponding changes to model or application development risk deployment delays or failures. This marks a key\\ndecision for the project lifecycle, as this expensive ML deployment risk is common without MLTRL (see Figure 2).\\nLevel 6 data – Additional data should be collected and operationalized at this stage towards robustifying the ML\\nmodels, algorithms, and surrounding components. These include adversarial examples to check local robustness [ 15],\\nsemantically-equivalent perturbations to check consistency of the model with respect to domain assumptions [16, 17],\\nand collecting data from different sources and checking how well the trained model generalizes to them. These\\nconsiderations are even more vital in the challenging deployment domains mentioned above with limited data access.\\nLevel 6 review – Focus is on the code quality, the set of newly deﬁned product requirements, system SLA and SLO\\nrequirements, data pipelines spec, and an AI ethics revisit now that we are closer to a real-world use-case. In particular,\\nregulatory compliance is mandated for this gated review; the data privacy and security laws are changing rapidly, and\\nmissteps with compliance can make or break the project.\\nLevel 7 - Integrations For integrating the technology into existing production systems, we recommend the working\\ngroup has a balance of infrastructure engineers andapplied AI engineers – this stage of development is vulnerable\\nto latent model assumptions and failure modes, and as such cannot be safely developed solely by software engineers.\\nImportant tools for them to build together include:\\n•Tests that run use-case speciﬁc critical scenarios and data-slices – a proper risk-quantiﬁcation table will\\nhighlight these.\\n•A “golden dataset” should be deﬁned to baseline the performance of each model and succession of models –see\\nthe computer vision app example in Figure 4–for use in the continuous integration and deployment (CI/CD)\\ntests.\\n•Metamorphic testing : a software engineering methodology for testing a speciﬁc set of relations between the\\noutputs of multiple inputs. When integrating ML modules into larger systems, a codiﬁed list of metamorphic\\nrelations[18] can provide valuable veriﬁcation and validation measures and steps.\\n•Data intervention tests that seek data bugs at various points in the pipelines, downstream to measure the\\npotential effects of data processing and ML on consumers or users of that data, as well as upstream at data\\ningestion or creation. Rather than using model performance as a proxy for data quality, it is crucial to use\\nintervention tests that instead catch data errors with mechanisms speciﬁc to data validation.\\nThese tests in particular help mitigate underspeciﬁcation in ML pipelines, a key obstacle to reliably training models that\\nbehave as expected in deployment[ 19]. On the note of reliability, it is important that quality assurance engineers (QA)\\nplay a key role here and through Level 9, overseeing data processes to ensure privacy and security, and covering audits\\nfor downstream accountability of AI methods.\\nLevel 7 data – In addition to the data for test suites discussed above, this level calls for QA to prioritize data governance :\\nhow data is obtained, managed, used, and secured by the organization. This was earlier suggested in level 5 (in order to\\npreempt related technical debt), and essential here at the main junction for integration, which may create additional\\ngovernance challenges in light of downstream effects and consumers.\\nLevel 7 review – The review should focus on the data pipelines and test suites; a scorecard like the ML Testing\\nRubric[ 20] is useful. The group should also emphasize ethical considerations at this stage, as they may be more\\nadequately addressed now (where there are many test suites put into place) rather than close to shipping later.\\nLevel 8 - Flight-ready The technology is demonstrated to work in its ﬁnal form and under expected conditions.\\nThere should be additional tests implemented at this stage covering deployment aspects, notably A/B tests, blue/green\\ndeployment tests, shadow testing, and canary testing, which enable proactive and gradual testing for changing ML\\nmethods and data. Ahead of deployment, the CI/CD system should be ready to regularly stress test the overall system\\nand ML components. In practice, problems stemming from real-world data are impossible to anticipate and design for –\\nan upstream data provider could change formats unexpectedly or a physical event could cause the customer behavior to\\nchange. Running models in shadow mode for a period of time would help stress test the infrastructure and evaluate how\\nsusceptible the ML model(s) will be to performance regressions caused by data. We observe that ML systems with\\ndata-oriented architectures are more readily tested in this manner, and better surface data quality issues, data drifts, and\\nconcept drifts – this is discussed later in the Beyond Software Engineering section. To close this stage, the key decision\\nis go or no-go for deployment, and when.\\nLevel 8 data – If not already in place, there absolutely needs to be mechanisms for automatically logging data\\ndistributions alongside model performance once deployed.\\n6'),\n",
       " Document(metadata={'page': 10, 'source': 'random.pdf'}, page_content='\\x15A?U?HEJC\\x03?H=OOEBE?=PEKJ\\x03LELAHEJA\\x03\\n\\xa0=¡\\xa0>¡\\n,V\\x03PRGHO\\x03FRQğGHQFH\\x03\\x1f\\x03WKUHVKROG\",V\\x03GHWHFWHG\\x03REMHFW\\x03LQ\\x03WDUJHW\\x03VHW\"3URYLGH\\x03FRUUHVSRQGLQJ\\x03UHF\\\\FOLQJ\\x03LQVWUXFWLRQV,QLWLDWH\\x03KXPDQ\\x10\\x03LQ\\x10WKH\\x10ORRS\\x03SURWRFRO12<(6<(6Figure 4: Computer vision pipeline for an automated recycling application (a), which contains multiple ML models,\\nuser input, and image data from various sources. Complicated logic such as this can mask ML model performance lags\\nand failures, and also emphasized the need for R&D-to-product hand off described in MLTRL. Additional emphasis is\\nplaced on ML tests that consider the mix of real-world data with user annotations (b, right) and synthetic data generated\\nby Unity AI’s Perception tool and structured domain randomization (b, left).\\ncomplement real-world data sources (Figure 4). This application exempliﬁes three important challenges in ML product\\ndevelopment that MLTRL helps overcome:\\n•Multiple and disparate data sources are common in deployed ML pipelines yet often ignored in R&D.\\nFor instance, upstream data providers can change formats unexpectedly, or a physical event could cause the\\ncustomer behavior to change. It is nearly impossible to anticipate and design for all potential problems with\\nreal-world data and deployment. This computer vision system implemented pipelines and extended test suites\\nto cover open-source benchmark data, real user data, and synthetic data.\\n•Hidden performance degradation can be challenging to detect and debug in ML systems because gradual\\nchanges in performance may not be immediately visible. Common reasons for this challenge are that the\\nML component may be one step in a series. Additionally, local/isolated changes to an ML component’s\\nperformance may not directly affect the observed downstream performance. We can see both issues in the\\nillustrated logic diagram for the automated recycling app (Figure 4). A slight degradation in the initial CV\\nmodel may not heavily inﬂuence the following user input. However, when an uncommon input image appears\\nin the future, the app fails altogether.\\n•Model usage requirements can make or break an ML product. For example, the Netﬂix “$1M Prize” solution\\nwas never fully deployed because of signiﬁcant engineering costs in real-world scenariosv. For example,\\nengineering teams must communicate memory usage, compute power requirements, hardware availability,\\nnetwork privacy, and latency to the ML teams. ML teams often only understand the statistics or ML theory\\nbehind a model but not the system requirements or how it scales.\\nWe next elucidate these challenges and how MLTRL helps overcome them in the context of this project’s lifecycle. This\\nproject started at level 4, using largely existing ML methods with a target use-case.\\n•Level 4 – For this project, we validated most of the components in other projects. Speciﬁcally, the computer\\nvision (CV) model for object recognition and classiﬁcation was an off-the-shelf model. The synthetic data\\ngeneration method used Unity Perception, a well-established open-source project. Though this allowed us to\\nvnetﬂixtechblog.com/netﬂix-recommendations-beyond-the-5-stars-part-1\\n11'),\n",
       " Document(metadata={'page': 0, 'source': 'random.pdf'}, page_content='TECHNOLOGY READINESS LEVELS\\nFOR MACHINE LEARNING SYSTEMS\\nAlexander Lavin∗\\nPasteur LabsCiarán M. Gilligan-Lee\\nSpotifyAlessya Visnjic\\nWhyLabsSiddha Ganju\\nNvidiaDava Newman\\nMIT\\nAtılım Güne¸ s Baydin\\nUniversity of OxfordSujoy Ganguly\\nUnity AIDanny Lange\\nUnity AIAmit Sharma\\nMicrosoft Research\\nStephan Zheng\\nSalesforce ResearchEric P. Xing\\nPetuumAdam Gibson\\nKonduitJames Parr\\nNASA Frontier Development Lab\\nChris Mattmann\\nNASA Jet Propulsion LabYarin Gal\\nAlan Turing Institute\\nABSTRACT\\nThe development and deployment of machine learning (ML) systems can be executed easily with\\nmodern tools, but the process is typically rushed and means-to-an-end. The lack of diligence can\\nlead to technical debt, scope creep and misaligned objectives, model misuse and failures, and\\nexpensive consequences. Engineering systems, on the other hand, follow well-deﬁned processes\\nand testing standards to streamline development for high-quality, reliable results. The extreme is\\nspacecraft systems, where mission critical measures and robustness are ingrained in the development\\nprocess. Drawing on experience in both spacecraft engineering and ML (from research through\\nproduct across domain areas), we have developed a proven systems engineering approach for machine\\nlearning development and deployment. Our Machine Learning Technology Readiness Levels (MLTRL)\\nframework deﬁnes a principled process to ensure robust, reliable, and responsible systems while\\nbeing streamlined for ML workﬂows, including key distinctions from traditional software engineering.\\nEven more, MLTRL deﬁnes a lingua franca for people across teams and organizations to work\\ncollaboratively on artiﬁcial intelligence and machine learning technologies. Here we describe the\\nframework and elucidate it with several real world use-cases of developing ML methods from basic\\nresearch through productization and deployment, in areas such as medical diagnostics, consumer\\ncomputer vision, satellite imagery, and particle physics.\\nKeywords: Machine Learning; Systems Engineering; Data Management; Medical AI; Space Sciences\\nIntroduction\\nThe accelerating use of artiﬁcial intelligence (AI) and machine learning (ML) technologies in systems of software,\\nhardware, data, and people introduces vulnerabilities and risks due to dynamic and unreliable behaviors; fundamentally,\\nML systems learn from data, introducing known and unknown challenges in how these systems behave and interact with\\ntheir environment. Currently the approach to building AI technologies is siloed: models and algorithms are developed\\nin testbeds isolated from real-world environments, and without the context of larger systems or broader products they’ll\\nbe integrated within for deployment. A main concern is models are typically trained and tested on only a handful of\\ncurated datasets, without measures and safeguards for future scenarios, and oblivious of the downstream tasks and\\nusers. Even more, models and algorithms are often integrated into a software stack without regard for the inherent\\nstochasticity –for instance, the massive effect random seeds have on deep reinforcement learning model performance\\n[1] – and failure modes of the ML components, which can be dangerously hidden in layers of software and abstraction.\\nOther domains of engineering, such as civil and aerospace, follow well-deﬁned processes and testing standards to\\nstreamline development for high-quality, reliable results. Technology Readiness Level (TRL) is a systems engineering\\nprotocol for deep tech[ 2] and scientiﬁc endeavors at scale, ideal for integrating many interdependent components\\n∗lavin@simulation.science\\nPreprint. Under review.arXiv:2101.03989v2  [cs.LG]  29 Nov 2021')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"The development and deployment of machine learning (ML) systems can be executed easily with modern tools, but the process is typically rushed and means-to-an-end. \"\n",
    "retrieval = db.similarity_search(query)\n",
    "retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings_ollama = OllamaEmbeddings(model=\"mxbai-embed-large\") ## By default llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='mxbai-embed-large', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=False, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_ollama = embeddings_ollama.embed_documents(\"Hi my name is Omii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeded_ollama[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import tqdm\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm-venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings_hf = HuggingFaceEmbeddings(model_name=\"all-MiniLM-l6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is practice.\"\n",
    "result = embeddings_hf.embed_query(text)\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Databases and Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 807, which is longer than the specified 200\n",
      "Created a chunk of size 582, which is longer than the specified 200\n",
      "Created a chunk of size 891, which is longer than the specified 200\n",
      "Created a chunk of size 419, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('speech.txt')\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size =200, chunk_overlap = 0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings()\n",
    "db = FAISS.from_documents(docs,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'speech.txt'}, page_content='As a teenager, Malcolm Little made his way to New York, where he took the street name Detroit Red and became a pimp and petty criminal. In 1946, Malcolm Little was sent to prison for burglary. He read voraciously while serving time and converted to the Black Muslim faith. He joined the Nation of Islam (NOI) and changed his name to Malcolm X, eliminating that part of his identity he called a white-imposed slave name.')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is Malcom X?\"\n",
    "result = db.similarity_search(query)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 807, which is longer than the specified 200\n",
      "Created a chunk of size 582, which is longer than the specified 200\n",
      "Created a chunk of size 891, which is longer than the specified 200\n",
      "Created a chunk of size 419, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "loader = TextLoader('speech.txt')\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size =200, chunk_overlap = 0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings()\n",
    "vectordb = Chroma.from_documents(docs,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'speech.txt'}, page_content='As a teenager, Malcolm Little made his way to New York, where he took the street name Detroit Red and became a pimp and petty criminal. In 1946, Malcolm Little was sent to prison for burglary. He read voraciously while serving time and converted to the Black Muslim faith. He joined the Nation of Islam (NOI) and changed his name to Malcolm X, eliminating that part of his identity he called a white-imposed slave name.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is Malcom X?\"\n",
    "result = vectordb.similarity_search(query)\n",
    "result[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
